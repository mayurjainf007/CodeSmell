code_blocks_index,kernel_id,code_block_id,code_block
0,28356656,0,"import numpy as np 
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
from catboost import Pool, cv, CatBoostClassifier, CatBoostRegressor

from sklearn.metrics import mean_squared_error, classification_report
from sklearn.preprocessing import LabelEncoder

from sklearn.model_selection import train_test_split

# import xgboost
# import lightgbm as lgb
# from lightgbm import LGBMClassifier

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))
        
import gc
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn import metrics
pd.set_option('max_rows', 300)
import re

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

pd.set_option('display.max_columns', 300)
np.random.seed(566)
pd.set_option('display.max_rows', 200)
pd.set_option('display.width', 1000)
pd.set_option('display.float_format', '{:20,.2f}'.format)
pd.set_option('display.max_colwidth', -1)"
1,28356656,1,"TARGET_COL = ""hospital_death"""
2,28356656,2,"df = pd.read_csv(""/kaggle/input/widsdatathon2020/training_v2.csv"")
print(df.shape)
display(df.nunique())
df.head()"
3,28356656,3,df.isna().sum()
4,28356656,4,df.describe()
5,28356656,5,"test = pd.read_csv(""/kaggle/input/widsdatathon2020/unlabeled.csv"")
print(test.shape)
display(test.nunique())
test.head()"
6,28356656,6,test.isna().sum()
7,28356656,7,"print([c for c in df.columns if 7<df[c].nunique()<800])
## 
# categorical_cols = ['hospital_id','apache_3j_bodysystem', 'apache_2_bodysystem',
# ""hospital_admit_source"",""icu_id"",""ethnicity""]"
8,28356656,8,"## print non numeric columns : We may need to
## define them as categorical / encode as numeric with label encoder, depending on ml model used
print([c for c in df.columns if (1<df[c].nunique()) & (df[c].dtype != np.number)& (df[c].dtype != int) ])"
9,28356656,9,"categorical_cols =  ['hospital_id',
 'ethnicity', 'gender', 'hospital_admit_source', 'icu_admit_source', 'icu_stay_type', 'icu_type', 'apache_3j_bodysystem', 'apache_2_bodysystem']

#['apache_3j_bodysystem', 'apache_2_bodysystem',
# ""hospital_admit_source"",""icu_id"",""ethnicity""]"
10,28356656,10,"display(df[categorical_cols].dtypes)
display(df[categorical_cols].tail(3))
display(df[categorical_cols].isna().sum())"
11,28356656,11,"df[categorical_cols] = df[categorical_cols].fillna("""")

# same transformation for test data
test[categorical_cols] = test[categorical_cols].fillna("""")

df[categorical_cols].isna().sum()"
12,28356656,12,"## useful ""hidden"" function - df._get_numeric_data()  - returns only numeric columns from a pandas dataframe. Useful for scikit learn models! 

X_train = df.drop([TARGET_COL],axis=1)
y_train = df[TARGET_COL]"
13,28356656,13,"## catBoost Pool object
train_pool = Pool(data=X_train,label = y_train,cat_features=categorical_cols,
#                   baseline= X_train[""""], ## 
#                   group_id = X_train['hospital_id']
                 )

### OPT/TODO:  do train test split for early stopping then add that as an eval pool object : "
14,28356656,14,"model_basic = CatBoostClassifier(verbose=False,iterations=50)#,learning_rate=0.1, task_type=""GPU"",)
model_basic.fit(train_pool, plot=True,silent=True)
print(model_basic.get_best_score())"
15,28356656,15,"### hyperparameter tuning example grid for catboost : 
grid = {'learning_rate': [0.04, 0.1],
        'depth': [7, 11],
#         'l2_leaf_reg': [1, 3,9],
#        ""iterations"": [500],
       ""custom_metric"":['Logloss', 'AUC']}

model = CatBoostClassifier()

## can also do randomized search - more efficient typically, especially for large search space - `randomized_search`
grid_search_result = model.grid_search(grid, 
                                       train_pool,
                                       plot=True,
                                       refit = True, #  refit best model on all data
                                      partition_random_seed=42)

print(model.get_best_score())"
16,28356656,16,"print(""best model params: \n"",grid_search_result[""params""])"
17,28356656,17,"feature_importances = model.get_feature_importance(train_pool)
feature_names = X_train.columns
for score, name in sorted(zip(feature_importances, feature_names), reverse=True):
    if score > 0.05:
        print('{0}: {1:.2f}'.format(name, score))"
18,28356656,18,"import shap
shap.initjs()

explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(train_pool)

# visualize the training set predictions
# SHAP plots for all the data is very slow, so we'll only do it for a sample. Taking the head instead of a random sample is dangerous! 
shap.force_plot(explainer.expected_value,shap_values[0,:400], X_train.iloc[0,:400])"
19,28356656,19,"# summarize the effects of all the features
shap.summary_plot(shap_values, X_train)"
20,28356656,20,"test[TARGET_COL] = model.predict(test.drop([TARGET_COL],axis=1),prediction_type='Probability')[:,1]"
21,28356656,21,"test[[""encounter_id"",""hospital_death""]].to_csv(""submission.csv"",index=False)"
22,29140873,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the ""../input/"" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# Any results you write to the current directory are saved as output.
import lightgbm as lgb
from sklearn.metrics import roc_auc_score
from sklearn.metrics import auc
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from datetime import datetime, date, time, timedelta

def label_var(data,variables_cat):
    lb=[]
    for m in variables_cat:
        l=LabelEncoder()
        lb.append(l.fit(list(data[m].dropna())))
    
    return lb

def label_enc(data,l,categorical_features):
    i=0
    for m in categorical_features:
        data.loc[data[m].notnull(),m]=l[i].transform(data.loc[data[m].notnull(),m])
        i=i+1

df_tr = pd.read_csv(""/kaggle/input/widsdatathon2020/training_v2.csv"")
df_ts = pd.read_csv(""/kaggle/input/widsdatathon2020/unlabeled.csv"")

train_columns = [x for x in df_tr.columns if x not in ['encounter_id','patient_id','hospital_death','readmission_status']]

categorical_features = []
for m in train_columns:
    if(df_tr[m].dtypes=='object'):
        categorical_features.append(m)
        
df_tr_ts = pd.concat([df_tr[categorical_features],df_ts[categorical_features]])

l = label_var(df_tr_ts, categorical_features)
label_enc(df_tr,l,categorical_features)
label_enc(df_ts,l,categorical_features)

for df in [df_tr, df_ts]:
    for m in categorical_features:
        df[m] = df[m].astype(float)
        
categorical_index = [train_columns.index(x) for x in categorical_features]

target = df_tr['hospital_death']

param = {'task': 'train',
         'boosting': 'gbdt',
         'objective':'binary',
         'metric': 'auc',
         'num_leaves': 15,
         'min_data_in_leaf': 90,
         'learning_rate': 0.01,
         'max_depth': 5,
         'feature_fraction': 0.1,
         'bagging_freq': 1,
         'bagging_fraction': 0.75,
         'use_missing': True,
         'nthread': 4
        }

folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=256)
oof = np.zeros(len(df_tr))
r=[]
predictions = np.zeros(len(df_ts))
for fold_, (trn_idx, val_idx) in enumerate(folds.split(df_tr,target.values)):
    strLog = ""fold {}"".format(fold_)
    print(strLog)
    
    trn_data = lgb.Dataset(df_tr.iloc[trn_idx][train_columns], label=target.iloc[trn_idx])
    val_data = lgb.Dataset(df_tr.iloc[val_idx][train_columns], label=target.iloc[val_idx],reference=trn_data)

    num_round = 7000
    clf = lgb.train(param,trn_data,num_round,valid_sets=val_data,early_stopping_rounds=100,verbose_eval=200,categorical_feature=categorical_index)
    oof[val_idx] = clf.predict(df_tr.iloc[val_idx][train_columns], num_iteration=clf.best_iteration)
   
    a=roc_auc_score(target.loc[val_idx],clf.predict(df_tr.loc[val_idx,train_columns].values, num_iteration=clf.best_iteration))
    r.append(a)
    
    #predictions
    predictions += clf.predict(df_ts[train_columns], num_iteration=clf.best_iteration) / folds.n_splits
    
strAUC = roc_auc_score(target, oof)
print(strAUC)
print (""mean: ""+str(np.mean(np.array(r))))
print (""std: ""+str(np.std(np.array(r))))

df_sub = pd.DataFrame({'encounter_id': df_ts['encounter_id']})
df_sub['hospital_death'] = predictions

df_sub.to_csv(""sub1.csv"",index=False)"
23,28710497,0,"import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt"
24,28710497,1,"train = pd.read_csv(""../input/widsdatathon2020/training_v2.csv"")
test = pd.read_csv(""../input/widsdatathon2020/unlabeled.csv"")"
25,28710497,2,"train.shape, test.shape"
26,28710497,3,train.head()
27,28710497,4,test.head()
28,28710497,5,train.describe()
29,28710497,6,test.describe()
30,28710497,7,train.isnull().sum()/len(train)*100
31,28710497,8,"f,ax=plt.subplots(1,2,figsize=(18,8))
train['hospital_death'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',ax=ax[0],shadow=True)
ax[0].set_title('hospital_death')
ax[0].set_ylabel('')
sns.countplot('hospital_death',data=train,ax=ax[1])
ax[1].set_title('hospital_death')
plt.show()"
32,28710497,9,"train.drop(""readmission_status"",inplace=True,axis=1)
test.drop(""readmission_status"",inplace=True,axis=1)"
33,28710497,10,"test.drop(""hospital_death"",inplace=True,axis=1)
y_train = train[['encounter_id', 'patient_id', 'hospital_id',""hospital_death""]].copy()
train.drop(""hospital_death"",inplace=True,axis=1)"
34,28710497,11,"non_categorical = train.loc[:,train.dtypes!=""object""].columns"
35,28710497,12,"categorical = [c for c in train[non_categorical].columns if (train[c].nunique()<10)]
non_categorical = [c for c in train[non_categorical].columns if (train[c].nunique()>=10)]"
36,28710497,13,"print(train[non_categorical].isnull().sum()/len(train))
a = train[non_categorical].isnull().sum()/len(train)>0.40 ## editable
missing_m40 = train[non_categorical].loc[:,a].columns

a = train[non_categorical].isnull().sum()/len(train)<=0.40 ## editable
missing_l40 = train[non_categorical].loc[:,a].columns
del a"
37,28710497,14,"a = train.isnull().sum()/len(train)!= 0 ## editable
missing = train.loc[:,a].columns
del a
for i in missing:
    train[str(i)+""_Na""]=pd.get_dummies(train[i].isnull(),prefix=i).iloc[:,0]
    test[str(i)+""_Na""]=pd.get_dummies(test[i].isnull(),prefix=i).iloc[:,0]

for i in missing_l40:
    for j in train.hospital_id.unique():
        train[i][train.hospital_id==j]=train[i][train.hospital_id==j].fillna(train[i][train.hospital_id==j].median())
    for k in test.hospital_id.unique():
        test[i][test.hospital_id==k]=test[i][test.hospital_id==k].fillna(test[i][test.hospital_id==k].median())"
38,28710497,15,"train[""apache_4a_hospital_death_prob""]=train[""apache_4a_hospital_death_prob""].replace({-1:np.nan})
test[""apache_4a_hospital_death_prob""]=test[""apache_4a_hospital_death_prob""].replace({-1:np.nan})

train[""apache_4a_icu_death_prob""]=train[""apache_4a_icu_death_prob""].replace({-1:np.nan})
test[""apache_4a_icu_death_prob""]=test[""apache_4a_icu_death_prob""].replace({-1:np.nan})"
39,28710497,16,"a = train[non_categorical].isnull().sum()/len(train)<=0.40 ## editable
missing_l40 = train[non_categorical].loc[:,a].columns
for i in missing_l40:
    train[i] = train[i].fillna(train[i].median())
    
a = test[non_categorical].isnull().sum()/len(test)<=0.40 ## editable
missing_l40 = test[non_categorical].loc[:,a].columns
for i in missing_l40:
    test[i] = test[i].fillna(train[i].median())
del a, missing_l40, missing_m40"
40,28710497,17,"categorical=np.concatenate([train.loc[:,train.dtypes==""object""].columns.tolist(),categorical])"
41,28710497,18,train[categorical].isnull().sum()/len(train)
42,28710497,19,test[categorical].isnull().sum()/len(test)
43,28710497,20,train[categorical].nunique()
44,28710497,21,"## imputador gender
train[""gender""][train.height>167]=train[""gender""][train.height>167].fillna(""M"")
train[""gender""][train.height<=167]=train[""gender""][train.height<=167].fillna(""F"")"
45,28710497,22,"for i in categorical:
    train[i] = train[i].fillna(train[i].value_counts().index[0])
    test[i] = test[i].fillna(train[i].value_counts().index[0])"
46,28710497,23,"categorical = train.loc[:,train.dtypes==""object""].columns.tolist()"
47,28710497,24,train[categorical].nunique()
48,28710497,25,"train[""hospital_admit_source""]=train[""hospital_admit_source""].replace({'Other ICU':""ICU"",'ICU to SDU':""SDU"",
                                       'Step-Down Unit (SDU)':""SDU"",
                                      'Acute Care/Floor':""Floor"",
                                      'Other Hospital':""Other""})
test[""hospital_admit_source""]=test[""hospital_admit_source""].replace({'Other ICU':""ICU"",'ICU to SDU':""SDU"",
                                       'Step-Down Unit (SDU)':""SDU"",
                                      'Acute Care/Floor':""Floor"",
                                      'Other Hospital':""Other""})
train[""apache_2_bodysystem""] = train[""apache_2_bodysystem""].replace({'Undefined Diagnoses':""UD"",
                                                                    'Undefined diagnoses':""UD""})
test[""apache_2_bodysystem""] = test[""apache_2_bodysystem""].replace({'Undefined Diagnoses':""UD"",
                                                                    'Undefined diagnoses':""UD""})"
49,28710497,26,"train = train.join(pd.get_dummies(train[categorical]).drop(""gender_F"",axis=1))
test = test.join(pd.get_dummies(test[categorical]).drop(""gender_F"",axis=1))
train.drop(categorical,axis=1,inplace=True)
test.drop(categorical,axis=1,inplace=True)"
50,28710497,27,"non = ['encounter_id', 'patient_id', 'hospital_id','icu_id']"
51,28710497,28,"correlated_features = set()
train1 = train.drop(non,axis=1) 
correlation_matrix = train1.corr()
del train1"
52,28710497,29,"for i in range(len(correlation_matrix.columns)):
     for j in range(i):
            if abs(correlation_matrix.iloc[i, j]) ==  1:
                colname = correlation_matrix.columns[i]
                correlated_features.add(colname)
correlated_features=list(correlated_features)"
53,28710497,30,"train.drop(correlated_features,axis=1,inplace=True)
test.drop(correlated_features,axis=1,inplace=True)"
54,28710497,31,"train.shape, test.shape"
55,28710497,32,"train.drop(""hospital_admit_source_Observation"",axis=1,inplace=True)"
56,28710497,33,"train = train.set_index(""encounter_id"")
test = test.set_index(""encounter_id"")
y_train = y_train.set_index(""encounter_id"")"
57,28710497,34,test = test.fillna(0)
58,28710497,35,"from lightgbm import LGBMClassifier
from sklearn.model_selection import StratifiedKFold, GroupKFold
drop_cols = ['patient_id', 'hospital_id','icu_id']
#drop_cols = np.concatenate([drop_cols,perdidos])
gf = GroupKFold(n_splits=4)
groups = np.array(train.hospital_id)
test_probs = []
train_probs = []

for i,(a,b) in enumerate(gf.split(train,y_train.loc[train.index, ""hospital_death""],groups)) :
    Xt = train.iloc[a,:]
    yt = y_train.loc[Xt.index, ""hospital_death""]
    Xt = Xt.drop(drop_cols, axis=1)
    Xt = Xt.fillna(0)
    
    Xv = train.iloc[b,:]
    yv = y_train.loc[Xv.index, ""hospital_death""]
    Xv = Xv.drop(drop_cols, axis=1)
    Xv = Xv.fillna(0)
    print(""*+*+*+*+*entrenando fold: {} "".format(i+1))
    
    learner = LGBMClassifier(n_estimators=10,learning_rate=0.03,num_iterations=3400,lambda_l2=7 ,lambda_l1 =7
                                 ,num_leaves =7,max_depth=5,min_data_in_leaf =500,early_stopping_rounds=200,feature_fraction= 0.8
                            ,bagging_fraction=0.85,bagging_freq=10)
    
    learner.fit(Xt, yt  , eval_metric=""auc"",eval_set= [(Xt, yt),(Xv, yv)], verbose=50)
    
    
    train_probs.append(pd.Series(learner.predict_proba(Xv)[:, -1],
                                index=Xv.index, name=""probs""+ str(i)))
    test_probs.append(pd.Series(learner.predict_proba(test.drop(drop_cols, axis=1))[:, -1],
                                index=test.index, name=""fold_"" + str(i)  ))
      
test_probs = pd.concat(test_probs, axis=1).mean(axis=1)
train_probs = pd.concat(train_probs, axis=1)"
59,28710497,36,"test_probs = pd.DataFrame(test_probs.rename(""hospital_death""))
test_probs.to_csv(""lightgbm_baseline.csv"", header=True)"
60,28985739,0,"# In this notebook I have used fancyimpute technique to impute missing values in the entire WiDS2020 dataset. 
# I have used LGBM to train the model. I got accuracy of 0.88. Parameter tuning might help in bettring this score.
"
61,28985739,1,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import seaborn as sns

# Input data files are available in the ""../input/"" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# Any results you write to the current directory are saved as output."
62,28985739,2,"from fancyimpute import KNN
from sklearn.preprocessing import OrdinalEncoder
from sklearn.model_selection import train_test_split
import lightgbm as lgb"
63,28985739,3,"#read the data and drop noisy columns
train=pd.read_csv(""/kaggle/input/widsdatathon2020/training_v2.csv"")
test=pd.read_csv(""/kaggle/input/widsdatathon2020/unlabeled.csv"")
solution = pd.read_csv(""/kaggle/input/solutiontemplate/solution_template.csv"")
trainv1=train.drop(['encounter_id','patient_id','icu_id', 'hospital_id', 'readmission_status','ethnicity'],axis=1)
testv1=test.drop(['encounter_id','patient_id','icu_id', 'hospital_id', 'readmission_status','hospital_death','ethnicity'],axis=1)
print(""number of rows and columns in training set is \n"",trainv1.shape)
print(""number of rows and columns in test set is \n"",testv1.shape)"
64,28985739,4,"#Exploring the data
trainv1.info()
trainv1.describe()
trainv1.isna().sum()
testv1.isna().sum()
trainv1['hospital_death'].value_counts()*100/len(trainv1['hospital_death'])
sns.countplot(trainv1['hospital_death'])"
65,28985739,5,"#Seperate categorical and numerical variables
cattrain=trainv1.select_dtypes('object')
numtrain=trainv1.select_dtypes('number')
cattest=testv1.select_dtypes('object')
numtest=testv1.select_dtypes('number')"
66,28985739,6,"#encoding categorical test variables
#instantiate both packages to use
encoder = OrdinalEncoder()
imputer = KNN()
# create a list of categorical columns to iterate over
cat_cols = cattest.columns

def encode(data):
    '''function to encode non-null data and replace it in the original data'''
    #retains only non-null values
    nonulls = np.array(data.dropna())
    #reshapes the data for encoding
    impute_reshape = nonulls.reshape(-1,1)
    #encode data
    impute_ordinal = encoder.fit_transform(impute_reshape)
    #Assign back encoded values to non-null values
    data.loc[data.notnull()] = np.squeeze(impute_ordinal)
    return data

#create a for loop to iterate through each column in the data
for columns in cat_cols:
    encode(cattest[columns])"
67,28985739,7,"#encoding categorical train variables
#instantiate both packages to use
encoder = OrdinalEncoder()
imputer = KNN()
# create a list of categorical columns to iterate over
cat_cols = cattrain.columns

def encode(data):
    '''function to encode non-null data and replace it in the original data'''
    #retains only non-null values
    nonulls = np.array(data.dropna())
    #reshapes the data for encoding
    impute_reshape = nonulls.reshape(-1,1)
    #encode data
    impute_ordinal = encoder.fit_transform(impute_reshape)
    #Assign back encoded values to non-null values
    data.loc[data.notnull()] = np.squeeze(impute_ordinal)
    return data

#create a for loop to iterate through each column in the data
for columns in cat_cols:
    encode(cattrain[columns])"
68,28985739,8,"#splitting train values into sections for faster imputing
numtrain1=numtrain[0:20000]
numtrain2=numtrain[20000:40000]
numtrain3=numtrain[40000:60000]
numtrain4=numtrain[60000:80000]
numtrain5=numtrain[80000:]

cattrain1=cattrain[0:20000]
cattrain2=cattrain[20000:40000]
cattrain3=cattrain[40000:60000]
cattrain4=cattrain[60000:80000]
cattrain5=cattrain[80000:]"
69,28985739,9,"#splitting test values into sections for faster imputing
cattest1=cattest[0:20000]
cattest2=cattest[20000:]

numtest1=numtest[0:20000]
numtest2=numtest[20000:]"
70,28985739,10,"# impute catgorical test data and convert                                                                                                                                                   
encode_testdata1 = pd.DataFrame(np.round(imputer.fit_transform(cattest1)),columns = cattest.columns)
encode_testdata2 = pd.DataFrame(np.round(imputer.fit_transform(cattest2)),columns = cattest.columns)
"
71,28985739,11,"# impute catgorical train data and convert                                                                                                                                                   
encode_data1 = pd.DataFrame(np.round(imputer.fit_transform(cattrain1)),columns = cattrain.columns)
encode_data2 = pd.DataFrame(np.round(imputer.fit_transform(cattrain2)),columns = cattrain.columns)
encode_data3 = pd.DataFrame(np.round(imputer.fit_transform(cattrain3)),columns = cattrain.columns)
encode_data4 = pd.DataFrame(np.round(imputer.fit_transform(cattrain4)),columns = cattrain.columns)
encode_data5 = pd.DataFrame(np.round(imputer.fit_transform(cattrain5)),columns = cattrain.columns)"
72,28985739,12,"cattrainfill=pd.concat([encode_data1,encode_data2,encode_data3,encode_data4,encode_data5])
cattestfill=pd.concat([encode_testdata1,encode_testdata2])"
73,28985739,13,"#impute numerical test data
encode_testdatanum = pd.DataFrame(np.round(imputer.fit_transform(numtest1)),columns = numtest.columns)
encode_testdatanum2 = pd.DataFrame(np.round(imputer.fit_transform(numtest2)),columns = numtest.columns)
"
74,28985739,14,"#impute numerical train data
encode_datanum1 = pd.DataFrame(np.round(imputer.fit_transform(numtrain1)),columns = numtrain.columns)
encode_datanum2 = pd.DataFrame(np.round(imputer.fit_transform(numtrain2)),columns = numtrain.columns)
encode_datanum3 = pd.DataFrame(np.round(imputer.fit_transform(numtrain3)),columns = numtrain.columns)
encode_datanum4 = pd.DataFrame(np.round(imputer.fit_transform(numtrain4)),columns = numtrain.columns)
encode_datanum5 = pd.DataFrame(np.round(imputer.fit_transform(numtrain5)),columns = numtrain.columns)"
75,28985739,15,"numtrainfill=pd.concat([encode_datanum1,encode_datanum2,encode_datanum3,encode_datanum4,encode_datanum5])
numtestfill=pd.concat([encode_testdatanum,encode_testdatanum2])"
76,28985739,16,"trainv6=pd.concat([numtrainfill,cattrainfill],axis=1,join='inner')
testv6=pd.concat([numtestfill,cattestfill],axis=1,join='inner')"
77,28985739,17,"y=trainv6['hospital_death']
trainv7=trainv6.drop(['hospital_death'], axis=1)"
78,28985739,18,"# Split into training and validation set
x_train, x_val, y_train, y_val = train_test_split(trainv7, y, test_size = 0.25, random_state = 1)"
79,28985739,19,"#Model building
d_train = lgb.Dataset(x_train, label=y_train)
params = {}
params['learning_rate'] = 0.003
params['boosting_type'] = 'gbdt'
params['objective'] = 'binary'
params['metric'] = 'binary_logloss'
params['sub_feature'] = 0.5
params['num_leaves'] = 100
params['min_data'] = 50
params['max_depth'] = 10
clf = lgb.train(params, d_train, 100)"
80,28985739,20,"#Prediction
y_pred=clf.predict(x_val)
y_pred1=np.round(y_pred)"
81,28985739,21,"#Measure accuracy
#Confusion matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_val, y_pred1)
print (cm)
#Accuracy
from sklearn.metrics import accuracy_score
accuracy=accuracy_score(y_pred1,y_val)
print (accuracy)"
82,28985739,22,"#Prediction on Test variables
pred_on_test=clf.predict(testv6)"
83,28985739,23,"solution.hospital_death = pred_on_test
solution.to_csv(""submissionlgbm.csv"", index=0)"
84,34088724,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the ""../input/"" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# Any results you write to the current directory are saved as output.
import lightgbm as lgb
from sklearn.metrics import roc_auc_score
from sklearn.metrics import auc
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from datetime import datetime, date, time, timedelta

def label_var(data,variables_cat):
    lb=[]
    for m in variables_cat:
        l=LabelEncoder()
        lb.append(l.fit(list(data[m].dropna())))
    
    return lb

def label_enc(data,l,categorical_features):
    i=0
    for m in categorical_features:
        data.loc[data[m].notnull(),m]=l[i].transform(data.loc[data[m].notnull(),m])
        i=i+1
        

df_tr = pd.read_csv(""/kaggle/input/widsdatathon2020/training_v2.csv"")
df_ts = pd.read_csv(""/kaggle/input/widsdatathon2020/unlabeled.csv"")"
85,34088724,1,"feature_importance_dfs = {}

for key, value in feature_importance_dfs.items():
    print('Feature Importance for type ', encoders['type'].classes_[key], ' :')
    print(
    value.groupby(['Feature'])[['importance']].mean().sort_values(
        ""importance"", ascending=False).head(20)
         )"
86,34088724,2,feature_importance_dfs = {}
87,34088724,3,"train_columns = [x for x in df_tr.columns if x not in ['encounter_id','patient_id','hospital_death','readmission_status']]
categorical_features = []
for m in train_columns:

    if(df_tr[m].dtypes=='object'):
        categorical_features.append(m)
        "
88,34088724,4,"df_tr_ts = pd.concat([df_tr[categorical_features],df_ts[categorical_features]])

l = label_var(df_tr_ts, categorical_features)
label_enc(df_tr,l,categorical_features)
label_enc(df_ts,l,categorical_features)

for df in [df_tr, df_ts]:
    for m in categorical_features:
        df[m] = df[m].astype(float)"
89,34088724,5,"categorical_index = [train_columns.index(x) for x in categorical_features]

target = df_tr['hospital_death']

param = {'task': 'train',
         'boosting': 'gbdt',
         'objective':'binary',
         'metric': 'auc',
         'num_leaves': 15,
         'min_data_in_leaf': 90,
         'learning_rate': 0.01,
         'max_depth': 5,
         'feature_fraction': 0.1,
         'bagging_freq': 1,
         'bagging_fraction': 0.75,
         'use_missing': True,
         'nthread': 4
        }"
90,34088724,6,"folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=256)
oof = np.zeros(len(df_tr))
r=[]
predictions = np.zeros(len(df_ts))
feature_importance_df = pd.DataFrame()
features = train_columns#[col for col in df_tr.columns if col != 'fc' and col != 'id' and col not in ['hospital_death']]
evals_result = None
for fold_, (trn_idx, val_idx) in enumerate(folds.split(df_tr,target.values)):
    strLog = ""fold {}"".format(fold_)
    print(strLog)
    evals_result = {}
    trn_data = lgb.Dataset(df_tr.iloc[trn_idx][train_columns], label=target.iloc[trn_idx])
    val_data = lgb.Dataset(df_tr.iloc[val_idx][train_columns], label=target.iloc[val_idx],reference=trn_data)

    num_round = 7000
    clf = lgb.train(param,trn_data,num_round,valid_sets=val_data,early_stopping_rounds=100,verbose_eval=200,categorical_feature=categorical_index,evals_result=evals_result)
    oof[val_idx] = clf.predict(df_tr.iloc[val_idx][train_columns], num_iteration=clf.best_iteration)
    
    fold_importance_df = pd.DataFrame()
    fold_importance_df[""Feature""] = features
    fold_importance_df[""importance""] = clf.feature_importance()
    fold_importance_df[""fold""] = fold_ + 1
    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)
        
    a=roc_auc_score(target.loc[val_idx],clf.predict(df_tr.loc[val_idx,train_columns].values, num_iteration=clf.best_iteration))
    r.append(a)
    
    
    #predictions
    predictions += clf.predict(df_ts[train_columns], num_iteration=clf.best_iteration) / folds.n_splits
    
strAUC = roc_auc_score(target, oof)
print(strAUC)
print (""mean: ""+str(np.mean(np.array(r))))
print (""std: ""+str(np.std(np.array(r))))

df_sub = pd.DataFrame({'encounter_id': df_ts['encounter_id']})
df_sub['hospital_death'] = predictions

df_sub.to_csv(""sub1.csv"",index=False)




"
91,34088724,7,len(evals_result['valid_0']['auc'])
92,34088724,8,"df = pd.DataFrame({ 'iteration': [i for i in range(len(evals_result['valid_0']['auc']))],'auc': evals_result['valid_0']['auc']})

fig = px.line(df, x=""iteration"", y=""auc"", title='LGBM Validation AUC')
fig.show()"
93,34088724,9,"feature_importance_df.head(5).sort_values(by=['importance'], ascending=False)"
94,34088724,10,"feature_importance_df.groupby(['Feature'])[['importance']].mean().sort_values(
        ""importance"", ascending=False).head(10)"
95,34088724,11,"import plotly.express as px
df = feature_importance_df.groupby(['Feature'])[['importance']].mean().sort_values(
        ""importance"", ascending=True).tail(30).reset_index()
fig = px.bar(df, x=""importance"", y=""Feature"", orientation='h')
fig.show()"
96,34088724,12,clf.
97,34088724,13,"# for key, value in feature_importance_dfs.items():
#     print('Feature Importance for type ', encoders['type'].classes_[key], ' :')
#     print(
#     value.groupby(['Feature'])[['importance']].mean().sort_values(
#         ""importance"", ascending=False).head(20)
#          )
"
98,34088724,14,"grouped = df_tr.groupby(['apache_3j_diagnosis'])[
'd1_heartrate_min',
'apache_4a_hospital_death_prob',
'age',
'd1_temp_max',
'd1_platelets_min','bmi','d1_spo2_min',
'd1_heartrate_max',
'd1_creatinine_max',
'd1_wbc_min',
'urineoutput_apache',
'h1_heartrate_max',
'heart_rate_apache',
'd1_sodium_max',
'glucose_apache',
'd1_glucose_min',
'd1_lactate_min',
'weight',
'd1_bun_min',
'd1_bun_max',
'd1_arterial_ph_max',
'd1_glucose_max',
'd1_wbc_max',
'wbc_apache',
'd1_hemaglobin_max',
'd1_sysbp_noninvasive_max',
'd1_resprate_min',
'd1_creatinine_min',
'd1_calcium_min',
'd1_resprate_max',
'h1_heartrate_min',
'd1_pao2fio2ratio_min',
'd1_pao2fio2ratio_max',
'd1_hco3_max',
'd1_platelets_min',
'd1_lactate_max',
'hematocrit_apache',
'h1_temp_max',
'd1_temp_min',
'd1_arterial_pco2_max',
'bun_apache',
'd1_hematocrit_min',
'creatinine_apache','pre_icu_los_days'].mean()
"
99,34088724,15,"df_tr = pd.merge(df_tr, grouped, how='left', on=['apache_3j_diagnosis'])

df_ts = pd.merge(df_ts, grouped, how='left', on=['apache_3j_diagnosis'])
grouped"
100,34088724,16,"l = label_var(df_tr_ts, categorical_features)
label_enc(df_tr,l,categorical_features)
label_enc(df_ts,l,categorical_features)

for df in [df_tr, df_ts]:
    for m in categorical_features:
        df[m] = df[m].astype(float)
        
categorical_index = [train_columns.index(x) for x in categorical_features]

target = df_tr['hospital_death']

param = {'task': 'train',
         'boosting': 'gbdt',
         'objective':'binary',
         'metric': 'auc',
         'num_leaves': 15,
         'min_data_in_leaf': 90,
         'learning_rate': 0.01,
         'max_depth': 5,
         'feature_fraction': 0.1,
         'bagging_freq': 1,
         'bagging_fraction': 0.75,
         'use_missing': True,
         'nthread': 4
        }

folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=256)
oof = np.zeros(len(df_tr))
r=[]
predictions = np.zeros(len(df_ts))
feature_importance_df = pd.DataFrame()
features = train_columns#[col for col in df_tr.columns if col != 'fc' and col != 'id' and col not in ['hospital_death']]

for fold_, (trn_idx, val_idx) in enumerate(folds.split(df_tr,target.values)):
    strLog = ""fold {}"".format(fold_)
    print(strLog)
    
    trn_data = lgb.Dataset(df_tr.iloc[trn_idx][train_columns], label=target.iloc[trn_idx])
    val_data = lgb.Dataset(df_tr.iloc[val_idx][train_columns], label=target.iloc[val_idx],reference=trn_data)

    num_round = 7000
    clf = lgb.train(param,trn_data,num_round,valid_sets=val_data,early_stopping_rounds=100,verbose_eval=200,categorical_feature=categorical_index)
    oof[val_idx] = clf.predict(df_tr.iloc[val_idx][train_columns], num_iteration=clf.best_iteration)
    
    fold_importance_df = pd.DataFrame()
    fold_importance_df[""Feature""] = features
    fold_importance_df[""importance""] = clf.feature_importance()
    fold_importance_df[""fold""] = fold_ + 1
    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)
        
    a=roc_auc_score(target.loc[val_idx],clf.predict(df_tr.loc[val_idx,train_columns].values, num_iteration=clf.best_iteration))
    r.append(a)
    
    
    #predictions
    predictions += clf.predict(df_ts[train_columns], num_iteration=clf.best_iteration) / folds.n_splits
    
strAUC = roc_auc_score(target, oof)
print(strAUC)
print (""mean: ""+str(np.mean(np.array(r))))
print (""std: ""+str(np.std(np.array(r))))

df_sub = pd.DataFrame({'encounter_id': df_ts['encounter_id']})
df_sub['hospital_death'] = predictions

df_sub.to_csv(""sub1.csv"",index=False)




"
101,34088724,17,"np.mean(r), np.std(r)"
102,34088724,18,SHAP Feature Importance
103,34088724,19,"def plot_importances(importances_):
    mean_gain = importances_[['gain', 'feature']].groupby('feature').mean()
    importances_['mean_gain'] = importances_['feature'].map(mean_gain['gain'])
    plt.figure(figsize=(18, 44))
    data_imp = importances_.sort_values('mean_gain', ascending=False)
    sns.barplot(x='gain', y='feature', data=data_imp[:300])
    plt.tight_layout()
    plt.savefig('importances.png')
    plt.show()"
104,34088724,20,
105,29070286,0,"import numpy as np
import pandas as pd

import random
random.seed(28)
np.random.seed(28)

import matplotlib.pyplot as plt
from matplotlib_venn import venn2

from sklearn.metrics import (confusion_matrix, precision_recall_curve, auc,
                             roc_curve, recall_score, classification_report, f1_score,
                             precision_recall_fscore_support)
import os
import copy

pd.options.display.precision = 15

from collections import defaultdict
import lightgbm as lgb
import xgboost as xgb
import time
from collections import Counter
import datetime
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, GroupKFold, GridSearchCV, train_test_split, TimeSeriesSplit, RepeatedStratifiedKFold
from sklearn import metrics
from sklearn.metrics import mean_absolute_error
import gc
import seaborn as sns
import warnings
warnings.filterwarnings(""ignore"")
from bayes_opt import BayesianOptimization
#import eli5
import shap
from IPython.display import HTML
import json

import matplotlib.pyplot as plt
%matplotlib inline
import os

pd.set_option('max_rows', 500)
import re

pd.set_option('display.max_columns', 1000)
pd.set_option('display.max_rows', 500)
pd.set_option('display.width', 1000)
pd.set_option('display.float_format', '{:20,.2f}'.format)
pd.set_option('display.max_colwidth', -1)

np.random.seed(2206)"
106,29070286,1,"train = pd.read_csv(""../input/widsdatathon2020/training_v2.csv"")
samplesubmission = pd.read_csv(""../input/widsdatathon2020/samplesubmission.csv"")
test = pd.read_csv(""../input/widsdatathon2020/unlabeled.csv"")
dictionary = pd.read_csv(""../input/widsdatathon2020/WiDS Datathon 2020 Dictionary.csv"")
solution_template = pd.read_csv(""../input/widsdatathon2020/solution_template.csv"")

print('train ' , train.shape)
print('test ' , test.shape)
print('samplesubmission ' , samplesubmission.shape)
print('solution_template ' , solution_template.shape)
print('dictionary ' , dictionary.shape)"
107,29070286,2,"dico = pd.DataFrame(dictionary.T.head(6))
dico.columns=list(dico.loc[dico.index == 'Variable Name'].unstack())
dico = dico.loc[dico.index != 'Variable Name']
dico.columns
train_stat = pd.DataFrame(train.describe())
train_stat2 = pd.concat([dico,train_stat],axis=0)
train_stat2.head(20)"
108,29070286,3,train_stat2.T.head(200)
109,29070286,4,"# Missing Values
train.isna().sum()"
110,29070286,5,"# function to evaluate the score of our model
def eval_auc(pred,real):
    false_positive_rate, recall, thresholds = roc_curve(real, pred)
    roc_auc = auc(false_positive_rate, recall)
    return roc_auc    "
111,29070286,6,"# a wrapper class  that we can have the same ouput whatever the model we choose
class Base_Model(object):
    
    def __init__(self, train_df, test_df, features, categoricals=[], n_splits=5, verbose=True,ps={}):
        self.train_df = train_df
        self.test_df = test_df
        self.features = features
        self.n_splits = n_splits
        self.categoricals = categoricals
        self.target = 'hospital_death'
        self.cv = self.get_cv()
        self.verbose = verbose
#         self.params = self.get_params()
        self.params = self.set_params(ps)
        self.y_pred, self.score, self.model , self.oof_pred = self.fit()
        
    def train_model(self, train_set, val_set):
        raise NotImplementedError
        
    def get_cv(self):
        cv = StratifiedKFold(n_splits=self.n_splits, shuffle=True, random_state=42)
        return cv.split(self.train_df, self.train_df[self.target])
    
    def get_params(self):
        raise NotImplementedError
        
    def convert_dataset(self, x_train, y_train, x_val, y_val):
        raise NotImplementedError
        
    def convert_x(self, x):
        return x
        
    def fit(self):
        oof_pred = np.zeros((len(self.train_df), ))
        y_pred = np.zeros((len(self.test_df), ))
        for fold, (train_idx, val_idx) in enumerate(self.cv):
            x_train, x_val = self.train_df[self.features].iloc[train_idx], self.train_df[self.features].iloc[val_idx]
            y_train, y_val = self.train_df[self.target][train_idx], self.train_df[self.target][val_idx]
            train_set, val_set = self.convert_dataset(x_train, y_train, x_val, y_val)
            model = self.train_model(train_set, val_set)
            conv_x_val = self.convert_x(x_val)
            oof_pred[val_idx] = model.predict(conv_x_val).reshape(oof_pred[val_idx].shape)
            x_test = self.convert_x(self.test_df[self.features])
            y_pred += model.predict(x_test).reshape(y_pred.shape) / self.n_splits

            print('Partial score of fold {} is: {}'.format(fold,eval_auc(oof_pred[val_idx],y_val) ))
        #print(oof_pred, self.train_df[self.target].values)
        loss_score = eval_auc(oof_pred,self.train_df[self.target].values) 
        if self.verbose:
            print('Our oof AUC score is: ', loss_score)
        return y_pred, loss_score, model , oof_pred"
112,29070286,7,"#we choose to try a LightGbM using the Base_Model class
class Lgb_Model(Base_Model):
    
    def train_model(self, train_set, val_set):
        verbosity = 100 if self.verbose else 0
        return lgb.train(self.params, train_set, valid_sets=[train_set, val_set], verbose_eval=verbosity)
        
    def convert_dataset(self, x_train, y_train, x_val, y_val):
        train_set = lgb.Dataset(x_train, y_train, categorical_feature=self.categoricals)
        val_set   = lgb.Dataset(x_val,    y_val,  categorical_feature=self.categoricals)
        return train_set, val_set
        
    def get_params(self):
        params = {'n_estimators':5000,
                  'boosting_type': 'gbdt',
                  'objective': 'binary',
                  'metric': 'auc',
                  'subsample': 0.75,
                  'subsample_freq': 1,
                  'learning_rate': 0.1,
                  'feature_fraction': 0.9,
                  'max_depth': 15,
                  'lambda_l1': 1,  
                  'lambda_l2': 1,
                  'early_stopping_rounds': 100,
                  #'is_unbalance' : True ,
                  'scale_pos_weight' : 3,
                  'device': 'gpu',
                  'gpu_platform_id': 0,
                  'gpu_device_id': 0,
                  'num_leaves': 31
                    }
        return params
    def set_params(self,ps={}):
        params = self.get_params()
        if 'subsample_freq' in ps:
            params['subsample_freq']=int(ps['subsample_freq'])
            params['learning_rate']=ps['learning_rate']
            params['feature_fraction']=ps['feature_fraction']
            params['lambda_l1']=ps['lambda_l1']
            params['lambda_l2']=ps['lambda_l2']
            params['scale_pos_weight']=ps['scale_pos_weight']
            params['max_depth']=int(ps['max_depth'])
            params['subsample']=ps['subsample']
            params['num_leaves']=int(ps['num_leaves'])
            params['min_split_gain']=ps['min_split_gain']
#             params['min_child_weight']=ps['min_child_weight']
        
        return params  "
113,29070286,8,"def plot_importances(importances_, plot_name):
    mean_gain = importances_[['gain', 'feature']].groupby('feature').mean()
    importances_['mean_gain'] = importances_['feature'].map(mean_gain['gain'])
    plt.figure(figsize=(18, 44))
    data_imp = importances_.sort_values('mean_gain', ascending=False)
    sns.barplot(x='gain', y='feature', data=data_imp[:300])
    plt.tight_layout()
    plt.savefig('{}.png'.format(plot_name))
    plt.show()"
114,29070286,9,"# Replace values

print('Replacing: {}'.format('hospital_admit_source'))

replace_hospital_admit_source =  {'Other ICU': 'ICU',
                                  'ICU to SDU':'SDU', 
                                  'Step-Down Unit (SDU)': 'SDU', 
                                  'Other Hospital':'Other',
                                  'Observation': 'Recovery Room',
                                  'Acute Care/Floor': 'Acute Care'}
train['hospital_admit_source'].replace(replace_hospital_admit_source, inplace=True)
test['hospital_admit_source'].replace(replace_hospital_admit_source, inplace=True)

#combined_dataset['icu_type'] = combined_dataset['icu_type'].replace({'CCU-CTICU': 'Grpd_CICU', 'CTICU':'Grpd_CICU', 'Cardiac ICU':'Grpd_CICU'})

print('Replacing: {}'.format('apache_2_bodysystem'))

replace_apache_2_bodysystem =  {'Undefined diagnoses': 'Undefined Diagnoses'}
train['apache_2_bodysystem'].replace(replace_apache_2_bodysystem, inplace=True)
test['apache_2_bodysystem'].replace(replace_apache_2_bodysystem, inplace=True)"
115,29070286,10,"#we are going to drop these columns because we dont want our ML model to be bias toward these consideration
#(we also remove the target and the ids.)
to_drop = ['gender','ethnicity' ,'encounter_id', 'patient_id',  'hospital_death']

# this is a list of features that look like to be categorical
categoricals_features = ['hospital_id','ethnicity','gender','hospital_admit_source','icu_admit_source',
                         'icu_stay_type','icu_type','apache_3j_bodysystem','apache_2_bodysystem']
categoricals_features = [col for col in categoricals_features if col not in to_drop]

# this is the list of all input feature we would like our model to use 
features = [col for col in train.columns if col not in to_drop ]
print('numerber of features ' , len(features))
print('shape of train / test ', train.shape , test.shape)"
116,29070286,11,"# categorical feature need to be transform to numeric for mathematical purpose.
# different technics of categorical encoding exists here we will rely on our model API to deal with categorical
# still we need to encode each categorical value to an id , for this purpose we use LabelEncoder

print('Transform all String features to category.\n')
for usecol in categoricals_features:
    train[usecol] = train[usecol].astype('str')
    test[usecol] = test[usecol].astype('str')
    
    #Fit LabelEncoder
    le = LabelEncoder().fit(
            np.unique(train[usecol].unique().tolist()+
                      test[usecol].unique().tolist()))

    #At the end 0 will be used for dropped values
    train[usecol] = le.transform(train[usecol])+1
    test[usecol]  = le.transform(test[usecol])+1
    
    train[usecol] = train[usecol].replace(np.nan, 0).astype('int').astype('category')
    test[usecol]  = test[usecol].replace(np.nan, 0).astype('int').astype('category')"
117,29070286,12,"def adversarial_validation(train, test, features):
    tr_data   = train.copy()
    tst_data = test.copy()
    tr_data['target']  = 0 
    tst_data['target'] = 1
    av_data = pd.concat([tr_data, tst_data], axis = 0)
    av_data.reset_index(drop = True)        
    params = {
            'learning_rate': 0.1, 
            'seed': 50,
            'objective':'binary',
            'boosting_type':'gbdt',
            'metric': 'auc',
        }    
    # define a KFold strategy
    kf = StratifiedKFold(n_splits = 5, shuffle=True, random_state=42)
    target = 'target'
    oof_pred = np.zeros(len(av_data))
    important_features = pd.DataFrame()
    fold_auc = []    
    
    for fold, (tr_ind, val_ind) in enumerate(kf.split(av_data, av_data[target])) :
        print('Fold {}'.format(fold + 1))
        x_train, x_val = av_data[features].iloc[tr_ind], av_data[features].iloc[val_ind]
        y_train, y_val = av_data[target].iloc[tr_ind], av_data[target].iloc[val_ind]
        train_set = lgb.Dataset(x_train, y_train)
        val_set   = lgb.Dataset(x_val, y_val)
        
        model = lgb.train(params, train_set, num_boost_round = 1000, early_stopping_rounds = 20, valid_sets = [train_set, val_set], verbose_eval = 100)
        
        fold_importance = pd.DataFrame()
        fold_importance['feature'] = features
        fold_importance['gain'] = model.feature_importance()
        important_features = pd.concat([important_features, fold_importance], axis = 0)
        
        oof_pred[val_ind] = model.predict(x_val)
        fold_auc.append(metrics.roc_auc_score(y_train, model.predict(x_train)))
        
    print('Our mean train roc auc score is :', np.mean(fold_auc))
    print('Our oof roc auc score is :', metrics.roc_auc_score(av_data[target], oof_pred))
    return important_features"
118,29070286,13,"# run the adversatial model with all the feature we used :
    
adversarial_features = adversarial_validation(train, test, features)"
119,29070286,14,"# AUC is almost perfect so we can expect that some feature are perfectly different between train / test

adversarial_features = adversarial_features[['gain', 'feature']].groupby('feature').mean().reset_index()
adversarial_features= adversarial_features.sort_values('gain', ascending=False)

plot_importances(adversarial_features, 'importances-lgb-v6')"
120,29070286,15,"# So icu_id columns seems to be the feature that dominate the feature importance for the adversarial 
# validation model, so it is likely to be totally different between train and test, 
# lets check the distribution of the top features :

def plot_differente_between_train_test(adversarial_features):
    import warnings
    warnings.filterwarnings(""ignore"")
    warnings.simplefilter(action='ignore', category=UserWarning)
    i=0
    for index, row in adversarial_features.sort_values(by=['gain'],ascending=False).iterrows():  
        column=row['feature']
        if i< 10:
                print(column,i,""gain :"",row['gain'])
                df1      = train.copy()
                df2      = test.copy()

                fig = plt.figure(figsize=(20,4))
                sns.distplot(df1[column].dropna(),  color='yellow', label='train', kde=True); 
                sns.distplot(df2[column].dropna(),  color='violet', label='test', kde=True); 
                fig=plt.legend(loc='best')
                plt.xlabel(column, fontsize=12);
                plt.show()
                i=i+1

plot_differente_between_train_test(adversarial_features)"
121,29070286,16,"# it is .... Let's remove icu_id and see the results ..
adversarial_features2 = adversarial_validation(train, test, [ f for f in features if f not in ['icu_id'] ])"
122,29070286,17,"# Let`s check again the difference between train / test

adversarial_features2 = adversarial_features2[['gain', 'feature']].groupby('feature').mean().reset_index()
adversarial_features2= adversarial_features2.sort_values('gain', ascending=False)

plot_importances(adversarial_features2, 'importances-lgb-v6')"
123,29070286,18,plot_differente_between_train_test(adversarial_features2)
124,29070286,19,"# hospital_id seems to be also from a different distribution. 
# We can check it directly, obviously only few hospital are common to both dataset ..

common_id  = list([id for id in train['hospital_id'].unique() if id in test['hospital_id'].unique() ])
id_only_in_train  = [id for id in train['hospital_id'].unique() if id not in test['hospital_id'].unique() ]
id_only_in_test   = [id for id in test['hospital_id'].unique()  if id not in train['hospital_id'].unique() ]
count_common_train = train.loc[train['hospital_id'].isin(common_id)].shape[0]
count_common_test  = test.loc[test['hospital_id'].isin(common_id)].shape[0]

count_train = train.loc[train['hospital_id'].isin(id_only_in_train)].shape[0]
count_test  = test.loc[test['hospital_id'].isin(id_only_in_test)].shape[0]

 
fig = plt.figure(figsize=(20,6))
venn2(subsets = (count_train,  count_test, count_common_train+count_common_test), set_labels = ('Hospital only in train', 'Hospital only in test'),set_colors=('purple', 'yellow'), alpha = 0.7);
plt.show()"
125,29070286,20,"# Let's do an ultimate try without 'icu_id','hospitaadversarial_features3 = adversarial_validation(train, test, [ f for f in features if f not in ['icu_id','hospital_id'] ])l_id'
adversarial_features3 = adversarial_validation(train, test, [ f for f in features if f not in ['icu_id','hospital_id'] ])"
126,29070286,21,"# I leave it to you to see what you can do with other features..
adversarial_features3 = adversarial_features3[['gain', 'feature']].groupby('feature').mean().reset_index()
adversarial_features3= adversarial_features3.sort_values('gain', ascending=False)

plot_importances(adversarial_features3, 'importances-lgb-v6')"
127,29070286,22,plot_differente_between_train_test(adversarial_features3)
128,29070286,23,"# Lets remove hospital_id and icu_id

print('Difference between train and teste> -- hospital_id: ')
print(len(list(set(train['hospital_id']) - set(test['hospital_id']))))

print('\nDifference between train and teste> -- icu_id: ')
print(len(list(set(train['icu_id']) - set(test['icu_id']))))


# Drop features with zero importance
print('\nLength train features: {}'.format(len(features)))
for feat_to_remove in ['icu_id', 'hospital_id']:
    if feat_to_remove in categoricals_features:
        print('Removing from categoricals_features....{}'.format(feat_to_remove))
        categoricals_features.remove(feat_to_remove)
    if feat_to_remove in features:
        print('Removing from features....{}'.format(feat_to_remove))
        features.remove(feat_to_remove)
    
print('\nNew length train features: {}'.format(len(features)))"
129,29070286,24,"# percentage of death , hopefully it s a bit unbalanced
train['hospital_death'].sum()/train['hospital_death'].count()"
130,29070286,25,"# You want Bayesian Optimization?

boll_BayesianOptimization = False
# boll_BayesianOptimization = True"
131,29070286,26,"%time

def LGB_Beyes(subsample_freq,
                    learning_rate,
                    feature_fraction,
                    max_depth,
                    lambda_l1,
                    lambda_l2,
                    scale_pos_weight,
                    subsample,
                    num_leaves,
                    min_split_gain):
#                     min_child_weight):
    params={}
    params['subsample_freq']=subsample_freq
    params['learning_rate']=learning_rate
    params['feature_fraction']=feature_fraction
    params['lambda_l1']=lambda_l1
    params['lambda_l2']=lambda_l2
    params['max_depth']=max_depth
    params['scale_pos_weight']=scale_pos_weight
    params['subsample']=subsample
    params['num_leaves']=num_leaves
    params['min_split_gain']=min_split_gain
   # params['min_child_weight']=min_child_weight
    
    
    lgb_model= Lgb_Model(train, test, features, categoricals=categoricals_features,ps=params)
    print('auc: ',lgb_model.score)
    return lgb_model.score

bounds_LGB = {
    'max_depth': (5, 17),
    'subsample': (0.5, 1),
    'num_leaves': (10, 45),
    'feature_fraction': (0.1, 1),
    'min_split_gain': (0.0, 0.1),
#     'min_child_weight': (1e-3, 50),
    'subsample_freq': (1, 10),
    'learning_rate': (0.005, 0.02),
    'lambda_l1': (0, 5),
    'lambda_l2': (0, 5),
    'scale_pos_weight': (1, 10)
}

# ACTIVATE it if you want to search for better parameter
if boll_BayesianOptimization: 
    LGB_BO = BayesianOptimization(LGB_Beyes, bounds_LGB, random_state=1029)
    import warnings
    init_points = 16
    n_iter = 16
    with warnings.catch_warnings():
        warnings.filterwarnings('ignore')    
        LGB_BO.maximize(init_points=init_points, n_iter=n_iter, acq='ucb', xi=0.0, alpha=1e-6)"
132,29070286,27,"if boll_BayesianOptimization and LGB_BO:
    print(LGB_BO.max['params'])"
133,29070286,28,"# params = {'feature_fraction': 0.9,
#  'lambda_l1': 1,
#  'lambda_l2': 1,
#  'learning_rate': 0.1,
#  'max_depth': 13,
#  'subsample_freq': 1,
#  'scale_pos_weight':1}

# Best Hyperparams from Bayesian Optimization in notebook lgb-v2
# params = {'feature_fraction': 0.524207414205945,
#  'lambda_l1': 4.171808735757517,
#  'lambda_l2': 4.6435328298317256,
#  'learning_rate': 0.007897539397989824,
#  'max_depth': 16.62053004755999,
#  'scale_pos_weight': 1.2199266532301127,
#  'subsample_freq': 1.0276518730971627}


# # Best Hyperparams from Bayesian Optimization in notebook lgb-v3
# params = {'feature_fraction': 0.524207414205945,
#  'lambda_l1': 4.171808735757517,
#  'lambda_l2': 4.6435328298317256,
#  'learning_rate': 0.007897539397989824,
#  'max_depth': 16.62053004755999,
#  'scale_pos_weight': 1.2199266532301127,
#  'subsample_freq': 1.0276518730971627}

# # Best Hyperparams from Bayesian Optimization in notebook lgb-v4
# params = {'feature_fraction': 0.5348508368206359,
#  'lambda_l1': 0.0009370993396629057,
#  'lambda_l2': 4.743745312344983,
#  'learning_rate': 0.012891827059322746,
#  'max_depth': 15.784155449197529,
#  'scale_pos_weight': 1.0325760631926175,
#  'subsample_freq': 1.0744384574974872}


# Best Hyperparams from Bayesian Optimization in notebook lgb-v5
# params = {'feature_fraction': 0.3245039721724266,
#  'lambda_l1': 1.416727346446085,
#  'lambda_l2': 2.779776916582821,
#  'learning_rate': 0.006854369969433722,
#  'max_depth': 16.673905691676964,
#  'min_split_gain': 0.05643417986130283,
#  'num_leaves': 44.8672896759208,
#  'scale_pos_weight': 1.1577974342088542,
#  'subsample': 0.630352165410007,
#  'subsample_freq': 1.2158674819047501}

# Best Hyperparams from Bayesian Optimization in notebook lgb-v6 -- BEST MODEL
params = {
   ""feature_fraction"":0.1743912077888097,
   ""lambda_l1"":2.838660318794291,
   ""lambda_l2"":0.292397357257721,
   ""learning_rate"":0.012602188092427687,
   ""max_depth"":16.575351761228106,
   ""min_split_gain"":0.04631934372471113,
   ""num_leaves"":44.81666226482246,
   ""scale_pos_weight"":1.0897617979884857,
   ""subsample"":0.8260779721854892,
   ""subsample_freq"":1.2473380372944387
}"
134,29070286,29,"%time

if boll_BayesianOptimization: # ACTIVATE it if you want to search/use for better parameter
    lgb_model = Lgb_Model(train,test, features, categoricals=categoricals_features, ps= LGB_BO.max['params'])
else :
    lgb_model = Lgb_Model(train,test, features, categoricals=categoricals_features, ps=params)"
135,29070286,30,"imp_df = pd.DataFrame()
imp_df['feature'] = features
imp_df['gain']  = lgb_model.model.feature_importance(importance_type='gain')
imp_df['split'] = lgb_model.model.feature_importance(importance_type='split')"
136,29070286,31,"plot_importances(imp_df, 'importances-lgb-v6-lgb_model')"
137,29070286,32,"import shap
explainer   =  shap.TreeExplainer(lgb_model.model)
shap_values = explainer.shap_values(train[features].iloc[:1000,:])
shap.summary_plot(shap_values, train[features].iloc[:1000,:])"
138,29070286,33,"print('AUC Version 1: ', lgb_model.score)
#print('AUC:Version 2: ', lgb_model_v2.score)"
139,29070286,34,"test[""hospital_death""] = lgb_model.y_pred
#test[[""encounter_id"",""hospital_death""]].to_csv(""submission6-lgb-v6.csv"",index=False)

test[[""encounter_id"",""hospital_death""]].head()"
140,27795188,0,"import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))
#Linear Algebra
import numpy as np
#Data preprocessing
import pandas as pd

#setting display options
pd.set_option('display.max_rows', 5000)
pd.set_option('display.max_columns', 500)
pd.set_option('max_colwidth', 500)
np.set_printoptions(linewidth =400)

from matplotlib import pyplot as plt
%matplotlib inline
#Advance-style plotting
import seaborn as sns
color =sns.color_palette()
sns.set_style('darkgrid')

#Ignore annoying warning from sklearn and seaborn
import warnings
def ignore_warn(*args, **kwargs):
    pass
warnings.warn = ignore_warn

#other libraiaries
import os
import copy
from collections import defaultdict
from collections import Counter
from sklearn import metrics
import matplotlib.pyplot as plt
%matplotlib inline
import os
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import metrics
import re
import plotly.graph_objs as go
from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot
init_notebook_mode(connected=True)"
141,27795188,1,description = pd.read_csv('/kaggle/input/widsdatathon2020/WiDS Datathon 2020 Dictionary.csv'); description
142,27795188,2,"#read the data
train = pd.read_csv('/kaggle/input/widsdatathon2020/training_v2.csv')
test  = pd.read_csv('/kaggle/input/widsdatathon2020/unlabeled.csv')"
143,27795188,3,train.columns
144,27795188,4,"print(train.shape , test.shape)"
145,27795188,5,"#column 1 Unique identifier associated with a patient unit stay
print (train['encounter_id'].nunique() , test['encounter_id'].nunique())"
146,27795188,6,"#column 2 Unique identifier associated with a hospital
print (train['hospital_id'].nunique() , test['hospital_id'].nunique())"
147,27795188,7,"#column 3 Unique identifier associated with a patient
print (train['patient_id'].nunique() , test['patient_id'].nunique())"
148,27795188,8,"#column 4
Yes = len(train[train.hospital_death ==1])
No = len(train[train.hospital_death ==0])
Total = len(train)
print ('There are imbalanace datset with a %i/%i ratio'%((No/Total*100), (Yes/Total*100)+1))"
149,27795188,9,"sns.catplot(x ='hospital_death', kind ='count',palette='pastel', data = train);"
150,27795188,10,"#columnn 5
train['age'].describe()"
151,27795188,11,#Hint: ensure all units of each columns are having relationship with respect to each other..
152,28727928,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import StratifiedKFold, KFold
import matplotlib.pyplot as plt 
from sklearn.metrics import roc_auc_score
pd.set_option('display.max_columns', 500)
pd.set_option('display.max_rows', 500)

import lightgbm

# Input data files are available in the ""../input/"" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))
        


# Any results you write to the current directory are saved as output."
153,28727928,1,"drop_cols = ['encounter_id','patient_id','icu_id', 'hospital_id', 'readmission_status']
drop_cols_test = ['encounter_id','patient_id','icu_id','hospital_death','hospital_id', 'readmission_status']

train = pd.read_csv(""/kaggle/input/widsdatathon2020/training_v2.csv"").drop(drop_cols, axis=1)
# sample_submission = pd.read_csv(""/kaggle/input/widsdatathon2020/samplesubmission.csv"")
test = pd.read_csv(""/kaggle/input/widsdatathon2020/unlabeled.csv"").drop(drop_cols_test, axis=1)
data_dictionary = pd.read_csv(""/kaggle/input/widsdatathon2020/WiDS Datathon 2020 Dictionary.csv"")
solution_template = pd.read_csv(""/kaggle/input/widsdatathon2020/solution_template.csv"")

target = 'hospital_death'"
154,28727928,2,"print(f'Rows in train data : {train.shape[0]} and columns in train data: {train.shape[1]}')
print(f'Rows in test data  : {test.shape[0]} and columns in train data: {test.shape[1]}')"
155,28727928,3,"np.round(train[target].value_counts()*100/len(train[target]),2)"
156,28727928,4,"ax = sns.countplot(train[target])
for p in ax.patches:
    ax.annotate('{}'.format(p.get_height()), (p.get_x()+0.15, p.get_height()+1))
plt.show()"
157,28727928,5,train.info()
158,28727928,6,train.head()
159,28727928,7,"np.round(train.isna().mean()*100,2)"
160,28727928,8,"np.round(test.isna().mean()*100,2)"
161,28727928,9,train.dtypes
162,28727928,10,train.select_dtypes(include='O').columns.values.tolist()
163,28727928,11,test.select_dtypes(include='O').columns.values.tolist()
164,28727928,12,"def explore_variable(col_name):
    """"""
    Helper function for categorical variable
    """"""
    print(f""Unique values in train: {train[col_name].unique()}"")
    print(f""Unique values in test:  {test[col_name].unique()}"")
    print(f""Number of unique values in train : {train[col_name].nunique()}"") 
    print(f""Number of unique values in test: {test[col_name].nunique()}"")

def count_plot(col_name, fig_size=(10,10)):
    """"""
    Helper function for count plot. 
    Here in count plot I have ordered by train[col].value_counts so it is easy compare distribution between train and test
    """"""
    fig = plt.figure(figsize=fig_size)
    fig.add_subplot(2,1,1)            
    ax1 = sns.countplot(x=col_name, data=train, order = train[col_name].value_counts().index)
    for p in ax1.patches:
        ax1.annotate('{}'.format(p.get_height()), (p.get_x()+0.15, p.get_height()+1))
    ax1.set_title(""Train distribution"", fontsize='large')
    ax1.set_ylabel(col_name)
    fig.add_subplot(2,1,2)            
    ax2 = sns.countplot(x=col_name, data=test, order = train[col_name].value_counts().index)
    ax2.set_title(""Test distribution"", fontsize='large')
    ax2.set_ylabel(col_name)
    for p in ax2.patches:
        ax2.annotate('{}'.format(p.get_height()), (p.get_x()+0.15, p.get_height()+1))
    
    plt.show()                        "
165,28727928,13,explore_variable('ethnicity')
166,28727928,14,"count_plot(col_name ='ethnicity', fig_size=(20,10))"
167,28727928,15,explore_variable('age')
168,28727928,16,"count_plot(col_name ='age', fig_size=(40,15))"
169,28727928,17,explore_variable('hospital_admit_source')
170,28727928,18,"count_plot(col_name ='hospital_admit_source', fig_size=(30,12))"
171,28727928,19,explore_variable('icu_admit_source')
172,28727928,20,"count_plot(col_name ='icu_admit_source', fig_size=(30,12))"
173,28727928,21,explore_variable('icu_stay_type')
174,28727928,22,"count_plot(col_name ='icu_stay_type', fig_size=(30,10))"
175,28727928,23,explore_variable('icu_type')
176,28727928,24,"count_plot(col_name ='icu_type', fig_size=(20,10))"
177,28727928,25,explore_variable('apache_3j_bodysystem')
178,28727928,26,"count_plot(col_name ='apache_3j_bodysystem', fig_size=(25,12))"
179,28727928,27,explore_variable('apache_2_bodysystem')
180,28727928,28,"count_plot(col_name ='apache_2_bodysystem', fig_size=(25,12))"
181,28727928,29,"cat_cols =  train.select_dtypes(include='O').columns.values.tolist()
for col in cat_cols: 
    if col in train.columns: 
        le = LabelEncoder() 
        le.fit(list(train[col].astype(str).values) + list(test[col].astype(str).values)) 
        train[col] = le.transform(list(train[col].astype(str).values)) 
        test[col] = le.transform(list(test[col].astype(str).values)) "
182,28727928,30,"y=train[target]
train=train.drop(target, axis=1)"
183,28727928,31,"# Parameters
params = {""objective"": ""binary"", 
          ""boosting"": ""gbdt"",
          ""metric"": ""auc"",
          ""n_jobs"":-1,
          ""verbose"":-1}

num_folds = 10
roc_auc = list()
feature_importances = pd.DataFrame()
feature_importances['feature'] = train.columns
pred_on_test = np.zeros(test.shape[0])


kf = StratifiedKFold(n_splits=num_folds,shuffle=True, random_state=2020)
for index, (train_index, valid_index) in enumerate(kf.split(X=train,y=y)):
    print(f""FOLD {index+1}"")

    X_train_fold, y_train_fold = train.iloc[train_index], y.iloc[train_index]
    X_valid_fold, y_valid_fold = train.iloc[valid_index], y.iloc[valid_index]

    dtrain = lightgbm.Dataset(X_train_fold, label=y_train_fold)
    dvalid = lightgbm.Dataset(X_valid_fold, label=y_valid_fold)

    lgb = lightgbm.train(params=params, train_set=dtrain, num_boost_round=2000, 
                         valid_sets=[dtrain, dvalid], verbose_eval=250, early_stopping_rounds=500)

    feature_importances[f'fold_{index + 1}'] = lgb.feature_importance()

    y_valid_pred = (lgb.predict(X_valid_fold,num_iteration=lgb.best_iteration))
    pred_on_test += (lgb.predict(test,num_iteration=lgb.best_iteration)) / num_folds

    # winsorization
    y_valid_pred = np.clip(a=y_valid_pred, a_min=0, a_max=1)
    pred_on_test = np.clip(a=pred_on_test, a_min=0, a_max=1)

    print(f""FOLD {index+1}: ROC_AUC  => {np.round(roc_auc_score(y_true=y_valid_fold, y_score=y_valid_pred),5)}"")
    roc_auc.append(roc_auc_score(y_true=y_valid_fold, y_score=y_valid_pred)/num_folds)
    
print(f""Mean roc_auc for {num_folds} folds: {np.round(sum(roc_auc),5)}"")"
184,28727928,32,"def plot_feature_importance(df, k_fold_object):
    df['average_feature_imp'] = df[['fold_{}'.format(fold + 1) for fold in range(k_fold_object.n_splits)]].mean(axis=1)
    plt.figure(figsize=(10, 40))
    sns.barplot(data=df.sort_values(by='average_feature_imp', ascending=False), x='average_feature_imp', y='feature');
    plt.title('Feature importance over {} folds average'.format(k_fold_object.n_splits))
    plt.show()"
185,28727928,33,"plot_feature_importance(df=feature_importances, k_fold_object=kf)"
186,28727928,34,"solution_template.hospital_death = pred_on_test
solution_template.to_csv(""Version_1.csv"", index=0)"
187,27606681,0,from fastai.tabular import * 
188,27606681,1,"train = pd.read_csv(""../input/widsdatathon2020/training_v2.csv"")
test = pd.read_csv(""../input/widsdatathon2020/unlabeled.csv"")"
189,27606681,2,"procs = [FillMissing, Categorify, Normalize]
"
190,27606681,3,"valid_idx = range(int(len(train)*0.9), len(train))"
191,27606681,4,dep_var = 'hospital_death'
192,27606681,5,"def to_cat(c): 
    train[c] = train[c].apply(str)
    test[c]  = test[c].apply(str)
    
[to_cat(c) for c in ['apache_3j_diagnosis', 'apache_2_diagnosis']]"
193,27606681,6,"cat_names = ['ethnicity', 'gender', 'hospital_admit_source', 'icu_admit_source',  
             'icu_stay_type', 'icu_type', 'apache_3j_bodysystem', 'apache_2_bodysystem', 
             'elective_surgery', 'apache_post_operative', 'arf_apache', 'gcs_eyes_apache', 
             'gcs_motor_apache', 'gcs_unable_apache', 'gcs_verbal_apache', 'intubated_apache', 
             'ventilated_apache', 'aids', 'cirrhosis', 'diabetes_mellitus', 'hepatic_failure', 
             'immunosuppression', 'leukemia', 'lymphoma', 'solid_tumor_with_metastasis', 
             'icu_id', 'apache_3j_diagnosis' , 'apache_2_diagnosis']"
194,27606681,7,"cont_names = list(set(train)-set(cat_names)-
                  {dep_var, 'hospital_id', 'patient_id', 'encounter_id'})
cont_names"
195,27606681,8,"train[""apache_4a_hospital_death_prob""]=train[""apache_4a_hospital_death_prob""].replace({-1:np.nan})
test[""apache_4a_hospital_death_prob""]=test[""apache_4a_hospital_death_prob""].replace({-1:np.nan})

train[""apache_4a_icu_death_prob""]=train[""apache_4a_icu_death_prob""].replace({-1:np.nan})
test[""apache_4a_icu_death_prob""]=test[""apache_4a_icu_death_prob""].replace({-1:np.nan})"
196,27606681,9,"data = TabularDataBunch.from_df('.', train, dep_var, 
                                valid_idx=valid_idx, procs=procs, 
                                cat_names=cat_names, cont_names=cont_names)"
197,27606681,10,"learn = tabular_learner(data, layers=[100,100], ps=0.5, emb_drop=0.5, metrics=[accuracy, AUROC()])"
198,27606681,11,"learn.fit_one_cycle(3, 1e-2)"
199,27606681,12,"data.add_test(TabularList.from_df(test,path='.' ,cat_names=cat_names, cont_names=cont_names))"
200,27606681,13,"probs = learn.get_preds(DatasetType.Test)[0][:,1]
probs"
201,27606681,14,"sub = pd.read_csv('../input/widsdatathon2020/solution_template.csv')
sub['hospital_death'] = probs
sub.head()"
202,27606681,15,"sub.to_csv(""sub.csv"", header=True, index=False)"
203,27606681,16,! head sub.csv
204,27654742,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the ""../input/"" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# Any results you write to the current directory are saved as output.

import warnings
warnings.filterwarnings(""ignore"")

import seaborn as sns
import math
import matplotlib as p
import matplotlib.pyplot as plt
%matplotlib inline
import scipy.stats as sps
import re"
205,27654742,1,"train = pd.read_csv('../input/widsdatathon2020/training_v2.csv')
test = pd.read_csv('../input/widsdatathon2020/unlabeled.csv')
st = pd.read_csv('../input/widsdatathon2020/solution_template.csv')
ss = pd.read_csv('../input/widsdatathon2020/samplesubmission.csv')
dictionary = pd.read_csv('../input/widsdatathon2020/WiDS Datathon 2020 Dictionary.csv')

pd.set_option('display.max_columns', 500)
print('solution template shape', st.shape)
display(st.head())
print('dictionary shape', dictionary.shape)
display(dictionary.T.head())
print('train shape', train.shape)
display(train.head())
print('test shape', test.shape)
display(test.head())"
206,27654742,2,"# Dropping patient_id for now
train = train.copy().drop('patient_id', axis = 1)
test = test.copy().drop('patient_id', axis = 1)"
207,27654742,3,"from sklearn.model_selection import train_test_split

Train, Validation = train_test_split(train, test_size = 0.3)"
208,27654742,4,"X_train = Train.copy().drop('hospital_death', axis = 1)
y_train = Train[['encounter_id','hospital_death']]
X_val = Validation.copy().drop('hospital_death', axis = 1)
y_val = Validation[['encounter_id','hospital_death']]"
209,27654742,5,"X_test = test.copy().drop('hospital_death', axis = 1)
y_test = test[['encounter_id','hospital_death']]"
210,27654742,6,"sns.catplot('hospital_death', data= train, kind='count', alpha=0.7, height=6, aspect=1)

# Get current axis on current figure
ax = plt.gca()

# Max value to be set
y_max = train['hospital_death'].value_counts().max() 

# Iterate through the list of axes' patches
for p in ax.patches:
    ax.text(p.get_x() + p.get_width()/5., p.get_height(),'%d' % int(p.get_height()),
            fontsize=13, color='blue', ha='center', va='bottom')
plt.title('Frequency plot of Hospital Deaths', fontsize = 20, color = 'black')
plt.show()"
211,27654742,7,"plt.figure(figsize=(30,15))
ethnicity_vs_death = sns.catplot(x='ethnicity', col='hospital_death', kind='count', data=train, 
                                 order = train['ethnicity'].value_counts().index, height = 7, aspect = 1);
ethnicity_vs_death.set_xticklabels(rotation=90);"
212,27654742,8,"plt.figure(figsize=(30,15))
has_vs_death = sns.catplot(x='hospital_admit_source', col='hospital_death', kind='count', data=train, 
                           order = train['hospital_admit_source'].value_counts().index, height = 7, aspect = 1.5);
has_vs_death.set_xticklabels(rotation=90);"
213,27654742,9,"plt.figure(figsize=(30,15))
ias_vs_death = sns.catplot(x='icu_admit_source', col='hospital_death', kind='count', data=train, 
                           order = train['icu_admit_source'].value_counts().index, height = 7, aspect = 1.5);
ias_vs_death.set_xticklabels(rotation=90);"
214,27654742,10,"# Freq plot of Hospital ID for hospital_death = 0
train_hID_0 = train[train['hospital_death'] == 0]
plt.figure(figsize=(30,15))
hID_vs_death = sns.catplot(y='hospital_id',  orient = ""v"", kind='count', data=train_hID_0, order = train_hID_0['hospital_id'].value_counts().index, 
                           height = 30, aspect = 1)"
215,27654742,11,"# Freq plot of Hospital ID for hospital_death = 1
train_hID_1 = train[train['hospital_death'] != 0]
plt.figure(figsize=(30,20))
hID_vs_death = sns.catplot(y='hospital_id',  orient = ""v"", kind='count', data=train_hID_1, order = train_hID_1['hospital_id'].value_counts().index, 
                           height = 30, aspect = 1);"
216,27654742,12,"# Freq plot of Hospital ID for hospital_death = 0 & 1
plt.figure(figsize=(30,40))
hID_vs_death = sns.catplot(x = 'hospital_id', col='hospital_death', kind='count', data=train, order = train['hospital_id'].value_counts().index, 
                           height = 5, aspect = 2.8);
hID_vs_death.set_xticklabels(rotation=90);"
217,27654742,13,"plt.figure(figsize = (15,5))
sns.kdeplot(train_hID_1['age'], shade=True, color=""r"")
sns.kdeplot(train_hID_0['age'], shade=True, color=""b"")"
218,27654742,14,"sns.jointplot(x=""age"", y=""bmi"", data=train, kind = ""kde"")
sns.jointplot(x=""age"", y=""height"", data=train, kind = ""kde"")
sns.jointplot(x=""age"", y=""weight"", data=train, kind = ""kde"")"
219,27654742,15,"dataset = pd.concat(objs=[X_train, X_val], axis=0)"
220,27654742,16,col_1 = dataset.columns
221,27654742,17,"for i in col_1:
    if X_train[i].nunique() == 1:
        print('in Train', i)
    if X_val[i].nunique() == 1:
        print('in Val', i)
    if X_test[i].nunique() == 1:
        print('in Test', i)
    "
222,27654742,18,"# Dropping 'readmission_status'
X_train = X_train.drop(['readmission_status'], axis=1)
X_val = X_val.drop(['readmission_status'], axis=1)
X_test = X_test.drop(['readmission_status'], axis=1)"
223,27654742,19,"print('For Train')
d1 = X_train.nunique()
print(sorted(d1))
print(""=============================="")
print('For Validation')
d2 = X_val.nunique()
print(sorted(d2))

# Considering columns with <= 15 unique values for conversion"
224,27654742,20,"d = pd.concat(objs=[X_train, X_val], axis=0)"
225,27654742,21,col = d.columns 
226,27654742,22,"# For Train data
l1 = []
for i in col:
    if X_train[i].nunique() <= 15:
        l1.append(i)
        
l1"
227,27654742,23,"# For Val data
l2 = []
for i in col:
    if X_val[i].nunique() <= 15:
        l2.append(i)
        
l2"
228,27654742,24,"# For Test data
l3 = []
for i in col:
    if X_test[i].nunique() <= 15:
        l3.append(i)
        
l3"
229,27654742,25,"# Checking for columns in X_train and X_validation
set(l1) & set(l2)"
230,27654742,26,"# Checking for columns in X_train and X_test
set(l1) & set(l3)"
231,27654742,27,"print('Train', len(l1))
print('Validation', len(l2))
print('Common', len(set(l1) & set(l2)))"
232,27654742,28,"print('Train', len(l1))
print('Test', len(l3))
print('Common', len(set(l1) & set(l3)))"
233,27654742,29,X_train[l1].dtypes
234,27654742,30,"X_val[l2].dtypes
# Not a necessary step since we already confirmed the common columns. Included just for reference. "
235,27654742,31,"X_train[l1] = pd.Categorical(X_train[l1])
X_val[l2] = pd.Categorical(X_val[l2])
X_test[l3] = pd.Categorical(X_test[l3])
print('Train dtypes:')
print(X_train[l1].dtypes)
print('======================================')
print('Validation dtypes:')
print(X_val[l2].dtypes)
print('======================================')
print('Test dtypes:')
print(X_test[l3].dtypes)"
236,27654742,32,"# On train data
pd.set_option('display.max_rows', 500)
NA_col = pd.DataFrame(X_train.isna().sum(), columns = ['NA_Count'])
NA_col['% of NA'] = (NA_col.NA_Count/len(X_train))*100
NA_col.sort_values(by = ['% of NA'], ascending = False, na_position = 'first')"
237,27654742,33,"# On val data
pd.set_option('display.max_rows', 500)
NA_col = pd.DataFrame(X_val.isna().sum(), columns = ['NA_Count'])
NA_col['% of NA'] = (NA_col.NA_Count/len(X_val))*100
NA_col.sort_values(by = ['% of NA'], ascending = False, na_position = 'first')"
238,27654742,34,"# On test data
pd.set_option('display.max_rows', 500)
NA_col = pd.DataFrame(X_test.isna().sum(), columns = ['NA_Count'])
NA_col['% of NA'] = (NA_col.NA_Count/len(X_test))*100
NA_col.sort_values(by = ['% of NA'], ascending = False, na_position = 'first')"
239,27654742,35,"cols = X_train.columns
num_cols = X_train._get_numeric_data().columns
cat_cols = list(set(cols) - set(num_cols))
cat_cols"
240,27654742,36,"# Courtesy: https://www.kaggle.com/jayjay75/wids2020-lgb-starter-script
from sklearn.preprocessing import LabelEncoder,OneHotEncoder
for usecol in cat_cols:
    X_train[usecol] = X_train[usecol].astype('str')
    X_val[usecol] = X_val[usecol].astype('str')
    X_test[usecol] = X_test[usecol].astype('str')
    
    #Fit LabelEncoder
    le = LabelEncoder().fit(
            np.unique(X_train[usecol].unique().tolist()+
                      X_val[usecol].unique().tolist()+
                     X_test[usecol].unique().tolist()))

    #At the end 0 will be used for dropped values
    X_train[usecol] = le.transform(X_train[usecol])+1
    X_val[usecol]  = le.transform(X_val[usecol])+1
    X_test[usecol]  = le.transform(X_test[usecol])+1
    
    X_train[usecol] = X_train[usecol].replace(np.nan, 0).astype('int').astype('category')
    X_val[usecol]  = X_val[usecol].replace(np.nan, 0).astype('int').astype('category')
    X_test[usecol]  = X_test[usecol].replace(np.nan, 0).astype('int').astype('category')"
241,27654742,37,"X_train.set_index('encounter_id', inplace = True)
y_train.set_index('encounter_id', inplace = True)
X_val.set_index('encounter_id', inplace = True)
y_val.set_index('encounter_id', inplace = True)
X_test.set_index('encounter_id', inplace = True)
y_test.set_index('encounter_id', inplace = True)"
242,27654742,38,# y_test.hospital_death = y_test.hospital_death.fillna(0)
243,27654742,39,"# y_train['hospital_death'] = pd.Categorical(y_train['hospital_death'])
# y_train.dtypes"
244,27654742,40,"# y_test['hospital_death'] = pd.Categorical(y_test['hospital_death'])
# y_test.dtypes"
245,27654742,41,"# from sklearn.preprocessing import LabelEncoder,OneHotEncoder
# from pandas import Series
# l=LabelEncoder() 
# l.fit(y_train['hospital_death']) 
# l.classes_ 
# y_train['hospital_death']=Series(l.transform(y_train['hospital_death']))  #label encoding our target variable 
# y_train['hospital_death'].value_counts() "
246,27654742,42,"# l.fit(y_test['hospital_death']) 
# l.classes_ 
#y_test['hospital_death'].fillna(0.0, inplace = True)
# y_test['hospital_death']=Series(l.transform(y_test['hospital_death']))  #label encoding our target variable 
# y_test['hospital_death'].value_counts() "
247,27654742,43,"import lightgbm as lgbm

lgbm_train = lgbm.Dataset(X_train, y_train, categorical_feature=cat_cols)
# lgbm_test = lgbm.Dataset(X_test, y_test, categorical_feature=cat_cols)
lgbm_val = lgbm.Dataset(X_val, y_val, reference = lgbm_train)"
248,27654742,44,"params = {'feature_fraction': 0.9,
          'lambda_l1': 1,
          'lambda_l2': 1,
          'learning_rate': 0.01,
          'max_depth': 10,
          'metric': 'auc',
          'num_leaves': 500,
          'min_data_in_leaf': 100,
          'subsample_freq': 1,
          'scale_pos_weight':1,
          'metric': 'auc',
          'is_unbalance': 'true',
          'boosting': 'gbdt',
          'bagging_fraction': 0.5,
          'bagging_freq': 10,}"
249,27654742,45,"evals_result = {}  # to record eval results for plotting
model_lgbm = lgbm.train(params,
                lgbm_train,
                num_boost_round=100,
                valid_sets=[lgbm_train, lgbm_val],
                feature_name=['f' + str(i + 1) for i in range(X_train.shape[-1])],
                categorical_feature= [182],
                evals_result=evals_result,
                verbose_eval=10)"
250,27654742,46,"ax = lgbm.plot_metric(evals_result, metric='auc', figsize=(15, 8))
plt.show()"
251,27654742,47,"test[""hospital_death""] = model_lgbm.predict(X_test, predition_type = 'Probability')
test[[""encounter_id"",""hospital_death""]].to_csv(""submission_lgbm1.csv"",index=False)"
252,27654742,48,"test[[""encounter_id"",""hospital_death""]].head()"
253,29099590,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the ""../input/"" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# Any results you write to the current directory are saved as output.

import lightgbm as lgb
from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold
from sklearn.metrics import roc_auc_score

from tqdm import tqdm_notebook
import gc"
254,29099590,1,"train = pd.read_csv('/kaggle/input/widsdatathon2020/training_v2.csv')
test = pd.read_csv('/kaggle/input/widsdatathon2020/unlabeled.csv')
sample_submission = pd.read_csv('/kaggle/input/widsdatathon2020/samplesubmission.csv')
solution_template = pd.read_csv('/kaggle/input/widsdatathon2020/solution_template.csv')
train.shape, test.shape"
255,29099590,2,train.sample(5)
256,29099590,3,test.head(5)
257,29099590,4,"def make_submit(y_pred, filename='submission.csv'):
    solution_template['hospital_death'] = y_pred
    solution_template.to_csv(f'{filename}', index=False)
    print('solution file created. Commit notebook and submit file...')
    solution_template['hospital_death'].hist()
    "
258,29099590,5,"# LightGBM GBDT with KFold or Stratified KFold

def kfold_lightgbm(train, test, target_col, params, cols_to_drop=None, cat_features=None, num_folds=5, stratified = False, 
                   debug= False):
    
    print(""Starting LightGBM. Train shape: {}, test shape: {}"".format(train.shape, test.shape))


    
    # Cross validation model
    if stratified:
        folds = StratifiedKFold(n_splits= num_folds, shuffle=True, random_state=1)
    else:
        folds = KFold(n_splits= num_folds, shuffle=True, random_state=1)

    # Create arrays and dataframes to store results
    oof_preds = np.zeros(train.shape[0])
    sub_preds = np.zeros(test.shape[0])
    feature_importance_df = pd.DataFrame()
    if cols_to_drop == None:
        feats = [f for f in train.columns if f not in [target_col]]
    else:
        feats = [f for f in train.columns if f not in cols_to_drop+[target_col]]

    # k-fold
    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train[feats], train[target_col])):
        train_x, train_y = train[feats].iloc[train_idx], train[target_col].iloc[train_idx]
        valid_x, valid_y = train[feats].iloc[valid_idx], train[target_col].iloc[valid_idx]

        # set data structure
        lgb_train = lgb.Dataset(train_x,
                                label=train_y,
                                categorical_feature=cat_features,
                                free_raw_data=False)
        lgb_test = lgb.Dataset(valid_x,
                               label=valid_y,
                               categorical_feature=cat_features,
                               free_raw_data=False)

        # params after optimization
        reg = lgb.train(
                        params,
                        lgb_train,
                        valid_sets=[lgb_train, lgb_test],
                        valid_names=['train', 'test'],
#                         num_boost_round=10000,
#                         early_stopping_rounds= 200,
                        verbose_eval=False
                        )

        roc_auc = []
        oof_preds[valid_idx] = reg.predict(valid_x, num_iteration=reg.best_iteration)
        sub_preds += reg.predict(test[feats], num_iteration=reg.best_iteration) / folds.n_splits

        fold_importance_df = pd.DataFrame()
        fold_importance_df[""feature""] = feats
        fold_importance_df[""importance""] = np.log1p(reg.feature_importance(importance_type='gain', 
                                                                           iteration=reg.best_iteration))
        fold_importance_df[""fold""] = n_fold + 1
        
        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)
        print('Fold %2d ROC-AUC : %.6f' % (n_fold + 1, roc_auc_score(valid_y, oof_preds[valid_idx])))
        roc_auc.append(roc_auc_score(valid_y, oof_preds[valid_idx]))
        del reg, train_x, train_y, valid_x, valid_y
        gc.collect()
        
    print('Mean ROC-AUC : %.6f' % (np.mean(roc_auc)))
    return sub_preds"
259,29099590,6,cat_features = [x for x in train.columns if train[x].dtype == 'object' ]
260,29099590,7,"from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()

#   
for col in tqdm_notebook(cat_features):
    train[col] = train[col].astype('str')
    train[col] = le.fit_transform(train[col])
    
for col in tqdm_notebook(cat_features):
    test[col] = test[col].astype('str')
    test[col] = le.fit_transform(test[col])"
261,29099590,8,"params ={
    'objective': 'binary',
    'metric': 'roc_auc',
    'categorical_features': cat_features
                }
params_best = {
    'bagging_fraction': 0.15760757965010433, 
    'feature_fraction': 0.11161740354830015, 
    'learning_rate': 0.03, 
    'max_depth': 50, 
    'min_child_weight': 0.008857217513412136, 
    'min_data_in_leaf': 20, 
    'n_estimators': 815, 
    'num_leaves': 96, 
    'reg_alpha': 1.5292311993088907, 
    'reg_lambda': 1.903834634991243}"
262,29099590,9,"submit_best_params = kfold_lightgbm(train, test.drop('hospital_death', axis=1), cat_features=cat_features, 
                                 target_col='hospital_death', params=params_best)"
263,29099590,10,"make_submit(submit_best_params, 'submission.csv')"
264,29099590,11,"baseline_submit = kfold_lightgbm(train, test.drop('hospital_death', axis=1), cat_features=cat_features, 
                                 target_col='hospital_death', params=params)"
265,29099590,12,"def bayes_auc_lgb(
    n_estimators,
    learning_rate,
    num_leaves, 
    bagging_fraction,
    feature_fraction,
    min_child_weight, 
    min_data_in_leaf,
    max_depth,
    reg_alpha,
    reg_lambda):
    
    """"""
            
    """"""
    
    #   LightGBM        . 
    n_estimators = int(n_estimators)
    num_leaves = int(num_leaves)
    min_data_in_leaf = int(min_data_in_leaf)
    max_depth = int(max_depth)
    
    assert type(n_estimators) == int
    assert type(num_leaves) == int
    assert type(min_data_in_leaf) == int
    assert type(max_depth) == int
    
    params = {
              'n_estimators': n_estimators,
              'num_leaves': num_leaves, 
              'min_data_in_leaf': min_data_in_leaf,
              'min_child_weight': min_child_weight,
              'bagging_fraction' : bagging_fraction,
              'feature_fraction' : feature_fraction,
              'learning_rate' : learning_rate,
              'max_depth': max_depth,
              'reg_alpha': reg_alpha,
              'reg_lambda': reg_lambda,
              'objective': 'binary',
              'save_binary': True,
              'seed': 1337,
              'feature_fraction_seed': 1337,
              'bagging_seed': 1337,
              'drop_seed': 1337,
              'data_random_seed': 1337,
              'boosting_type': 'gbdt',
              'verbose': 1,
              'is_unbalance': False,
              'boost_from_average': True,
              'metric':'f1'}

    
    # -
    folds = StratifiedKFold(n_splits= 5, shuffle=True, random_state=1)


    #    
    oof_preds = np.zeros(df.shape[0])

    feats = [f for f in df.columns if f not in ['hospital_death']]

    # k-fold
    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(df[feats], df['hospital_death'])):
        train_x, train_y = df[feats].iloc[train_idx], df['hospital_death'].iloc[train_idx]
        valid_x, valid_y = df[feats].iloc[valid_idx], df['hospital_death'].iloc[valid_idx]

        #   
        lgb_train = lgbm.Dataset(train_x,
                                label=train_y,
                                categorical_feature=cat_f,
                                free_raw_data=False)
        lgb_test = lgbm.Dataset(valid_x,
                               label=valid_y,
                               categorical_feature=cat_f,
                               free_raw_data=False)

        # 
        clf = lgbm.train(
                        params,
                        lgb_train,
                        valid_sets=[lgb_train, lgb_test],
                        verbose_eval=False
                        )

        auc = []
        oof_preds[valid_idx] = clf.predict(valid_x, num_iteration=clf.best_iteration)



        auc.append(roc_auc_score(valid_y, oof_preds[valid_idx]))

    return np.mean(auc)"
266,29099590,13,"#       
bounds_lgb = {
    'n_estimators': (10, 1000),
    'num_leaves': (10, 500), 
    'min_data_in_leaf': (20, 200),
    'bagging_fraction' : (0.1, 0.9),
    'feature_fraction' : (0.1, 0.9),
    'learning_rate': (0.01, 0.3),
    'min_child_weight': (0.00001, 0.01),   
    'reg_alpha': (1, 2), 
    'reg_lambda': (1, 2),
    'max_depth':(-1,50),
}"
267,29099590,14,"def bayes_lgb(score_func, bound_lgb, init_points: int = 10, n_iter:int = 100):
    """"""
            . 
    :param score_func:   .  .
    :param bounds_lgb:      ().  
    :param n_iter:  
    :return:     
    """"""
    #  
    lgb_bo = BayesianOptimization(score_func, bounds_lgb, verbose=0)
    
    # 
    lgb_bo.maximize(init_points=init_points, n_iter=n_iter, xi=0.0, alpha=1e-6)
    
    print(""  : "",lgb_bo.max['target'])
    print("" : "", lgb_bo.max['params'])
    
    return lgb_bo.max['params']"
268,29099590,15,from bayes_opt import BayesianOptimization
269,29099590,16,cat_features.remove('icu_stay_type')
270,29099590,17,"df = train.copy().drop(['encounter_id', 'patient_id', 'hospital_id', 'icu_stay_type', 'icu_id'], axis=1)
cat_f=cat_features

bo_best_params = bayes_lgb(bayes_auc_lgb, bounds_lgb)"
271,28312981,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the ""../input/"" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# Any results you write to the current directory are saved as output."
272,28312981,1,"# Data
train = pd.read_csv(""/kaggle/input/widsdatathon2020/training_v2.csv"")
test = pd.read_csv(""/kaggle/input/widsdatathon2020/unlabeled.csv"")
# Dictionary
dictionary = pd.read_csv(""/kaggle/input/widsdatathon2020/WiDS Datathon 2020 Dictionary.csv"")
# Samples
samplesubmission = pd.read_csv(""/kaggle/input/widsdatathon2020/samplesubmission.csv"")
solution_template = pd.read_csv(""/kaggle/input/widsdatathon2020/solution_template.csv"")"
273,28312981,2,dictionary
274,28312981,3,dictionary.Category.unique()
275,28312981,4,#dictionary[dictionary['Data Type'] == 'numeric']
276,28312981,5,train
277,28312981,6,test
278,28312981,7,train.hospital_death.describe()
279,28312981,8,"print('unlabeled data: {}\ntraining data:  {}'.format(test.shape, train.shape))"
280,28312981,9,"from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score"
281,28312981,10,"train.fillna(method='ffill', inplace=True)
test.fillna(method='ffill', inplace=True)"
282,28312981,11,"# Train
X_train = train[['age','weight']].values
y_train = train['hospital_death'].values
# Test
X_test = test[['age','weight']].values"
283,28312981,12,"model = LogisticRegression()
model.fit(X_train, y_train)"
284,28312981,13,y_predict = model.predict_proba(X_test)
285,28312981,14,"test['hospital_death'] = y_predict[:,0]"
286,28312981,15,test
287,28312981,16,"test[[""encounter_id"",""hospital_death""]].to_csv(""submission.csv"",index=False)"
288,28312981,17,"test[[""encounter_id"",""hospital_death""]]"
289,82247998,0,"#Import Packages

import numpy as np
import pandas as pd
import os
import matplotlib.pyplot as plt

from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity='all' # shows outputs of all commands executed in 1 cell"
290,82247998,1,"# Input data files are available in the ""../input/"" directory.

# List all files under the input directory

input_path = '/kaggle/input/widsdatathon2020'

for dirpath, dirname, filenames in os.walk(input_path):
    for name in filenames:
        print (os.path.join(dirpath , name))
        
# Any results you write to the current directory are saved as output."
291,82247998,2,"# read file
fname = 'training_v2.csv'
train_df = pd.read_csv(os.path.join(input_path , fname))

fname = 'unlabeled.csv'
test_df = pd.read_csv(os.path.join(input_path , fname))

fname = 'solution_template.csv'
solution_df = pd.read_csv(os.path.join(input_path , fname))

"
292,82247998,3,"print('solution_df')
solution_df.head() 
solution_df.info()
solution_df.shape"
293,82247998,4,solution_df['encounter_id'].describe()
294,82247998,5,"print('test_df')
test_df.head()
test_df.info()
test_df.shape
test_df['encounter_id'].describe()"
295,82247998,6,"print('train_df')
train_df.head() 
train_df.info()
train_df.shape"
296,82247998,7,"train_df['hospital_death'].dtype
test_df['hospital_death'].dtype
"
297,82247998,8,"def display_columns_properties(df):
    for i, col in enumerate(df.columns.tolist()):
         print('\n ({} {})  Missing: {}  UniqValsSz: {}'.format(i,col, df[col].isnull().sum() ,df[col].unique().size))
    print('\n')"
298,82247998,9,display_columns_properties(train_df)
299,82247998,10,display_columns_properties(test_df)
300,82247998,11,"cat_train_df = train_df.select_dtypes(include='object')
cat_train_df.head()
cat_train_df.info()"
301,82247998,12,"cat_test_df = test_df.select_dtypes(include='object')
cat_test_df.head()
cat_test_df.info()"
302,82247998,13,"def display_columns_uniqvals(df):
    for i, col in enumerate(df.columns.tolist()):
         print('\n ({} {}) Uniq: {}'.format(i,col, df[col].unique() ))
    print('\n')"
303,82247998,14,display_columns_uniqvals(cat_test_df)
304,82247998,15,"from sklearn.model_selection import train_test_split

# copy the data
train = train_df.copy()

# Select target
y = train['hospital_death']


# To keep things simple, we'll use only numerical predictors
predictors = train.drop(['hospital_death'], axis=1)
X = predictors.select_dtypes(exclude=['object'])



# Divide data into training and validation subsets
X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,
                                                      random_state=0)

X_train.shape
X_valid.shape
"
305,82247998,16,"from sklearn.impute import SimpleImputer

# Imputation
my_imputer = SimpleImputer()
imputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))
imputed_X_valid = pd.DataFrame(my_imputer.transform(X_valid))

# Imputation removed column names; put them back
imputed_X_train.columns = X_train.columns
imputed_X_valid.columns = X_valid.columns
"
306,82247998,17,"
display_columns_properties(imputed_X_train)"
307,82247998,18,"
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_absolute_error


# Define model. Specify a number for random_state to ensure same results each run.
dt_model = DecisionTreeRegressor(random_state=1)

# Fit model using Traing data
dt_model.fit(imputed_X_train, y_train)

# get predicted prices on validation data
predicted_values = dt_model.predict(imputed_X_valid)

# Find difference
score = mean_absolute_error(y_valid, predicted_values)
print('MAE:', score)"
308,82247998,19,"
test = test_df.copy()

#Separate target
y_test = test['hospital_death']

# To keep things simple, we'll use only numerical predictors
predictors_test = test.drop(['hospital_death'], axis=1)
X_test = predictors_test.select_dtypes(exclude=['object'])



X_test.shape
X_test.head()"
309,82247998,20,"# Imputation
my_imputer = SimpleImputer()
imputed_X_test = pd.DataFrame(my_imputer.fit_transform(X_test))


# Imputation removed column names; put them back
imputed_X_test.columns = X_test.columns"
310,82247998,21,imputed_X_test.head()
311,82247998,22,"
# get predictions on test data
preds = dt_model.predict(imputed_X_test)

# Save predictions in format used for competition scoring
output = pd.DataFrame({'encounter_id': imputed_X_test.encounter_id,
                       'hospital_death': preds},dtype=np.int32)
 
output.to_csv('submission.csv', index=False)
"
312,82247998,23,output.columns.dtype
313,82247998,24,"### Conclusion
# Used Decision tree model, simple imputation and only numerical columns.
# Random forest Training is taking too long and not getting complete."
314,27625781,0,"#codes from Rodrigo Lima  @rodrigolima82
from IPython.display import Image
Image(url = 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcS0WNpRevX2b8A237rMQ2VQaXkQSv20nnmGW2lOFJFlwjI43aGO2w&s',width=400,height=400)"
315,27625781,1,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go

# Input data files are available in the ""../input/"" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# Any results you write to the current directory are saved as output."
316,27625781,2,"wids = pd.read_csv(""../input/widsdatathon2020/training_v2.csv"")
test = pd.read_csv(""../input/widsdatathon2020/unlabeled.csv"")
sub = pd.read_csv('../input/widsdatathon2020/samplesubmission.csv')"
317,27625781,3,"#codes from Rodrigo Lima  @rodrigolima82
from IPython.display import Image
Image(url = 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSQ44CM9UB1kKjsFpFGU-DTRVmlvGugTz3wPRllnbucMXplydui6w&s',width=400,height=400)"
318,27625781,4,wids.head()
319,27625781,5,wids.dtypes
320,27625781,6,"#codes from Rodrigo Lima  @rodrigolima82
from IPython.display import Image
Image(url = 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTJE4rPsgdkEJgNcZCknrweDTpASeH3LggoTzdqsZd8iUDTaRz_&s',width=400,height=400)"
321,27625781,7,wids.describe()
322,27625781,8,"print(""The number of nulls in each column are \n"", wids.isna().sum())"
323,27625781,9,"sns.countplot(wids[""hospital_death""])
plt.xticks(rotation=45)
plt.yticks(rotation=45)
plt.show()"
324,27625781,10,"#codes from Rodrigo Lima  @rodrigolima82
from IPython.display import Image
Image(url = 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcT1XBLX-CmEdjlURIK4ovG5Aflo8QKS_hnycP5kDwlw50C5QRWx&s',width=400,height=400)"
325,27625781,11,"sns.distplot(wids[""hospital_death""])"
326,27625781,12,"sns.scatterplot(x='age',y='hospital_death',data=wids)"
327,27625781,13,"print (""Skew is:"", wids.hospital_death.skew())
plt.hist(wids.hospital_death, color='pink')
plt.show()"
328,27625781,14,"#codes from Rodrigo Lima  @rodrigolima82
from IPython.display import Image
Image(url = 'https://i.pinimg.com/236x/ac/fb/8b/acfb8b6026740e5c50063307c468f12f.jpg',width=400,height=400)"
329,27625781,15,"# Necessary Functions: 
def pie_plot(labels, values, colors, title):
    fig = {
      ""data"": [
        {
          ""values"": values,
          ""labels"": labels,
          ""domain"": {""x"": [0, .48]},
          ""name"": ""Job Type"",
          ""sort"": False,
          ""marker"": {'colors': colors},
          ""textinfo"":""percent+label+value"",
          ""textfont"": {'color': '#FFFFFF', 'size': 10},
          ""hole"": .6,
          ""type"": ""pie""
        } ],
        ""layout"": {
            ""title"":title,
            ""annotations"": [
                {
                    ""font"": {
                        ""size"": 25,

                    },
                    ""showarrow"": False,
                    ""text"": """"

                }
            ]
        }
    }
    return fig"
330,27625781,16,"sns.boxplot(x=""hospital_death"", y=""patient_id"", data=wids)
"
331,27625781,17,"#codes from Rodrigo Lima  @rodrigolima82
from IPython.display import Image
Image(url = 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcS8TMbfKeH67PQNL0ghQrPrwG75wjca1M7g38Bi9Jbj1T3dsh0F&s',width=400,height=400)"
332,27625781,18,"#codes from PSVishnu @psvishnu
hospital = [
    'patient_id','hospital_id','hospital_death','encounter_id']"
333,27625781,19,"#codes from Rodrigo Lima  @rodrigolima82
from IPython.display import Image
Image(url = 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTxDDQDIfj58v2qLXz1NnNM9fqYg1xdM3cGdJ5d3ktKlMvRpALqqw&s',width=400,height=400)"
334,27625781,20,"sns.pairplot(data=wids,diag_kind='kde',vars=hospital,hue='hospital_death')
plt.show()"
335,27625781,21,"import plotly.offline as py
value_counts = wids['hospital_id'].value_counts()
labels = value_counts.index.tolist()
py.iplot(pie_plot(labels, value_counts,['#1B9E77', '#7570B3'], ""Hospital Id""))"
336,27625781,22,"#codes from Rodrigo Lima  @rodrigolima82
from IPython.display import Image
Image(url = 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSVbT0PMefnpoA7dQYRZKmGvxVO3kXqb6MVCgTcy-guauA-xBX2Cw&s',width=400,height=400)"
337,27625781,23,"from collections import Counter
import json
from IPython.display import HTML
import altair as alt
from  altair.vega import v5"
338,27625781,24,"#codes from Rodrigo Lima  @rodrigolima82
from IPython.display import Image
Image(url = 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcR-TVKsdRr0lRfeZHuaMMrzA4g7qN1pOsU-hnd3MoedAtQKTC3T&s',width=400,height=400)"
339,27625781,25,"
##-----------------------------------------------------------
# This whole section 
vega_url = 'https://cdn.jsdelivr.net/npm/vega@' + v5.SCHEMA_VERSION
vega_lib_url = 'https://cdn.jsdelivr.net/npm/vega-lib'
vega_lite_url = 'https://cdn.jsdelivr.net/npm/vega-lite@' + alt.SCHEMA_VERSION
vega_embed_url = 'https://cdn.jsdelivr.net/npm/vega-embed@3'
noext = ""?noext""

paths = {
    'vega': vega_url + noext,
    'vega-lib': vega_lib_url + noext,
    'vega-lite': vega_lite_url + noext,
    'vega-embed': vega_embed_url + noext
}

workaround = """"""
requirejs.config({{
    baseUrl: 'https://cdn.jsdelivr.net/npm/',
    paths: {}
}});
""""""

#------------------------------------------------ Defs for future rendering
def add_autoincrement(render_func):
    # Keep track of unique <div/> IDs
    cache = {}
    def wrapped(chart, id=""vega-chart"", autoincrement=True):
        if autoincrement:
            if id in cache:
                counter = 1 + cache[id]
                cache[id] = counter
            else:
                cache[id] = 0
            actual_id = id if cache[id] == 0 else id + '-' + str(cache[id])
        else:
            if id not in cache:
                cache[id] = 0
            actual_id = id
        return render_func(chart, id=actual_id)
    # Cache will stay outside and 
    return wrapped

@add_autoincrement
def render(chart, id=""vega-chart""):
    chart_str = """"""
    <div id=""{id}""></div><script>
    require([""vega-embed""], function(vg_embed) {{
        const spec = {chart};     
        vg_embed(""#{id}"", spec, {{defaultStyle: true}}).catch(console.warn);
        console.log(""anything?"");
    }});
    console.log(""really...anything?"");
    </script>
    """"""
    return HTML(
        chart_str.format(
            id=id,
            chart=json.dumps(chart) if isinstance(chart, dict) else chart.to_json(indent=None)
        )
    )



HTML("""".join((
    ""<script>"",
    workaround.format(json.dumps(paths)),
    ""</script>"")))"
340,27625781,26,"def word_cloud(df, pixwidth=6000, pixheight=350, column=""index"", counts=""count""):
    data= [dict(name=""dataset"", values=df.to_dict(orient=""records""))]
    wordcloud = {
        ""$schema"": ""https://vega.github.io/schema/vega/v5.json"",
        ""width"": pixwidth,
        ""height"": pixheight,
        ""padding"": 0,
        ""title"": ""Hospital - Women in Data Science 2020"",
        ""data"": data
    }
    scale = dict(
        name=""color"",
        type=""ordinal"",
        range=[""cadetblue"", ""royalblue"", ""steelblue"", ""navy"", ""teal""]
    )
    mark = {
        ""type"":""text"",
        ""from"":dict(data=""dataset""),
        ""encode"":dict(
            enter=dict(
                text=dict(field=column),
                align=dict(value=""center""),  
                baseline=dict(value=""alphabetic""),
                fill=dict(scale=""color"", field=column),
                tooltip=dict(signal=""datum.count + ' occurrances'"")
            )
        ),
            ""transform"": [{
            ""type"": ""wordcloud"",
            ""text"": dict(field=column),
            ""size"": [pixwidth, pixheight],
            ""font"": ""Helvetica Neue, Arial"",
            ""fontSize"": dict(field=""datum.{}"".format(counts)),
            ""fontSizeRange"": [10, 60],
            ""padding"": 2
        }]
    }
    wordcloud[""scales""] = [scale]
    wordcloud[""marks""] = [mark]
    
    return wordcloud

from collections import defaultdict

def wordcloud_create(wids):
    ult = {}
    corpus = wids.icu_type.values.tolist()
    final = defaultdict(int) #Declaring an empty dictionary for count (Saves ram usage)
    for words in corpus:
        for word in words.split():
             final[word]+=1
    temp = Counter(final)
    for k, v in  temp.most_common(200):
        ult[k] = v
    corpus = pd.Series(ult) #Creating a dataframe from the final default dict
    return render(word_cloud(corpus.to_frame(name=""count"").reset_index(), pixheight=600, pixwidth=900))"
341,27625781,27,wordcloud_create(wids)
342,27625781,28,"#codes from Rodrigo Lima  @rodrigolima82
from IPython.display import Image
Image(url = 'https://www.kdnuggets.com/wp-content/uploads/career-progression.jpg',width=400,height=400)"
343,27625781,29,"#word cloud
from wordcloud import WordCloud, ImageColorGenerator
text = "" "".join(str(each) for each in wids.apache_2_bodysystem)
# Create and generate a word cloud image:
wordcloud = WordCloud(max_words=200, background_color=""black"").generate(text)
plt.figure(figsize=(10,6))
plt.figure(figsize=(15,10))
# Display the generated image:
plt.imshow(wordcloud, interpolation='Bilinear')
plt.axis(""off"")
plt.show()"
344,27625781,30,"#codes from Rodrigo Lima  @rodrigolima82
from IPython.display import Image
Image(url = 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQhs0JIJjvon1mlAC_QGIIIZDTwGUPY5ZByFTdFaSE9f3l2RC3L2g&s',width=400,height=400)"
345,29148454,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.api as sm

import warnings
warnings.simplefilter(action = 'ignore')

# Standardization
from sklearn import preprocessing
from sklearn.preprocessing import StandardScaler

# Train-Test split
from sklearn.model_selection import train_test_split 

# Importing classification report and confusion matrix from sklearn metrics
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# Importing the PCA module
from sklearn.decomposition import PCA

# Importing random forest classifier from sklearn library
from sklearn.ensemble import RandomForestClassifier

# Importing Ridge, Lasso and GridSearch
from sklearn.linear_model import Ridge
from sklearn.linear_model import Lasso
from sklearn.model_selection import GridSearchCV

# Importing XGBoost libraries
from sklearn.ensemble import AdaBoostClassifier

import gc # for deleting unused variables

# Importing the below library and configuring to display all columns in a dataframe
from IPython.display import display
pd.options.display.max_columns = None

# Input data files are available in the ""../input/"" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# Any results you write to the current directory are saved as output.
"
346,29148454,1,"training_df = pd.read_csv('/kaggle/input/widsdatathon2020/training_v2.csv')
training_df.head()"
347,29148454,2,"test_df = pd.read_csv('/kaggle/input/widsdatathon2020/unlabeled.csv')
df_unlabel = test_df.copy()
test_df.head()"
348,29148454,3,"print(training_df.shape)
print(test_df.shape)"
349,29148454,4,train_len = len(training_df)
350,29148454,5,"training_df = pd.concat(objs = [training_df,test_df], axis = 0)
training_df.shape"
351,29148454,6,training_df.info(verbose = True)
352,29148454,7,training_df.isnull().sum()
353,29148454,8,"# percentage of missing values in columns greater than 50%
null_cols = training_df.columns[round(training_df.isnull().sum()/len(training_df.index)*100,2) > 50].tolist()
null_cols"
354,29148454,9,"# deleting cols having missing %age greater than 50%
print(training_df.shape)
training_df = training_df.drop(null_cols,axis = 1)
print(training_df.shape)"
355,29148454,10,"# numeric columns
df_numeric = training_df.select_dtypes(include=['int16', 'int32', 'int64', 'float16', 'float32', 'float64'])
df_numeric.head() "
356,29148454,11,"# missing values in columns
num_null_cols = df_numeric.columns[df_numeric.isnull().any()].tolist()
print(num_null_cols)
print(len(num_null_cols))"
357,29148454,12,"# dividing the null columns in 5 part 
num_null_cols_1 = num_null_cols[:19]
num_null_cols_2 = num_null_cols[19:38]
num_null_cols_3 = num_null_cols[38:57]
num_null_cols_4 = num_null_cols[57:76]
num_null_cols_5 = num_null_cols[76:]"
358,29148454,13,"# Visualizing first part of num_null_cols which is num_null_cols_1 using box_plot
plt.figure(figsize=(15,15))
sns.boxplot(x=""value"", y=""variable"", data=pd.melt(df_numeric[num_null_cols_1]))
plt.show()"
359,29148454,14,"# Visualizing first part of num_null_cols which is num_null_cols_2 using box_plot
plt.figure(figsize=(15,15))
sns.boxplot(x=""value"", y=""variable"", data=pd.melt(df_numeric[num_null_cols_2]))
plt.show()"
360,29148454,15,"# Visualizing first part of num_null_cols which is num_null_cols_3 using box_plot
plt.figure(figsize=(15,15))
sns.boxplot(x=""value"", y=""variable"", data=pd.melt(df_numeric[num_null_cols_3]))
plt.show()"
361,29148454,16,"# Visualizing first part of num_null_cols which is num_null_cols_4 using box_plot
plt.figure(figsize=(15,15))
sns.boxplot(x=""value"", y=""variable"", data=pd.melt(df_numeric[num_null_cols_4]))
plt.show()"
362,29148454,17,"# Visualizing first part of num_null_cols which is num_null_cols_5 using box_plot
plt.figure(figsize=(15,15))
sns.boxplot(x=""value"", y=""variable"", data=pd.melt(df_numeric[num_null_cols_5]))
plt.show()"
363,29148454,18,"# imputing missing values
df_numeric = df_numeric.fillna(df_numeric.median())
print(df_numeric.isnull().sum())"
364,29148454,19,"# non numeric columns
df_non_numeric = training_df.select_dtypes(exclude=['int16', 'int32', 'int64', 'float16', 'float32', 'float64'])
print(df_non_numeric.head())"
365,29148454,20,"# percentage of missing values
round(df_non_numeric.isnull().sum()/len(training_df.index)*100,2)"
366,29148454,21,"# Visualizing them using countplot
plt.figure(figsize=(10,15))
plt.subplot(4,1,1)
sns.countplot(y = 'ethnicity',data= df_non_numeric)
plt.subplot(4,1,2)
sns.countplot(y = 'gender',data= df_non_numeric)
plt.subplot(4,1,3)
sns.countplot(y = 'hospital_admit_source',data= df_non_numeric)
plt.subplot(4,1,4)
sns.countplot(y = 'icu_admit_source',data= df_non_numeric)
plt.show()"
367,29148454,22,"# Visualizing them using countplot
plt.figure(figsize=(10,15))
plt.subplot(4,1,1)
sns.countplot(y = 'icu_stay_type',data= df_non_numeric)
plt.subplot(4,1,2)
sns.countplot(y = 'icu_type',data= df_non_numeric)
plt.subplot(4,1,3)
sns.countplot(y = 'apache_3j_bodysystem',data= df_non_numeric)
plt.subplot(4,1,4)
sns.countplot(y = 'apache_2_bodysystem',data= df_non_numeric)
plt.show()"
368,29148454,23,"for column in df_non_numeric.columns:
    df_non_numeric[column].fillna(df_non_numeric[column].mode()[0], inplace = True)"
369,29148454,24,df_non_numeric.isnull().sum()
370,29148454,25,"# merging df_numeric and df_non_numeric on their index
df_train = pd.concat([df_numeric, df_non_numeric], axis=1)
df_train.head()"
371,29148454,26,df_train.shape
372,29148454,27,"# checking outliers
df_train.describe(percentiles=[.25,.5,.75,.90,.95,.99])"
373,29148454,28,"df_train['hospital_death'].value_counts().plot('bar')
plt.show()"
374,29148454,29,df_train['hospital_death'].sum()/len(df_train['hospital_death'].index)*100
375,29148454,30,"# get correlation of 'hospital_death' with other variables
plt.figure(figsize=(30,9))
df_train.corr()['hospital_death'].sort_values(ascending = False).plot('bar')
plt.show()"
376,29148454,31,df_train.head()
377,29148454,32,df_train.shape
378,29148454,33,"df_train = df_train.drop(['encounter_id','patient_id'], axis = 1)
df_train.columns"
379,29148454,34,"import copy
train = copy.copy(df_train[:train_len])
test = copy.copy(df_train[train_len:])
print('train             ', train.shape)
print('test              ', test.shape)"
380,29148454,35,"X = train.drop('hospital_death',1)
X.head()"
381,29148454,36,X.shape
382,29148454,37,"y = train['hospital_death']
y.head()"
383,29148454,38,"test = test.drop('hospital_death', axis = 1)
test.shape"
384,29148454,39,X.info(verbose=True)
385,29148454,40,X.shape
386,29148454,41,test.shape
387,29148454,42,X_len = len(X)
388,29148454,43,"combined_df = pd.concat(objs = [X,test], axis = 0)
combined_df.shape"
389,29148454,44,"combined_df[['elective_surgery','readmission_status','apache_post_operative','arf_apache','gcs_unable_apache','intubated_apache','ventilated_apache','aids','cirrhosis','diabetes_mellitus','hepatic_failure','immunosuppression','leukemia','lymphoma','solid_tumor_with_metastasis']] = combined_df[['elective_surgery','readmission_status','apache_post_operative','arf_apache','gcs_unable_apache','intubated_apache','ventilated_apache','aids','cirrhosis','diabetes_mellitus','hepatic_failure','immunosuppression','leukemia','lymphoma','solid_tumor_with_metastasis']].astype(object)"
390,29148454,45,"combined_df_categorical = combined_df[['elective_surgery','readmission_status','apache_post_operative','arf_apache','gcs_unable_apache',
                   'intubated_apache','ventilated_apache','aids','cirrhosis','diabetes_mellitus','hepatic_failure',
                   'immunosuppression','leukemia','lymphoma','solid_tumor_with_metastasis','ethnicity','gender','hospital_admit_source',
                   'icu_stay_type','icu_type','apache_3j_bodysystem','apache_2_bodysystem','icu_admit_source']]

# convert into dummies
combined_dummies = pd.get_dummies(combined_df_categorical, drop_first=True)

# drop cateorical variables from X dataframe
combined_df = combined_df.drop(combined_df_categorical, axis = 1)

# concat dummy variables with X dataframe
combined_df = pd.concat([combined_df, combined_dummies], axis = 1)
print(combined_df.shape)"
391,29148454,46,"import copy
X = copy.copy(combined_df[:X_len])
test = copy.copy(combined_df[X_len:])
print('X             ', X.shape)
print('test          ', test.shape)"
392,29148454,47,X.corr()
393,29148454,48,"# create correlation matrix
corr_matrix = X.corr().abs()

# select upper triangle of correlation matrix
upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape),k=1).astype(np.bool))

# find index of feature columns with correlation greater than 0.80
to_drop = [column for column in upper.columns if any((upper[column]>0.80))]
to_drop"
394,29148454,49,"X = X.drop(to_drop,1)
print(X.shape)"
395,29148454,50,"test = test.drop(to_drop ,1)
print(test.shape)"
396,29148454,51,"# plotting heat map to see correlation 
plt.figure(figsize=(15,10))
sns.heatmap(data = X.corr())
plt.show()"
397,29148454,52,X.info(verbose = True)
398,29148454,53,"# standardization of X
scaler = preprocessing.StandardScaler().fit(X)
X = scaler.transform(X)
X = pd.DataFrame(X)

# test
test = scaler.transform(test)
test = pd.DataFrame(test)"
399,29148454,54,test.shape
400,29148454,55,"# splitting the data into train and test
X_train, X_test, y_train, y_test = train_test_split(X,y, train_size = 0.7, test_size = 0.3, random_state = 100)"
401,29148454,56,"X_orig = X.copy()
y_orig = y.copy()"
402,29148454,57,"from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics

X = X_orig.copy()
y = y_orig.copy()

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=100)

model_rf = RandomForestClassifier()
model_rf.fit(X_train, y_train)

# Make predictions
prediction_test = model_rf.predict(X_test)
print(classification_report(y_test,prediction_test))"
403,29148454,58,"# Printing confusion matrix
print(confusion_matrix(y_test, prediction_test))"
404,29148454,59,"# Defining a genric function to calculate sensitivity
def sensitivity_score(inp_y_test, inp_y_pred):
    positives = confusion_matrix(inp_y_test,inp_y_pred)[1]
    # print('True Positives: ', positives[1], ' False Positives: ', positives[0])
    return (positives[1]/(positives[1] + positives[0]))"
405,29148454,60,"print ('Random Forest Accuracy with Default Hyperparameter', metrics.accuracy_score(y_test, prediction_test))
print ('Random Forest Sensitivity with Default Hyperparameter', sensitivity_score(y_test, prediction_test))"
406,29148454,61,"import lightgbm as lgb
from sklearn.model_selection import RandomizedSearchCV
clf = lgb.LGBMClassifier(silent=True, random_state = 304, metric='roc_auc', n_jobs=4)"
407,29148454,62,"from scipy.stats import randint as sp_randint
from scipy.stats import uniform as sp_uniform
params ={'cat_smooth' : sp_randint(1, 100), 'min_data_per_group': sp_randint(1,1000), 'max_cat_threshold': sp_randint(1,100)}"
408,29148454,63,"fit_params={""early_stopping_rounds"":2, 
            ""eval_metric"" : 'auc', 
            ""eval_set"" : [(X_train, y_train),(X_test,y_test)],
            'eval_names': ['train','valid'],
            'verbose': 300,
            'categorical_feature': 'auto'}"
409,29148454,64,"gs = RandomizedSearchCV( estimator=clf, param_distributions=params, scoring='roc_auc',cv=3, refit=True,random_state=304,verbose=True)"
410,29148454,65,"gs.fit(X_train, y_train, **fit_params)
print('Best score reached: {} with params: {} '.format(gs.best_score_, gs.best_params_))"
411,29148454,66,"gs.best_params_, gs.best_score_"
412,29148454,67,"# clf2 = lgb.LGBMClassifier(random_state=304, metric = 'roc_auc', n_jobs=4)
clf2 = lgb.LGBMClassifier(random_state=304, metric = 'roc_auc', cat_smooth = 32, max_cat_threshold = 75, min_data_per_group = 82, n_jobs=4)"
413,29148454,68,"params_2 = {'learning_rate': [0.04, 0.05, 0.08],   
            'num_iterations': [1400,1600, 1800]}"
414,29148454,69,"gs2 = GridSearchCV(clf2,params_2, scoring='roc_auc',cv=3)"
415,29148454,70,"gs2.fit(X_train, y_train, **fit_params)
print('Best score reached: {} with params: {} '.format(gs2.best_score_, gs2.best_params_))"
416,29148454,71,"gs2.best_params_, gs2.best_score_"
417,29148454,72,"params_2 = {
 'bagging_fraction': 0.4,
 'boosting': 'dart',
 'num_iterations': 1400, 
 'learning_rate': 0.04,
 'colsample_bytree': 0.5048747931447324,
 'cat_smooth': 32, 
 'max_cat_threshold':75, 
 'min_data_per_group': 82,
 'max_bin': 1312,
 'max_depth': 12,
 'num_leaves': 4090,
 'min_child_samples': 407,
 'min_child_weight': 0.1,
 'min_data_in_leaf': 2420,
 'reg_alpha': 0.1,
 'reg_lambda': 20,
 'scale_pos_weight': 3,
 'subsample': 0.7340872997512691,
 'subsample_for_bin': 512,
 'scoring': 'roc_auc',
 'metric': 'auc',
 'objective': 'binary'}"
418,29148454,73,"lgbm_train2 = lgb.Dataset(X_train, y_train)
lgbm_val2 = lgb.Dataset(X_test, y_test)"
419,29148454,74,"evals_result = {}  # to record eval results for plotting
model_lgbm_2 = lgb.train(params_2,
                lgbm_train2,
                num_boost_round=250,
                valid_sets=[lgbm_train2, lgbm_val2],
                feature_name=['f' + str(i + 1) for i in range(X_train.shape[-1])],
                categorical_feature= [182],
                evals_result=evals_result,
                verbose_eval=100)"
420,29148454,75,"ax = lgb.plot_metric(evals_result, metric='auc', figsize=(15, 8))
plt.show()"
421,29148454,76,"df_unlabel[""hospital_death""] = model_lgbm_2.predict(test, pred_contrib=False)"
422,29148454,77,df_unlabel.shape
423,29148454,78,df_unlabel.head()
424,29148454,79,"df_unlabel[['encounter_id','hospital_death']].to_csv('submission.csv',index = False)
df_unlabel[['encounter_id','hospital_death']].head()"
425,29148454,80,
426,28395914,0,"# Necessary libraries
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import LabelEncoder
import statsmodels.api as sm
from tqdm.notebook import tqdm
from scipy import stats"
427,28395914,1,"# Import dataframes
df_train = pd.read_csv(""/kaggle/input/widsdatathon2020/training_v2.csv"")
df_test = pd.read_csv(""/kaggle/input/widsdatathon2020/unlabeled.csv"")"
428,28395914,2,"# Convert categorical labels into numerical values
categorical_columns = ['gender', 'apache_2_bodysystem', 'ethnicity', 'apache_3j_bodysystem', 
    'icu_admit_source', 'icu_stay_type', 'apache_2_diagnosis', 'apache_3j_diagnosis', 'icu_type']

cat_labenc_mapping = {
    col: LabelEncoder()
    for col in categorical_columns
}

for col in tqdm(categorical_columns):
    df_train[col] = df_train[col].astype('str')
    cat_labenc_mapping[col] = cat_labenc_mapping[col].fit(
        np.unique(df_train[col].unique().tolist() + df_test[col].unique().tolist())
    )
    df_train[col] = cat_labenc_mapping[col].transform(df_train[col])
    
    df_test[col] = df_test[col].astype('str')
    df_test[col] = cat_labenc_mapping[col].transform(df_test[col])"
429,28395914,3,"predictives = ['height','diabetes_mellitus']
dependents = ['weight']

#Load in the data columns we need and drop NA rows
test = df_train[(predictives+dependents)].dropna()

#Add the intercept to the model
X2 = sm.add_constant(test[predictives])

#create regression object and fit it
estWeight = sm.OLS(test[dependents], X2).fit()
print(estWeight.summary())"
430,28395914,4,"cw = estWeight.params[0] # constant
h = estWeight.params[1] # height
db = estWeight.params[2] # diabetes mellitus

index = df_train['weight'].isna() & ~df_train['height'].isna() & ~df_train['diabetes_mellitus'].isna()
n = 0
for idx,row in df_train[index].iterrows():
    df_train.loc[idx,'weight'] = cw + df_train.loc[idx,'height'] * h + df_train.loc[idx,'diabetes_mellitus'] * db
    n+=1
print('Filled up '+str(n)+' weight values')"
431,28395914,5,"predictives = ['weight','gender','ethnicity']
dependents = ['height']

#Load in the data columns we need and drop NA rows
test = df_train[(predictives+dependents)].dropna()

#Add the intercept to the model
X2 = sm.add_constant(test[predictives])

#create regression object and fit it
estHeight = sm.OLS(test[dependents], X2).fit()
print(estHeight.summary())"
432,28395914,6,"ch = estHeight.params[0] # constant
w = estHeight.params[1] # weight
g = estHeight.params[2] # gender
e = estHeight.params[3] # ethnicity

index = df_train['height'].isna() & ~df_train['weight'].isna() & ~df_train['gender'].isna() & ~df_train['ethnicity'].isna()
n = 0
for idx,row in df_train[index].iterrows():
    df_train.loc[idx,'height'] = ch + df_train.loc[idx,'weight'] * w + df_train.loc[idx,'gender'] * g + df_train.loc[idx,'ethnicity'] * e
    n+=1
print('Filled up '+str(n)+' height values')"
433,28395914,7,"index = df_train['bmi'].isna()
for idx,row in df_train[index].iterrows():
    df_train.loc[idx,'bmi'] = df_train.loc[idx,'weight'] / (df_train.loc[idx,'height']/100)**2

print('Calculated '+str(len(df_train[index]))+' bmi values')"
434,28577122,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the ""../input/"" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# Any results you write to the current directory are saved as output.

import warnings
warnings.filterwarnings(""ignore"")

import seaborn as sns
import math
import matplotlib as p
import matplotlib.pyplot as plt
%matplotlib inline
import scipy.stats as sps
import re"
435,28577122,1,"train = pd.read_csv('../input/widsdatathon2020/training_v2.csv')
test = pd.read_csv('../input/widsdatathon2020/unlabeled.csv')
st = pd.read_csv('../input/widsdatathon2020/solution_template.csv')
ss = pd.read_csv('../input/widsdatathon2020/samplesubmission.csv')
dictionary = pd.read_csv('../input/widsdatathon2020/WiDS Datathon 2020 Dictionary.csv')

pd.set_option('display.max_columns', 500)
print('solution template shape', st.shape)
display(st.head())
print('dictionary shape', dictionary.shape)
display(dictionary.T.head())
print('train shape', train.shape)
display(train.head())
print('test shape', test.shape)
display(test.head())"
436,28577122,2,"# Dropping patient_id for now
train = train.copy().drop('patient_id', axis = 1)
test = test.copy().drop('patient_id', axis = 1)"
437,28577122,3,"from sklearn.model_selection import train_test_split

Train, Validation = train_test_split(train, test_size = 0.3)"
438,28577122,4,"X_train = Train.copy().drop('hospital_death', axis = 1)
y_train = Train[['encounter_id','hospital_death']]
X_val = Validation.copy().drop('hospital_death', axis = 1)
y_val = Validation[['encounter_id','hospital_death']]"
439,28577122,5,"X_test = test.copy().drop('hospital_death', axis = 1)
y_test = test[['encounter_id','hospital_death']]"
440,28577122,6,"sns.catplot('hospital_death', data= train, kind='count', alpha=0.7, height=6, aspect=1)

# Get current axis on current figure
ax = plt.gca()

# Max value to be set
y_max = train['hospital_death'].value_counts().max() 

# Iterate through the list of axes' patches
for p in ax.patches:
    ax.text(p.get_x() + p.get_width()/5., p.get_height(),'%d' % int(p.get_height()),
            fontsize=13, color='blue', ha='center', va='bottom')
plt.title('Frequency plot of Hospital Deaths', fontsize = 20, color = 'black')
plt.show()"
441,28577122,7,"plt.figure(figsize=(30,15))
ethnicity_vs_death = sns.catplot(x='ethnicity', col='hospital_death', kind='count', data=train, 
                                 order = train['ethnicity'].value_counts().index, height = 7, aspect = 1);
ethnicity_vs_death.set_xticklabels(rotation=90);"
442,28577122,8,"plt.figure(figsize=(30,15))
has_vs_death = sns.catplot(x='hospital_admit_source', col='hospital_death', kind='count', data=train, 
                           order = train['hospital_admit_source'].value_counts().index, height = 7, aspect = 1.5);
has_vs_death.set_xticklabels(rotation=90);"
443,28577122,9,"plt.figure(figsize=(30,15))
ias_vs_death = sns.catplot(x='icu_admit_source', col='hospital_death', kind='count', data=train, 
                           order = train['icu_admit_source'].value_counts().index, height = 7, aspect = 1.5);
ias_vs_death.set_xticklabels(rotation=90);"
444,28577122,10,"# Freq plot of Hospital ID for hospital_death = 0
train_hID_0 = train[train['hospital_death'] == 0]
plt.figure(figsize=(30,15))
hID_vs_death = sns.catplot(y='hospital_id',  orient = ""v"", kind='count', data=train_hID_0, order = train_hID_0['hospital_id'].value_counts().index, 
                           height = 30, aspect = 1)"
445,28577122,11,"# Freq plot of Hospital ID for hospital_death = 1
train_hID_1 = train[train['hospital_death'] != 0]
plt.figure(figsize=(30,20))
hID_vs_death = sns.catplot(y='hospital_id',  orient = ""v"", kind='count', data=train_hID_1, order = train_hID_1['hospital_id'].value_counts().index, 
                           height = 30, aspect = 1);"
446,28577122,12,"# Freq plot of Hospital ID for hospital_death = 0 & 1
plt.figure(figsize=(30,40))
hID_vs_death = sns.catplot(x = 'hospital_id', col='hospital_death', kind='count', data=train, order = train['hospital_id'].value_counts().index, 
                           height = 5, aspect = 2.8);
hID_vs_death.set_xticklabels(rotation=90);"
447,28577122,13,"plt.figure(figsize = (15,5))
sns.kdeplot(train_hID_1['age'], shade=True, color=""r"")
sns.kdeplot(train_hID_0['age'], shade=True, color=""b"")"
448,28577122,14,"sns.jointplot(x=""age"", y=""bmi"", data=train, kind = ""kde"")
sns.jointplot(x=""age"", y=""height"", data=train, kind = ""kde"")
sns.jointplot(x=""age"", y=""weight"", data=train, kind = ""kde"")"
449,28577122,15,"dataset = pd.concat(objs=[X_train, X_val], axis=0)"
450,28577122,16,col_1 = dataset.columns
451,28577122,17,"for i in col_1:
    if X_train[i].nunique() == 1:
        print('in Train', i)
    if X_val[i].nunique() == 1:
        print('in Val', i)
    if X_test[i].nunique() == 1:
        print('in Test', i)
    "
452,28577122,18,"# Dropping 'readmission_status'
X_train = X_train.drop(['readmission_status'], axis=1)
X_val = X_val.drop(['readmission_status'], axis=1)
X_test = X_test.drop(['readmission_status'], axis=1)"
453,28577122,19,"print('For Train')
d1 = X_train.nunique()
print(sorted(d1))
print(""=============================="")
print('For Validation')
d2 = X_val.nunique()
print(sorted(d2))

# Considering columns with <= 15 unique values for conversion"
454,28577122,20,"d = pd.concat(objs=[X_train, X_val], axis=0)"
455,28577122,21,col = d.columns 
456,28577122,22,"# For Train data
l1 = []
for i in col:
    if X_train[i].nunique() <= 15:
        l1.append(i)
        
l1"
457,28577122,23,"# For Val data
l2 = []
for i in col:
    if X_val[i].nunique() <= 15:
        l2.append(i)
        
l2"
458,28577122,24,"# For Test data
l3 = []
for i in col:
    if X_test[i].nunique() <= 15:
        l3.append(i)
        
l3"
459,28577122,25,"# Checking for columns in X_train and X_validation
set(l1) & set(l2)"
460,28577122,26,"# Checking for columns in X_train and X_test
set(l1) & set(l3)"
461,28577122,27,"print('Train', len(l1))
print('Validation', len(l2))
print('Common', len(set(l1) & set(l2)))"
462,28577122,28,"print('Train', len(l1))
print('Test', len(l3))
print('Common', len(set(l1) & set(l3)))"
463,28577122,29,X_train[l1].dtypes
464,28577122,30,"X_val[l2].dtypes
# Not a necessary step since we already confirmed the common columns. Included just for reference. "
465,28577122,31,"X_train[l1] = pd.Categorical(X_train[l1])
X_val[l2] = pd.Categorical(X_val[l2])
X_test[l3] = pd.Categorical(X_test[l3])
print('Train dtypes:')
print(X_train[l1].dtypes)
print('======================================')
print('Validation dtypes:')
print(X_val[l2].dtypes)
print('======================================')
print('Test dtypes:')
print(X_test[l3].dtypes)"
466,28577122,32,"# On train data
pd.set_option('display.max_rows', 500)
NA_col = pd.DataFrame(X_train.isna().sum(), columns = ['NA_Count'])
NA_col['% of NA'] = (NA_col.NA_Count/len(X_train))*100
NA_col.sort_values(by = ['% of NA'], ascending = False, na_position = 'first')"
467,28577122,33,"# On val data
pd.set_option('display.max_rows', 500)
NA_col = pd.DataFrame(X_val.isna().sum(), columns = ['NA_Count'])
NA_col['% of NA'] = (NA_col.NA_Count/len(X_val))*100
NA_col.sort_values(by = ['% of NA'], ascending = False, na_position = 'first')"
468,28577122,34,"# On test data
pd.set_option('display.max_rows', 500)
NA_col = pd.DataFrame(X_test.isna().sum(), columns = ['NA_Count'])
NA_col['% of NA'] = (NA_col.NA_Count/len(X_test))*100
NA_col.sort_values(by = ['% of NA'], ascending = False, na_position = 'first')"
469,28577122,35,"cols = X_train.columns
num_cols = X_train._get_numeric_data().columns
cat_cols = list(set(cols) - set(num_cols))
cat_cols"
470,28577122,36,"# Courtesy: https://www.kaggle.com/jayjay75/wids2020-lgb-starter-script
from sklearn.preprocessing import LabelEncoder,OneHotEncoder
for usecol in cat_cols:
    X_train[usecol] = X_train[usecol].astype('str')
    X_val[usecol] = X_val[usecol].astype('str')
    X_test[usecol] = X_test[usecol].astype('str')
    
    #Fit LabelEncoder
    le = LabelEncoder().fit(
            np.unique(X_train[usecol].unique().tolist()+
                      X_val[usecol].unique().tolist()+
                     X_test[usecol].unique().tolist()))

    #At the end 0 will be used for dropped values
    X_train[usecol] = le.transform(X_train[usecol])+1
    X_val[usecol]  = le.transform(X_val[usecol])+1
    X_test[usecol]  = le.transform(X_test[usecol])+1
    
    X_train[usecol] = X_train[usecol].replace(np.nan, 0).astype('int').astype('category')
    X_val[usecol]  = X_val[usecol].replace(np.nan, 0).astype('int').astype('category')
    X_test[usecol]  = X_test[usecol].replace(np.nan, 0).astype('int').astype('category')"
471,28577122,37,"X_train.set_index('encounter_id', inplace = True)
y_train.set_index('encounter_id', inplace = True)
X_val.set_index('encounter_id', inplace = True)
y_val.set_index('encounter_id', inplace = True)
X_test.set_index('encounter_id', inplace = True)
y_test.set_index('encounter_id', inplace = True)"
472,28577122,38,# y_test.hospital_death = y_test.hospital_death.fillna(0)
473,28577122,39,"# y_train['hospital_death'] = pd.Categorical(y_train['hospital_death'])
# y_train.dtypes"
474,28577122,40,"# y_test['hospital_death'] = pd.Categorical(y_test['hospital_death'])
# y_test.dtypes"
475,28577122,41,"# from sklearn.preprocessing import LabelEncoder,OneHotEncoder
# from pandas import Series
# l=LabelEncoder() 
# l.fit(y_train['hospital_death']) 
# l.classes_ 
# y_train['hospital_death']=Series(l.transform(y_train['hospital_death']))  #label encoding our target variable 
# y_train['hospital_death'].value_counts() "
476,28577122,42,"# l.fit(y_test['hospital_death']) 
# l.classes_ 
#y_test['hospital_death'].fillna(0.0, inplace = True)
# y_test['hospital_death']=Series(l.transform(y_test['hospital_death']))  #label encoding our target variable 
# y_test['hospital_death'].value_counts() "
477,28577122,43,"import lightgbm as lgbm

lgbm_train = lgbm.Dataset(X_train, y_train, categorical_feature=cat_cols)
# lgbm_test = lgbm.Dataset(X_test, y_test, categorical_feature=cat_cols)
lgbm_val = lgbm.Dataset(X_val, y_val, reference = lgbm_train)"
478,28577122,44,"params = {'feature_fraction': 0.9,
          'lambda_l1': 1,
          'lambda_l2': 1,
          'learning_rate': 0.01,
          'max_depth': 10,
          'metric': 'auc',
          'num_leaves': 500,
          'min_data_in_leaf': 100,
          'subsample_freq': 1,
          'scale_pos_weight':1,
          'metric': 'auc',
          'is_unbalance': 'true',
          'boosting': 'gbdt',
          'bagging_fraction': 0.5,
          'bagging_freq': 10,}"
479,28577122,45,"evals_result = {}  # to record eval results for plotting
model_lgbm = lgbm.train(params,
                lgbm_train,
                num_boost_round=100,
                valid_sets=[lgbm_train, lgbm_val],
                feature_name=['f' + str(i + 1) for i in range(X_train.shape[-1])],
                categorical_feature= [182],
                evals_result=evals_result,
                verbose_eval=10)"
480,28577122,46,"ax = lgbm.plot_metric(evals_result, metric='auc', figsize=(15, 8))
plt.show()"
481,28577122,47,"test[""hospital_death""] = model_lgbm.predict(X_test, predition_type = 'Probability')
test[[""encounter_id"",""hospital_death""]].to_csv(""submission_lgbm.csv"",index=False)"
482,28577122,48,"test[[""encounter_id"",""hospital_death""]].head()"
483,28722761,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
from sklearn.preprocessing import LabelEncoder
import os
from keras import regularizers
import tensorflow as tf
from sklearn import preprocessing
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import SGD, Adam
from tensorflow.keras.callbacks import ModelCheckpoint
from keras.callbacks import Callback, EarlyStopping
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import seaborn as sns
# roc curve and auc score
from sklearn import preprocessing
from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score"
484,28722761,1,"# Input data files are available in the ""../input/"" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

trainData = pd.read_csv(""../input/widsdatathon2020/training_v2.csv"")
testData = pd.read_csv(""../input/widsdatathon2020/unlabeled.csv"")

# Any results you write to the current directory are saved as output."
485,28722761,2,"categorical_cols = [c for c in trainData.columns if (trainData[c].dtype != np.number)& (trainData[c].dtype != int) ]
Categorical_df= trainData[categorical_cols]
# for col in categorical_cols:
#     print(col, ""*****************"")
#     print(Categorical_df[col].value_counts(), Categorical_df[col].unique())"
486,28722761,3,"# frequency encoder
for col in ['ethnicity', 'hospital_admit_source', 'icu_admit_source', 'apache_3j_bodysystem', 'apache_2_bodysystem', 'icu_type']:
    trainData[col] = trainData[col].astype('str')
    freq = trainData.groupby(col).size()/len(trainData[col])
    trainData[col] = trainData[col].map(freq)
    
    testData[col] = testData[col].astype('str')
    freq = testData.groupby(col).size()/len(testData[col])
    testData[col] = testData[col].map(freq)

trainData.head()"
487,28722761,4,"# ordinal encoder
icu_st_dict={'admit':0,'readmit':1,'transfer':2}

trainData['icu_stay_type'] = trainData['icu_stay_type'].astype('str')
trainData['icu_stay_type'] = trainData['icu_stay_type'].map(icu_st_dict)

testData['icu_stay_type'] = testData['icu_stay_type'].astype('str')
testData['icu_stay_type'] = testData['icu_stay_type'].map(icu_st_dict)"
488,28722761,5,"trainData['icu_stay_type'].tail(10)
trainData['icu_type'].tail(10)"
489,28722761,6,"# label encoding the data gender
le = LabelEncoder()
for col in ['gender']:
    trainData[col] = trainData[col].astype('str')

    #Fit LabelEncoder
    le.fit(np.unique(trainData[col].unique()))

    #At the end 0 will be used for null values so we start at 1 
    trainData[col] = le.transform(trainData[col])+1
    trainData[col] = trainData[col].replace(np.nan, 0).astype('int')
    
    testData[col] = testData[col].astype('str')

    #Fit LabelEncoder
    le.fit(np.unique(testData[col].unique()))

    #At the end 0 will be used for null values so we start at 1 
    testData[col] = le.transform(testData[col])+1
    testData[col] = testData[col].replace(np.nan, 0).astype('int')

testData.head()"
490,28722761,7,"# replace na with mean for following categories

for col in ['age', 'bmi', 'weight', 'height']:
    mean = trainData[col].mean()
    trainData[col] = trainData[col].replace(np.nan, mean).astype('int')
    
    mean = testData[col].mean()
    testData[col] = testData[col].replace(np.nan, mean).astype('int')"
491,28722761,8,"x = testData.isnull().sum(axis=0)
x"
492,28722761,9,"trainData = trainData.replace(np.nan, 0).astype('int')
testData = testData.replace(np.nan, 0).astype('int')
testData.head(5)"
493,28722761,10,"to_drop = ['gender','ethnicity' ,'encounter_id', 'patient_id',  'hospital_death', 'hospital_id']
testDataOld = testData
trainLabel = trainData['hospital_death']
for col in to_drop:
    trainData = trainData.drop(col, axis = 1)
    testData = testData.drop(col, axis = 1)

trainData.head()"
494,28722761,11,"cols_with_missing = (col for col in y_test.columns if y_test[col].isnull().any())
for col in cols_with_missing:
    y_test[col + '_was_missing'] = y_test[col].isnull()
    y_test[col + '_was_missing'] = y_test[col].isnull()"
495,28722761,12,"x_train, y_test ,x_label, y_label = train_test_split(trainData, trainLabel, test_size=0.3, random_state=1)
std_scale = preprocessing.StandardScaler().fit(x_train)
x_train = std_scale.transform(x_train)
y_test  = std_scale.transform(y_test)
testData = std_scale.transform(testData)
x_train.shape"
496,28722761,13,"# from sklearn.linear_model import LinearRegression
# from sklearn.feature_selection import RFE
# from sklearn.feature_selection import SelectKBest, SelectFpr, f_classif

# reg = LinearRegression()
# x_train_new = SelectFpr(f_classif, alpha=0.01).fit_transform(x_train, x_label)
# fit = reg.fit(x_train_new, x_label)
# pred = fit.predict(y_test)
# print(testData.shape, pred.shape)

# auc = roc_auc_score(y_label, pred)
# fpr, tpr, thresholds = roc_curve(y_label, pred)
# plot_roc_curve(fpr, tpr)
# print(auc)
# pred = fit.predict(testData)
# testDataOld[""hospital_death""] = pred
# testDataOld[[""encounter_id"",""hospital_death""]].to_csv(""submission.csv"",index=False)
# testDataOld[[""encounter_id"",""hospital_death""]].head()"
497,28722761,14,"def plot_roc_curve(fpr, tpr):
    plt.plot(fpr, tpr, color='orange', label='ROC')
    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic (ROC) Curve')
    plt.legend()
    plt.show()"
498,28722761,15,"checkpoint_callback = ModelCheckpoint(""model.h5"", monitor='accuracy', save_best_only=True, save_freq=2)
y_test.shape"
499,28722761,16,"model = Sequential()
model.add(Dense(64, input_shape=(180,),kernel_initializer='normal', activation='sigmoid', name='fc1'))
model.add(Dense(32, activation='sigmoid',kernel_initializer='normal', name='fc3'))
model.add(Dense(1, name='output'))
optimizer = tf.keras.optimizers.RMSprop(0.0001)"
500,28722761,17,"model.compile(optimizer, loss='mse', metrics=['accuracy', 'mse'])"
501,28722761,18,"model.fit(x_train, x_label, batch_size=60, epochs=50, callbacks=[checkpoint_callback])"
502,28722761,19,"probs = model.predict_proba(y_test).flatten()
auc = roc_auc_score(y_label, probs)
fpr, tpr, thresholds = roc_curve(y_label, probs)
plot_roc_curve(fpr, tpr)
print(""AUC-ROC :"",auc)
probs"
503,28722761,20,"probstest = model.predict_proba(testData)
probstest = probstest[:]
print(probstest)
testDataOld[""hospital_death""] = probstest
testDataOld[[""encounter_id"",""hospital_death""]].to_csv(""submission3.csv"",index=False)
testDataOld[[""encounter_id"",""hospital_death""]].head()
testDataOld[[""encounter_id"",""hospital_death""]].head()"
504,28722761,21,"from IPython.display import FileLink
import os
os.chdir(r'/kaggle/working')
FileLink(r'submission3.csv')"
505,29100312,0,"
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt
import seaborn as sns


# Input data files are available in the ""../input/"" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))
         import os
    os.chdir(r'kaggle/working')

# Any results you write to the current directory are saved as output."
506,29100312,1,"import h2o
from h2o.automl import H2OAutoML
h2o.init()
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt
%matplotlib inline

import plotly as py
#plotly.offline doesn't push your charts to the clouds
import plotly.offline as pyo
import plotly.graph_objs as go
pyo.offline.init_notebook_mode()
import plotly.express as px

from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier ,AdaBoostClassifier,VotingClassifier
from sklearn.model_selection import train_test_split

import matplotlib.pyplot as plt
import seaborn as sns
# roc curve and auc score
from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score
from sklearn import preprocessing

pd.set_option('display.max_columns', None)"
507,29100312,2,"# loading dataset 
df= pd.read_csv(""../input/widsdatathon2020/training_v2.csv"")
test=pd.read_csv(""..//input/widsdatathon2020/unlabeled.csv"")"
508,29100312,3,"df.sample(5)

"
509,29100312,4,"#By using the above code, maybe I can check how missing values vary by thresholds?
for x in range(30):
    df_check = df.dropna(thresh=x)
    print(x,"" variables = "",df_check.shape)"
510,29100312,5,"
# Drop columns based on threshold limit
threshold = len(df) * 0.60
df_thresh=df.dropna(axis=1, thresh=threshold)

# View columns in the dataset
#We can see that 74 columns have been dropped as they cant be used for predictions as they are missing lots of data
df_thresh.shape"
511,29100312,6,"# Drop columns based on threshold limit
threshold = len(test) * 0.50
test=test.dropna(axis=1, thresh=threshold)

# View columns in the dataset
#We can see that 74 columns have been dropped as they cant be used for predictions as they are missing lots of data
test.shape"
512,29100312,7,"#Getting the numerical data from the dataset
df_thresh._get_numeric_data()

"
513,29100312,8,"#Checking for missing values in IDs
df_thresh[['encounter_id','hospital_id','patient_id','icu_id']].isnull().sum()"
514,29100312,9,"#Applying skelarn imputer for numerical values 
#Using mean as the imputation value as most of the numerical data are clinical data that can be approximated to mean
imputer_skdf = df_thresh._get_numeric_data()
colNames=imputer_skdf.columns;
from sklearn.impute import SimpleImputer
imputer = SimpleImputer(missing_values=np.nan, strategy='mean')
# Fit and transform to the parameters
imputer_skdf = pd.DataFrame(imputer.fit_transform(imputer_skdf))
imputer_skdf.columns = colNames;
# Checking for any null values
imputer_skdf.isna().sum()
imputer_skdf.info()"
515,29100312,10,"#Applying skelarn imputer for numerical values 
#Using mean as the imputation value as most of the numerical data are clinical data that can be approximated to mean
imputer_sktest =test._get_numeric_data()
colNames1=imputer_sktest.columns;
from sklearn.impute import SimpleImputer
imputer1 = SimpleImputer(missing_values=np.nan, strategy='mean')
# Fit and transform to the parameters
imputer_sktest = pd.DataFrame(imputer.fit_transform(imputer_skdf))
imputer_sktest.columns = colNames1;
# Checking for any null values
imputer_sktest.isna().sum()
imputer_sktest.info()"
516,29100312,11,"#categorical values
categ_df=df_thresh.select_dtypes(exclude=['int','float'])
column_names=categ_df.columns
column_names"
517,29100312,12,"#categorical values for test
categ_df1=test.select_dtypes(exclude=['int','float'])
column_names1=categ_df1.columns
column_names1

"
518,29100312,13,"# Replacing null values in categorical data with most frequent value for test set
imputer2 = SimpleImputer(missing_values=np.nan, strategy='most_frequent')
categ_df1 = pd.DataFrame(imputer2.fit_transform(categ_df1))
categ_df1.columns =column_names1;"
519,29100312,14,"# Replacing null values in categorical data with most frequent value
imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')
categ_df = pd.DataFrame(imputer.fit_transform(categ_df))
categ_df.columns =column_names;"
520,29100312,15,"#merging Categorical and numerical data to a single dataset for test set
categ_df1['encounter_id']=imputer_sktest.encounter_id

test=pd.merge(imputer_sktest,categ_df1,on='encounter_id')
test.shape"
521,29100312,16,"#merging Categorical and numerical data to a single dataset 
categ_df['encounter_id']=imputer_skdf.encounter_id

df1=pd.merge(imputer_skdf,categ_df,on='encounter_id')
df1.shape

"
522,29100312,17,"#Exploratory data anlaysis and feature engineering 
#Checking the gender distribution using a pie chart
labels = df1['gender'].value_counts().index
values = df1['gender'].value_counts().values

colors = ['#eba796', '#96ebda']

fig = {'data' : [{'type' : 'pie',
                  'name' : ""Patients by Gender: Pie chart"",
                 'labels' : df1['gender'].value_counts().index,
                 'values' : df1['gender'].value_counts().values,
                 'direction' : 'clockwise',
                 'marker' : {'colors' : ['#9cc359', '#e96b5c']}}], 'layout' : {'title' : 'Patients by Gender'}}

pyo.iplot(fig)"
523,29100312,18,"#Visualizing the ICU admit source Distribution 
#we could see that accident and emergency, operating room and floor are major contributors for ICU admissions
colors = ['#eba796', '#96ebda']


fig = {'data' : [{'type' : 'pie',
                  'name' : ""ICU admit source"",
                 'labels' : df1['icu_admit_source'].value_counts().index,
                 'values' : df1['icu_admit_source'].value_counts().values,
                 'direction' : 'clockwise',
                 'marker' : {'colors' : ['#9cc359', '#e96b5c']}}], 'layout' : {'title' : 'ICU admit source distribution'}}

pyo.iplot(fig)"
524,29100312,19,"#Visualizing the Hospital admit source Distribution 
#we could see that accident and emergency are major contributors for ICU admissions also floor constitute 11% of the hospital admissions 
colors = ['#eba796', '#96ebda']


fig = {'data' : [{'type' : 'pie',
                  'name' : ""Hospital admit source"",
                 'labels' : df1['hospital_admit_source'].value_counts().index,
                 'values' : df['hospital_admit_source'].value_counts().values,
                 'direction' : 'clockwise',
                 'marker' : {'colors' : ['#9cc359', '#e96b5c']}}], 'layout' : {'title' : 'hospital admit source distribution'}}

pyo.iplot(fig)"
525,29100312,20,"#Visualizing the Hospital admit source Distribution 
#we could see that accident and emergency are major contributors for ICU admissions also floor constitute 11% of the hospital admissions 
colors = ['#eba796', '#96ebda']


fig = {'data' : [{'type' : 'pie',
                  'name' : ""Hospital admit source"",
                 'labels' : df1['hospital_admit_source'].value_counts().index,
                 'values' : df['hospital_admit_source'].value_counts().values,
                 'direction' : 'clockwise',
                 'marker' : {'colors' : ['#9cc359', '#e96b5c']}}], 'layout' : {'title' : 'hospital admit source distribution'}}

pyo.iplot(fig)"
526,29100312,21,"#taking a subset of the data to understand the effect of features on dataset
df1['hospital_death'].value_counts(normalize=True)


"
527,29100312,22,"#Checking the distribution of the icu admit source
df1['icu_admit_source'].value_counts()"
528,29100312,23,"#Checking the distribution of icu_type
df1['icu_type'].value_counts()"
529,29100312,24,"#Visualizing the age 
#We can see that most hospital death occur for patients in the 60 -70 age group
fig=plt.figure() #Plots in matplotlib reside within a figure object, use plt.figure to create new figure
#Create one or more subplots using add_subplot, because you can't create blank figure
ax = fig.add_subplot(1,1,1)
#Variable
ax.hist(df1['age'],bins = 7) # Here you can play with number of bins

plt.title('Age distribution')
plt.xlabel('Age')
plt.ylabel('Patient')
plt.show()"
530,29100312,25,"#gender and #age distrubution
import seaborn as sns 
sns.violinplot(df1['age'], df1['gender']) #Variable Plot
sns.despine()"
531,29100312,26,"var = df1.groupby('gender').hospital_death.sum() #grouped sum of sales at Gender level
fig = plt.figure()
ax1 = fig.add_subplot(1,1,1)
ax1.set_xlabel('gender')
ax1.set_ylabel('Sum of deaths')
ax1.set_title(""Gender wise Sum of deaths"")
var.plot(kind='bar')"
532,29100312,27,"#Visualizing ethnicity
var = df1.groupby('ethnicity').hospital_death.sum() 
fig = plt.figure()
ax1 = fig.add_subplot(1,1,1)
ax1.set_xlabel('ethnicity')
ax1.set_ylabel('Sum of deaths')
ax1.set_title(""Ethnicity wise Sum of deaths"")
var.plot(kind='bar')"
533,29100312,28,"var = df1.groupby('bmi').hospital_death.sum()
fig = plt.figure()
ax1 = fig.add_subplot(1,1,1)
ax1.set_xlabel('bmi')
ax1.set_ylabel('Sum of deaths')
ax1.set_title(""BMI wise Sum of deaths"")
var.plot(kind='line')"
534,29100312,29,df1['apache_3j_diagnosis'].value_counts()
535,29100312,30,df1['apache_2_bodysystem'].value_counts()
536,29100312,31,"
#Visualizing hospital death by apache_2_bodySystem
#We could see that cardio vascular conditions account for the most hospital deaths
var = df1.groupby('apache_2_bodysystem').hospital_death.sum() 
fig = plt.figure()
ax1 = fig.add_subplot(1,1,1)
ax1.set_xlabel('apache_2_bodysystem')
ax1.set_ylabel('Sum of deaths')
ax1.set_title(""apache bodysystem wise Sum of deaths"")
var.plot(kind='bar')"
537,29100312,32,test.columns=df1.columns
538,29100312,33,"df_encoded=pd.get_dummies(df1, columns=['ethnicity', 'gender', 'hospital_admit_source', 'icu_admit_source',
       'icu_stay_type', 'icu_type', 'apache_3j_bodysystem',
       'apache_2_bodysystem'])
df_encoded.columns"
539,29100312,34,"# creating independent features X and dependent feature Y
y =df_encoded['hospital_death']
X = df_encoded
X = df_encoded.drop(columns=['hospital_death'],axis='columns')
test=pd.get_dummies(test, columns=['ethnicity', 'gender', 'hospital_admit_source', 'icu_admit_source',
       'icu_stay_type', 'icu_type', 'apache_3j_bodysystem',
       'apache_2_bodysystem'])"
540,29100312,35,"# Split into training and validation set
X_train, valid_features, Y_train, valid_y = train_test_split(X, y, test_size = 0.25, random_state = 1)"
541,29100312,36,"# Gradient Boosting Classifier
GBC = GradientBoostingClassifier(random_state=1)"
542,29100312,37,"# Random Forest Classifier
RFC = RandomForestClassifier(n_estimators=100)"
543,29100312,38,"# Voting Classifier with soft voting 
votingC = VotingClassifier(estimators=[('rfc', RFC),('gbc',GBC)], voting='soft')
votingC = votingC.fit(X_train, Y_train)"
544,29100312,39,predict_y = votingC.predict(valid_features)
545,29100312,40,"def plot_roc_curve(fpr, tpr):
    plt.plot(fpr, tpr, color='orange', label='ROC')
    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic (ROC) Curve')
    plt.legend()
    plt.show()"
546,29100312,41,"probs = votingC.predict_proba(valid_features)
probs = probs[:, 1]
auc = roc_auc_score(valid_y, probs)
fpr, tpr, thresholds = roc_curve(valid_y, probs)
plot_roc_curve(fpr, tpr)
print(""AUC-ROC :"",auc)"
547,29100312,42,"test1 = test.copy()

test1[""hospital_death""] = votingC.predict(test)
test1.hospital_death =test1.hospital_death.astype(int)
test1.encounter_id =test1.encounter_id.astype(int)
test1[[""encounter_id"",""hospital_death""]].to_csv(""..//output/kaggle/working/submission5.csv"",index=False)
test1[[""encounter_id"",""hospital_death""]].head()
from IPython.display import FileLink
FileLink(r'submission5.csv')"
548,29100312,43,"
"
549,29100312,44,
550,29100312,45,
551,29100312,46,
552,29100312,47,
553,29100312,48,
554,29100312,49,
555,29100312,50,
556,29100312,51,"
"
557,29100312,52,
558,29100312,53,
559,29100312,54,"
"
560,29100312,55,
561,29100312,56,
562,29100312,57,
563,29100312,58,"
"
564,29100312,59,"
"
565,29100312,60,
566,29100312,61,
567,29100312,62,
568,29100312,63,
569,29100312,64,
570,29100312,65,
571,29100312,66,"
"
572,35177928,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the ""../input/"" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# Any results you write to the current directory are saved as output."
573,35177928,1,"import seaborn as sns
import lightgbm
import matplotlib.pyplot as plt
from decimal import *
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import train_test_split, cross_val_score,cross_validate
from xgboost import XGBClassifier
from sklearn import ensemble
from sklearn.ensemble import (RandomForestClassifier, ExtraTreesClassifier, VotingClassifier, AdaBoostClassifier, BaggingRegressor)
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, classification_report
from sklearn.metrics import mean_absolute_error
"
574,35177928,2,"train_data = pd.read_csv('/kaggle/input/widsdatathon2020/training_v2.csv')
test_data = pd.read_csv('/kaggle/input/widsdatathon2020/unlabeled.csv')"
575,35177928,3,"print(train_data.shape)
print(test_data.shape)"
576,35177928,4,train_data.describe()
577,35177928,5,test_data.describe()
578,35177928,6,"plt.figure(figsize=(15,9))
plt.xticks(rotation=90)
sns.countplot(x='age', hue='hospital_death', data= train_data);"
579,35177928,7,"sns.countplot(x='gender', hue='hospital_death', data=train_data);"
580,35177928,8,"sns.countplot(x='hospital_death', data=train_data);
"
581,35177928,9,"np.round(train_data['hospital_death'].value_counts()/train_data.shape[0]*100,2)"
582,35177928,10,train_data.head()
583,35177928,11,"drop_columns = ['encounter_id','patient_id', 'hospital_id', 'icu_id']"
584,35177928,12,"train_data = train_data.drop(drop_columns, axis=1)
test_data = test_data.drop(drop_columns, axis=1)"
585,35177928,13,"# How much of the data is missing 
np.round(train_data.isna().sum()/train_data.shape[0]*100,2).sort_values(ascending=False)
"
586,35177928,14,"np.round(test_data.isna().sum()/train_data.shape[0]*100,2)
"
587,35177928,15,"null_values = train_data.isnull().sum()/len(train_data)*100
missing_features = null_values[null_values > 30].index
"
588,35177928,16,"train_data.drop(missing_features, axis=1, inplace=True)
train_data.shape"
589,35177928,17,"test_data.drop(missing_features, axis=1, inplace=True)
test_data.shape"
590,35177928,18,len(missing_features)
591,35177928,19,"#list of features to dummy
todummy_list = []

# Check how many unique categories I have 
for col_name in train_data.columns:
    if train_data[col_name].dtype == 'object':
        unique_cat = len(train_data[col_name].unique())
        todummy_list.append(col_name)
        print('Feature {col_name} has {unique_cat} unique categories'.format(col_name=col_name,unique_cat=unique_cat ))"
592,35177928,20,"print(train_data['ethnicity'].value_counts(normalize=True, ascending=False)*100)"
593,35177928,21,"train_data['ethnicity'] = ['Caucasian' if value == 'Caucasian' else 'Others' for value in train_data['ethnicity']]
test_data['ethnicity'] = ['Caucasian' if value == 'Caucasian' else 'Others' for value in test_data['ethnicity']]

print(train_data['ethnicity'].value_counts(normalize=True, ascending=False)*100)"
594,35177928,22,"print(train_data['hospital_admit_source'].value_counts(normalize=True, ascending=False)*100)"
595,35177928,23,"train_data['hospital_admit_source'] = ['Emergency Department' if value == 'Emergency Department' else 'Others' for value in train_data['hospital_admit_source']]
test_data['hospital_admit_source'] = ['Emergency Department' if value == 'Emergency Department' else 'Others' for value in test_data['hospital_admit_source']]

print(train_data['hospital_admit_source'].value_counts(normalize=True, ascending=False)*100)"
596,35177928,24,"print(train_data['icu_admit_source'].value_counts(normalize=True, ascending=False)*100)"
597,35177928,25,"train_data['icu_admit_source'] = ['Accident & Emergency' if value == 'Accident & Emergency' else 'Others' for value in train_data['icu_admit_source']]
test_data['icu_admit_source'] = ['Accident & Emergency' if value == 'Accident & Emergency' else 'Others' for value in test_data['icu_admit_source']]

print(train_data['icu_admit_source'].value_counts(normalize=True, ascending=False)*100)"
598,35177928,26,"print(train_data['icu_stay_type'].value_counts(normalize=True, ascending=False)*100)"
599,35177928,27,"train_data['icu_stay_type'] = ['admit' if value == 'admit' else 'Others' for value in train_data['icu_stay_type']]
test_data['icu_stay_type'] = ['admit' if value == 'admit' else 'Others' for value in test_data['icu_stay_type']]

print(train_data['icu_stay_type'].value_counts(normalize=True, ascending=False)*100)"
600,35177928,28,"print(train_data['icu_type'].value_counts(normalize=True, ascending=False)*100)"
601,35177928,29,"train_data['icu_type'] = ['Med-Surg ICU' if value == 'Med-Surg ICU' else 'Others' for value in train_data['icu_type']]
test_data['icu_type'] = ['Med-Surg ICU' if value == 'Med-Surg ICU' else 'Others' for value in test_data['icu_type']]

print(train_data['icu_type'].value_counts(normalize=True, ascending=False)*100)"
602,35177928,30,"print(train_data['apache_3j_bodysystem'].value_counts(normalize=True, ascending=False)*100)"
603,35177928,31,"print(train_data['apache_2_bodysystem'].value_counts(normalize=True, ascending=False)*100)"
604,35177928,32,"#function to dummy all the categorical variables used for modeling
def dummy_df(df, todummy_list):
    for feature in todummy_list:
        dummies = pd.get_dummies(df[feature], prefix= feature, dummy_na = False)
        df = df.drop(feature, 1)
        df = pd.concat([df, dummies], axis =1)
    return df"
605,35177928,33,"train_data = dummy_df(train_data, todummy_list)
test_data = dummy_df(test_data, todummy_list)"
606,35177928,34,"print(train_data.shape)
print(test_data.shape)"
607,35177928,35,"#Assign X a DataFrame of features and y as a Series of outcome variable
X = train_data.drop('hospital_death', 1)
y = train_data.hospital_death"
608,35177928,36,"X_test = test_data.drop('hospital_death', 1)
y_test = test_data.hospital_death"
609,35177928,37,"pd.DataFrame(X).fillna(X.median(), inplace=True)
pd.DataFrame(X_test).fillna(X_test.median(), inplace=True)"
610,35177928,38,X.isna().sum().sort_values(ascending=False)
611,35177928,39,"scaler = StandardScaler()
scaler.fit(X)
X_scaled = scaler.transform(X)"
612,35177928,40,"print('XGBClassifier Model:')
XGB_CV = pd.DataFrame(cross_validate(XGBClassifier(), X_scaled, y, cv = 3, return_train_score=True, scoring = ['accuracy', 'precision', 'recall', 'f1']))
XGB_CV.mean()"
613,35177928,41,import sklearn.feature_selection
614,35177928,42,"select = sklearn.feature_selection.SelectKBest(k=50)
selected_features = select.fit(X,y)
indices_selected = selected_features.get_support(indices=True)
col_names_selected = [X.columns[i] for i in indices_selected]

X_selected = X[col_names_selected]
"
615,35177928,43,col_names_selected
616,35177928,44,"scaler_2 = StandardScaler()
scaler.fit(X_scaled)
X_scaled_2 = scaler.transform(X_scaled)
"
617,35177928,45,"print('LogisticRegression Model:')
log_CV = pd.DataFrame(cross_validate(LogisticRegression(), X_scaled_2, y, cv = 3, return_train_score=True, scoring = ['accuracy', 'precision', 'recall', 'f1']))
log_CV.mean()"
618,35177928,46,"print('XGBClassifier Model:')
XGB_CV = pd.DataFrame(cross_validate(XGBClassifier(), X_scaled_2, y, cv = 3, return_train_score=True, scoring = ['accuracy', 'precision', 'recall', 'f1']))
XGB_CV.mean()"
619,35177928,47,"print('AdaBoostClassifier Model:')
ada_model = pd.DataFrame(cross_validate(ensemble.AdaBoostClassifier(), X_selected, y, cv = 3, return_train_score=True, scoring = ['accuracy', 'precision', 'recall', 'f1']))
ada_model.mean()"
620,35177928,48,from imblearn.over_sampling import SMOTE
621,35177928,49,"# transform the dataset
oversample = SMOTE()
X, y = oversample.fit_resample(X, y)"
622,35177928,50,"sns.countplot(x=y, data=train_data);"
623,35177928,51,"np.round(y.value_counts()/len(y)*100,2)"
624,35177928,52,"scaler_3 = StandardScaler()
scaler.fit(X)
X_scaled_3= scaler.transform(X)
X_test_scaled = scaler.transform(X_test)"
625,35177928,53,"print('XGBClassifier Model:')
XGB_CV = pd.DataFrame(cross_validate(XGBClassifier(), X_scaled_3, y, cv = 3, return_train_score=True, scoring = ['accuracy', 'precision', 'recall', 'f1']))
XGB_CV.mean()"
626,35177928,54,"def print_confusion_matrix(confusion_matrix, class_names, figsize = (10,7), fontsize=18):
    """"""Prints a confusion matrix, as returned by sklearn.metrics.confusion_matrix, as a heatmap.
    
    Arguments
    ---------
    confusion_matrix: numpy.ndarray
        The numpy.ndarray object returned from a call to sklearn.metrics.confusion_matrix. 
        Similarly constructed ndarrays can also be used.
    class_names: list
        An ordered list of class names, in the order they index the given confusion matrix.
    figsize: tuple
        A 2-long tuple, the first value determining the horizontal size of the ouputted figure,
        the second determining the vertical size. Defaults to (10,7).
    fontsize: int
        Font size for axes labels. Defaults to 14.
        
    Returns
    -------
    matplotlib.figure.Figure
        The resulting confusion matrix figure
    """"""
    df_cm = pd.DataFrame(confusion_matrix, index=class_names, columns=class_names, )
    fig = plt.figure(figsize=figsize)
    try:
        heatmap = sns.heatmap(df_cm, annot=True, fmt=""d"")
    except ValueError:
        raise ValueError(""Confusion matrix values must be integers."")
    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)
    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)
    plt.xlim(0,len(class_names))
    plt.ylim(len(class_names),0)
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    return fig"
627,35177928,55,"def model_test(model, X, y):
    # perform train/val split
    X, X_test, y, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    model.fit(X, y,  eval_metric='auc')
    pred = model.predict(X_test)
    
    print('Mean Absolute Error:', mean_absolute_error(y_test,pred))
    print('Accuracy score:', accuracy_score(y_test, pred))
    conf = confusion_matrix(y_test, pred)
    print(classification_report(y_test, pred))
    print_confusion_matrix(conf, ['1', '0'])"
628,35177928,56,"model = XGBClassifier()
model_test(model, X_scaled_3, y)
#model.fit(X_scaled_3, y)"
629,35177928,57,y_test = model.predict(X_test_scaled)
630,35177928,58,"solution_template = pd.read_csv(""/kaggle/input/widsdatathon2020/solution_template.csv"")"
631,35177928,59,"print(y_test.shape)
print(solution_template.shape)"
632,35177928,60,"solution_template.hospital_death = y_test
solution_template.to_csv(""Version_2.csv"", index=0)"
633,35177928,61,"from keras.models import Sequential
from keras.layers import Dense, Dropout"
634,35177928,62,"# define the keras model
nn_model = Sequential()
nn_model.add(Dense(15, input_dim=X.shape[1], activation='relu'))
nn_model.add(Dense(10, activation='relu'))
nn_model.add(Dense(8, activation='relu'))
nn_model.add(Dropout(.2))
nn_model.add(Dense(1, activation='sigmoid'))"
635,35177928,63,"# compile the keras model
nn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
"
636,35177928,64,"# fit the keras model on the dataset
nn_model.fit(X, y, epochs=150, batch_size=10)"
637,35177928,65,"# save the model
nn_model.save('nn_model.h5')"
638,35177928,66,"# make class predictions with the model
predictions = nn_model.predict_classes(X_test)
"
639,35177928,67,"solution_template.hospital_death = predictions
solution_template.to_csv(""Version_3.csv"", index=0)"
640,35177928,68,
641,35177928,69,
642,29626958,0,"# data analysis and wrangling
import pandas as pd
import numpy as np
import random as rnd

# visualization
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline 

# machine learning
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler, normalize"
643,29626958,1,"import warnings
warnings.filterwarnings('ignore')"
644,29626958,2,"#Load the training and test dataset
train_df = pd.read_csv('/kaggle/input/widsdatathon2020/training_v2.csv')
test_df = pd.read_csv('/kaggle/input/widsdatathon2020/unlabeled.csv')"
645,29626958,3,"#Know the features of the dataset
print(train_df.info())
print(train_df.shape)"
646,29626958,4,"# Train missing values (in percent)
train_missing = (train_df.isnull().sum() / len(train_df)).sort_values(ascending = False)
train_missing.head()
train_missing = train_missing.index[train_missing > 0.75]
print('There are %d columns with more than 75%% missing values' % len(train_missing))
print('The missing columns are %s' % train_missing)
df_train = train_df.drop(columns = train_missing)
df_test = test_df.drop(columns = train_missing)
df_train.shape
df_test.shape"
647,29626958,5,"columns_to_drop = train_df[['patient_id', 'hospital_id','icu_id','readmission_status','hospital_death']]
target = train_df['hospital_death']
df_train = df_train.drop(columns = columns_to_drop)
df_test = df_test.drop(columns = columns_to_drop)"
648,29626958,6,"print(df_train.shape)
print(df_test.shape)"
649,29626958,7,"# Remove duplicates from training and test data
df_train = df_train.drop_duplicates(subset=None, keep='first', inplace=False).copy()
df_test = df_test.drop_duplicates(subset=None, keep='first', inplace=False).copy()
print(df_train.shape)
print(df_test.shape)"
650,29626958,8,"# Identifying the datatypes of the variables
continous_attrib = df_train.select_dtypes(include=np.number).columns
binary_attrib = df_train[['apache_post_operative', 'arf_apache', 'cirrhosis', 'diabetes_mellitus', 'immunosuppression',
'hepatic_failure', 'leukemia', 'lymphoma', 'solid_tumor_with_metastasis', 'gcs_unable_apache',
'intubated_apache', 'ventilated_apache']].columns
continous_attrib = continous_attrib.drop(binary_attrib)
categorical_attrib =   df_train.select_dtypes(include=['object']).columns
selected_attributes = list(set(continous_attrib)) + list(set(categorical_attrib)) + list(set(binary_attrib))
print(len(selected_attributes))
df_train,y_train = df_train[selected_attributes],target
df_test= df_test[selected_attributes]
df_train.head()
df_test.head()
"
651,29626958,9,"#Imputing features in train and test data
from sklearn.impute import SimpleImputer
# Replacing NAN values in numerical columns with mean

imputer_Num = SimpleImputer(missing_values=np.nan, strategy='mean')
imputer_Cat_Bin = SimpleImputer(missing_values=np.nan, strategy='most_frequent')

continous_attrib = list(set(continous_attrib)) 
categorical_attrib = list(set(categorical_attrib)) 
binary_attrib = list(set(binary_attrib))

#One hot coding for train data
df_Cat_attrib = pd.DataFrame(df_train[categorical_attrib])
df_Cat_OneHotCoded = pd.get_dummies(df_Cat_attrib)
df_Cat_OneHotCoded.head()

# Fit and transform to the parameters
df_imputed_Num = pd.DataFrame(imputer_Num.fit_transform(df_train[continous_attrib]))
df_imputed_Num.columns = continous_attrib

df_imputed_Cat = pd.DataFrame(imputer_Cat_Bin.fit_transform(df_Cat_OneHotCoded[df_Cat_OneHotCoded.columns]))
df_imputed_Cat.columns = df_Cat_OneHotCoded.columns

df_imputed_binary = pd.DataFrame(imputer_Num.fit_transform(df_train[binary_attrib]))
df_imputed_binary.columns = binary_attrib

train_imputed_df = pd.concat([df_imputed_Num,df_imputed_Cat,df_imputed_binary], axis=1).dropna()


#One hot coding for test data
df_Cat_attrib = pd.DataFrame(df_test[categorical_attrib])
df_Cat_OneHotCoded = pd.get_dummies(df_Cat_attrib)
df_Cat_OneHotCoded.head()

# Fit and transform to the parameters
df_imputed_Num = pd.DataFrame(imputer_Num.fit_transform(df_test[continous_attrib]))
df_imputed_Num.columns = continous_attrib

df_imputed_Cat = pd.DataFrame(imputer_Cat_Bin.fit_transform(df_Cat_OneHotCoded[df_Cat_OneHotCoded.columns]))
df_imputed_Cat.columns = df_Cat_OneHotCoded.columns

df_imputed_binary = pd.DataFrame(imputer_Num.fit_transform(df_test[binary_attrib]))
df_imputed_binary.columns = binary_attrib

test_imputed_df = pd.concat([df_imputed_Num,df_imputed_Cat,df_imputed_binary], axis=1).dropna()

train_imputed_df.head(5)
test_imputed_df.head(5)

"
652,29626958,10,"# The number of features in the train and test data are different.  Concat and use the get_dummies to even 
# them out such that training and test data have the same number of attributes
train_objs_num = len(train_imputed_df)
dataset = pd.concat(objs=[train_imputed_df, test_imputed_df], axis=0,sort='False')
train_X_df = dataset[:train_objs_num].copy()
test_X_df = dataset[train_objs_num:].copy()

train_X_df = train_X_df.round(decimals=2)
test_X_df = test_X_df.round(decimals=2)

train_X_df.columns.values
test_X_df.head(5)"
653,29626958,11,"#Distribution of train data
fig = plt.figure(figsize=(20,30))
for i in range(int(len(train_X_df.columns)-1)):
    fig.add_subplot(40,5,i+1)
    sns.distplot(train_X_df.iloc[:,i+1].dropna())
    plt.xlabel(train_X_df.columns[i])
plt.show()"
654,29626958,12,"threshold = 0.9
# Absolute value correlation matrix - train data
corr_matrix_train = train_X_df.corr().abs()
corr_matrix_train.head()

# test data
corr_matrix_test = test_X_df.corr().abs()
corr_matrix_test.head()"
655,29626958,13,"# Upper triangle of correlations - train data
upper = corr_matrix_train.where(np.triu(np.ones(corr_matrix_train.shape), k=1).astype(np.bool))
# Select columns with correlations above threshold
to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]
print(to_drop)
print('There are %d columns to remove.' % (len(to_drop)))
#Drop the columns with high correlations
train_X_df = train_X_df.drop(columns = to_drop)"
656,29626958,14,"# Upper triangle of correlations - test data
upper = corr_matrix_test.where(np.triu(np.ones(corr_matrix_test.shape), k=1).astype(np.bool))
# Select columns with correlations above threshold
to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]
print(to_drop)
print('There are %d columns to remove.' % (len(to_drop)))
#Drop the columns with high correlations
test_X_df = test_X_df.drop(columns = to_drop)
train_X_df = train_X_df.drop(columns = 'd1_hematocrit_max')"
657,29626958,15,"print(train_X_df.shape)
print(test_X_df.shape)
"
658,29626958,16,"from sklearn.model_selection import train_test_split
X_train, X_eval, Y_train, Y_eval = train_test_split(train_X_df, y_train, test_size=0.15, stratify=y_train)
X_train.shape, X_eval.shape, Y_train.shape, Y_eval.shape"
659,29626958,17,"from sklearn.model_selection import  GridSearchCV, RandomizedSearchCV
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import KFold
from xgboost import XGBClassifier
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import StratifiedKFold, KFold

from sklearn.metrics import (confusion_matrix, precision_recall_curve, auc,
                             roc_curve, recall_score, classification_report, f1_score,
                             precision_recall_fscore_support, roc_auc_score)

gkf = KFold(n_splits=3, shuffle=True, random_state=42).split(X=X_train, y=Y_train)
fit_params_of_xgb = {
    ""early_stopping_rounds"":100, 
    ""eval_metric"" : 'auc', 
    ""eval_set"" : [(X_eval,Y_eval)],
    # 'callbacks': [lgb.reset_parameter(learning_rate=learning_rate_010_decay_power_099)],
    'verbose': 500,
}
params = {
    'booster': [""gbtree""],
    'learning_rate': [0.01],
    'n_estimators': [3000],#range(1000, 2000, 3000),#range(100, 500, 100)
    'min_child_weight': [1],#1
    'gamma': [0],
    'subsample': [0.4],
    'colsample_bytree': [0.8],
    'max_depth': [4],
    ""scale_pos_weight"": [1],
    ""reg_alpha"":[1],#0.08
}
xgb_estimator = XGBClassifier(
    objective='binary:logistic',
    silent=True,
)

gsearch = GridSearchCV(
    estimator=xgb_estimator,
    param_grid=params,
    scoring='roc_auc',
    n_jobs=-1,
    cv=gkf, verbose=3
)
xgb_model = gsearch.fit(X=X_train, y=Y_train, **fit_params_of_xgb)
gsearch.best_params_, gsearch.best_score_"
660,29626958,18,"fit_params_of_xgb = {
    ""early_stopping_rounds"":100, 
    ""eval_metric"" : 'auc', 
    ""eval_set"" : [(X_eval,Y_eval)],
    # 'callbacks': [lgb.reset_parameter(learning_rate=learning_rate_010_decay_power_099)],
    'verbose': 500
}
xgb_estimator = XGBClassifier(
    objective='binary:logistic',
    silent=True,    
    booster= ""gbtree"",
    learning_rate= 0.01,
    n_estimators=3000,#range(1000, 2000, 3000),#range(100, 500, 100)
    min_child_weight= 1,#1
    gamma= 0,
    subsample= 0.4,
    colsample_bytree= 0.8,
    max_depth= 4,
    scale_pos_weight=1,
    reg_alpha=1,#0.08
)
xgb_estimator.fit(X=X_train, y=Y_train, **fit_params_of_xgb)
gsearch.best_params_, gsearch.best_score_"
661,29626958,19,"Y_pred = xgb_estimator.predict(test_X_df)
submission = pd.DataFrame({
        ""encounter_id"": test_df[""encounter_id""],
        ""hospital_death"": Y_pred
    })
submission.to_csv(""hospital_death.csv"",index=False)"
662,28701986,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the ""../input/"" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# Any results you write to the current directory are saved as output."
663,28701986,1,!pip install fastai==0.7.0
664,28701986,2,"from fastai.imports import *
from fastai.structured import *
from pandas_summary import DataFrameSummary
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
from IPython.display import display
from sklearn import metrics"
665,28701986,3,!pip install ggplot
666,28701986,4,"import pathlib
PATH = '../input/fifa2019wages'
working_path = '/kaggle/working/'

path = pathlib.Path(PATH)
path_w = pathlib.Path(working_path)"
667,28701986,5,!head -n 100000 {path}/FifaTrainNew.csv > {path_w}/FifaTrainNew.csv
668,28701986,6,"df_raw = pd.read_csv(f'{working_path}/FifaTrainNew.csv', low_memory=False, 
                     parse_dates=[""Joined"",'Contract Valid Until'])"
669,28701986,7,df_raw.head()
670,28701986,8,df_raw.tail().T
671,28701986,9,"def display_all(df):
    with pd.option_context(""display.max_rows"", 1000, ""display.max_columns"", 1000): 
        display(df)"
672,28701986,10,display_all(df_raw.tail().T)
673,28701986,11,df_raw['Contract Valid Until'].unique()
674,28701986,12,display_all(df_raw.describe(include='all').T)
675,28701986,13,"df_raw = df_raw.drop('Ob' , axis = 1)"
676,28701986,14,train_cats(df_raw)
677,28701986,15,df_raw.Club.unique()
678,28701986,16,display_all(df_raw.head())
679,28701986,17,display_all(df_raw.isnull().sum().sort_index()/len(df_raw))
680,28701986,18,"c=0
for col in df_raw.columns:
    if(str(df_raw[col].dtype)!=""category""):
        print(""'""+col+""',"")"
681,28701986,19,df_raw['LongPassing']
682,28701986,20,"add_datepart(df_raw, 'Joined')
add_datepart(df_raw, 'Contract Valid Until')"
683,28701986,21,"df_trn, y_trn, nas= proc_df(df_raw,y_fld= 'WageNew')"
684,28701986,22,df_raw
685,28701986,23,"m = RandomForestRegressor(n_jobs=-1)
m.fit(df_trn, y)
m.score(df_trn,y)"
686,28701986,24,?proc_df
687,28701986,25,len(df)
688,28701986,26,"def split_vals(a,n): return a[:n], a[n:]
n_valid = 6000
n_trn = len(df_trn)-n_valid
X_train, X_valid = split_vals(df_trn, n_trn)
y_train, y_valid = split_vals(y_trn, n_trn)
raw_train, raw_valid = split_vals(df_raw, n_trn)"
689,28701986,27,"def print_score(m,imp_cols=None):
    if(imp_cols is not None):
        res = [ m.score(X_train[imp_cols], y_train), m.score(X_valid[imp_cols], y_valid)]
    else:
        res = [ m.score(X_train, y_train), m.score(X_valid, y_valid)]
    if hasattr(m, 'oob_score_'): res.append(m.oob_score_)
    print(res)"
690,28701986,28,len(df_raw)
691,28701986,29,"#set_rf_samples(6000)
reset_rf_samples()"
692,28701986,30,??set_rf_samples
693,28701986,31,"m = RandomForestRegressor(n_estimators=60, min_samples_leaf=3,max_features=0.5, n_jobs=-1, oob_score=True)
m.fit(X_train, y_train)
print_score(m)"
694,28701986,32,"%time preds = np.stack([t.predict(X_valid) for t in m.estimators_])
np.mean(preds[:,0]), np.std(preds[:,0])"
695,28701986,33,"def get_preds(t): return t.predict(X_valid)
%time preds = np.stack(parallel_trees(m, get_preds))
np.mean(preds[:,0]), np.std(preds[:,0])"
696,28701986,34,display_all(df_trn)
697,28701986,35,"fi = rf_feat_importance(m, df_trn); fi[:10]"
698,28701986,36,"fi.plot('cols', 'imp', figsize=(10,6), legend=False);"
699,28701986,37,"def plot_fi(fi): return fi.plot('cols', 'imp', 'barh', figsize=(12,7), legend=False)"
700,28701986,38,plot_fi(fi[:30]);
701,28701986,39,plot_fi(fi[:12]);
702,28701986,40,to_keep = fi[fi.imp>0.005].cols; len(to_keep)
703,28701986,41,to_keep
704,28701986,42,"df_keep = df_trn[to_keep].copy()
X_train, X_valid = split_vals(df_keep, n_trn)"
705,28701986,43,"m = RandomForestRegressor(n_estimators=80, min_samples_leaf=3, max_features=0.5,
                          n_jobs=-1, oob_score=True)
m.fit(X_train, y_train)
print_score(m)"
706,28701986,44,"fi = rf_feat_importance(m, df_keep)
plot_fi(fi);"
707,28701986,45,fi
708,28701986,46,"df_trn2, y_trn, nas = proc_df(df_raw, 'WageNew', max_n_cat=7)
X_train, X_valid = split_vals(df_trn2, n_trn)

m = RandomForestRegressor(n_estimators=80, min_samples_leaf=3, max_features=0.6, n_jobs=-1, oob_score=True)
m.fit(X_train, y_train)
print_score(m)"
709,28701986,47,"fi = rf_feat_importance(m, df_trn2)
plot_fi(fi[:25]);"
710,28701986,48,from scipy.cluster import hierarchy as hc
711,28701986,49,?scipy.stats.spearmanr
712,28701986,50,"corr = np.round(scipy.stats.spearmanr(df_keep).correlation, 4)
corr_condensed = hc.distance.squareform(1-corr)
z = hc.linkage(corr_condensed, method='average')
fig = plt.figure(figsize=(16,10))
dendrogram = hc.dendrogram(z, labels=df_keep.columns, orientation='left', leaf_font_size=16)
plt.show()"
713,28701986,51,"def get_oob(df):
    m = RandomForestRegressor(n_estimators=80, min_samples_leaf=5, max_features=0.6, n_jobs=-1, oob_score=True)
    x, _ = split_vals(df, n_trn)
    m.fit(x, y_train)
    return m.oob_score_"
714,28701986,52,get_oob(df_keep)
715,28701986,53,df_keep.columns
716,28701986,54,"for c in ( 'LCM', 'RCM', 'RAM', 'CAM', 'RW' , 'LM'):
    print(c, get_oob(df_keep.drop(c, axis=1)))"
717,28701986,55,"to_drop = ['CM', 'CAM', 'RW']
get_oob(df_keep.drop(to_drop, axis=1))"
718,28701986,56,"df_keep.drop(to_drop, axis=1, inplace=True)
X_train, X_valid = split_vals(df_keep, n_trn)"
719,28701986,57,"np.save('/kaggle/working/keep_cols.npy', np.array(df_keep.columns))"
720,28701986,58,"keep_cols = np.load('/kaggle/working/keep_cols.npy' , allow_pickle=True)
df_keep = df_trn[keep_cols]"
721,28701986,59,reset_rf_samples()
722,28701986,60,"m = RandomForestRegressor(n_estimators=80, min_samples_leaf=4, max_features=0.5, n_jobs=-1, oob_score=True)
m.fit(X_train, y_train)
print_score(m)"
723,28701986,61,"from pdpbox import pdp
from plotnine import *"
724,28701986,62,reset_rf_samples()
725,28701986,63,"df_trn2, y_trn, nas = proc_df(df_raw, 'WageNew', max_n_cat=7)
X_train, X_valid = split_vals(df_trn2, n_trn)
m = RandomForestRegressor(n_estimators=80, min_samples_leaf=3, max_features=0.6, n_jobs=-1, oob_score=True)
m.fit(X_train, y_train);"
726,28701986,64,print_score(m)
727,28701986,65,"plot_fi(rf_feat_importance(m, df_trn2)[:10]);"
728,28701986,66,"df_raw.plot('Reactions', 'JoinedElapsed', 'scatter', alpha=0.01, figsize=(10,8));"
729,28701986,67,sum(df_raw['Reactions']>45)
730,28701986,68,"x_all = get_sample(df_raw[df_raw.Reactions>40], 500)"
731,28701986,69,??get_sample
732,28701986,70,!pip install scikit-misc
733,28701986,71,"ggplot(x_all, aes('Reactions', 'WageNew'))+stat_smooth(se=True, method='loess')"
734,28701986,72,"x = get_sample(X_train[X_train.Reactions>45], 500)
def plot_pdp(feat, clusters=None, feat_name=None):
    feat_name = feat_name or feat
    p = pdp.pdp_isolate(m, x, x.columns, feat)
    return pdp.pdp_plot(p, feat_name, plot_lines=True,
                        cluster=clusters is not None,
                        n_cluster_centers=clusters)"
735,28701986,73,plot_pdp('Reactions')
736,28701986,74,"feats = ['JoinedElapsed', 'Reactions']
p = pdp.pdp_interact(m, x, x.columns, feats)
pdp.pdp_interact_plot(p, feats)"
737,28701986,75,df_keep.describe
738,28701986,76,df_keep.Age.describe
739,28701986,77,!pip install treeinterpreter
740,28701986,78,from treeinterpreter import treeinterpreter as ti
741,28701986,79,
742,28701986,80,"df_raw = pd.read_csv(f'{working_path}/FifaTrainNew.csv', low_memory=False, 
                     parse_dates=[""Joined"",'Contract Valid Until'])"
743,28701986,81,obj_cols = df_raw.dtypes[df_raw.dtypes == object].index.tolist()
744,28701986,82,"for col in obj_cols:
    print(f'{col}\t\t{df_raw[col].unique()}')"
745,28701986,83,"df_raw.drop('Ob',axis=1,inplace=True)"
746,28701986,84,train_cats(df_raw)
747,28701986,85,"for col in obj_cols:
    print(f'{col}\t{df_raw[col].unique()}')"
748,28701986,86,"add_datepart(df_raw, 'Joined')
add_datepart(df_raw, 'Contract Valid Until')
"
749,28701986,87,"def split_vals(a,n): return a[:n],a[n:]

df_trn, y_trn, nas = proc_df(df_raw,y_fld='WageNew', max_n_cat=10)

n_valid = 7500
n_train = len(df_trn) - n_valid

X_train, X_valid = split_vals(df_trn,n_train)
y_train, y_valid = split_vals(y_trn, n_train)
train_raw, valid_raw = split_vals(df_raw, n_train)"
750,28701986,88,set_rf_samples(5000)
751,28701986,89,"def print_score(m):
    res = [ m.score(X_train, y_train), m.score(X_valid, y_valid)]
    if hasattr(m, 'oob_score_'): res.append(m.oob_score_)
    print(res)"
752,28701986,90,"def print_score(m,imp_cols=None):
    if(imp_cols is not None):
        res = [ m.score(X_train[imp_cols], y_train), m.score(X_valid[imp_cols], y_valid)]
    else:
        res = [ m.score(X_train, y_train), m.score(X_valid, y_valid)]
    if hasattr(m, 'oob_score_'): res.append(m.oob_score_)
    print(res)"
753,28701986,91,"m = RandomForestRegressor(n_estimators=1000,min_samples_leaf=3,max_features=0.555,n_jobs=-1,warm_start=True,oob_score=True)
m.fit(X_train,y_train)
print_score(m)"
754,28701986,92,?RandomForestRegressor
755,28701986,93,"fi = rf_feat_importance(m,df_trn)
def plot_fi(fi): return fi.plot('cols','imp','barh',figsize=(12,10),legend=True)"
756,28701986,94,plot_fi(fi[fi['imp'] > 0.005])
757,28701986,95,"imps_cols = fi[fi['imp'] > 0.002]['cols'].tolist()
imps_cols"
758,28701986,96,len(imps_cols)
759,28701986,97,X_train.columns.tolist()
760,28701986,98,"m = RandomForestRegressor(n_estimators=1000,min_samples_leaf=3,max_features=0.555,n_jobs=-1,warm_start=True,oob_score=True)
m.fit(X_train[imps_cols],y_train)
print_score(m,imps_cols)"
761,28701986,99,"fi = rf_feat_importance(m,X_train[imps_cols])
def plot_fi(fi): return fi.plot('cols','imp','barh',figsize=(12,10),legend=True)
plot_fi(fi)"
762,28701986,100,plot_fi(fi[:25])
763,28701986,101,"imps_cols = fi[:25]['cols'].tolist()
df_trn[imps_cols].dtypes"
764,28701986,102,from scipy.cluster import hierarchy as hc
765,28701986,103,"corr = np.round(scipy.stats.spearmanr(X_train[imps_cols]).correlation, 4)
corr_condensed = hc.distance.squareform(1-corr)
z = hc.linkage(corr_condensed, method='average')
fig = plt.figure(figsize=(16,10))
dendrogram = hc.dendrogram(z, labels=X_train[imps_cols].columns, orientation='left', leaf_font_size=16)
plt.show()"
766,28701986,104,"def get_oob(df):
    m = RandomForestRegressor(n_estimators=1000,min_samples_leaf=3,max_features=0.555,n_jobs=-1,warm_start=True,oob_score=True)
    x,_ = split_vals(df,n_train)
    m.fit(x,y_train)
    return m.oob_score_"
767,28701986,105,get_oob(X_train[imps_cols])
768,28701986,106,"for col in ['RCM','LCM','CM','RDM','LDM','CDM']:
    print(col,get_oob(X_train[imps_cols].drop(col,axis=1)))"
769,28701986,107,"to_drop = ['RCM','LCM','CM','RDM','LDM','CDM']
get_oob(X_train[imps_cols].drop(to_drop,axis=1))
"
770,28701986,108,"m = RandomForestRegressor(n_estimators=1000,min_samples_leaf=3,max_features=0.555,n_jobs=-1,warm_start=True,oob_score=True)
cols = X_train[imps_cols].drop(to_drop,axis=1).columns
m.fit(X_train[cols],y_train)
print_score(m,imp_cols=cols)"
771,28701986,109,"df_test = pd.read_csv(f'{path}/FifaNoY.csv', low_memory=False, 
                     parse_dates=[""Joined"",'Contract Valid Until'])"
772,28701986,110,train_cats(df_test)
773,28701986,111,"add_datepart(df_test, 'Joined')
add_datepart(df_test, 'Contract Valid Until')
"
774,28701986,112,"df_test1, y_trn, nas = proc_df(df_test, max_n_cat=10)
"
775,28701986,113,predictions = m.predict(df_test1[cols])
776,28701986,114,?proc_df
777,28701986,115,"submission = pd.DataFrame({'Ob':df_test1['Ob'],'WageNew':predictions})"
778,28701986,116,submission
779,28701986,117,"filename = '/kaggle/working/FIFA2019Wages.csv'

submission.to_csv(filename,index=False)

print('Saved file: ' + filename)"
780,28701986,118,ls
781,28701986,119,pwd
782,28701986,120,
783,28537757,0,"# Data manipulation packages import.
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt # Data visualization
import seaborn as sns # More data visualization (pretty)

# Supress warnings.
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

# Sklearn and XGBoost import.
from sklearn.decomposition import PCA
from sklearn.linear_model import Lasso, Ridge, LogisticRegression
from xgboost import XGBClassifier, XGBRegressor
from sklearn.model_selection import RandomizedSearchCV
from sklearn.model_selection import StratifiedKFold
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, AdaBoostClassifier
from sklearn.svm import SVC
from sklearn.metrics import roc_auc_score as auc
from sklearn.cluster import AgglomerativeClustering as LinkCluster
from sklearn.cluster import KMeans

# Tensorflow import.
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras import Model

# File Download.
import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))"
784,28537757,1,"def normalise(data, test_data):
    mean = data.mean()
    std = data.std()
    
    data = (data - mean) / std
    test_data = (test_data - mean) / std
    
    return data, test_data

def prep(data, mean=None, std=None, fancy_nan=False):
    '''
    Separate the data into X and Y values, normalise the X data, and handle NaN values.
    '''
    
    # Handle the NaN values based on the algorithm.
    data = handle_nan(data, fancy=fancy_nan)
    
    # Separate the y-values if the exist.
    if 'y' in data.columns:
        features = data.drop(['y'], axis=1)
        labels = data['y']
    else:
        features = data
    
    if 'y' in data.columns:
        return features, labels
    else:
        return features

def handle_nan(data, fancy=False):
    '''
    Handle NaN values.
    '''
    
    if fancy:  # If we want to use Lasso regression to fill in NaN values.
        # Deal with NaN numbers.
        fill_X_open = data[['d_open_interest', 'transacted_qty', 'opened_position_qty ']].dropna(axis=0)
        fill_X_closed = data[['d_open_interest', 'transacted_qty', 'closed_position_qty']].dropna(axis=0)

        # The scale is a bit off (i.e. max d_open_interestis 1361 vs 42 of opened_position_qty), but it should be okay
        lasso_fill_open = Lasso()
        lasso_fill_open.fit(fill_X_open.drop(['opened_position_qty '], axis=1), fill_X_open['opened_position_qty '])

        lasso_fill_closed = Lasso()
        lasso_fill_closed.fit(fill_X_closed.drop(['closed_position_qty'], axis=1), fill_X_closed['closed_position_qty'])

        # Fill in the NaN values for the training set.
        elements = data.loc[data[['closed_position_qty', 'opened_position_qty ']].isna().any(axis=1), ['d_open_interest', 'transacted_qty']]

        data.loc[elements.index, 'opened_position_qty '] = lasso_fill_open.predict(elements)
        data.loc[elements.index, 'closed_position_qty'] = lasso_fill_closed.predict(elements)
    else:  # If we just want to drop the columns with NaN values.
        data = data.fillna(data.mean())
        pass # data = data.dropna(axis=1)
        
    return data


def pca(data, test_data, run=True, n_components=None):
    '''
    Run PCA on the data set.
    '''
    
    if run:
        # Run PCA on the data set.
        transform = PCA(n_components=n_components)
        transform.fit(data)

        # Transform the data set.
        data = transform.fit_transform(data)
        test_data = transform.fit_transform(test_data)
        
        # Convert data back to DataFrame
        data = pd.DataFrame(data)
        test_data = pd.DataFrame(test_data)
    
    return data, test_data


def logistic(array):
    '''
    Convert raw scores to probabilities.
    '''
    
    array = (array - array.mean()) / array.std()
    return np.exp(array) / (1 + np.exp(array))


def export(predictions, train_data=None, path='preds.csv'):
    '''
    Export a list of predictions to a CSV file.
    '''
    
    # Convert the predictions to the proper format.
    predictions = pd.DataFrame(predictions)
    predictions[1] = predictions.index + train_data.shape[0] if train_data is not None else 0
    predictions = predictions[[1, 0]]
    predictions.columns = ['id', 'Predicted']
    predictions = predictions.set_index(predictions.index + 1)
    
    # Load the predictions to a CSV file.
    predictions.to_csv(path, index=False)"
785,28537757,2,"def predict(model, data):
    return model.predict(data)[:, 0]"
786,28537757,3,"def add_features(data):
    data.loc[:, 'diff'] = data.loc[:, 'ask1'] - data.loc[:, 'bid1']
    data.loc[:, 'bid1/ask1'] = data.loc[:, 'bid1'] / data.loc[:, 'ask1']
    data.loc[:, 'bid_spread'] = data.loc[:, 'bid1'] / data.loc[:, 'bid5']
    data.loc[:, 'ask_spread'] = data.loc[:, 'ask1'] / data.loc[:, 'ask5']
    data.loc[:, 'imbalance1'] = data.loc[:, 'ask1vol'] / data.loc[:, 'bid1vol']
    data.loc[:, 'imbalance2'] = data.loc[:, 'ask2vol'] / data.loc[:, 'bid2vol']
    data.loc[:, 'imbalance3'] = data.loc[:, 'ask3vol'] / data.loc[:, 'bid3vol']
    data.loc[:, 'imbalance4'] = data.loc[:, 'ask4vol'] / data.loc[:, 'bid4vol']
    data.loc[:, 'imbalance5'] = data.loc[:, 'ask5vol'] / data.loc[:, 'bid5vol']
    data.loc[:, 'spread'] = data.loc[:, 'ask1vol'] - data.loc[:, 'bid1vol']
    data.loc[:, 'imb*spread'] = data.loc[:, 'imbalance1'] * data.loc[:, 'spread']
    data.loc[:, 'askvol*ask_spread'] = data.loc[:, 'ask1vol'] * data.loc[:, 'ask_spread']
    data.loc[:, 'askvol*diff'] = data.loc[:, 'ask1vol'] * data.loc[:, 'diff']
    data.loc[:, 'bidvol*diff'] = data.loc[:, 'bid1vol'] * data.loc[:, 'diff']
    
    return data

def add_clusters(data, test_data, clusters=5):
    # K means clustering.
    for c in clusters:
        print(c)
        cluster = KMeans(n_clusters=c, n_jobs=-1).fit(data)
    
        data.loc[:, f'kmeans/{c}'] = cluster.labels_
        test_data.loc[:, f'kmeans/{c}'] = cluster.predict(test_data)
    
    return data, test_data"
787,28537757,4,"CLUSTERS = [10, 15, 20]

testing = False

if testing:
    # Load the data, and split it into training and testing.
    input_data = pd.read_csv('/kaggle/input/caltech-cs155-2020/train.csv').set_index('id')
    input_data = input_data.iloc[np.random.permutation(len(input_data))].reset_index(drop=True)

    # Split the data into training and testing.
    t = 4 * input_data.shape[0] // 5

    train = input_data.iloc[:t]
    test = input_data.iloc[t:]
else:
    train = pd.read_csv('/kaggle/input/caltech-cs155-2020/train.csv').set_index('id')
    test = pd.read_csv('/kaggle/input/caltech-cs155-2020/test.csv').set_index('id')

# Let's add some features!!
train = add_features(train)
test = add_features(test)

# Prep the data and split into X and Y.
mean = train.drop(['y'], axis=1).mean(axis=0)
std = train.drop(['y'], axis=1).std(axis=0)

X, Y = prep(train, mean=mean, std=std)

if testing:
    Xt, Yt = prep(test, mean=mean, std=std)
else:
    Xt = prep(test, mean=mean, std=std)

# Add clusters.
# X, Xt = add_clusters(X, Xt, clusters=CLUSTERS)

# Normalise the data.
X, Xt = normalise(X, Xt)"
788,28537757,5,"N_MODELS = 1

# Keep track of predictions for each model.
train_preds = pd.DataFrame()
test_preds = pd.DataFrame()

# Add the XGB models to the ensemble.
for i in range(N_MODELS):
    print(f'XGB {i + 1}')
    # Randomly sample the columns
    cols = np.random.choice(X.columns, size=np.random.randint(3, 7), replace=False)
    
    # Train the model.
    model = XGBRegressor(n_jobs=-1, tree_method='gpu_hist', objective='reg:squarederror')
    model.fit(X[cols], Y)

    # Make the predictions.
    train_preds.loc[:, f'xgb{i}'] = model.predict(X[cols])
    test_preds.loc[:, f'xgb{i}'] = model.predict(Xt[cols])

# Add the Logistic models to the ensemble.
for i in range(N_MODELS):
    print(f'Log {i + 1}')
    # Randomly sample the columns.
    cols = np.random.choice(X.columns, size=np.random.randint(3, 7), replace=False)
    
    # Train the model.
    model = LogisticRegression(C=30)
    model.fit(X[cols], Y)

    # Make the predictions.
    train_preds.loc[:, f'log{i}'] = model.predict_proba(X[cols])[:, 1]
    test_preds.loc[:, f'log{i}'] = model.predict_proba(Xt[cols])[:, 1]
    

# One hot encode the labels so that the probability of each can be attained.
Y_1hc = pd.DataFrame(Y)
Y_1hc.columns = ['1']
Y_1hc['0'] = 1 - Y_1hc['1']  

# Add Neural Nets to the ensemble.
for i in range(1):
    print(f'Neural Net {i + 1}')
    # Create Neural Network.
    model = Sequential()
    model.add(Dense(100, input_dim=X.shape[1], activation='relu'))
    model.add(Dense(100, activation='relu'))
    model.add(Dense(70, activation='relu'))
    model.add(Dense(50, activation='relu'))
    model.add(Dense(30, activation='relu'))
    model.add(Dense(20, activation='relu'))
    model.add(Dense(10, activation='relu'))
    model.add(Dense(5, activation='relu'))
    model.add(Dense(2, activation='softmax'))

    # Train the model.
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', tf.keras.metrics.AUC()])
    model.fit(X, Y_1hc, epochs=1, batch_size=32)

    # Make the predictions.
    train_preds.loc[:, f'nn{i}'] = predict(model, X)
    test_preds.loc[:, f'nn{i}'] = predict(model, Xt)

print('Complete.')"
789,28537757,6,"# Normalise the pooled predictions.
mean = train_preds.mean()
std = train_preds.std()

train_preds = (train_preds - mean) / std
test_preds = (test_preds - mean) / std

# If we want, we can pass the original data to the nn as well.
original = False
if original:
    train_preds = pd.concat([train_preds, X.reset_index(drop=True)], axis=1)
    test_preds = pd.concat([test_preds, Xt.reset_index(drop=True)], axis=1)"
790,28537757,7,"# One hot encode the labels so that the probability of each can be attained.
Y_1hc = pd.DataFrame(Y)
Y_1hc.columns = ['1']
Y_1hc['0'] = 1 - Y_1hc['1']

# Create Neural Network.
nn_model = Sequential()
nn_model.add(Dense(200, input_dim=train_preds.shape[1], activation='relu'))
nn_model.add(Dense(300, activation='relu'))
nn_model.add(Dense(200, activation='relu'))
nn_model.add(Dense(50, activation='relu'))
nn_model.add(Dense(2, activation='softmax'))

# Train the model.
nn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', tf.keras.metrics.AUC()])
nn_model.fit(train_preds, Y_1hc, epochs=1, batch_size=32)

# In-sample error
preds_in = predict(nn_model, train_preds)
train_acc = auc(Y, preds_in)
print(f'Train Acc: {train_acc:.8f}')

# Out-of-sample error.
preds = predict(nn_model, test_preds)

if testing:
    test_acc = auc(Yt, preds)
    print(f'Test Acc:  {test_acc:.8f}')"
791,28537757,8,"if not testing:
    export(preds, X)"
1843,29007121,0,"import pandas as pd
from keras.models import Sequential
from keras.layers import Dense, Dropout, LeakyReLU
from keras.wrappers.scikit_learn import KerasClassifier
from keras.utils import np_utils
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold
from sklearn.preprocessing import LabelEncoder
from sklearn.pipeline import Pipeline
import numpy as np"
1844,29007121,1,"data=pd.read_csv('/kaggle/input/bitsf312-lab1/train.csv',index_col=0)"
1845,29007121,2,"data = data.replace({'?': np.nan})
data = data.fillna(data.mean())
data.fillna(value = data.mode().loc[0], inplace = True)"
1846,29007121,3,data.head(20)
1847,29007121,4,"data = pd.get_dummies(data, columns= ['Size'], prefix = ['Size'])
data.head(100)"
1848,29007121,5,data = data.astype('float64')
1849,29007121,6,data.dtypes
1850,29007121,7,# data = data.fillna(value = data.mean())
1851,29007121,8,"# from sklearn.preprocessing import StandardScaler

# data_scaled = data.copy()
# scaler = StandardScaler()

# data_scaled = pd.DataFrame(scaler.fit_transform(data_scaled), columns=data.columns)"
1852,29007121,9,"
data.head()
"
1853,29007121,10,"X = data.drop('Class', axis= 1)
#X = X.drop('ID', axis = 1)
y=data['Class']
y.value_counts()"
1854,29007121,11,"# encoder = LabelEncoder()
# encoder.fit(y)
# encoded_Y = encoder.transform(y)
# # convert integers to dummy variables (i.e. one hot encoded)
# dummy_y = np_utils.to_categorical(encoded_Y)
 
# # define baseline model
# def baseline_model():
# 	# create model
# 	model = Sequential()
# 	model.add(Dense(8, input_dim=13, activation='relu'))
# 	model.add(Dense(7, activation='softmax'))
# 	# Compile model
# 	model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
# 	return model
 
# estimator = KerasClassifier(build_fn=baseline_model, epochs=200, batch_size=5, verbose=0)
# kfold = KFold(n_splits=10, shuffle=True)
# results = cross_val_score(estimator, X, dummy_y, cv=kfold)
# print(""Baseline: %.2f%% (%.2f%%)"" % (results.mean()*100, results.std()*100))

"
1855,29007121,12,"encoder = LabelEncoder()
encoder.fit(y)
encoded_Y = encoder.transform(y)
# convert integers to dummy variables (i.e. one hot encoded)
dummy_y = np_utils.to_categorical(encoded_Y)
 
# define baseline model

# create model
model = Sequential()
model.add(Dense(64, input_dim=13, activation='relu'))
model.add(Dropout(rate = 0.2))
model.add(Dense(32, activation='relu'))
model.add(Dropout(rate = 0.2))
model.add(Dense(32, activation='tanh'))
model.add(LeakyReLU(alpha=0.3))
model.add(Dropout(rate = 0.2))
model.add(Dense(6, activation='softmax'))
# Compile model
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
history=model.fit(X, dummy_y, validation_split=0.2, epochs=200,batch_size=15)
model.summary()

 
# estimator = KerasClassifier(build_fn=baseline_model, epochs=200, batch_size=5, verbose=0)
# kfold = KFold(n_splits=10, shuffle=True)
# results = cross_val_score(estimator, X, dummy_y, cv=kfold)
# print(""Baseline: %.2f%% (%.2f%%)"" % (results.mean()*100, results.std()*100))"
1856,29007121,13,"test = pd.read_csv('/kaggle/input/bitsf312-lab1/test.csv')
test = test.replace({'?': np.nan})
test = test.fillna(data.mean())
test.fillna(value = test.mode().loc[0], inplace = True)"
1857,29007121,14,test.head()
1858,29007121,15,"test = pd.get_dummies(test, columns= ['Size'], prefix = ['size'])"
1859,29007121,16,"y2 = test['ID']
test = test.drop(['ID'], axis = 1)"
1860,29007121,17,"
# test_scaled = test.copy()
# scaler = StandardScaler()

# test_scaled = pd.DataFrame(scaler.fit_transform(test_scaled), columns=test.columns)"
1861,29007121,18,test.head()
1862,29007121,19,
1863,29007121,20,ans = model.predict(test)
1864,29007121,21,labels = ans.argmax(-1)
1865,29007121,22,len(labels)
1866,29007121,23,"labels = pd.DataFrame(data = labels, index = y2)"
1867,29007121,24,labels.head(20)
1868,29007121,25,labels.to_csv('nnfl4.csv')
1869,29007121,26,"from IPython.display import HTML
import pandas as pd
import numpy as np
import base64def create_download_link(df, title = ""Download CSV file"", filename = ""data.csv""):    csv = df.to_csv(index=False)    b64 = base64.b64encode(csv.encode())    payload = b64.decode()html='<adownload=""{filename}""href=""data:text/csv;base64,{payload}""target=""_blank"">{title}</a>'    html = html.format(payload=payload,title=title,filename=filename)    
return HTML(html)create_download_link(<submission_DataFrame_name>)"
1870,29007121,27,
2551,32052230,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the ""../input/"" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# Any results you write to the current directory are saved as output."
2552,32052230,1,"trainurl = ""/kaggle/input/hecmontrealdeeplearningcourse/train.csv""
testurl = ""/kaggle/input/hecmontrealdeeplearningcourse/test.csv""
referenceurl = ""/kaggle/input/hecmontrealdeeplearningcourse/reference.csv""
texturl = ""/kaggle/input/hecmontrealdeeplearningcourse/text.csv""
sampleurl = ""/kaggle/input/hecmontrealdeeplearningcourse/sample.csv""
train1 = pd.read_csv(trainurl)
test1 = pd.read_csv(testurl)
reference = pd.read_csv(referenceurl)
text = pd.read_csv(texturl)
sample = pd.read_csv(sampleurl)
test1.tail(5)"
2553,32052230,2,"newtest = pd.merge(test1, text, how = 'left', on = 'id')
newtrain = pd.merge(train1, text, how = 'left', on = 'id')
newtrain.head()
newtrain['label'].max()
print(text[""title""].map(len).max()) #Finding max length of sentence"
2554,32052230,3,classes =  list(set(newtrain.title))
2555,32052230,4,"train = newtrain[['title', 'label']]"
2556,32052230,5,"train = pd.get_dummies(train, prefix='', prefix_sep='', columns=['label',])
"
2557,32052230,6,"import pandas as pd, numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
2558,32052230,7,"import re, string
re_tok = re.compile(f'([{string.punctuation}])')
def tokenize(s): return re_tok.sub(r' \1 ', s).split()"
2559,32052230,8,"label_cols = ['0','1', '2','3','4']"
2560,32052230,9,"n = train.shape[0]
vec = TfidfVectorizer(ngram_range=(1,2), tokenizer=tokenize,
               min_df=3, max_df=0.9, strip_accents='unicode', use_idf=1,
               smooth_idf=1, sublinear_tf=1 )
trn_term_doc = vec.fit_transform(train.title)
test_term_doc = vec.transform(newtest.title)"
2561,32052230,10,"#A sparse matrix is created 
trn_term_doc, test_term_doc"
2562,32052230,11,"def pr(y_i, y):
    p = x[y==y_i].sum(0)
    return (p+1) / ((y==y_i).sum()+1)"
2563,32052230,12,"x = trn_term_doc
test_x = test_term_doc
"
2564,32052230,13,"def get_mdl(y):
    y = y.values
    r = np.log(pr(1,y) / pr(0,y))
    m = LogisticRegression(C=4, dual=True, solver = 'liblinear')
    x_nb = x.multiply(r)
    return m.fit(x_nb, y), r"
2565,32052230,14,"preds = np.zeros((len(newtest), len(label_cols)))

for i, j in enumerate(label_cols):
    print('fit', j)
    m,r = get_mdl(train[j])
    preds[:,i] = m.predict_proba(test_x.multiply(r))[:,1]"
2566,32052230,15,"submid = pd.DataFrame({'id': newtest[""id""]})"
2567,32052230,16,"answer1 = pd.concat([submid, pd.DataFrame(preds, columns = label_cols)], axis=1)
answer1.to_csv('answer1.csv', index=False)"
2568,32052230,17,answer1.head()
2569,32052230,18,answer1.shape
2570,32052230,19,"import sys, os, re, csv, codecs, numpy as np, pandas as pd

from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation
from keras.layers import Bidirectional, GlobalMaxPool1D
from keras.models import Model
from keras import initializers, regularizers, constraints, optimizers, layers"
2571,32052230,20,"EMBEDDING_FILE= ""/kaggle/input/glovedataset/glove.6B.50d.txt"""
2572,32052230,21,"embed_size = 50 # how big is each word vector
max_features = 20000 # how many unique words to use (i.e num rows in embedding vector)
maxlen = 100 # max number of words in a title"
2573,32052230,22,"list_classes = [""0"", ""1"", ""2"", ""3"", ""4""]"
2574,32052230,23,y = train[list_classes].values
2575,32052230,24,"list_sentences_train = train[""title""].values
list_sentences_test = newtest['title'].values"
2576,32052230,25,"tokenizer = Tokenizer(num_words=max_features)
tokenizer.fit_on_texts(list(list_sentences_train))
list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)
list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)
X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)
X_te = pad_sequences(list_tokenized_test, maxlen=maxlen)"
2577,32052230,26,"def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')
embeddings_index = dict(get_coefs(*o.strip().split()) for o in open(EMBEDDING_FILE))"
2578,32052230,27,"all_embs = np.stack(embeddings_index.values())
emb_mean,emb_std = all_embs.mean(), all_embs.std()
emb_mean,emb_std"
2579,32052230,28,"word_index = tokenizer.word_index
nb_words = min(max_features, len(word_index))+1
embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))
for word, i in word_index.items():
    if i >= max_features: continue
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
2580,32052230,29,embedding_matrix.shape
2581,32052230,30,"inp = Input(shape=(maxlen,))
inp.shape"
2582,32052230,31,"inp = Input(shape=(maxlen,))

x = Embedding(9322, embed_size, weights=[embedding_matrix])(inp)
x = Bidirectional(LSTM(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)
x = GlobalMaxPool1D()(x)
x = Dense(50, activation=""relu"")(x)
x = Dropout(0.1)(x)
x = Dense(5, activation=""softmax"")(x)
model = Model(inputs=inp, outputs=x)
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
2583,32052230,32,"model.fit(X_t, y, batch_size=32, epochs=8, validation_split=0.1);"
2584,32052230,33,"y_test = model.predict([X_te], batch_size=1024, verbose=1)"
2585,32052230,34,y_test.shape
2586,32052230,35,print(y_test)
2587,32052230,36,"answer2 = pd.concat([submid, pd.DataFrame(y_test, columns = label_cols)], axis=1)
answer2.to_csv('answer2.csv', index=False)"
2588,32052230,37,answer2.head()
2589,32052230,38,"answer3 = answer2.copy()
answer3[label_cols] = (answer1[label_cols] + answer1[label_cols]) / 2"
2590,32052230,39,answer3.head()
2591,32052230,40,df = answer3[label_cols]
2592,32052230,41,type(df)
2593,32052230,42,print(df)
2594,32052230,43,df = df.to_numpy()
2595,32052230,44,"df2 = np.argmax(df,axis=1)"
2596,32052230,45,
2597,32052230,46,"prediction10 = pd.DataFrame({'id':newtest.id,'label':df2},columns=['id', 'label'])
prediction10.to_csv('prediction10.csv', index=False)
prediction10.shape"
2598,32052230,47,prediction10.head()
2599,30296882,0,"import os
import numpy as np 
import pandas as pd 
import json"
2600,30296882,1,"%%time

with open('../input/herbarium-2020-fgvc7/nybg2020/train/metadata.json', ""r"", encoding=""ISO-8859-1"") as file:
    train = json.load(file)

train_img = pd.DataFrame(train['images'])
train_ann = pd.DataFrame(train['annotations']).drop(columns='image_id')
train_df = train_img.merge(train_ann, on='id')
train_df.head()"
2601,30296882,2,"%%time

with open('../input/herbarium-2020-fgvc7/nybg2020/test/metadata.json', ""r"", encoding=""ISO-8859-1"") as file:
    test = json.load(file)

test_df = pd.DataFrame(test['images'])
test_df.head()"
2602,30296882,3,train_df['category_id'].value_counts()
2603,30296882,4,"from sklearn import preprocessing

le = preprocessing.LabelEncoder()
le.fit(train_df['category_id'])
train_df['category_id'] = le.transform(train_df['category_id'])"
2604,30296882,5,"# ====================================================
# Library
# ====================================================

import sys

import gc
import os
import random
import time
from contextlib import contextmanager
from pathlib import Path

import cv2
from PIL import Image
import numpy as np
import pandas as pd
import scipy as sp

import sklearn.metrics

from functools import partial

import torch
import torch.nn as nn
from torch.optim import Adam, SGD
from torch.optim.lr_scheduler import CosineAnnealingLR
from torch.utils.data import DataLoader, Dataset

from albumentations import Compose, Normalize, Resize
from albumentations.pytorch import ToTensorV2


device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device"
2605,30296882,6,"# ====================================================
# Utils
# ====================================================

@contextmanager
def timer(name):
    t0 = time.time()
    LOGGER.info(f'[{name}] start')
    yield
    LOGGER.info(f'[{name}] done in {time.time() - t0:.0f} s.')

    
def init_logger(log_file='train.log'):
    from logging import getLogger, DEBUG, FileHandler,  Formatter,  StreamHandler
    
    log_format = '%(asctime)s %(levelname)s %(message)s'
    
    stream_handler = StreamHandler()
    stream_handler.setLevel(DEBUG)
    stream_handler.setFormatter(Formatter(log_format))
    
    file_handler = FileHandler(log_file)
    file_handler.setFormatter(Formatter(log_format))
    
    logger = getLogger('Herbarium')
    logger.setLevel(DEBUG)
    logger.addHandler(stream_handler)
    logger.addHandler(file_handler)
    
    return logger

LOG_FILE = 'train.log'
LOGGER = init_logger(LOG_FILE)


def seed_torch(seed=777):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True

SEED = 777
seed_torch(SEED)"
2606,30296882,7,"N_CLASSES = 32093


class TrainDataset(Dataset):
    def __init__(self, df, labels, transform=None):
        self.df = df
        self.labels = labels
        self.transform = transform
        
    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        file_name = self.df['file_name'].values[idx]
        file_path = f'../input/herbarium-2020-fgvc7/nybg2020/train/{file_name}'
        image = cv2.imread(file_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        label = self.labels.values[idx]
        
        if self.transform:
            augmented = self.transform(image=image)
            image = augmented['image']
        
        return image, label"
2607,30296882,8,"HEIGHT = 128
WIDTH = 128


def get_transforms(*, data):
    
    assert data in ('train', 'valid')
    
    if data == 'train':
        return Compose([
            Resize(HEIGHT, WIDTH),
            Normalize(
                mean=[0.485, 0.456, 0.406],
                std=[0.229, 0.224, 0.225],
            ),
            ToTensorV2(),
        ])
    
    elif data == 'valid':
        return Compose([
            Resize(HEIGHT, WIDTH),
            Normalize(
                mean=[0.485, 0.456, 0.406],
                std=[0.229, 0.224, 0.225],
            ),
            ToTensorV2(),
        ])"
2608,30296882,9,"from sklearn.model_selection import StratifiedKFold

DEBUG = False

if DEBUG:
    folds = train_df.sample(n=10000, random_state=0).reset_index(drop=True).copy()
else:
    folds = train_df.copy()
train_labels = folds['category_id'].values
kf = StratifiedKFold(n_splits=2)
for fold, (train_index, val_index) in enumerate(kf.split(folds.values, train_labels)):
    folds.loc[val_index, 'fold'] = int(fold)
folds['fold'] = folds['fold'].astype(int)
folds.to_csv('folds.csv', index=None)
folds.head()"
2609,30296882,10,"FOLD = 0
trn_idx = folds[folds['fold'] != FOLD].index
val_idx = folds[folds['fold'] == FOLD].index
print(trn_idx.shape, val_idx.shape)"
2610,30296882,11,"train_dataset = TrainDataset(folds.loc[trn_idx].reset_index(drop=True), 
                             folds.loc[trn_idx]['category_id'], 
                             transform=get_transforms(data='train'))
valid_dataset = TrainDataset(folds.loc[val_idx].reset_index(drop=True), 
                             folds.loc[val_idx]['category_id'], 
                             transform=get_transforms(data='valid'))"
2611,30296882,12,"batch_size = 512

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)"
2612,30296882,13,"import torchvision.models as models

model = models.resnet18(pretrained=True)
model.avgpool = nn.AdaptiveAvgPool2d(1)
model.fc = nn.Linear(model.fc.in_features, N_CLASSES)"
2613,30296882,14,"from torch.optim.lr_scheduler import ReduceLROnPlateau
from sklearn.metrics import f1_score
from tqdm import tqdm


with timer('Train model'):
    
    n_epochs = 1
    lr = 4e-4
    
    model.to(device)
    
    optimizer = Adam(model.parameters(), lr=lr, amsgrad=False)
    scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.75, patience=5, verbose=True, eps=1e-6)
    
    criterion = nn.CrossEntropyLoss()
    best_score = 0.
    best_loss = np.inf
    
    for epoch in range(n_epochs):
        
        start_time = time.time()

        model.train()
        avg_loss = 0.

        optimizer.zero_grad()

        for i, (images, labels) in tqdm(enumerate(train_loader)):

            images = images.to(device)
            labels = labels.to(device)
            
            y_preds = model(images)
            loss = criterion(y_preds, labels)
            
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()

            avg_loss += loss.item() / len(train_loader)
            
        model.eval()
        avg_val_loss = 0.
        preds = np.zeros((len(valid_dataset)))

        for i, (images, labels) in enumerate(valid_loader):
            
            images = images.to(device)
            labels = labels.to(device)
            
            with torch.no_grad():
                y_preds = model(images)
            
            preds[i * batch_size: (i+1) * batch_size] = y_preds.argmax(1).to('cpu').numpy()

            loss = criterion(y_preds, labels)
            avg_val_loss += loss.item() / len(valid_loader)
        
        scheduler.step(avg_val_loss)
            
        score = f1_score(folds.loc[val_idx]['category_id'].values, preds, average='macro')

        elapsed = time.time() - start_time

        LOGGER.debug(f'  Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  F1: {score:.6f}  time: {elapsed:.0f}s')

        if score>best_score:
            best_score = score
            LOGGER.debug(f'  Epoch {epoch+1} - Save Best Score: {best_score:.6f} Model')
            torch.save(model.state_dict(), f'fold{FOLD}_best_score.pth')

        if avg_val_loss<best_loss:
            best_loss = avg_val_loss
            LOGGER.debug(f'  Epoch {epoch+1} - Save Best Loss: {best_loss:.4f} Model')
            torch.save(model.state_dict(), f'fold{FOLD}_best_loss.pth')"
2614,33188284,0,"import matplotlib.pyplot as plt
from PIL import Image
import seaborn as sns
import pandas as pd
import numpy as np
import math
import sklearn
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error,r2_score,mean_absolute_error
import matplotlib.pyplot as plt
import seaborn as sb
from sklearn import preprocessing
from sklearn.feature_extraction.text import CountVectorizer
import time
import warnings 
warnings.filterwarnings('ignore')"
2615,33188284,1,"import json, codecs
with codecs.open(""../input/herbarium-2020-fgvc7/nybg2020/train/metadata.json"", 'r',
                 encoding='utf-8', errors='ignore') as f:
    train= json.load(f)
    
with codecs.open(""../input/herbarium-2020-fgvc7/nybg2020/test/metadata.json"", 'r',
                 encoding='utf-8', errors='ignore') as f:
    test = json.load(f)"
2616,33188284,2,"display(train.keys())
"
2617,33188284,3,"train_data = pd.DataFrame(train['annotations'])
display(train_data)"
2618,33188284,4,"Cat = pd.DataFrame(train['categories'])
display(Cat)"
2619,33188284,5,"train_img = pd.DataFrame(train['images'])
train_img.columns = ['file_name', 'height', 'image_id', 'license', 'width']
display(train_img)"
2620,33188284,6,"licenses = pd.DataFrame(train['licenses'])
display(licenses)"
2621,33188284,7,"regions = pd.DataFrame(train['regions'])
display(regions)"
2622,33188284,8,"train_data = train_data.merge(Cat, on='id', how='outer')
train_data = train_data.merge(train_img, on='image_id', how='outer')
train_data = train_data.merge(regions, on='id', how='outer')"
2623,33188284,9,"print(train_data.info())

display(train_data)"
2624,33188284,10,"test_data = pd.DataFrame(test['images'])
test_data.columns = ['file_name', 'height', 'image_id', 'license', 'width']
print(test_data.info())
display(test_data)"
2625,33188284,11,"print(len(train_data.id.unique()))
"
2626,33188284,12,"sub = pd.DataFrame()
sub['Id'] = test_data.image_id
sub['Predicted'] = list(map(int, np.random.randint(1, 32093, (test_data.shape[0]))))
display(sub)
sub.to_csv('submission.csv', index=False)"
2627,30313596,0,"import time
start_time = time.time()"
2628,30313596,1,"import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        if filename.endswith('.jpg'):
            break
        print(os.path.join(dirname, filename))"
2629,30313596,2,"sample_sub = pd.read_csv('../input/herbarium-2020-fgvc7/sample_submission.csv')
display(sample_sub)"
2630,30313596,3,"import json, codecs
with codecs.open(""../input/herbarium-2020-fgvc7/nybg2020/train/metadata.json"", 'r',
                 encoding='utf-8', errors='ignore') as f:
    train_meta = json.load(f)
    
with codecs.open(""../input/herbarium-2020-fgvc7/nybg2020/test/metadata.json"", 'r',
                 encoding='utf-8', errors='ignore') as f:
    test_meta = json.load(f)"
2631,30313596,4,display(train_meta.keys())
2632,30313596,5,"train_df = pd.DataFrame(train_meta['annotations'])
display(train_df)"
2633,30313596,6,"train_cat = pd.DataFrame(train_meta['categories'])
train_cat.columns = ['family', 'genus', 'category_id', 'category_name']
display(train_cat)"
2634,30313596,7,"train_img = pd.DataFrame(train_meta['images'])
train_img.columns = ['file_name', 'height', 'image_id', 'license', 'width']
display(train_img)"
2635,30313596,8,"train_reg = pd.DataFrame(train_meta['regions'])
train_reg.columns = ['region_id', 'region_name']
display(train_reg)"
2636,30313596,9,"train_df = train_df.merge(train_cat, on='category_id', how='outer')
train_df = train_df.merge(train_img, on='image_id', how='outer')
train_df = train_df.merge(train_reg, on='region_id', how='outer')"
2637,30313596,10,"print(train_df.info())
display(train_df)"
2638,30313596,11,"na = train_df.file_name.isna()
keep = [x for x in range(train_df.shape[0]) if not na[x]]
train_df = train_df.iloc[keep]"
2639,30313596,12,"dtypes = ['int32', 'int32', 'int32', 'int32', 'object', 'object', 'object', 'object', 'int32', 'int32', 'int32', 'object']
for n, col in enumerate(train_df.columns):
    train_df[col] = train_df[col].astype(dtypes[n])
print(train_df.info())
display(train_df)"
2640,30313596,13,"test_df = pd.DataFrame(test_meta['images'])
test_df.columns = ['file_name', 'height', 'image_id', 'license', 'width']
print(test_df.info())
display(test_df)"
2641,30313596,14,"train_df.to_csv('full_train_data.csv', index=False)
test_df.to_csv('full_test_data.csv', index=False)"
2642,30313596,15,"print(""Total Unique Values for each columns:"")
print(""{0:10s} \t {1:10d}"".format('train_df', len(train_df)))
for col in train_df.columns:
    print(""{0:10s} \t {1:10d}"".format(col, len(train_df[col].unique())))"
2643,30313596,16,"family = train_df[['family', 'genus', 'category_name']].groupby(['family', 'genus']).count()
display(family.describe())"
2644,30313596,17,"from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPool2D, Flatten, BatchNormalization, Input, concatenate
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.utils import plot_model
from sklearn.model_selection import train_test_split as tts

in_out_size = (120*120) + 3 #We will resize the image to 120*120 and we have 3 outputs
def xavier(shape, dtype=None):
    return np.random.rand(*shape)*np.sqrt(1/in_out_size)

def fg_model(shape, lr=0.001):
    '''Family-Genus model receives an image and outputs two integers indicating both the family and genus index.'''
    i = Input(shape)
    
    x = Conv2D(3, (3, 3), activation='relu', padding='same', kernel_initializer=xavier)(i)
    x = Conv2D(3, (5, 5), activation='relu', padding='same', kernel_initializer=xavier)(x)
    x = MaxPool2D(pool_size=(3, 3), strides=(3,3))(x)
    x = BatchNormalization()(x)
    x = Dropout(0.5)(x)
    x = Conv2D(16, (5, 5), activation='relu', padding='same', kernel_initializer=xavier)(x)
    #x = Conv2D(16, (5, 5), activation='relu', padding='same', kernel_initializer=xavier)(x)
    x = MaxPool2D(pool_size=(5, 5), strides=(5,5))(x)
    x = BatchNormalization()(x)
    x = Dropout(0.5)(x)
    x = Flatten()(x)
    
    o1 = Dense(310, activation='softmax', name='family', kernel_initializer=xavier)(x)
    
    o2 = concatenate([o1, x])
    o2 = Dense(3678, activation='softmax', name='genus', kernel_initializer=xavier)(o2)
    
    o3 = concatenate([o1, o2, x])
    o3 = Dense(32094, activation='softmax', name='category_id', kernel_initializer=xavier)(o3)
    
    x = Model(inputs=i, outputs=[o1, o2, o3])
    
    opt = Adam(lr=lr, amsgrad=True)
    x.compile(optimizer=opt, loss=['sparse_categorical_crossentropy', 
                                   'sparse_categorical_crossentropy', 
                                   'sparse_categorical_crossentropy'],
                 metrics=['accuracy'])
    return x

model = fg_model((120, 120, 3))
model.summary()
plot_model(model, to_file='full_model_plot.png', show_shapes=True, show_layer_names=True)"
2645,30313596,18,"from tensorflow.keras.preprocessing.image import ImageDataGenerator

train_datagen = ImageDataGenerator(featurewise_center=False,
                                     featurewise_std_normalization=False,
                                     rotation_range=180,
                                     width_shift_range=0.1,
                                     height_shift_range=0.1,
                                     zoom_range=0.2)"
2646,30313596,19,"m = train_df[['file_name', 'family', 'genus', 'category_id']]
fam = m.family.unique().tolist()
m.family = m.family.map(lambda x: fam.index(x))
gen = m.genus.unique().tolist()
m.genus = m.genus.map(lambda x: gen.index(x))
display(m)"
2647,30313596,20,"train, verif = tts(m, test_size=0.2, shuffle=True, random_state=17)
train = train[:40000]
verif = verif[:10000]
shape = (120, 120, 3)
epochs = 2
batch_size = 32

model = fg_model(shape, 0.007)

#Disable the last two output layers for training the Family
for layers in model.layers:
    if layers.name == 'genus' or layers.name=='category_id':
        layers.trainable = False

#Train Family for 2 epochs
model.fit_generator(train_datagen.flow_from_dataframe(dataframe=train,
                                                      directory='../input/herbarium-2020-fgvc7/nybg2020/train/',
                                                      x_col=""file_name"",
                                                      y_col=[""family"", ""genus"", ""category_id""],
                                                      target_size=(120, 120),
                                                      batch_size=batch_size,
                                                      class_mode='multi_output'),
                    validation_data=train_datagen.flow_from_dataframe(
                        dataframe=verif,
                        directory='../input/herbarium-2020-fgvc7/nybg2020/train/',
                        x_col=""file_name"",
                        y_col=[""family"", ""genus"", ""category_id""],
                        target_size=(120, 120),
                        batch_size=batch_size,
                        class_mode='multi_output'),
                    epochs=epochs,
                    steps_per_epoch=len(train)//batch_size,
                    validation_steps=len(verif)//batch_size,
                    verbose=1,
                    workers=8,
                    use_multiprocessing=False)

#Reshuffle the inputs
train, verif = tts(m, test_size=0.2, shuffle=True, random_state=17)
train = train[:40000]
verif = verif[:10000]

#Make the Genus layer Trainable
for layers in model.layers:
    if layers.name == 'genus':
        layers.trainable = True
        
#Train Family and Genus for 2 epochs
model.fit_generator(train_datagen.flow_from_dataframe(dataframe=train,
                                                      directory='../input/herbarium-2020-fgvc7/nybg2020/train/',
                                                      x_col=""file_name"",
                                                      y_col=[""family"", ""genus"", ""category_id""],
                                                      target_size=(120, 120),
                                                      batch_size=batch_size,
                                                      class_mode='multi_output'),
                    validation_data=train_datagen.flow_from_dataframe(
                        dataframe=verif,
                        directory='../input/herbarium-2020-fgvc7/nybg2020/train/',
                        x_col=""file_name"",
                        y_col=[""family"", ""genus"", ""category_id""],
                        target_size=(120, 120),
                        batch_size=batch_size,
                        class_mode='multi_output'),
                    epochs=epochs,
                    steps_per_epoch=len(train)//batch_size,
                    validation_steps=len(verif)//batch_size,
                    verbose=1,
                    workers=8,
                    use_multiprocessing=False)

#Reshuffle the inputs
train, verif = tts(m, test_size=0.2, shuffle=True, random_state=17)
train = train[:40000]
verif = verif[:10000]

#Make the category_id layer Trainable
for layers in model.layers:
    if layers.name == 'category_id':
        layers.trainable = True
        
#Train them all for 2 epochs
model.fit_generator(train_datagen.flow_from_dataframe(dataframe=train,
                                                      directory='../input/herbarium-2020-fgvc7/nybg2020/train/',
                                                      x_col=""file_name"",
                                                      y_col=[""family"", ""genus"", ""category_id""],
                                                      target_size=(120, 120),
                                                      batch_size=batch_size,
                                                      class_mode='multi_output'),
                    validation_data=train_datagen.flow_from_dataframe(
                        dataframe=verif,
                        directory='../input/herbarium-2020-fgvc7/nybg2020/train/',
                        x_col=""file_name"",
                        y_col=[""family"", ""genus"", ""category_id""],
                        target_size=(120, 120),
                        batch_size=batch_size,
                        class_mode='multi_output'),
                    epochs=epochs,
                    steps_per_epoch=len(train)//batch_size,
                    validation_steps=len(verif)//batch_size,
                    verbose=1,
                    workers=8,
                    use_multiprocessing=False)

'''
for i in range(epochs):
    n = 1
    for X, Y in train_datagen.flow_from_dataframe(dataframe=train,
                                                  directory='../input/herbarium-2020-fgvc7/nybg2020/train/',
                                                  x_col=""file_name"",
                                                  y_col=[""family"", ""genus"", ""category_id""],
                                                  target_size=(120, 120),
                                                  batch_size=batch_size,
                                                  class_mode='multi_output'):
        model.train_on_batch(X, Y, reset_metrics=False)
        loss, fam_loss, gen_loss, cat_loss, fam_acc, gen_acc, cat_acc = model.evaluate(X, Y, verbose=False)
        if n%10==0:
            print(f""For epoch {i} batch {n}: {loss}, {fam_loss}, {gen_loss}, {cat_loss}, {fam_acc}, {gen_acc}, {cat_acc}"")
            for layers in model.layers:
                if layers.name == 'family' and fam_acc>0.90:
                    layers.trainable=False
                elif layers.name == 'genus':
                    if fam_acc>0.75:
                        layers.trainable=True
                    else:
                        layers.trainable=False
                elif layers.name == 'category_id':
                    if fam_acc>0.75 and gen_acc>0.5:
                        layers.trainable=True
                    else:
                        layers.trainable=False
        n += 1
'''"
2648,30313596,21,model.save('fg_model.h5')
2649,30313596,22,"batch_size = 32
test_datagen = ImageDataGenerator(featurewise_center=False,
                                  featurewise_std_normalization=False)

generator = test_datagen.flow_from_dataframe(
        dataframe = test_df.iloc[:10000], #Limiting the test to the first 10,000 items
        directory = '../input/herbarium-2020-fgvc7/nybg2020/test/',
        x_col = 'file_name',
        target_size=(120, 120),
        batch_size=batch_size,
        class_mode=None,  # only data, no labels
        shuffle=False)

family, genus, category = model.predict_generator(generator, verbose=1)"
2650,30313596,23,"sub = pd.DataFrame()
sub['Id'] = test_df.image_id
sub['Id'] = sub['Id'].astype('int32')
sub['Predicted'] = np.concatenate([np.argmax(category, axis=1), 23718*np.ones((len(test_df.image_id)-len(category)))], axis=0)
sub['Predicted'] = sub['Predicted'].astype('int32')
display(sub)
sub.to_csv('category_submission.csv', index=False)"
2651,30313596,24,"sub['Predicted'] = np.concatenate([np.argmax(family, axis=1), np.zeros((len(test_df.image_id)-len(family)))], axis=0)
sub['Predicted'] = sub['Predicted'].astype('int32')
display(sub)
sub.to_csv('family_submission.csv', index=False)"
2652,30313596,25,"sub['Predicted'] = np.concatenate([np.argmax(genus, axis=1), np.zeros((len(test_df.image_id)-len(genus)))], axis=0)
sub['Predicted'] = sub['Predicted'].astype('int32')
display(sub)
sub.to_csv('genus_submission.csv', index=False)"
2653,30313596,26,"end_time = time.time()
total = end_time - start_time
h = total//3600
m = (total%3600)//60
s = total%60
print(""Total time spent: %i hours, %i minutes, and %i seconds"" %(h, m, s))"
2654,30042790,0,"import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        if filename.endswith('.jpg'):
            break
        print(os.path.join(dirname, filename))"
2655,30042790,1,"sample_sub = pd.read_csv('../input/herbarium-2020-fgvc7/sample_submission.csv')
display(sample_sub)"
2656,30042790,2,"import json, codecs
with codecs.open(""../input/herbarium-2020-fgvc7/nybg2020/train/metadata.json"", 'r',
                 encoding='utf-8', errors='ignore') as f:
    train_meta = json.load(f)
    
with codecs.open(""../input/herbarium-2020-fgvc7/nybg2020/test/metadata.json"", 'r',
                 encoding='utf-8', errors='ignore') as f:
    test_meta = json.load(f)"
2657,30042790,3,display(train_meta.keys())
2658,30042790,4,"train_df = pd.DataFrame(train_meta['annotations'])
display(train_df)"
2659,30042790,5,"train_cat = pd.DataFrame(train_meta['categories'])
train_cat.columns = ['family', 'genus', 'category_id', 'categort_name']
display(train_cat)"
2660,30042790,6,"train_img = pd.DataFrame(train_meta['images'])
train_img.columns = ['file_name', 'height', 'image_id', 'license', 'width']
display(train_img)"
2661,30042790,7,"train_reg = pd.DataFrame(train_meta['regions'])
train_reg.columns = ['region_id', 'region_name']
display(train_reg)"
2662,30042790,8,"train_df = train_df.merge(train_cat, on='category_id', how='outer')
train_df = train_df.merge(train_img, on='image_id', how='outer')
train_df = train_df.merge(train_reg, on='region_id', how='outer')"
2663,30042790,9,"print(train_df.info())

display(train_df)
"
2664,30042790,10,"na = train_df.file_name.isna()
keep = [x for x in range(train_df.shape[0]) if not na[x]]
train_df = train_df.iloc[keep]"
2665,30042790,11,"dtypes = ['int32', 'int32', 'int32', 'int32', 'object', 'object', 'object', 'object', 'int32', 'int32', 'int32', 'object']
for n, col in enumerate(train_df.columns):
    train_df[col] = train_df[col].astype(dtypes[n])
print(train_df.info())
display(train_df)"
2666,30042790,12,"test_df = pd.DataFrame(test_meta['images'])
test_df.columns = ['file_name', 'height', 'image_id', 'license', 'width']
print(test_df.info())
display(test_df)"
2667,30042790,13,"#train_df.to_csv('full_train_data.csv', index=False)
#test_df.to_csv('full_test_data.csv', index=False)"
2668,30042790,14,print(len(train_df.category_id.unique()))
2669,30042790,15,"sub = pd.DataFrame()
sub['Id'] = test_df.image_id
sub['Predicted'] = list(map(int, np.random.randint(1, 32093, (test_df.shape[0]))))
display(sub)
sub.to_csv('submission.csv', index=False)"
2670,30646081,0,"import os, json, time, sys, math
import numpy as np
import pandas as pd 
from matplotlib import pyplot as plt
from PIL import Image

if 'google.colab' in sys.modules:
    %tensorflow_version 2.x
import tensorflow as tf

print(""Tensorflow version "" + tf.__version__)
AUTO = tf.data.experimental.AUTOTUNE 

start_time = time.time()"
2671,30646081,1,"df = pd.read_csv('../input/data-for-datatse-herbarium/data.csv')
df1 = df.copy()
df1['file_name'] = df['file_name'].map(lambda x: x.split('/')[-1])
df1.head()
"
2672,30646081,2,"TRAIN_PATTERN = '/kaggle/input/herbarium-2020-fgvc7/nybg2020/train/images/*/*/*.jpg'
TEST_PATTERN = '/kaggle/input/herbarium-2020-fgvc7/nybg2020/test/images/*/*/*.jpg'"
2673,30646081,3,shard_size = 32
2674,30646081,4,"def display_from_dataset(dataset):
    plt.figure(figsize=(13,13))
    subplot=331
    for i, (image, label) in enumerate(dataset):
        plt.subplot(subplot)
        plt.axis('off')
        plt.imshow(image.numpy().astype(np.uint8))
        plt.title(label.numpy().decode(""utf-8""), fontsize=16)
        subplot += 1
        if i==8:
            break
    plt.tight_layout()
    plt.subplots_adjust(wspace=0.1, hspace=0.1)
    plt.show()
    
def decode_jpeg_and_label(filename):
    bits = tf.io.read_file(filename)
    image = tf.image.decode_jpeg(bits)
    label = tf.strings.split(tf.expand_dims(filename, axis=-1), sep='/')
    label = label.values[-1]
    return image, label

def resize_and_crop_image(image, label):
    w = tf.shape(image)[0]
    h = tf.shape(image)[1]
    tw = w//5
    th = h//5
    resize_crit = (w * th) / (h * tw)
    image = tf.cond(resize_crit < 1,
                    lambda: tf.image.resize(image, [w*tw/w, h*tw/w]), # if true
                    lambda: tf.image.resize(image, [w*th/h, h*th/h])  # if false
                   )
    nw = tf.shape(image)[0]
    nh = tf.shape(image)[1]
    image = tf.image.crop_to_bounding_box(image, (nw - tw) // 2, (nh - th) // 2, tw, th)
    return image, label

def recompress_image(image, label):
    height = tf.shape(image)[0]
    width = tf.shape(image)[1]
    image = tf.cast(image, tf.uint8)
    image = tf.image.encode_jpeg(image, optimize_size=True, chroma_downsampling=False)
    return image, label, height, width"
2675,30646081,5,"filenames = tf.data.Dataset.list_files(TRAIN_PATTERN) 
dataset   = filenames.map(decode_jpeg_and_label, num_parallel_calls=AUTO)
dataset   = dataset.map(resize_and_crop_image, num_parallel_calls=AUTO) 
dataset   = dataset.map(recompress_image, num_parallel_calls=AUTO)
dataset   = dataset.batch(shard_size)
dataset   = dataset.prefetch(AUTO)"
2676,30646081,6,!mkdir tfrecords
2677,30646081,7,"
def _bytestring_feature(list_of_bytestrings):
    return tf.train.Feature(bytes_list=tf.train.BytesList(value=list_of_bytestrings))

def _int_feature(list_of_ints): # int64
    return tf.train.Feature(int64_list=tf.train.Int64List(value=list_of_ints))

def _float_feature(list_of_floats): # float32
    return tf.train.Feature(float_list=tf.train.FloatList(value=list_of_floats))
  

def to_tfrecord(tfrec_filewriter, img_bytes, label, family, genus, category_id, width, height):
    one_hot_family = np.eye(310)[family]
    one_hot_genus = np.eye(3678)[genus]
    one_hot_category_id = np.eye(32094)[category_id]

    feature = {
      ""image"": _bytestring_feature([img_bytes]),
      ""label"":  _bytestring_feature([label]),
        
      ""family"": _int_feature([family]),
      ""genus"": _int_feature([genus]),
      ""category_id"": _int_feature([category_id]),
        
      ""one_hot_family"": _float_feature(one_hot_family.tolist()),
      ""one_hot_genus"": _float_feature(one_hot_genus.tolist()),
      ""one_hot_category_id"": _float_feature(one_hot_category_id.tolist()),
        
      ""size"":  _int_feature([width, height])
    }
    return tf.train.Example(features=tf.train.Features(feature=feature))

print(""Writing TFRecords"")
stoped = 0
for shard, (image, label, height, width) in enumerate(dataset):
    if stoped > 0:
        break
    stoped +=1
    shard_size = image.numpy().shape[0]
    filename   = './tfrecords/' + ""{:02d}-{}.tfrec"".format(shard, shard_size)
  
    with tf.io.TFRecordWriter(filename) as out_file:
        for i in range(shard_size):
            lbl         = label.numpy()[i]
            family      = df1.loc[df1.file_name == lbl.decode('utf-8')]['family'].values[0]
            genus       = df1.loc[df1.file_name == lbl.decode('utf-8')]['genus'].values[0]
            category_id = df1.loc[df1.file_name == lbl.decode('utf-8')]['category_id'].values[0]
            
            example = to_tfrecord(out_file,
                            image.numpy()[i],
                            lbl,
                            family, 
                            genus, 
                            category_id,
                            height.numpy()[i],
                            width.numpy()[i])
            out_file.write(example.SerializeToString())
        print(""Wrote file {} containing {} records"".format(filename, shard_size))"
2678,30646081,8,"def read_tfrecord(example):
    features = {
        ""image"": tf.io.FixedLenFeature([], tf.string), 
        ""label"": tf.io.FixedLenFeature([], tf.string),
        
        ""family"": tf.io.FixedLenFeature([], tf.int64), 
        ""genus"": tf.io.FixedLenFeature([], tf.int64), 
        ""category_id"": tf.io.FixedLenFeature([], tf.int64), 
        
        ""one_hot_family"": tf.io.VarLenFeature(tf.float32) ,
        ""one_hot_genus"": tf.io.VarLenFeature(tf.float32) ,
        ""one_hot_category_id"": tf.io.VarLenFeature(tf.float32) ,

        ""size"": tf.io.FixedLenFeature([2], tf.int64) 
    }
    example = tf.io.parse_single_example(example, features)
    width = example['size'][0]
    height  = example['size'][1]
    image = tf.image.decode_jpeg(example['image'], channels=3)
    image = tf.reshape(image, [width,height, 3])
    
    label = example['label']
    
  
    return image, label

option_no_order = tf.data.Options()
option_no_order.experimental_deterministic = False

filenames = tf.io.gfile.glob('/kaggle/working/tfrecords/' + ""*.tfrec"")
dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO)
dataset = dataset.with_options(option_no_order)
dataset = dataset.map(read_tfrecord, num_parallel_calls=AUTO)
dataset = dataset.shuffle(300)"
2679,30646081,9,display_from_dataset(dataset)
2680,30646081,10,"end_time = time.time()
total = end_time - start_time
h = total//3600
m = (total%3600)//60
s = total%60
print(""Total time spent: %i hours, %i minutes, and %i seconds"" %(h, m, s))"
2681,34782705,0,"## Herbarium 2020 - FGVC7
# read and prerpocess the json file.
import json
import codecs
import pandas as pd

# split the dataset.
from sklearn.model_selection import train_test_split

# deeplearning framework.
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Dense, Input, concatenate, Flatten, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.utils import plot_model
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import tensorflow as tf

# other python package.
from PIL import Image
import numpy as np
import os

## I. Data Preprocessing define function :
#-------------------<Data Preprocessing>----------------------------------------#
##   Given path of json file, return the DataFrame type data 
##   ** (merge all info into 1 DataFrame) **
def load_data(train_path, test_path):
    
    def load_jsonfile(json_path):
        with codecs.open(json_path,'r',encoding='utf-8',errors='ignore') as file:
            meta_data = json.load(file)  ## return dictionary type data.
        return meta_data 
    
    train_meta = load_jsonfile(train_path)
    test_meta = load_jsonfile(test_path)
    
    ## Extract from each information into corresponding DataFrame.
    ## 1. Annotation information.
    annotations = pd.DataFrame(train_meta['annotations'])
    ## Debug view : 
    #print(""\n\nAnnotations dataframe infomation : \n"")
    #print(annotations.info())
    
    ## 2. Categories information.
    categories = pd.DataFrame(train_meta['categories'])
    categories.columns = ['family', 'genus', 'category_id', 'category_name'] ## replace columns name.
    ## Debug view :
    #print(""\n\nCategories dataframe infomation : \n"")
    #print(categories.info())
    
    ## 3. Images information.
    images = pd.DataFrame(train_meta['images'])
    images.columns = ['file_name', 'height', 'image_id','license','width']
    ## Debug view :
    #print(""\n\nImages dataframe infomation : \n"")
    #print(images.info())
   
    ## 4. Regions information.
    regions = pd.DataFrame(train_meta['regions'])
    regions.columns = ['region_id','name']
    ## Debug view :
    #print(""\n\nRegions dataframe infomation : \n"")
    #print(regions.info())
    
    ## Merge all dataframe to get raw training dataframe.
    raw_train_df = annotations.merge(categories,on='category_id', how=""left""
                                     ).merge(images, on=""image_id"", how=""outer""
                                            ).merge(regions, on=""region_id"", how=""outer"")
    
    raw_train_df = raw_train_df[['file_name','family','genus','category_id']]
    ## Debug view :
    #print(""\n\nRaw dataframe of training set : \n"")
    #print(raw_train_df.info())
    #print(""\n\n The family, genus data type is object, which should be transformed into int type \
    #       via indicator (use index of list to present the corresponding content)"")
    
    ## Preprocess the dataframe.
    ## 1. unique the element in the frame, \
    ##           and replace the 'category type' data into corresponding 'index type' data in the list.
    name_list = raw_train_df['family'].unique().tolist()
    raw_train_df.loc[:,'family'] = raw_train_df['family'].map(lambda x:name_list.index(x))
    genus_list = raw_train_df['genus'].unique().tolist()
    raw_train_df.loc[:,'genus'] = raw_train_df['genus'].map(lambda x:genus_list.index(x))
    
    
    ## 2. redeclare the data type to shrink the memory usage.
    train_df = raw_train_df.astype({'family':'int16','genus':'int16','category_id':'int16'})
    
    ## Extract the test dataframe information.
    raw_test_df = pd.DataFrame(test_meta['images'])
    raw_test_df.columns = ['file_name', 'height', 'image_id','license','width']
    test_df = raw_test_df[['image_id','file_name']]
    return train_df, test_df

def crop(batch_x):
    cut1 = int(0.1*batch_x.shape[1])
    cut2 = int(0.05*batch_x.shape[2])
    return batch_x[:,cut1:-cut1,cut2:-cut2]
## II. Build Classifier :
#----------------------<self-build>------------------------#
def get_cnn_classifier(img_shape):
    '''
    def prerpocess_img(x):
        if isinstance(x, np.ndarray):  ## Training phase, instance input.
            return x/127.5 -1 
        else:  ## Symbolic preprocess.
            return tf.sub(tf.div(x, 127.5), 1)
    '''
    def build_classifier(img_shape):
        model = Sequential({
            Conv2D(filters=64, kernel_size=5, activation='relu',\
                   stride=2, input_shape=img_shape),
            MaxPooling2D(2),
            Conv2D(128, 3, activation='relu', padding='same'),
            Conv2D(128, 3, activation='relu', padding='same'),
            MaxPooling2D(2),
            Conv2D(256, 3, activation='relu', padding='same'),
            Conv2D(256, 3, activation='relu', padding='same'),
            MaxPooling2D(2),
            Flatten(),
            Dense(128, activation='relu'),
            Droupout(0.25),
            Dense(64, activation='relu'),
            Droupout(0.2),
            Dense(32094, activation='softmax')      
        })
        return model
    
#-------------------<keras applications>-------------------#
## VGG19
def get_VGG_based_Classifier(img_shape):
    ## Closure function : 
    def __preprocess_vgg(x):
        ## Take a HR image [-1, 1], convert to [0, 255], then to input for VGG network

        if isinstance(x, np.ndarray):  ## Training phase, instance input.
            return preprocess_input(x) 
        else:  ## Symbolic preprocess.
            return Lambda(lambda x: preprocess_input(x))(x)
    
    def use_raw_VGG():
        pass
        
    ## Build Basic VGG19 network :
    #prepro_VGG19 = prepro_no_top_VGG()
    #prepro_VGG19()
    ## The modification of output prediction :  output 320000 specimens
    ## Please take the reference of keras VGG application.
    #VGG_classifier = Model(inputs=prepro_VGG19.input, output=)
    
    VGG_classifier = VGG19(weights=None, include_top=True, \
                      input_shape=img_shape, classes=32000)
    VGG_classifier.compile(loss='mae',optimizer='adam')
    return VGG_classifier
    
## ResNet50
in_out_size = (120*120) + 3
def xavier(shape, dtype=None):
    return np.random.rand(*shape)*np.sqrt(1/in_out_size)
def create_model():
    actual_shape = (crop(np.zeros((1,img_shape[0],img_shape[1],img_shape[2]))).shape)[1:]
    i = Input(actual_shape)
    x = ResNet50(weights='imagenet', include_top=False, input_shape=actual_shape, pooling='max')(i)
    x = Dropout(0.5)(x)
    x = Flatten()(x)
    o1 = Dense(310, activation='softmax', name='family', kernel_initializer=xavier)(x)
    
    o2 = concatenate([o1, x])
    o2 = Dense(3678, activation='softmax', name='genus', kernel_initializer=xavier)(o2)
    
    o3 = concatenate([o1, o2, x])
    o3 = Dense(32094, activation='softmax', name='category_id', kernel_initializer=xavier)(o3)
    model = Model(inputs=i,outputs=[o1,o2,o3])
    model.layers[1].trainable = False
    model.get_layer('genus').trainable = False
    model.get_layer('category_id').trainable = False
    return model

def compile(model,learning_rate=0.005):
    model.compile(optimizer=Adam(learning_rate=0.005),loss=[""sparse_categorical_crossentropy"",
                                     ""sparse_categorical_crossentropy"",
                                     ""sparse_categorical_crossentropy""],
                                metrics=['accuracy'])
    


#TRAINSTEPS = (X_train.shape[0]//batchsize)+1
#VALSTEPS = (X_dev.shape[0]//batchsize)+1

## Main part :
# self-define params : *(argparse)
batch_size=64
learning_rate=0.01
epochs=1000
img_shape = (200, 200, 3) # original height = 1000, width= 682    219673 667    212347 676    190476
train_path = '/kaggle/input/herbarium-2020-fgvc7/nybg2020/train/metadata.json'
test_path = '/kaggle/input/herbarium-2020-fgvc7/nybg2020/test/metadata.json'

## I. Load & Preprocess data :
train_df, test_df = load_data(train_path, test_path)

nmb_cat = train_df['category_id'].max()+1
nmb_gen = train_df['genus'].max()+1
nmb_fam = train_df['family'].max()+1

train_data, test_data = train_test_split(train_df, test_size=0.05, shuffle=True, random_state=13)
print(train_data.info())

## II. Build classifier :
#VGG_classifier = get_VGG_based_Classifier(img_shape)
ResNet_classifier = create_model()

## III. Image Augumentation and Build the data flow :
                ## Image Augumentation.
train_datagen = ImageDataGenerator(horizontal_flip = True, vertical_flip = True, \
                                   rotation_range = 180, zoom_range=0.3, fill_mode='nearest')
                                   
            ## setting data flow extract from DataFrame type data.
data_flow = train_datagen.flow_from_dataframe(dataframe=train_data, \
                                  directory='../input/herbarium-2020-fgvc7/nybg2020/train/', \
                                  x_col=""file_name"", y_col=[""family"", ""genus"", ""category_id""], \
                                  target_size=img_shape, batch_size=batch_size, \
                                  class_mode='multi_output')

## IV. Training phase :
## In the training phase, we'll first predict the family, and then use the family to predict the genus,
##    and so on, use the predicted family & genus to predict category_id.
#VGG_classifier.fit_generator(data_flow, epochs=epochs, verbose=2, workers=4, use_multiprocessing=False)
#VGG_classifier.save('./classifier.h5')
model = create_model()
compile(model,learning_rate)
#model.summary()
filename=""classifier.h5""
model.save_weights(filename)
print(""Weights saved to {}"".format(filename))
plot_model(model, show_shapes=True, show_layer_names=True)"
2682,34782705,1,"## V. Prediction phase :
batch_size = 32
test_datagen = ImageDataGenerator(featurewise_center=False, 
                                    featurewise_std_normalization=False)

generator = test_datagen.flow_from_dataframe(
        dataframe = test_df.iloc[:10000], #Limiting the test to the first 10,000 items
        directory = '../input/herbarium-2020-fgvc7/nybg2020/test/',
        x_col = 'file_name',
        target_size=(120, 120),
        batch_size=batch_size,
        class_mode=None,  # only data, no labels
        shuffle=False)

family, genus, category = model.predict_generator(generator, verbose=1)"
2683,34782705,2,"sub = pd.DataFrame()
sub['Id'] = test_df.image_id
sub['Id'] = sub['Id'].astype('int32')
sub['Predicted'] = np.concatenate([np.argmax(category, axis=1), 23718*np.ones((len(test_df.image_id)-len(category)))], axis=0)
sub['Predicted'] = sub['Predicted'].astype('int32')
display(sub)
sub.to_csv('category_submission.csv', index=False)"
2684,34782705,3,"sub['Predicted'] = np.concatenate([np.argmax(family, axis=1), np.zeros((len(test_df.image_id)-len(family)))], axis=0)
sub['Predicted'] = sub['Predicted'].astype('int32')
display(sub)
sub.to_csv('family_submission.csv', index=False)"
2685,34782705,4,"sub['Predicted'] = np.concatenate([np.argmax(genus, axis=1), np.zeros((len(test_df.image_id)-len(genus)))], axis=0)
sub['Predicted'] = sub['Predicted'].astype('int32')
display(sub)
sub.to_csv('genus_submission.csv', index=False)"
2686,34373570,0,"''' This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only ""../input/"" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        if filename.endswith('.jpg'):
            break
        print(os.path.join(dirname, filename))

# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session'''"
2687,34373570,1,"#The kernal does not have the memory to train this model. But here's what I had done anyways.

'''import cv2
import re
from PIL import Image
import matplotlib.pyplot as plt
from keras.preprocessing.image import img_to_array, image
from keras.applications.resnet50 import preprocess_input, decode_predictions, resnet50
from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D
from keras.layers import Dropout, Flatten, Dense, Activation
from keras.models import Sequential 
from sklearn.model_selection import train_test_split
from keras import optimizers
from keras.applications.vgg16 import VGG16
from keras.applications.vgg19 import VGG19
from keras.models import Model
from keras.preprocessing.image import ImageDataGenerator
from keras import backend as K'''"
2688,34373570,2,"#%matplotlib inline
#import matplotlib.pyplot as plt
import numpy as np  
import pandas as pd
import torch

from torch import nn
from torch import optim
#import torch.nn.functional as F
from torchvision import datasets, transforms, models"
2689,34373570,3,"import json

with open('/kaggle/input/herbarium-2020-fgvc7/nybg2020/train/metadata.json', 'r', errors='ignore') as f:
    train_metadata = json.load(f)"
2690,34373570,4,#train_metadata.keys()
2691,34373570,5,"with open('/kaggle/input/herbarium-2020-fgvc7/nybg2020/test/metadata.json', 'r', errors='ignore') as f:
    test_metadata = json.load(f)"
2692,34373570,6,#test_metadata.keys()
2693,34373570,7,sub_samp = pd.read_csv('/kaggle/input/herbarium-2020-fgvc7/sample_submission.csv')
2694,34373570,8,sub_samp
2695,34373570,9,"test_dir = '/kaggle/input/herbarium-2020-fgvc7/nybg2020/test/'
train_dir = '/kaggle/input/herbarium-2020-fgvc7/nybg2020/train/'"
2696,34373570,10,train_ann = pd.DataFrame(train_metadata['annotations'])
2697,34373570,11,#train_ann
2698,34373570,12,#train_cat = pd.DataFrame(train_metadata['categories'])
2699,34373570,13,#train_cat
2700,34373570,14,#train_imgs = pd.DataFrame(train_metadata['images'])
2701,34373570,15,#train_imgs
2702,34373570,16,#train_metadata['info']
2703,34373570,17,#test_metadata['info']
2704,34373570,18,#train_metadata['licenses']
2705,34373570,19,#test_metadata['licenses']
2706,34373570,20,#train_metadata['regions']
2707,34373570,21,test_imgs = pd.DataFrame(test_metadata['images'])
2708,34373570,22,#test_imgs
2709,34373570,23,#train_files_names = train_imgs['file_name'].values
2710,34373570,24,#train_files_names
2711,34373570,25,#test_file_names = test_imgs['file_name'].values
2712,34373570,26,#test_file_names
2713,34373570,27,X = train_ann['image_id'].values
2714,34373570,28,#X
2715,34373570,29,y = train_ann['category_id'].values
2716,34373570,30,#y.size
2717,34373570,31,"train_torch_y = torch.tensor(y, dtype=torch.long ).view(35543, 29)"
2718,34373570,32,#train_torch_y[0].shape 
2719,34373570,33,#train_ann['category_id'].nunique()
2720,34373570,34,test_X = test_imgs['id'].values
2721,34373570,35,#test_X
2722,34373570,36,"'''def convert_to_tensor(path_img):
    img = image.load_img(path_img, target_size = (224,224))
    img_arr = image.img_to_array(img)
    return np.expand_dims(img_arr, axis = 0)'''"
2723,34373570,37,"'''def convert_all_tensor(paths_imgs):
    tensor_list = [convert_to_tensor(i) for i in paths_imgs]
    return np.vstack(tensor_list)'''"
2724,34373570,38,"#train_tensors = convert_all_tensor(train_dir+train_files_names).astype('float64')/255 #took too much memory
#test_tensors = convert_all_tensor(train_dir+train_files_names).astype('float64')/255"
2725,34373570,39,"#train_tensors.shape, test_tensors.shape"
2726,34373570,40,
2727,34373570,41,
2728,34373570,42,
2729,34373570,43,"data_transforms_train = transforms.Compose([transforms.RandomRotation(30),
                                       transforms.RandomResizedCrop(224),
                                       transforms.RandomHorizontalFlip(),
                                       transforms.ToTensor(),
                                       transforms.Normalize([0.485, 0.456, 0.406], 
                                                            [0.229, 0.224, 0.225])])



image_datasets_train =  datasets.ImageFolder(train_dir, transform=data_transforms_train)



dataloaders_train = torch.utils.data.DataLoader(image_datasets_train, batch_size=29)


"
2730,34373570,44,#next(iter(dataloaders_train))[0].shape
2731,34373570,45,"data_transforms_test = transforms.Compose([transforms.Resize(256),
                                      transforms.CenterCrop(224),
                                      transforms.ToTensor(),
                                      transforms.Normalize([0.485, 0.456, 0.406], 
                                                           [0.229, 0.224, 0.225])])

image_datasets_test = datasets.ImageFolder(test_dir, transform=data_transforms_test)

dataloaders_test = torch.utils.data.DataLoader(image_datasets_test, batch_size=44)"
2732,34373570,46,"device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")
model = models.vgg16_bn(pretrained=True)

for param in model.parameters():
    param.requires_grad = False"
2733,34373570,47,"model.classifier = nn.Sequential(nn.Linear(25088, 12544),
                                 nn.ReLU(),
                                 nn.Dropout(0.2),
                                 nn.Linear(12544, 32093),
                                 nn.LogSoftmax(dim=1))"
2734,34373570,48,"
criterion = nn.NLLLoss()
optimizer = optim.Adam(model.classifier.parameters(), lr=0.003)"
2735,34373570,49,#train_torch_y[0]
2736,34373570,50,model.to(device)
2737,34373570,51,"epochs = 2
steps = 0
running_loss = 0
print_every = 100
for epoch in range(epochs):
    for inputs, labels in zip(dataloaders_train,train_torch_y):
        steps += 1
        inputs, labels = inputs[0].to(device), labels.to(device)
        
        optimizer.zero_grad()
        
        logps = model.forward(inputs)
        loss = criterion(logps, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        
        if steps % print_every == 0:
               
            print(f""Epoch {epoch+1}/{epochs}.. ""
                  f""Train loss: {running_loss/print_every:.3f}.. "")
            running_loss = 0
        if steps % 2000 == 0:
            break #to get out of loop before memory is full"
2738,34373570,52,#torch.cuda.is_available()
2739,34373570,53,
2740,34373570,54,
2741,34373570,55,
2742,34373570,56,
2743,34373570,57,"model.eval()
top_c = []
top_probs = []
with torch.no_grad():
    for inputs, labels in dataloaders_test:
        inputs  = inputs.to(device) 
        logps = model.forward(inputs)
                           
        ps = torch.exp(logps)
        top_p, top_class = ps.topk(1, dim=1)
        top_c.append(top_class)
        top_probs.append(top_p)
model.train()"
2744,34373570,58,"

#print(top_c)"
2745,34373570,59,#print(top_probs)
2746,34373570,60,sub = sub_samp.copy()
2747,34373570,61,topc_list = [torch.Tensor.cpu(i).numpy().tolist() for i in top_c]
2748,34373570,62,"from functools import reduce
topc_list1 = reduce(lambda x,y: x+y, topc_list)
topc_list2 = reduce(lambda x,y: x+y, topc_list1)"
2749,34373570,63,#topc_list2
2750,34373570,64,
2751,34373570,65,"sub['Predicted'] = list(map(int,topc_list2))"
2752,34373570,66,"sub.to_csv('submission.csv', index=False)"
2753,34373570,67,sub
2754,34373570,68,
2755,31847564,0,"import numpy as np                                    # Array, Linear Algebra
from torch.utils.data.dataset import random_split     # spliting inTrain Val
import pandas as pd                                   # handling CSV
import os                                             # For File handling
import random                                         # Choosing from images dataset
import time                                           # timing Epochs  
from tqdm.notebook import tqdm                        # Testing
from os.path import join                              # File Handling
from torchvision import transforms                    # Data Aug
import torch                                          # Framework
from PIL import Image                                 # Loading Image
from torch.utils.data import Dataset, DataLoader      # Dataset
import torch.nn.functional as F                       # Function
import json                                           # Loading Metadat
from PIL import  ImageOps                             # Data Aug 
from PIL.Image import open as openIm                  # Image Handling
import matplotlib.pyplot  as plt                      # Ploting Image
import cv2"
2756,31847564,1,"TRAIN       = ""../input/herbarium-2020-fgvc7/nybg2020/train/""
TEST        = ""../input/herbarium-2020-fgvc7/nybg2020/test/""
META        = ""metadata.json""
BATCH_SIZE  = 7
NUM_WORKERS = 2
BATCH_EVAL  = 1
SHUFFLE     = True
EPOCHS      = 3
RESIZE      = (800, 600)
CLASSES     = 32094
LENGTH      = 2*CLASSES"
2757,31847564,2,"with open(join(TRAIN,META),""r"", encoding = ""ISO-8859-1"") as file:
    metadata = json.load(file)
print(""Metadata has {} sections. These section has all the Information regarding Images in dataset like class, id, size etc. "".format(len(list(metadata.keys()))))
print(""Let us see al the sections in metadata:- "", [print("" - "",i) for i in list(metadata.keys())])

print(""Number of Images in our Training set is:- "", len(metadata[""images""]))
print(""\n Let us see how every section of Dataset Looks like:-\n"")
for i in list(metadata.keys()):
    print("" - sample and number of elements in {} :- "".format(i),len(list(metadata[i])))
    print(""\t"",list(metadata[i])[0], end = ""\n\n"")"
2758,31847564,3,"with open(join(TEST,META),""r"", encoding = ""ISO-8859-1"") as file:
    metadata_test = json.load(file)
print(""Metadata has {} sections. These section has all the Information regarding Images in dataset like class, id, size etc. "".format(len(list(metadata_test.keys()))))
print(""Let us see al the sections in metadata:- "", [print("" - "",i) for i in list(metadata_test.keys())])

print(""Number of Images in our Training set is:- "", len(metadata_test[""images""]))
print(""\n Let us see how every section of Dataset Looks like:-\n"")
for i in list(metadata_test.keys()):
    print("" - sample and number of elements in {} :- "".format(i),len(list(metadata_test[i])))
    print(""\t"",list(metadata_test[i])[0], end = ""\n\n"")"
2759,31847564,4,"train_img = pd.DataFrame(metadata['images'])
train_ann = pd.DataFrame(metadata['annotations'])
train_df = pd.merge(train_ann, train_img, left_on='image_id', right_on='id', how='left').drop('image_id', axis=1).sort_values(by=['category_id'])
train_df.head()"
2760,31847564,5,"im = Image.open(""../input/herbarium-2020-fgvc7/nybg2020/train/images/156/72/354106.jpg"")
print(""Category Id is 15672 and Image Id is 354106 is shown below"")
im"
2761,31847564,6,"size_of_img = (40, 40)
fig=plt.figure(figsize=(80,80))
for i in range(60):
    ax=fig.add_subplot(20,20,i+1)
    img = cv2.imread(TRAIN + metadata[""images""][i][""file_name""])
    img = cv2.resize(img,size_of_img)
    ax.imshow(img)
plt.show()"
2762,31847564,7,"import time
start_time = time.time()"
2763,31847564,8,"import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        if filename.endswith('.jpg'):
            break
        print(os.path.join(dirname, filename))"
2764,31847564,9,"sample_sub = pd.read_csv('../input/herbarium-2020-fgvc7/sample_submission.csv')
display(sample_sub)"
2765,31847564,10,"import json, codecs
with codecs.open(""../input/herbarium-2020-fgvc7/nybg2020/train/metadata.json"", 'r',
                 encoding='utf-8', errors='ignore') as f:
    train_meta = json.load(f)
    
with codecs.open(""../input/herbarium-2020-fgvc7/nybg2020/test/metadata.json"", 'r',
                 encoding='utf-8', errors='ignore') as f:
    test_meta = json.load(f)"
2766,31847564,11,display(train_meta.keys())
2767,31847564,12,"train_df = pd.DataFrame(train_meta['annotations'])
display(train_df)"
2768,31847564,13,"train_cat = pd.DataFrame(train_meta['categories'])
train_cat.columns = ['family', 'genus', 'category_id', 'category_name']
display(train_cat)"
2769,31847564,14,"train_img = pd.DataFrame(train_meta['images'])
train_img.columns = ['file_name', 'height', 'image_id', 'license', 'width']
display(train_img)"
2770,31847564,15,"train_reg = pd.DataFrame(train_meta['regions'])
train_reg.columns = ['region_id', 'region_name']
display(train_reg)"
2771,31847564,16,"train_df = train_df.merge(train_cat, on='category_id', how='outer')
train_df = train_df.merge(train_img, on='image_id', how='outer')
train_df = train_df.merge(train_reg, on='region_id', how='outer')"
2772,31847564,17,"print(train_df.info())
display(train_df)"
2773,31847564,18,"na = train_df.file_name.isna()
keep = [x for x in range(train_df.shape[0]) if not na[x]]
train_df = train_df.iloc[keep]"
2774,31847564,19,"dtypes = ['int32', 'int32', 'int32', 'int32', 'object', 'object', 'object', 'object', 'int32', 'int32', 'int32', 'object']
for n, col in enumerate(train_df.columns):
    train_df[col] = train_df[col].astype(dtypes[n])
print(train_df.info())
display(train_df)"
2775,31847564,20,"test_df = pd.DataFrame(test_meta['images'])
test_df.columns = ['file_name', 'height', 'image_id', 'license', 'width']
print(test_df.info())
display(test_df)"
2776,31847564,21,"train_df.to_csv('full_train_data.csv', index=False)
test_df.to_csv('full_test_data.csv', index=False)"
2777,31847564,22,"print(""Total Unique Values for each columns:"")
print(""{0:10s} \t {1:10d}"".format('train_df', len(train_df)))
for col in train_df.columns:
    print(""{0:10s} \t {1:10d}"".format(col, len(train_df[col].unique())))"
2778,31847564,23,"family = train_df[['family', 'genus', 'category_name']].groupby(['family', 'genus']).count()
display(family.describe())"
2779,31847564,24,"from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPool2D, Flatten, BatchNormalization, Input, concatenate
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.utils import plot_model
from sklearn.model_selection import train_test_split as tts

in_out_size = (120*120) + 3 #We will resize the image to 120*120 and we have 3 outputs
def xavier(shape, dtype=None):
    return np.random.rand(*shape)*np.sqrt(1/in_out_size)

def fg_model(shape, lr=0.001):
    '''Family-Genus model receives an image and outputs two integers indicating both the family and genus index.'''
    i = Input(shape)
    
    x = Conv2D(3, (3, 3), activation='relu', padding='same', kernel_initializer=xavier)(i)
    x = Conv2D(3, (5, 5), activation='relu', padding='same', kernel_initializer=xavier)(x)
    x = MaxPool2D(pool_size=(3, 3), strides=(3,3))(x)
    x = BatchNormalization()(x)
    x = Dropout(0.5)(x)
    x = Conv2D(16, (5, 5), activation='relu', padding='same', kernel_initializer=xavier)(x)
    #x = Conv2D(16, (5, 5), activation='relu', padding='same', kernel_initializer=xavier)(x)
    x = MaxPool2D(pool_size=(5, 5), strides=(5,5))(x)
    x = BatchNormalization()(x)
    x = Dropout(0.5)(x)
    x = Flatten()(x)
    
    o1 = Dense(310, activation='softmax', name='family', kernel_initializer=xavier)(x)
    
    o2 = concatenate([o1, x])
    o2 = Dense(3678, activation='softmax', name='genus', kernel_initializer=xavier)(o2)
    
    o3 = concatenate([o1, o2, x])
    o3 = Dense(32094, activation='softmax', name='category_id', kernel_initializer=xavier)(o3)
    
    x = Model(inputs=i, outputs=[o1, o2, o3])
    
    opt = Adam(lr=lr, amsgrad=True)
    x.compile(optimizer=opt, loss=['sparse_categorical_crossentropy', 
                                   'sparse_categorical_crossentropy', 
                                   'sparse_categorical_crossentropy'],
                 metrics=['accuracy'])
    return x

model = fg_model((120, 120, 3))
model.summary()
plot_model(model, to_file='full_model_plot.png', show_shapes=True, show_layer_names=True)"
2780,31847564,25,"from tensorflow.keras.preprocessing.image import ImageDataGenerator

train_datagen = ImageDataGenerator(featurewise_center=False,
                                     featurewise_std_normalization=False,
                                     rotation_range=180,
                                     width_shift_range=0.1,
                                     height_shift_range=0.1,
                                     zoom_range=0.2)"
2781,31847564,26,"m = train_df[['file_name', 'family', 'genus', 'category_id']]
fam = m.family.unique().tolist()
m.family = m.family.map(lambda x: fam.index(x))
gen = m.genus.unique().tolist()
m.genus = m.genus.map(lambda x: gen.index(x))
display(m)"
2782,31847564,27,"train, verif = tts(m, test_size=0.2, shuffle=True, random_state=17)
train = train[:49000]
verif = verif[:1000]
shape = (120, 120, 3)
epochs = 2
batch_size = 32

model = fg_model(shape, 0.007)

#Disable the last two output layers for training the Family
for layers in model.layers:
    if layers.name == 'genus' or layers.name=='category_id':
        layers.trainable = False

#Train Family for 2 epochs
model.fit_generator(train_datagen.flow_from_dataframe(dataframe=train,
                                                      directory='../input/herbarium-2020-fgvc7/nybg2020/train/',
                                                      x_col=""file_name"",
                                                      y_col=[""family"", ""genus"", ""category_id""],
                                                      target_size=(120, 120),
                                                      batch_size=batch_size,
                                                      class_mode='multi_output'),
                    validation_data=train_datagen.flow_from_dataframe(
                        dataframe=verif,
                        directory='../input/herbarium-2020-fgvc7/nybg2020/train/',
                        x_col=""file_name"",
                        y_col=[""family"", ""genus"", ""category_id""],
                        target_size=(120, 120),
                        batch_size=batch_size,
                        class_mode='multi_output'),
                    epochs=epochs,
                    steps_per_epoch=len(train)//batch_size,
                    validation_steps=len(verif)//batch_size,
                    verbose=1,
                    workers=8,
                    use_multiprocessing=False)

#Reshuffle the inputs
train, verif = tts(m, test_size=0.2, shuffle=True, random_state=17)
train = train[:40000]
verif = verif[:10000]

#Make the Genus layer Trainable
for layers in model.layers:
    if layers.name == 'genus':
        layers.trainable = True
        
#Train Family and Genus for 2 epochs
model.fit_generator(train_datagen.flow_from_dataframe(dataframe=train,
                                                      directory='../input/herbarium-2020-fgvc7/nybg2020/train/',
                                                      x_col=""file_name"",
                                                      y_col=[""family"", ""genus"", ""category_id""],
                                                      target_size=(120, 120),
                                                      batch_size=batch_size,
                                                      class_mode='multi_output'),
                    validation_data=train_datagen.flow_from_dataframe(
                        dataframe=verif,
                        directory='../input/herbarium-2020-fgvc7/nybg2020/train/',
                        x_col=""file_name"",
                        y_col=[""family"", ""genus"", ""category_id""],
                        target_size=(120, 120),
                        batch_size=batch_size,
                        class_mode='multi_output'),
                    epochs=epochs,
                    steps_per_epoch=len(train)//batch_size,
                    validation_steps=len(verif)//batch_size,
                    verbose=1,
                    workers=8,
                    use_multiprocessing=False)

#Reshuffle the inputs
train, verif = tts(m, test_size=0.2, shuffle=True, random_state=17)
train = train[:40000]
verif = verif[:10000]

#Make the category_id layer Trainable
for layers in model.layers:
    if layers.name == 'category_id':
        layers.trainable = True
        
#Train them all for 2 epochs
model.fit_generator(train_datagen.flow_from_dataframe(dataframe=train,
                                                      directory='../input/herbarium-2020-fgvc7/nybg2020/train/',
                                                      x_col=""file_name"",
                                                      y_col=[""family"", ""genus"", ""category_id""],
                                                      target_size=(120, 120),
                                                      batch_size=batch_size,
                                                      class_mode='multi_output'),
                    validation_data=train_datagen.flow_from_dataframe(
                        dataframe=verif,
                        directory='../input/herbarium-2020-fgvc7/nybg2020/train/',
                        x_col=""file_name"",
                        y_col=[""family"", ""genus"", ""category_id""],
                        target_size=(120, 120),
                        batch_size=batch_size,
                        class_mode='multi_output'),
                    epochs=epochs,
                    steps_per_epoch=len(train)//batch_size,
                    validation_steps=len(verif)//batch_size,
                    verbose=1,
                    workers=8,
                    use_multiprocessing=False)

"
2783,31847564,28,model.save('fg_model.h5')
2784,31847564,29,"batch_size = 32
test_datagen = ImageDataGenerator(featurewise_center=False,
                                  featurewise_std_normalization=False)

generator = test_datagen.flow_from_dataframe(
        dataframe = test_df.iloc[:10000], #Limiting the test to the first 10,000 items
        directory = '../input/herbarium-2020-fgvc7/nybg2020/test/',
        x_col = 'file_name',
        target_size=(120, 120),
        batch_size=batch_size,
        class_mode=None,  # only data, no labels
        shuffle=False)

family, genus, category = model.predict_generator(generator, verbose=1)"
2785,31847564,30,"sub = pd.DataFrame()
sub['Id'] = test_df.image_id
sub['Id'] = sub['Id'].astype('int32')
sub['Predicted'] = np.concatenate([np.argmax(category, axis=1), 23718*np.ones((len(test_df.image_id)-len(category)))], axis=0)
sub['Predicted'] = sub['Predicted'].astype('int32')
display(sub)
sub.to_csv('category_submission.csv', index=False)"
2786,31847564,31,"sub['Predicted'] = np.concatenate([np.argmax(family, axis=1), np.zeros((len(test_df.image_id)-len(family)))], axis=0)
sub['Predicted'] = sub['Predicted'].astype('int32')
display(sub)
sub.to_csv('family_submission.csv', index=False)"
2787,31847564,32,"sub['Predicted'] = np.concatenate([np.argmax(genus, axis=1), np.zeros((len(test_df.image_id)-len(genus)))], axis=0)
sub['Predicted'] = sub['Predicted'].astype('int32')
display(sub)
sub.to_csv('genus_submission.csv', index=False)"
2788,31847564,33,"end_time = time.time()
total = end_time - start_time
h = total//3600
m = (total%3600)//60
s = total%60
print(""Total time spent: %i hours, %i minutes, and %i seconds"" %(h, m, s))"
2789,30800274,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the ""../input/"" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# Any results you write to the current directory are saved as output."
2790,30800274,1,"import matplotlib.pyplot as plt
import seaborn as sns"
2791,30800274,2,"data = pd.read_csv(""/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_train.csv"")"
2792,30800274,3,data.tail()
2793,30800274,4,data.info()
2794,30800274,5,"data.set_index('Date',inplace=True)"
2795,30800274,6,data.index = pd.to_datetime(data.index)
2796,30800274,7,"data.asfreq = ""D"""
2797,30800274,8,PositiveCase =  data[data['ConfirmedCases']>0]
2798,30800274,9,"
PositiveCase['ConfirmedCases'].plot()"
2799,30800274,10,"
PositiveCase['Fatalities'].plot()"
2800,30800274,11,"PositiveCase[""ConfirmedCases""][0]"
2801,30800274,12,"PositiveCase['FirstDerivative']= 0
for i in range(len(PositiveCase)):
    if i==0:
        PositiveCase[""FirstDerivative""][i]= 0
    else:
        PositiveCase['FirstDerivative'][i]= (PositiveCase['ConfirmedCases'][i]-PositiveCase['ConfirmedCases'][0])/i
    "
2802,30800274,13,PositiveCase.head()
2803,30800274,14,PositiveCase['FirstDerivative'].plot()
2804,30800274,15,"def adf_test(series,title=''):
    """"""
    Pass in a time series and an optional title, returns an ADF report
    """"""
    print(f'Augmented Dickey-Fuller Test: {title}')
    result = adfuller(series.dropna(),autolag='AIC') # .dropna() handles differenced data
    
    labels = ['ADF test statistic','p-value','# lags used','# observations']
    out = pd.Series(result[0:4],index=labels)

    for key,val in result[4].items():
        out[f'critical value ({key})']=val
        
    print(out.to_string())          # .to_string() removes the line ""dtype: float64""
    
    if result[1] <= 0.05:
        print(""Strong evidence against the null hypothesis"")
        print(""Reject the null hypothesis"")
        print(""Data has no unit root and is stationary"")
    else:
        print(""Weak evidence against the null hypothesis"")
        print(""Fail to reject the null hypothesis"")
        print(""Data has a unit root and is non-stationary"")"
2805,30800274,16,"Positiveseries = PositiveCase[['ConfirmedCases','Fatalities']]"
2806,30800274,17,Positiveseries.head()
2807,30800274,18,!pip install pmdarima
2808,30800274,19,"from statsmodels.tsa.statespace.varmax import VARMAX, VARMAXResults
from statsmodels.tsa.stattools import adfuller
# from pmdarima import auto_arima
from statsmodels.tools.eval_measures import rmse"
2809,30800274,20,"adf_test(Positiveseries['ConfirmedCases'],title=""Confirmed Cases"")"
2810,30800274,21,Positiveseries_tr = Positiveseries.diff()
2811,30800274,22,"Positiveseries_tred = Positiveseries_tr.dropna()
adf_test(Positiveseries_tred['ConfirmedCases'],title=""Confirmed Cases"")"
2812,30800274,23,Positive = Positiveseries_tred.diff()
2813,30800274,24,"Positived =  Positive.dropna()
adf_test(Positived['ConfirmedCases'],title=""Confirmed Cases"")"
2814,30800274,25,Positived1 = Positived.diff()
2815,30800274,26,"Positived1 =  Positived1.dropna()
adf_test(Positived1['ConfirmedCases'],title=""Confirmed Cases"")"
2816,30800274,27,Positived1.head()
2817,30800274,28,"from statsmodels.tsa.arima_model import ARMA,ARMAResults,ARIMA,ARIMAResults
"
2818,30800274,29,# Trying varius ARIMA model for 3 differencing level
2819,30800274,30,"ARIMA(1,3,1)"
2820,30800274,31,"model = ARIMA(Positiveseries['ConfirmedCases'],order=(1,0,1))
results = model.fit()
results.summary()"
2821,30800274,32,Positiveseries['ConfirmedCasesDiff1']= Positiveseries['ConfirmedCases'].diff()
2822,30800274,33,Positiveseries.head()
2823,30800274,34,"Positiveseries['Fatalitiesdiff1']= Positiveseries['Fatalities'].diff()
Positiveseries['ConfirmedCasesDiff2'] = Positiveseries['ConfirmedCasesDiff1'].diff()
Positiveseries['Fatalitiesdiff2']= Positiveseries['Fatalitiesdiff1'].diff()
Positiveseries['ConfirmedCasesDiff3'] = Positiveseries['ConfirmedCasesDiff2'].diff()
Positiveseries['Fatalitiesdiff3']= Positiveseries['Fatalitiesdiff2'].diff()"
2824,30800274,35,Positiveseries
2825,30800274,36,Positiveseries.plot()
2826,30800274,37,"Positiveseries[['ConfirmedCasesDiff3','Fatalitiesdiff3']].plot()"
2827,30800274,38,"#Adding rows for testing set
test = pd.read_csv(""/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_test.csv"",index_col='Date',parse_dates=True)"
2828,30800274,39,test.head()
2829,30800274,40,len(test)
2830,30800274,41,test = test.loc['25-03-2020':]
2831,30800274,42,
2832,30800274,43,"for i in list(Positiveseries.columns):
    test[i] = 0"
2833,30800274,44,"test.drop(['ForecastId','Province/State','Lat','Long'],inplace=True,axis=1)"
2834,30800274,45,"test.drop('Country/Region',axis=1,inplace=True)"
2835,30800274,46,test.head()
2836,30800274,47,Positiveseries = Positiveseries.append(test)
2837,30800274,48,Positiveseries.tail()
2838,30800274,49,Positiveseries
2839,30800274,50,"# Positiveseries.asfreq =""D""
for i in range(15,len(Positiveseries)):
    Positiveseries.iloc[i]['ConfirmedCasesDiff3'] = (Positiveseries.iloc[i-1]['ConfirmedCasesDiff3'] + Positiveseries.iloc[i-2]['ConfirmedCasesDiff3']+Positiveseries.iloc[i-3]['ConfirmedCasesDiff3']+Positiveseries.iloc[i-4]['ConfirmedCasesDiff3'])/4
    Positiveseries.iloc[i]['Fatalitiesdiff3'] = (Positiveseries.iloc[i-1]['Fatalitiesdiff3'] + Positiveseries.iloc[i-2]['Fatalitiesdiff3'])/2
    Positiveseries.iloc[i]['ConfirmedCasesDiff2'] = Positiveseries.iloc[i-1]['ConfirmedCasesDiff2']+ Positiveseries.iloc[i]['ConfirmedCasesDiff3']
# #     Positiveseries.iloc[i]['ConfirmedCasesDiff2'] = Positiveseries.iloc[i-1]['ConfirmedCasesDiff2']+ Positiveseries.iloc[i]['ConfirmedCasesDiff3']
    Positiveseries.iloc[i]['ConfirmedCasesDiff1'] = Positiveseries.iloc[i-1]['ConfirmedCasesDiff1']+ Positiveseries.iloc[i]['ConfirmedCasesDiff2']
    Positiveseries.iloc[i]['ConfirmedCases'] = Positiveseries.iloc[i-1]['ConfirmedCases']+ Positiveseries.iloc[i]['ConfirmedCasesDiff1']
    Positiveseries.iloc[i]['Fatalitiesdiff2']= Positiveseries.iloc[i-1]['Fatalitiesdiff2'] + Positiveseries.iloc[i]['Fatalitiesdiff3']
    Positiveseries.iloc[i]['Fatalitiesdiff1']= Positiveseries.iloc[i-1]['Fatalitiesdiff1'] + Positiveseries.iloc[i]['Fatalitiesdiff2']
    Positiveseries.iloc[i]['Fatalities']= Positiveseries.iloc[i-1]['Fatalities'] + Positiveseries.iloc[i]['Fatalitiesdiff1']
    
# Positiveseries['ConfirmedCasesDiff3']['24-03-2020']"
2840,30800274,51,"Output = Positiveseries[['ConfirmedCases','Fatalities']]"
2841,30800274,52,Output.loc['2020-04-03' :] =0
2842,30800274,53,Output.head()
2843,30800274,54,"submission = pd.read_csv(""/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_submission.csv"")"
2844,30800274,55,submission.head()
2845,30800274,56,submission['ConfirmedCases']= list(Output.loc['2020-03-12':]['ConfirmedCases'])
2846,30800274,57,submission['Fatalities']= list(Output.loc['2020-03-12':]['Fatalities'])
2847,30800274,58,"submission.to_csv(""submission.csv"",index=False)"
2848,30568127,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the ""../input/"" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# Any results you write to the current directory are saved as output."
2849,30568127,1,"import numpy as np
import pandas as pd
import scipy.optimize as opt
%matplotlib inline
import matplotlib.pyplot as plt
from tqdm import tqdm_notebook
from datetime import datetime, timedelta
from sklearn.metrics import mean_squared_log_error"
2850,30568127,2,"train = pd.read_csv('/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_train.csv')
train['Date'] = pd.to_datetime(train['Date'])
test = pd.read_csv('/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_test.csv')
test['Date'] = pd.to_datetime(test['Date'])
sub = pd.read_csv('/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_submission.csv')
# sub = sub.set_index('ForecastId').reset_index(drop=True)
print(test.shape)
print(train.shape)
print(sub.shape)"
2851,30568127,3,train.head(5)
2852,30568127,4,test.head(5)
2853,30568127,5,sub
2854,30568127,6,"def sigmoid(t, M, beta, alpha):
    return M / (1 + np.exp(-beta * (t - alpha)))"
2855,30568127,7,"NUMBER_UNCASEc= 47 
MAXIMUM = 1000
BOUNDS=(0, [2000, 1.0, 100])

x = list(range(len(train)))
min_date = min(train['Date'])
max_date = max(test['Date'])
cases = train['ConfirmedCases'].values
plt.plot(x, cases, label=""Train Data"")
print(""Case:"", cases)
popt, pcov = opt.curve_fit(sigmoid, x, cases, bounds=BOUNDS)
print(popt)
print(pcov)
M, beta, alpha = popt
plt.plot(x, sigmoid(x, M, beta, alpha), label=""Predict"")
# Place a legend to the right of this smaller subplot.
plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)

plt.show()"
2856,30568127,8,"M, beta, alpha = popt
x_forcast = list(range(train.shape[0], train.shape[0]+ test.shape[0]))
case_forecast = sigmoid(x_forcast, M, beta, alpha)
print(""case_forecast"",case_forecast)
sub[""ConfirmedCases""] = [int(i) for i in case_forecast]
plt.plot(x_forcast, case_forecast)"
2857,30568127,9,"deaths = train['Fatalities'].values
popt, pcov = opt.curve_fit(sigmoid, list(range(len(deaths))), deaths, bounds=BOUNDS)
M, beta, alpha = popt
death_forecast = sigmoid(x_forcast, M, beta, alpha)
print(""death_forecast"",death_forecast)
plt.plot(x_forcast, death_forecast)
sub[""Fatalities""] = [int(i) for i in death_forecast]
sub.to_csv('submission.csv', index=False)"
2858,30568127,10,sub
2859,30568127,11,
2860,30568127,12,
2861,30790194,0,"import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

from sklearn.linear_model import LinearRegression

import warnings
warnings.filterwarnings('ignore')

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))"
2862,30790194,1,"sub=pd.read_csv(""../input/covid19-local-us-ca-forecasting-week-1/ca_submission.csv"")
test=pd.read_csv(""../input/covid19-local-us-ca-forecasting-week-1/ca_test.csv"")
train=pd.read_csv(""../input/covid19-local-us-ca-forecasting-week-1/ca_train.csv"")"
2863,30790194,2,"print(train.shape)
train.head()"
2864,30790194,3,"#Only taking data with confirmed cases
train=train[train.ConfirmedCases>0]
print(train.shape)
train.head()"
2865,30790194,4,"sns.lineplot(train.Id, train.ConfirmedCases)"
2866,30790194,5,"sns.regplot(train.Id, np.log(train.ConfirmedCases))"
2867,30790194,6,"model_1= LinearRegression()
x1=np.array(train.Id).reshape(-1,1)
y1=np.log(train.ConfirmedCases)
model_1.fit(x1,y1)
print(""R-squared score : "",model_1.score(x1,y1))

gr=np.power(np.e, model_1.coef_[0])
print(""Growth Factor : "", gr)
print(f""Growth Rate : {round((gr-1)*100,2)}%"")"
2868,30790194,7,"sns.regplot(train.ConfirmedCases,train.Fatalities)"
2869,30790194,8,"model_2= LinearRegression()
x2=np.array(train.ConfirmedCases).reshape(-1,1)
y2=train.Fatalities
model_2.fit(x2,y2)
print(""R-Squared Score= "",model_2.score(x2,y2))"
2870,30790194,9,test.head()
2871,30790194,10,"#Making Id as unique key between test and train
test[""Id""]=50+test.ForecastId
test.head()"
2872,30790194,11,"test[""LogConf""]=model_1.predict(np.array(test.Id).reshape(-1,1))
test[""ConfirmedCases""]=np.exp(test.LogConf)//1
test[""Fatalities""]=model_2.predict(np.array(test.ConfirmedCases).reshape(-1,1))//1
test"
2873,30790194,12,"#Wherever confirmed cases and fatalities are available in train data, update it into test data
for id in train.Id:
    test.ConfirmedCases[test.Id==id]=train.ConfirmedCases[train.Id==id].sum()
    test.Fatalities[test.Id==id]=train.Fatalities[train.Id==id].sum()
test"
2874,30790194,13,### Prepare submission file
2875,30790194,14,"sub.ConfirmedCases=test.ConfirmedCases
sub.Fatalities=test.Fatalities
sub.to_csv(""submission.csv"", index=False)"
2876,30790194,15,
2877,30840463,0,"## Importing packages

# This R environment comes with all of CRAN and many other helpful packages preinstalled.
# You can see which packages are installed by checking out the kaggle/rstats docker image: 
# https://github.com/kaggle/docker-rstats

library(tidyverse) # metapackage with lots of helpful functions
library(dplyr)
library(readxl)
library(stats)
library(forecast)
library(fGarch)
library(tseries)
library(vars)
library(timeSeries)
library(readr)
## Running code

# In a notebook, you can run a single code cell by clicking in the cell and then hitting 
# the blue arrow to the left, or by clicking in the cell and pressing Shift+Enter. In a script, 
# you can run code by highlighting the code you want to run and then clicking the blue arrow
# at the bottom of this window.

## Reading in files

# You can access files from datasets you've added to this kernel in the ""../input/"" directory.
# You can see the files added to this kernel by running the code below. 

list.files(path = ""../input"")

## Saving data

# If you save any files or images, these will be put in the ""output"" directory. You 
# can see the output directory by committing and running your kernel (using the 
# Commit & Run button) and then checking out the compiled version of your kernel."
2878,30840463,1,"train <- read.csv('../input/covid19-local-us-ca-forecasting-week-1/ca_train.csv')
test <- read.csv('../input/covid19-local-us-ca-forecasting-week-1/ca_test.csv')
sample <- read.csv('../input/covid19-local-us-ca-forecasting-week-1/ca_submission.csv')"
2879,30840463,2,head(train)
2880,30840463,3,"cases <- ts(train$ConfirmedCases)
fatal <- ts(train$Fatalities)"
2881,30840463,4,auto.arima(cases)
2882,30840463,5,"forecast:::forecast.Arima(auto.arima(cases), 
                          h = 30, 
                          level = c(68, 90))$mean"
2883,30840463,6,auto.arima(fatal)
2884,30840463,7,"forecast:::forecast.Arima(auto.arima(fatal), 
                          h = 30, 
                          level = c(68, 90))$mean"
2885,30840463,8,"test[which(as.Date(test$Date) > as.Date('2020-03-24')),]"
2886,30840463,9,"train[which(as.Date(train$Date) > as.Date('2020-03-11')),]"
2887,30840463,10,"new_test <- merge(test, train, by = 'Date', all.x = TRUE)"
2888,30840463,11,"new_test[which(as.Date(new_test$Date) > as.Date('2020-03-24')),]$ConfirmedCases <- forecast:::forecast.Arima(auto.arima(cases), 
                          h = 30, 
                          level = c(68, 90))$mean"
2889,30840463,12,"new_test[which(as.Date(new_test$Date) > as.Date('2020-03-24')),]$Fatalities <- forecast:::forecast.Arima(auto.arima(fatal), 
                          h = 30, 
                          level = c(68, 90))$mean"
2890,30840463,13,new_test
2891,30840463,14,"sample$ConfirmedCases <- new_test$ConfirmedCases
sample$Fatalities <- new_test$Fatalities"
2892,30840463,15,"write.csv(sample, file = ""submission.csv"", row.names = FALSE)"
2893,30840463,16,
2894,30810228,0,"import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

import warnings
warnings.filterwarnings('ignore')

from sklearn.linear_model import LinearRegression 

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))"
2895,30810228,1,"sub=pd.read_csv(""../input/covid19-local-us-ca-forecasting-week-1/ca_submission.csv"")
test=pd.read_csv(""../input/covid19-local-us-ca-forecasting-week-1/ca_test.csv"")
train=pd.read_csv(""../input/covid19-local-us-ca-forecasting-week-1/ca_train.csv"")"
2896,30810228,2,"print(train.shape)
train.head()"
2897,30810228,3,"#Only taking data with confirmed cases
train=train[train.ConfirmedCases>0]
print(train.shape)
train.head()"
2898,30810228,4,"sns.lineplot(train.Id, train.ConfirmedCases)"
2899,30810228,5,"sns.regplot(train.Id, np.log(train.ConfirmedCases))"
2900,30810228,6,"model_1= LinearRegression()
x1=np.array(train.Id).reshape(-1,1)
y1=np.log(train.ConfirmedCases)
model_1.fit(x1,y1)
print(""R-squared score : "",model_1.score(x1,y1))

gr=np.power(np.e, model_1.coef_[0])
print(""Growth Factor : "", gr)
print(f""Growth Rate : {round((gr-1)*100,2)}%"")"
2901,30810228,7,"sns.regplot(train.ConfirmedCases,train.Fatalities)"
2902,30810228,8,"model_2= LinearRegression()
x2=np.array(train.ConfirmedCases).reshape(-1,1)
y2=train.Fatalities
model_2.fit(x2,y2)
model_2.score(x2,y2)"
2903,30810228,9,test.head()
2904,30810228,10,"#Making Id as unique key between test and train
test[""Id""]=50+test.ForecastId
test.head()"
2905,30810228,11,"test[""LogConf""]=model_1.predict(np.array(test.Id).reshape(-1,1))
test[""ConfirmedCases""]=np.exp(test.LogConf)//1
test[""Fatalities""]=model_2.predict(np.array(test.ConfirmedCases).reshape(-1,1))//1"
2906,30810228,12,"#Wherever confirmed cases and fatalities are available in train data, update it into test data
for id in train.Id:
    test.ConfirmedCases[test.Id==id]=train.ConfirmedCases[train.Id==id].sum()
    test.Fatalities[test.Id==id]=train.Fatalities[train.Id==id].sum()"
2907,30810228,13,"test[""Conf_d1""]=test.ConfirmedCases
test[""Fat_d1""]=test.Fatalities

rate=gr-1
for i in range(train.shape[0]-2,test.shape[0]):
    rate*=0.99
    test.Conf_d1[i]=(1+rate)*test.Conf_d1[i-1]//1
    test.Fat_d1[i]=model_2.predict(np.array(test.Conf_d1[i]).reshape(-1,1))[0]//1"
2908,30810228,14,"test[""Conf_d5""]=test.ConfirmedCases
test[""Fat_d5""]=test.Fatalities

rate=gr-1
for i in range(train.shape[0]-2,test.shape[0]):
    rate*=0.95
    test.Conf_d5[i]=(1+rate)*test.Conf_d5[i-1]//1
    test.Fat_d5[i]=model_2.predict(np.array(test.Conf_d5[i]).reshape(-1,1))[0]//1"
2909,30810228,15,"test[""Conf_i1""]=test.ConfirmedCases
test[""Fat_i1""]=test.Fatalities

rate=gr-1
for i in range(train.shape[0]-2,test.shape[0]):
    rate*=1.01
    test.Conf_i1[i]=(1+rate)*test.Conf_i1[i-1]//1
    test.Fat_i1[i]=model_2.predict(np.array(test.Conf_i1[i]).reshape(-1,1))[0]//1"
2910,30810228,16,"test[""Conf_15""]=test.ConfirmedCases
test[""Fat_15""]=test.Fatalities

rate=1.15
for i in range(train.shape[0]-2,test.shape[0]):
    test.Conf_15[i]=rate*test.Conf_15[i-1]//1
    test.Fat_15[i]=model_2.predict(np.array(test.Conf_15[i]).reshape(-1,1))[0]//1"
2911,30810228,17,"test[""Conf_20""]=test.ConfirmedCases
test[""Fat_20""]=test.Fatalities

rate=1.20
for i in range(train.shape[0]-2,test.shape[0]):
    test.Conf_20[i]=rate*test.Conf_20[i-1]//1
    test.Fat_20[i]=model_2.predict(np.array(test.Conf_20[i]).reshape(-1,1))[0]//1"
2912,30810228,18,"test[""Conf_25""]=test.ConfirmedCases
test[""Fat_25""]=test.Fatalities

rate=1.25
for i in range(train.shape[0]-2,test.shape[0]):
    test.Conf_25[i]=rate*test.Conf_25[i-1]//1
    test.Fat_25[i]=model_2.predict(np.array(test.Conf_25[i]).reshape(-1,1))[0]//1"
2913,30810228,19,"test[""Conf_30""]=test.ConfirmedCases
test[""Fat_30""]=test.Fatalities

rate=1.30
for i in range(train.shape[0]-2,test.shape[0]):
    test.Conf_30[i]=rate*test.Conf_30[i-1]//1
    test.Fat_30[i]=model_2.predict(np.array(test.Conf_30[i]).reshape(-1,1))[0]//1"
2914,30810228,20,"plt.figure(figsize=(12,8))
sns.lineplot(test.Id, test.Conf_20, label=""Constant 20%"")
sns.lineplot(test.Id, test.Conf_25, label=""Constant 25%"")
sns.lineplot(test.Id, test.Conf_30, label=""Constant 30%"", dashes=True)
sns.lineplot(test.Id, test.Conf_15, label=""Constant 15%"")
sns.lineplot(test.Id, test.Conf_i1, label=""Increasing by 1%"")
sns.lineplot(test.Id, test.Conf_d1, label=""Decreasing by 1%"")
sns.lineplot(test.Id, test.Conf_d5, label=""Decreasing by 5%"")
sns.lineplot(test.Id, test.ConfirmedCases, label=""Current Rate"")
sns.lineplot(train.Id, train.ConfirmedCases, label=""Train Data"")
plt.legend()
plt.show()"
2915,30810228,21,"plt.figure(figsize=(12,8))
sns.lineplot(test.Id, test.Fat_25, label=""Constant 25%"")
sns.lineplot(test.Id, test.Fat_20, label=""Constant 20%"")
sns.lineplot(test.Id, test.Fat_30, label=""Constant 30%"")
sns.lineplot(test.Id, test.Fat_15, label=""Constant 15%"")
sns.lineplot(test.Id, test.Fat_i1, label=""Increasing by 1%"")
sns.lineplot(test.Id, test.Fat_d1, label=""Decreasing by 1%"")
sns.lineplot(test.Id, test.Fat_d5, label=""Decreasing by 5%"")
sns.lineplot(test.Id, test.Fatalities, label=""Current Rate"")
sns.lineplot(train.Id, train.Fatalities, label=""Train Data"")
plt.legend()
plt.show()"
2916,30810228,22,"sub.ConfirmedCases=test.ConfirmedCases
sub.Fatalities=test.Fatalities
sub.to_csv(""submission.csv"", index=False)"
2917,30845748,0,"## Importing packages

library(tidyverse)
library(ggplot2)
library(ggthemes)
library(forcats)
library(readxl)
library(forecast)
library(tseries)

## Reading in tables

ca_submissions <- read.csv(""../input/covid19-local-us-ca-forecasting-week-1/ca_submission.csv"")
ca_test <- read.csv(""../input/covid19-local-us-ca-forecasting-week-1/ca_test.csv"")
ca_train <- read.csv(""../input/covid19-local-us-ca-forecasting-week-1/ca_train.csv"")

##external datasets
cases_03_22_2020 <- read.csv(""../input/california-county-level-data/cases_03_22_2020.csv"")
cases_03_23_2020 <- read.csv(""../input/california-county-level-data/cases_03_23_2020.csv"")
cases_03_24_2020 <- read.csv(""../input/california-county-level-data//cases_03_24_2020.csv"")
early_cases_cali <- read.csv(""../input/california-county-level-data/California Cases by County Xtime.csv"")
county_health_info <- read.csv(""../input/california-county-level-data/California County Health Rankings 2020.csv"")
county_health_addtnl_info <- read.csv(""../input/california-county-level-data/California County Health Rankings 2020 - AdditionalInfo.csv"")
health_info_with_cases <- read.csv(""../input/california-county-level-data/health_info_with_cases.csv"")
health_addtln_info_with_cases <- read.csv(""../input/california-county-level-data/health_addtln_info_with_cases.csv"")
ca_train_new <- read.csv(""../input/california-county-level-data/ca_train_new.csv"")

ca_submission <- read.csv(""../input/california-county-level-data/ca_submission1.csv"")

write_csv(ca_submission, 'submission.csv')"
2918,30845748,1,"head(ca_train)
head(ca_test)
head(ca_submissions)"
2919,30845748,2,"head(early_cases_cali)
head(cases_03_22_2020)
head(cases_03_23_2020)
head(cases_03_24_2020)"
2920,30845748,3,"head(county_health_info)
head(county_health_addtnl_info)"
2921,30845748,4,"ggplot(data = health_info_with_cases) +
  geom_point(aes(Average.Daily.PM2.5...Air.Pollution, Case, color = 'red')) +
  theme(legend.position = ""none"", title = element_text(size = 10)) +
  labs(x = ""Average Daily Particulate Matter"", y = ""Cases"", title = ""Average Daily Particulate Matter vs Cases"", color = 'Legend') +
  theme(plot.title = element_text(hjust = 0.5))"
2922,30845748,5,"ggplot(data = health_info_with_cases) +
  geom_point(aes(X..Severe.Housing.Problems, Case, color = 'red')) +
  theme(legend.position = ""none"", title = element_text(size = 10)) +
  labs(x = ""% Severe Housing Problems"", y = ""Cases"", title = ""% Severe Housing Problems vs Cases"", color = 'Legend') +
  theme(plot.title = element_text(hjust = 0.5))

ggplot(data = health_info_with_cases) +
  geom_point(aes(Overcrowding, Case, color = 'red')) +
  theme(legend.position = ""none"", title = element_text(size = 10)) +
  labs(x = ""Overcrowding"", y = ""Cases"", title = ""Overcrowding vs Cases"", color = 'Legend') +
  theme(plot.title = element_text(hjust = 0.5))"
2923,30845748,6,"ggplot(data = health_info_with_cases) +
  geom_point(aes(Presence.of.Water.Violation, Case, color = 'red')) +
  theme(legend.position = ""none"", title = element_text(size = 10)) +
  labs(x = ""Presence of Water Violation"", y = ""Cases"", title = ""Presence of Water Violation vs Cases"", color = 'Legend') +
  theme(plot.title = element_text(hjust = 0.5))"
2924,30845748,7,"ggplot(data = health_addtln_info_with_cases) +
  geom_point(aes(Average.Traffic.Volume.per.Meter.of.Major.Roadways, Case, color = 'red')) +
  theme(legend.position = ""none"", title = element_text(size = 10)) +
  labs(x = ""Average Traffic Volume/Meter of Major Roadways"", y = ""Cases"", title = ""Average Traffic Volume/Meter of Major Roadways vs Cases"", color = 'Legend') +
  theme(plot.title = element_text(hjust = 0.5))"
2925,30845748,8,"ggplot(data = health_addtln_info_with_cases) +
  geom_point(aes(X..65.and.over, Deaths, color = 'red')) +
  theme(legend.position = ""none"", title = element_text(size = 10)) +
  labs(x = ""% 65 and over"", y = ""Cases"", title = ""% 65 and over vs Cases"", color = 'Legend') +
  theme(plot.title = element_text(hjust = 0.5))"
2926,30845748,9,"##ARIMA for Cases
##identify optimal p,d,q values
auto.arima(ca_train_new$ConfirmedCases)

#use arima model ARIMA(2,0,2)
arima_model_cases <- Arima(ca_train_new$ConfirmedCases, order = c(2,0,2))
summary(arima_model_cases)

plot(forecast(arima_model_cases, h = 30))
"
2927,30845748,10,"##ARIMAX for fatality
##identify optimal p,d,q values
auto.arima(ca_train_new$Fatalities)

#use arima model ARIMA(0,2,1)
arima_model_deaths <- Arima(ca_train_new$Fatalities, order = c(0,2,1), xreg = ca_train_new$ConfirmedCases)
summary(arima_model_deaths)

plot(forecast(arima_model_deaths, xreg = forecast(arima_model_cases, h = 30)[[""mean""]][1:30], h = 30))"
2928,30845748,11,"#my submission!
head(ca_submission)"
2929,32669954,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the ""../input/"" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# Any results you write to the current directory are saved as output."
2930,32669954,1,"train=pd.read_csv(""/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_train.csv"")
test=pd.read_csv(""/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_test.csv"")
submission=pd.read_csv(""/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_submission.csv"")"
2931,32669954,2,"import matplotlib.pyplot as plt
%matplotlib inline"
2932,32669954,3,"train['Date']=pd.to_datetime(train['Date'])
train['Date'] = train['Date'].apply(lambda x:x.date().strftime('%m-%d'))
test['Date']=pd.to_datetime(test['Date'])
test['Date'] = test['Date'].apply(lambda x:x.date().strftime('%m-%d'))"
2933,32669954,4,"hor=train['Date']
ver=train['ConfirmedCases']
plt.figure(figsize=(20,10))
plt.plot(hor, ver)
plt.title('Time Series Confirmed Cases')
plt.show()"
2934,32669954,5,"hor=train['Date']
ver=train['Fatalities']
plt.figure(figsize=(20,10))
plt.plot(hor, ver)
plt.title('Time Series Fatalities')
plt.show()"
2935,32669954,6,"train1=train[48:]  #excluding first 48 values from train dataset as they are all zero
#train1=train
train1.head()"
2936,32669954,7,X_test1=test[['ForecastId']]+50 #matching the test data Id in line to training ID's
2937,32669954,8,"X1=train1[['Id']]
y_con=train1[['ConfirmedCases']]
y_fat=train1[['Fatalities']]"
2938,32669954,9,"from sklearn.preprocessing import PolynomialFeatures
poly=PolynomialFeatures(7) #Polynomial Feature with degree 7
X=poly.fit_transform(X1)
X_test=poly.fit_transform(X_test1)"
2939,32669954,10,"from sklearn.linear_model import Ridge, Lasso, SGDRegressor, LinearRegression
model_con=Lasso()
model_con.fit(X, y_con)"
2940,32669954,11,y_pred_con=model_con.predict(X_test)
2941,32669954,12,"model_fat=Lasso()
model_fat.fit(X, y_fat)"
2942,32669954,13,y_pred_fat=model_fat.predict(X_test)
2943,32669954,14,"y_pred_con1=y_pred_con.ravel()
y_pred_fat1=y_pred_fat.ravel()
"
2944,32669954,15,"y_pred_con1=y_pred_con1[13:]  #replacing 13 test prediction with training label as they overlap
y_con_t=train1['ConfirmedCases']
y_con_t=y_con_t[2:].ravel()  #getting those 13 labels from training set to put into prediction
#y_con_t=y_con_t[50:].ravel()
y_pred_con_final=np.round(np.append(y_con_t, y_pred_con1))
y_pred_con_final"
2945,32669954,16,"y_pred_fat1=y_pred_fat1[13:] #replacing 13 test prediction with training label as they overlap
y_fat_t=train1['Fatalities']
y_fat_t=y_fat_t[2:].ravel() #getting those 13 labels from training set to put into prediction
#y_fat_t=y_fat_t[50:].ravel()
y_pred_fat_final=np.round(np.append(y_fat_t, y_pred_fat1))
y_pred_fat_final"
2946,32669954,17,"data={'ForecastId':submission.ForecastId,'ConfirmedCases':y_pred_con_final, 'Fatalities':y_pred_fat_final}
result=pd.DataFrame(data, index=submission.index)
result.to_csv('/kaggle/working/submission.csv', index=False)
m1=pd.read_csv('/kaggle/working/submission.csv')
m1.head()"
2947,32669954,18,"hor=test.Date
ver=y_pred_con_final
plt.figure(figsize=(20,10))
plt.plot(hor, ver)
plt.title('Confirmed Cases Prediction')
plt.show()"
2948,32669954,19,"hor=test.Date
ver=y_pred_fat_final
plt.figure(figsize=(20,10))
plt.plot(hor, ver)
plt.title('Fatalities Prediction')
plt.show()"
2949,30831873,0,"# CORONA VIRUS - GROWTH RATE PREDICTION VIA TAYLOR SERIES MODEL
# REF: https://www.kaggle.com/rnglol/simple-taylor-series-model"
2950,30831873,1,"%%time
# IMPORTS
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# LOAD TRAIN DATA
train = pd.read_csv('/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_train.csv')

# SCRUB DATA
junk =['Id','Country/Region','Lat','Long','Province/State']
train.drop(junk, axis=1, inplace=True)"
2951,30831873,2,"# PREP TRAIN DATA 
X_train = train[48:]
X_train.reset_index(inplace = True, drop = True) 
print(X_train)"
2952,30831873,3,"# CALCULATE EXPANSION TABLE
diff_conf, conf_old = [], 0 
diff_fat, fat_old = [], 0
dd_conf, dc_old = [], 0
dd_fat, df_old = [], 0

for row in X_train.values:
    diff_conf.append(row[1]-conf_old)
    conf_old=row[1]
    diff_fat.append(row[2]-fat_old)
    fat_old=row[2]
    dd_conf.append(diff_conf[-1]-dc_old)
    dc_old=diff_conf[-1]
    dd_fat.append(diff_fat[-1]-df_old)
    df_old=diff_fat[-1]
    
print(len(diff_conf),X_train.shape)"
2953,30831873,4,"# SAMPLES
samples = len(diff_conf)
answer = samples - 1
key = answer - 1"
2954,30831873,5,"# POPULATE DATAFRAME FEATURES
pd.options.mode.chained_assignment = None  # default='warn'

X_train['diff_confirmed'] = diff_conf
X_train['diff_fatalities'] = diff_fat
X_train['dd_confirmed'] = dd_conf
X_train['dd_fatalities'] = dd_fat
    
X_train"
2955,30831873,6,"# CALCULATE SERIES AVERAGES
d_c = X_train.diff_confirmed.drop(0).mean()
dd_c = X_train.dd_confirmed.drop(0).drop(1).mean()
d_f = X_train.diff_fatalities.drop(0).mean()
dd_f = X_train.dd_fatalities.drop(0).drop(1).mean()


print(d_c, dd_c, d_f, dd_f)"
2956,30831873,7,"# ITERATE TAYLOR SERIES
pred_c, pred_f = list(X_train.ConfirmedCases.loc[2:answer]), list(X_train.Fatalities.loc[2:answer])

for i in range(1, 44 - key):
    pred_c.append(int((X_train.ConfirmedCases[answer] + (d_c + dd_c*i) * i) * 1.3))
    pred_f.append(int((X_train.Fatalities[answer] + (d_f + dd_f*i) * i)))"
2957,30831873,8,"# WRITE SUBMISSION
my_submission = pd.DataFrame({'ForecastId': list(range(1,44)), 'ConfirmedCases': pred_c, 'Fatalities': pred_f})

my_submission.to_csv('submission.csv', index=False)"
2958,30831873,9,print(my_submission)
2959,30790196,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

# import numpy as np # linear algebra
# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the ""../input/"" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

# import os
# for dirname, _, filenames in os.walk('/kaggle/input'):
#     for filename in filenames:
#         print(os.path.join(dirname, filename))

# Any results you write to the current directory are saved as output.

# -*- coding: utf-8 -*-
""""""KaggleTest.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rV0sSw8fMw2DkVnWwzV8_cnfQDYKhz9E
""""""
# -*- coding: utf-8 -*-
""""""KaggleTest.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rV0sSw8fMw2DkVnWwzV8_cnfQDYKhz9E
""""""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

from sklearn.linear_model import LinearRegression

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

submission = pd.read_csv(""../input/covid19-local-us-ca-forecasting-week-1/ca_submission.csv"")
test = pd.read_csv(""../input/covid19-local-us-ca-forecasting-week-1/ca_test.csv"")
train = pd.read_csv(""../input/covid19-local-us-ca-forecasting-week-1/ca_train.csv"")

train = train[train.ConfirmedCases>0]

model_1 = LinearRegression()
x1 = np.array(train.Id).reshape(-1,1)
y1 = np.log(train.ConfirmedCases)
model_1.fit(x1,y1)
print(""R-squared score : "",model_1.score(x1,y1))

model_2 = LinearRegression()
x2 = np.array(train.ConfirmedCases).reshape(-1,1)
y2 = train.Fatalities
model_2.fit(x2,y2)
print(""R-Squared Score= "",model_2.score(x2,y2))

test[""Id""] = 50 + test.ForecastId

test[""LogConf""] = model_1.predict(np.array(test.Id).reshape(-1,1))
test[""ConfirmedCases""] = np.exp(test.LogConf)//1
test[""Fatalities""] = model_2.predict(np.array(test.ConfirmedCases).reshape(-1,1))//1

# from google.colab import files

submission.ConfirmedCases = test.ConfirmedCases
submission.Fatalities = test.Fatalities
submission.to_csv(""submission.csv"", index=False)
# files.download(""submission.csv"")"
2960,30898457,0,"library(tidyverse)
library(ggplot2) 
library(deSolve)
options(scipen = 999)

path_target <- ""../input/covid19-local-us-ca-forecasting-week-1""

df_data_train <- read.csv(file = paste(path_target, ""ca_train.csv"", sep = ""/""))

df_data_test <- read.csv(file = paste(path_target, ""ca_test.csv"", sep = ""/""))
df_data_submission <- read.csv(file = paste(path_target, ""ca_submission.csv"", sep = ""/""))

df_train_eda <- df_data_train %>% 
  pivot_longer(cols = c(ConfirmedCases, Fatalities), names_to = ""metric_name"", values_to = ""metric_value"") %>% 
  mutate(metric_period = as.Date(as.character(Date)))

plot_cases <- ggplot(data = df_train_eda) + theme_bw() +
  geom_line(aes(x = metric_period, y = metric_value, group = metric_name, colour = metric_name), size = 1.2) +
  scale_x_date(date_breaks = ""1 week"", labels = scales::date_format(""%b %d"")) +
  scale_colour_manual(values = c(""ConfirmedCases"" = ""steelblue"", ""Fatalities"" = ""orangered"")) +
  theme(legend.position = ""bottom"") +
  labs(title = ""Confirmed Cases and Fatalities"",
       subtitle = ""California"",
       x = ""Week Commencing"", y = ""Cases"", colour = ""Case Type"")

print(plot_cases) 

df_growth <- df_data_train %>% 
  mutate(metric_period = as.Date(as.character(Date)),  
         cum_conf = cumsum(ConfirmedCases), 
         cum_fatal = cumsum(Fatalities), 
         cum_fatal_prop = cum_fatal / cum_conf, 
         cum_conf_lag = cumsum(dplyr::lag(ConfirmedCases, default = 0)), 
         grwth = ConfirmedCases/dplyr::lag(ConfirmedCases),
         grwth_cum =cum_conf/cum_conf_lag)

rate_growth <- df_growth %>% 
  filter(metric_period == max(metric_period)) %>% 
  pull(grwth_cum) 

rate_death <- df_growth %>% 
  filter(metric_period == max(metric_period)) %>% 
  pull(cum_fatal_prop) 

#--- Susceptible
fun_dS_dt <- function(S, I, beta_const, n_pop) {
  -beta_const * S * I / n_pop 
}

#--- Exposed 
fun_dE_dt <- function(S, E, I, beta_const, epsilon_const, n_pop) {
  beta_const * S * I / n_pop - epsilon_const * E
}

#--- Infected 
fun_dI_dt <- function(I, E, epsilon_const, gamma_const) {
  epsilon_const * E - gamma_const * I
}

#--- Recovered 
fun_dR_dt <- function(I, gamma_const) {
  gamma_const * I 
}

fun_seir <- function(t, x, parms) {  
  
  if(t <= parms[""beta_day""]) {
    beta_const <- parms[""beta_0""]
  } else {
    beta_const <- parms[""beta_1""]
  }  
  
  if(t <= parms[""gamma_day""]) {
    gamma_const <- parms[""gamma_0""]
  } else {
    gamma_const <- parms[""gamma_1""]
  } 
  
  S <- x[""S""]
  E <- x[""E""]
  I <- x[""I""] 
  
  epsilon_const <- parms[""epsilon_const""]
  n_pop <- parms[""n_pop""]
  
  dS <- fun_dS_dt(S, I, beta_const, n_pop) 
  dE <- fun_dE_dt(S, E, I, beta_const, epsilon_const, n_pop)
  dI <- fun_dI_dt(I, E, epsilon_const, gamma_const) 
  dR <- fun_dR_dt(I, gamma_const) 
  res <- c(dS, dE, dI, dR)
  list(res)
}

#--- model run length 
run_days = 100

#--- model initial parameters 
mdl_init <- c(S = 36000000, 
              E = 0, 
              I = 12, 
              R = 0)
n_pop <- sum(mdl_init)

#--- infection rate
beta_0 <- 1.01
beta_intervene_day = 60
beta_1 = 0.9

#--- days infectous 
infectious_days_0 <- 2.2 # days 
infectious_days_1 <- 2.0 # days 

gamma_0 <- 1 / infectious_days_0
gamma_intervene_day = 40
gamma_1 = 1 / infectious_days_1

#--- exposed hosts, who are latently infected but not yet infectious
latency_days <- 2 # days 

this_time_steps <- seq(from = 1, to = run_days) 

#--- Parameters 
params <- c(beta_0 = beta_0,
            beta_1 = beta_1, 
            beta_day = beta_intervene_day, 
            gamma_0 = gamma_0,
            gamma_1 = gamma_1, 
            gamma_day = gamma_intervene_day,  
            epsilon_const = 1 / latency_days,
            n_pop = n_pop) 

mdl_seir <- lsoda(y = mdl_init, times = this_time_steps, func = fun_seir, parms = params) 

df_output <- as.data.frame(mdl_seir) %>% 
  rename(Susceptable = S, Exposed = E,  Infected = I, Recovered = R) %>% 
  pivot_longer(cols = c(Susceptable, Exposed, Infected, Recovered), names_to = ""metric_name"", values_to = ""metric_value"") %>% 
  mutate(metric_name = factor(metric_name, levels = c(""Susceptable"", ""Exposed"", ""Infected"", ""Recovered"")))

metric_colours <- c(""Susceptable"" = ""black"", ""Exposed"" = ""steelblue"", ""Infected"" = ""tomato"", ""Recovered"" = ""darkolivegreen3"")

plot_output <- ggplot(data = df_output) + theme_bw() + 
  geom_line(aes(x = time, y = metric_value, group = metric_name, colour = metric_name), size = 1.2) +  
  scale_colour_manual(values = metric_colours) + 
  scale_x_continuous(breaks = seq(from = 0, to = run_days, by = 10),
                     minor_breaks = seq(from = 0, to = run_days, by = 1)) + 
  scale_y_continuous(breaks = seq(from = 0, to = n_pop, by = 2e6),
                     labels = scales::comma) + 
  theme(legend.position = ""bottom"") + 
  labs(title = ""SEIR"",
       x = ""Days"",
       y = ""Cases"",
       colour = ""Status"")
print(plot_output)


df_t <- df_data_train %>% 
  select(id = Id, train_cases = ConfirmedCases)

sync_val <- 36

df_m <- df_output %>% 
  filter(metric_name == ""Infected"") %>% 
  select(id_x = time, mdl_cases = metric_value) %>% 
  mutate(id = id_x + sync_val)

df_x <- left_join(df_t, df_m, by = ""id"") %>% 
  filter(train_cases > 0)


plot_x <- ggplot(data = df_x, aes(x = id)) + theme_bw() + 
  geom_line(aes(y = train_cases), colour = ""blue"") + 
  geom_line(aes(y = mdl_cases), colour = ""red"")
print(plot_x)

start_point <- 13
submission <- df_output %>%
  filter(metric_name == ""Infected"" &
           time >= start_point &
           time <= start_point + 42) %>% 
  mutate(ForecastID = 1:43) %>% 
  select(ForecastID,
         ConfirmedCases = metric_value) %>%
  mutate(ConfirmedCases = round(ConfirmedCases, 0),
         Fatalities = round(ConfirmedCases * rate_death, 0))

write.csv(x = submission, file = ""submission.csv"", row.names = FALSE)

"
2961,30801003,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the ""../input/"" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# Any results you write to the current directory are saved as output."
2962,30801003,1,"import matplotlib.pyplot as plt
%matplotlib inline"
2963,30801003,2,"df_train = pd.read_csv(""/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_train.csv"")
df_train.head(5)"
2964,30801003,3,"df_test = pd.read_csv(""/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_test.csv"")"
2965,30801003,4,"def from_date_to_day(train, test):
    date_train = pd.to_datetime(train[""Date""])
    
    beginning = date_train.min()
    
    days_train = date_train - beginning
    train[""Day""] = days_train.dt.days
    
    date_test = pd.to_datetime(test[""Date""])
    days_test = date_test - beginning
    test[""Day""] = days_test.dt.days"
2966,30801003,5,"from_date_to_day(df_train, df_test)"
2967,30801003,6,"# get rid of log(0) problem
def log_rectified(x):
    return np.log(max(x, 0) + 1.0)

np_log_rectified = np.vectorize(log_rectified)

def inv_log_rectified(x):
    return np.exp(x) - 1.0

np_inv_log_rectified = np.vectorize(inv_log_rectified)"
2968,30801003,7,"def plot_log_cases(data):
    
    X = data[""Day""].unique()
    y = np_log_rectified(data.groupby(""Day"").ConfirmedCases.sum())
    
    plt.plot(X, y, 'bo')"
2969,30801003,8,plot_log_cases(df_train)
2970,30801003,9,"from scipy import interpolate
class Model:
    def __init__(self, train, target):
        X = np.arange(0, train.Day.max() + 1)
        y_log = np_log_rectified(train.groupby(""Day"")[target].sum()[X])
        
        self.f1 = interpolate.interp1d(X, y_log, fill_value=""extrapolate"", kind=""linear"")
    
    def predict(self, test):
        return self.f1(test)"
2971,30801003,10,"def plot_prediction(data, model, target):
    X = np.linspace(0,65,100)
    y_pred = np_inv_log_rectified(model.predict(X))
    
    days = data[""Day""].unique()
    y = data.groupby(""Day"")[target].sum()
    
    plt.plot(days, y, 'bo')
    plt.plot(X, y_pred)"
2972,30801003,11,"def plot_error(data, model, target):
    days = data[""Day""].unique()
    y = data.groupby(""Day"")[target].sum()
    
    y_pred = np_inv_log_rectified(model.predict(days))
    
    plt.plot(days, abs(y - y_pred))"
2973,30801003,12,"model_case = Model(df_train, ""ConfirmedCases"")
plot_prediction(df_train, model_case, ""ConfirmedCases"")"
2974,30801003,13,"plot_error(df_train, model_case, ""ConfirmedCases"")"
2975,30801003,14,"model_fatal = Model(df_train, ""Fatalities"")
plot_prediction(df_train, model_fatal, ""Fatalities"")"
2976,30801003,15,"plot_error(df_train, model_fatal, ""Fatalities"")"
2977,30801003,16,"pred_cases = np_inv_log_rectified(model_case.predict(df_test.Day)).astype(int)
pred_fatal = np_inv_log_rectified(model_fatal.predict(df_test.Day)).astype(int)"
2978,30801003,17,"submission = pd.concat(
    [ pd.Series(np.arange(1, df_test.ForecastId.max() + 1)),
     pd.Series(pred_cases),
     pd.Series(pred_fatal)],
    axis=1)"
2979,30801003,18,submission.head(5)
2980,30801003,19,"submission.to_csv('submission.csv', header=['ForecastId', 'ConfirmedCases', 'Fatalities'], index=False)"
2981,30848066,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the ""../input/"" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# Any results you write to the current directory are saved as output."
2982,30848066,1,"import pandas as pd
sample_submission = pd.read_csv(""/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_submission.csv"")
test = pd.read_csv(""/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_test.csv"")
train = pd.read_csv(""/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_train.csv"")"
2983,30848066,2,"len(train)
"
2984,30848066,3,sample_submission.head()
2985,30848066,4,test.head()
2986,30848066,5,train.tail()
2987,30848066,6,"#make a heatmap

import folium
from folium import Choropleth, Marker
from folium.plugins import HeatMap, MarkerCluster
m = folium.Map(location=[37, -115], zoom_start=6) 
def embed_map(m, file_name):
    from IPython.display import IFrame
    m.save(file_name)
    return IFrame(file_name, width='100%', height='750px')

#merge test and training data
Full_data = pd.merge(test, train, on=['Lat','Long','Date'])

# Add a heatmap to the base map
HeatMap(data=Full_data[['Lat', 'Long']], radius=11).add_to(m)

# Show the map
embed_map(m, ""q_1.html"")"
2988,30848066,7,"#rename therefor the data columns
train.rename(columns={'Province/State':'Province'}, inplace=True)
train.rename(columns={'Country/Region':'Country'}, inplace=True)
train.rename(columns={'ConfirmedCases':'Confirmed'}, inplace=True)"
2989,30848066,8,"#and we do the same for test set
test.rename(columns={'Province/State':'Province'}, inplace=True)
test.rename(columns={'Country/Region':'Country'}, inplace=True)"
2990,30848066,9,"from sklearn.preprocessing import LabelEncoder
# creating initial dataframe
bridge_types = ('Lat', 'Date', 'Province', 'Country', 'Long', 'Confirmed',
       'ForecastId', 'Id')
countries = pd.DataFrame(train, columns=['Country'])
# creating instance of labelencoder
labelencoder = LabelEncoder()
# Assigning numerical values and storing in another column
train['Countries'] = labelencoder.fit_transform(train['Country'])

#do the same for test set
test['Countries'] = labelencoder.fit_transform(test['Country'])

#check label encoding 
train['Countries'].head()
"
2991,30848066,10,"train['Date']= pd.to_datetime(train['Date']) 
test['Date']= pd.to_datetime(test['Date']) "
2992,30848066,11,"train = train.set_index(['Date'])
test = test.set_index(['Date'])"
2993,30848066,12,"def create_time_features(df):
    """"""
    Creates time series features from datetime index
    """"""
    df['date'] = df.index
    df['hour'] = df['date'].dt.hour
    df['dayofweek'] = df['date'].dt.dayofweek
    df['quarter'] = df['date'].dt.quarter
    df['month'] = df['date'].dt.month
    df['year'] = df['date'].dt.year
    df['dayofyear'] = df['date'].dt.dayofyear
    df['dayofmonth'] = df['date'].dt.day
    df['weekofyear'] = df['date'].dt.weekofyear
    
    X = df[['hour','dayofweek','quarter','month','year',
           'dayofyear','dayofmonth','weekofyear']]
    return X"
2994,30848066,13,"create_time_features(train).head()
create_time_features(test).head()"
2995,30848066,14,train.head()
2996,30848066,15,"train.drop(""date"", axis=1, inplace=True)
test.drop(""date"", axis=1, inplace=True)"
2997,30848066,16,# train.isnull().sum()
2998,30848066,17,"#drop useless columns for train and test set
train.drop(['Country'], axis=1, inplace=True)
train.drop(['Province'], axis=1, inplace=True)"
2999,30848066,18,"test.drop(['Country'], axis=1, inplace=True)
test.drop(['Province'], axis=1, inplace=True)"
3000,30848066,19,"from sklearn.tree import DecisionTreeRegressor  
regressor = DecisionTreeRegressor(random_state = 0) "
3001,30848066,20,"# import xgboost as xgb
# from xgboost import plot_importance, plot_tree
from sklearn.metrics import mean_squared_error, mean_absolute_error

# reg= xgb.XGBRegressor(n_estimators=1000)"
3002,30848066,21,train.head()
3003,30848066,22,"# features that will be used in the model
x = train[['Lat', 'Long','Countries','dayofweek','month','dayofyear','weekofyear']]
y1 = train[['Confirmed']]
y2 = train[['Fatalities']]
x_test = test[['Lat', 'Long','Countries','dayofweek','month','dayofyear','weekofyear']]"
3004,30848066,23,x.head()
3005,30848066,24,"#use model on data 
regressor.fit(x,y1)
predict_1 = regressor.predict(x_test)
predict_1 = pd.DataFrame(predict_1)
predict_1.columns = [""Confirmed_predict""]"
3006,30848066,25,predict_1.head()
3007,30848066,26,"#use model on data 
regressor.fit(x,y2)
predict_2 = regressor.predict(x_test)
predict_2 = pd.DataFrame(predict_2)
predict_2.columns = [""Death_prediction""]
predict_2.head()"
3008,30848066,27,"# plot = plot_importance(regressor, height=0.9, max_num_features=20)"
3009,30848066,28,"Samle_submission = pd.read_csv(""/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_submission.csv"")
Samle_submission.columns
submission = Samle_submission[[""ForecastId""]]"
3010,30848066,29,"Final_submission = pd.concat([predict_1,predict_2,submission],axis=1)
Final_submission.head()"
3011,30848066,30,"Final_submission.columns = ['ConfirmedCases', 'Fatalities', 'ForecastId']
Final_submission = Final_submission[['ForecastId','ConfirmedCases', 'Fatalities']]

Final_submission[""ConfirmedCases""] = Final_submission[""ConfirmedCases""].astype(int)
Final_submission[""Fatalities""] = Final_submission[""Fatalities""].astype(int)"
3012,30848066,31,Final_submission.head()
3013,30848066,32,"Final_submission.to_csv(""submission.csv"",index=False)
print('Model ready for submission!')"
3014,30823052,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_squared_log_error
from sklearn.compose import TransformedTargetRegressor
from sklearn.linear_model import LinearRegression,  Ridge
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline

# Input data files are available in the ""../input/"" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# Any results you write to the current directory are saved as output."
3015,30823052,1,"train = pd.read_csv('/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_train.csv')
train.head()"
3016,30823052,2,"# Clean the data. only look at the data from the first confirmed case
train = train[train.ConfirmedCases>0]
train"
3017,30823052,3,"whole_world_data = pd.read_csv('/kaggle//input/covid19-global-forecasting-week-1/train.csv')
us_data = whole_world_data.loc[whole_world_data['Country/Region'] == 'US']
us_data"
3018,30823052,4,"possible_states = us_data['Province/State'].unique()[(us_data.groupby('Province/State').max().ConfirmedCases>144 ) &  (us_data.groupby('Province/State').min().ConfirmedCases<6)]
possible_states"
3019,30823052,5,"COLUMNS = ['ConfirmedCases', 'Fatalities', 'Province/State']
possible_starts = pd.DataFrame(columns=COLUMNS)
for country in possible_states:
    possible_starts = pd.concat([possible_starts, us_data[(us_data['Province/State'] == country) & (us_data['ConfirmedCases']<144) & (us_data['ConfirmedCases']>0)][COLUMNS]])

possible_starts"
3020,30823052,6,"# First, find which start is the best for an exponential pattern, and what is the lowest cross validation we get with it
def test_exponential_accuraccy(y):
    accuraccy = 0
    X = np.asarray(list(range(len(y)))) +1
    tscv = TimeSeriesSplit(len(X)-1)
    for train_index, test_index in tscv.split(X):
        X_train, X_test = X[train_index], X[test_index]
        y_train, y_test = y[train_index], y[test_index]

        # Transform the data with a log function and after prediction, apply the exp() function.
        regressor = TransformedTargetRegressor(regressor=LinearRegression(),
                                                         func=np.log1p,
                                                         inverse_func=np.expm1)
        regressor.fit(np.array(X_train).reshape(-1,1), np.array(y_train).reshape(-1,1))
        y_pred = regressor.predict(np.array(X_test).reshape(-1,1))
        accuraccy += mean_squared_log_error(y_pred, y_test)
        
        
    
    return accuraccy"
3021,30823052,7,"# For every state, compute its sliding window error.
best_accuraccy = 100
best_start_state = ''
for state in possible_starts['Province/State'].unique():
    possible_train_data = pd.concat([possible_starts[possible_starts['Province/State']==state], train])
    
    state_accuracy = test_exponential_accuraccy(possible_train_data['ConfirmedCases'].values) 
    if  state_accuracy < best_accuraccy:
        best_start_state = state
        best_accuraccy = state_accuracy
                                
print(best_start_state, best_accuraccy)"
3022,30823052,8,"# Now, lets find which start is the best for a polynomial pattern, what is the degree of the polynom,
# and what is the lowest cross validation we get with it
def test_polynomial_accuraccy(y, degree):
    accuraccy = 0
    X = np.asarray(list(range(len(y)))) +1
    tscv = TimeSeriesSplit(len(X)-1)
    for train_index, test_index in tscv.split(X):
        X_train, X_test = X[train_index], X[test_index]
        y_train, y_test = y[train_index], y[test_index]
        
        # Fit the data with a polynom of the given degree.
        model = make_pipeline(PolynomialFeatures(degree), Ridge())
        model.fit(X_train.reshape(-1,1), y_train.reshape(-1,1))
        y_pred = model.predict(X_test.reshape(-1,1))
#         print(type(y_pred))
        if y_pred < 0: 
            y_pred = np.array([0]) # this case can happen in the beggining of the sliding window, and a high degree polynom. y_pred=0 is a big enough error
        accuraccy += mean_squared_log_error(y_pred, y_test)
        
        
    
    return accuraccy"
3023,30823052,9,"# For every state and polynom degree, compute its sliding window error
# What we are going to do is look at a few options and decide where we get a low enough error, and not over generalize polynom.

degrees = [2,3,4,5,6]
lowest_errors = pd.DataFrame(columns=['accuraccy', 'state', 'degree'])
for degree in degrees:
    best_accuraccy = 100
    best_start_state = ''
    for state in possible_starts['Province/State'].unique():
        possible_train_data = pd.concat([possible_starts[possible_starts['Province/State']==state], train])
    
        state_accuracy = test_polynomial_accuraccy(possible_train_data['ConfirmedCases'].values, degree)
        if  state_accuracy < best_accuraccy:
            best_start_state = state
            best_accuraccy = state_accuracy
    
    lowest_errors = lowest_errors.append(pd.DataFrame({'accuraccy': [best_accuraccy], 'state': [best_start_state], 'degree': [degree]}))"
3024,30823052,10,lowest_errors
3025,30823052,11,possible_starts[possible_starts['Province/State'] == 'Massachusetts']
3026,30823052,12,"train = pd.concat([possible_starts[possible_starts['Province/State'] == 'Massachusetts'], train])[['ConfirmedCases', 'Fatalities']]
train = train.loc[train.ConfirmedCases>0]"
3027,30823052,13,train.reset_index()
3028,30823052,14,"days_to_predict = 43 # Change to 29
public_leader_board_first_column=7 # Change to 26
model = make_pipeline(PolynomialFeatures(3), Ridge())

#ConfirmedCases predictions
X_train = np.array(range(len(train))) + 1
X_test = np.array(range(public_leader_board_first_column,public_leader_board_first_column+days_to_predict)) + 1
y_train = train.ConfirmedCases.values
model.fit(X_train.reshape(-1,1), y_train.reshape(-1,1))
confirmed_cases_predictions = model.predict(X_test.reshape(-1,1))
confirmed_cases_predictions = list(map(lambda x: x[0], confirmed_cases_predictions.tolist()))

#Fatalities predictions
y_train = train.Fatalities.values
model.fit(X_train.reshape(-1,1), y_train.reshape(-1,1))
fatalities_predictions = model.predict(X_test.reshape(-1,1))
fatalities_predictions = list(map(lambda x: x[0], fatalities_predictions.tolist()))

submissions = pd.DataFrame({'ConfirmedCases': confirmed_cases_predictions, 'Fatalities': fatalities_predictions})
submissions.to_csv('submission.csv', index=False)

"
3029,30823052,15,
3030,30844920,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt

# Input data files are available in the ""../input/"" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# Any results you write to the current directory are saved as output."
3031,30844920,1,"def load_training_csv(path):
    """"""helper function to regularize the preprocessing of dataframes""""""
    df = pd.read_csv(path, header=0, parse_dates=['Date'])
    #df.drop(df[((df['ConfirmedCases'] == 0) & (df['Fatalities'] == 0))].index, inplace=True)
    df['ConfirmedCases_log1p'] = df['ConfirmedCases'].map(np.log1p)
    df['Fatalities_log1p'] = df['Fatalities'].map(np.log1p)
    df.drop(['Province/State', 'Country/Region','Lat','Long'], axis=1, inplace=True)
    return df

def load_case_data_from_csv(path):
    """"""load case count data where it's > 0 only and drop unneeded columns""""""
    df = pd.read_csv(path, header=0, parse_dates=['Date'])
    df.drop(df[(df['ConfirmedCases'] == 0)].index, inplace=True)
    df['ConfirmedCases_log1p'] = df['ConfirmedCases'].map(np.log1p)
    df.drop(['Fatalities', 'Province/State', 'Country/Region','Lat','Long'], axis=1, inplace=True)
    return df

def load_fatality_data_from_csv(path):
    """"""load fatality count data where it's > 0 only and drop unneeded columns""""""
    df = pd.read_csv(path, header=0, parse_dates=['Date'])
    df.drop(df[(df['Fatalities'] == 0)].index, inplace=True)
    df['Fatalities_log1p'] = df['Fatalities'].map(np.log1p)
    df.drop(['ConfirmedCases', 'Province/State', 'Country/Region','Lat','Long'], axis=1, inplace=True)
    return df

def rmsle(y_true, y_pred):
    """"""return the root mean squared logarithmic error: square root of the mean squared error of the natural log 
    of (value plus 1): sqrt(mean(power(log1p(p)-log1p(a),2)))
    """"""
    return np.sqrt(np.mean(np.power(np.log1p(y_true) - np.log1p(y_pred),2)))



from scipy.stats import linregress

full_df = load_training_csv(""../input/covid19-local-us-ca-forecasting-week-1/ca_train.csv"") # for reference
cdf = load_case_data_from_csv(""../input/covid19-local-us-ca-forecasting-week-1/ca_train.csv"") # just case counts
fdf = load_fatality_data_from_csv(""../input/covid19-local-us-ca-forecasting-week-1/ca_train.csv"") # just fatality counts

y_cases_train = cdf['ConfirmedCases_log1p'].values
y_cases_true = cdf['ConfirmedCases'].values
y_fatalities_train = fdf['Fatalities_log1p'].values
y_fatalities_true = fdf['Fatalities'].values

# make some x values for regression purposes
case_xs = range(0, len(y_cases_train))
fat_xs = range(0, len(y_fatalities_train))

#
# linear regressionon against log1p transformed data
#

cslope, cintercept, cr, cp, csterr = linregress(case_xs, y_cases_train)
fslope, fintercept, fr, fp, fsterr = linregress(fat_xs, y_fatalities_train)
print(""case log1p(y) = mx + b: {0:.4f}x + {1:.4f} sterr {2:.4f} r={3:.4f}"".format(cslope, cintercept, csterr, cr))
print(""fatality log1p(y) = mx + b:{0:.4f}x + {1:.4f} sterr {2:.4f} r={3:.4f}"".format(fslope, fintercept, fsterr, fr))

#
# polyfit the transformed data
#

poly_degrees = 5
zcase = np.polyfit(case_xs, y_cases_train, deg=poly_degrees)
zfat = np.polyfit(fat_xs, y_fatalities_train, deg=poly_degrees)
case_poly = np.poly1d(zcase)
fat_poly = np.poly1d(zfat)

#
# check linear fit against existing data
#

y_cases_pred_linear = np.round(np.expm1(case_xs * cslope + cintercept), 0)
y_fatalities_pred_linear = np.round(np.expm1(fat_xs * fslope + fintercept), 0)
case_rmsle = rmsle(y_cases_true, y_cases_pred_linear)
fat_rmsle = rmsle(y_fatalities_true, y_fatalities_pred_linear)
plt.plot(case_xs, y_cases_true,'o')
plt.plot(case_xs, y_cases_pred_linear, '-', label=""RMSLE={0:.4f}"".format(case_rmsle))
plt.plot(fat_xs, y_fatalities_true, 'o')
plt.plot(fat_xs, y_fatalities_pred_linear, '-', label=""RMSLE={0:.4f}"".format(fat_rmsle))
plt.legend()
plt.show()

#
# check poly fit against existing data
#

y_cases_pred_poly = np.round(np.expm1(case_poly(case_xs)))
y_fatalities_pred_poly = np.round(np.expm1(fat_poly(fat_xs)))
case_poly_rmsle = rmsle(y_cases_true, y_cases_pred_poly)
fat_poly_rmsle = rmsle(y_fatalities_true, y_fatalities_pred_poly)
plt.plot(case_xs, y_cases_true, 'o')
plt.plot(case_xs, y_cases_pred_poly, '-', label=""RMSLE={0:.4f}"".format(case_poly_rmsle))
plt.plot(fat_xs, y_fatalities_true, 'o')
plt.plot(fat_xs, y_fatalities_pred_poly, '-', label='RMSLE={0:.4f}'.format(fat_poly_rmsle))
plt.legend()
plt.show()
                                      
"
3032,30844920,2,"#print(full_df.tail())

# load test data and predict based on above regression
test_df = pd.read_csv(""../input/covid19-local-us-ca-forecasting-week-1/ca_test.csv"")
print(test_df.head())
print(test_df.shape)
test_xs = range(2,test_df.shape[0]+2)
print(len(test_xs))
#print(list(test_xs))
test_df['xs'] = test_xs
#test_df['ConfirmedCases'] = test_df.apply(lambda x:np.round(np.expm1(x['xs']*cslope + cintercept)), axis=1)
#test_df['Fatalities'] = test_df.apply(lambda x: np.round(np.expm1(x['xs']*fslope + fintercept)), axis=1)
test_df['ConfirmedCases'] = test_df.apply(lambda x: np.round(np.expm1(case_poly(case_xs))), axis=1)
test_df['Fatalities'] = test_df.apply(lambda x: np.round(np.expm1(fat_poly(fat_xs))), axis=1)

print(test_df[test_df['Date'] > '2020-03-18'])

sub_df = test_df[['ForecastId','ConfirmedCases','Fatalities']]
sub_df.to_csv('submission.csv', index=False, header=True)"
3033,30919301,0,"import pandas as pd
import plotly.graph_objects as go
import plotly.express as px
# Plotly installation: https://plot.ly/python/getting-started/#jupyterlab-support-python-35"
3034,30919301,1,"df = pd.read_csv('../input/covid19-local-us-ca-forecasting-week-1/ca_train.csv')
df=df[df[""Date""]>""2020-03-09""] # Only keep dates with confirmed cases
df.head()"
3035,30919301,2,"# Reference: https://plot.ly/python/time-series/
fig = go.Figure(
    [go.Scatter(x=df['Date'], y=df['ConfirmedCases'])],
    layout_title_text=""Confirmed Cases in California""
)
fig.update_layout(
    yaxis_type=""log"",
    margin=dict(l=20, r=20, t=50, b=20),
    template=""plotly_white"")
fig.show()"
3036,30919301,3,"df_test = pd.read_csv('../input/covid19-local-us-ca-forecasting-week-1/ca_test.csv')
print(df_test.shape)
df_test.head()"
3037,30919301,4,"public_leaderboard_start_date = ""2020-03-12""
last_public_leaderboard_train_date = ""2020-03-11""
public_leaderboard_end_date  = ""2020-03-26""

cases  = df[df[""Date""]==last_public_leaderboard_train_date][""ConfirmedCases""].values[0] * (2**(1/4))
df_test.insert(1, ""ConfirmedCases"", 0)"
3038,30919301,5,"for i in range(15):
    df_test.loc[i, ""ConfirmedCases""] = cases
    cases = cases * (2**(1/4))    
df_test.head()"
3039,30919301,6,"# Reference: https://plot.ly/python/time-series/
fig = go.Figure(
    [
        go.Scatter(x=df['Date'], y=df['ConfirmedCases'], name=""actual""),
        go.Scatter(x=df_test['Date'].iloc[:15], y=df_test['ConfirmedCases'], name=""predicted""),
    ],
    layout_title_text=""Confirmed Cases in California""
)
fig.update_layout(
    yaxis_type=""log"",
    margin=dict(l=20, r=20, t=50, b=20),
    template=""plotly_white"")
fig.show()"
3040,30919301,7,"df_growth = pd.DataFrame({
    ""Date"": df[""Date""].iloc[1:].values,
    ""Rate"": df[""ConfirmedCases""].iloc[1:].values / df[""ConfirmedCases""].iloc[:-1].values * 100
})"
3041,30919301,8,"# Reference: https://plot.ly/python/bar-charts/
fig = px.bar(df_growth, x='Date', y='Rate', width=600, height=400)
fig.update_layout(
    margin=dict(l=20, r=20, t=50, b=20),
    template=""plotly_white"",
    title=""Empirical Growth Rate"",
    yaxis_title=""Rate (%)""
)
fig.update_yaxes(range=[100, 135])
fig.show()"
3042,30919301,9,"# rate = df_growth[""Rate""].mean() / 100
rate = df_growth[""Rate""].iloc[:7].median() / 100
print(f""Rate used: {rate:.4f}"")
df_test = pd.read_csv('../input/covid19-local-us-ca-forecasting-week-1/ca_test.csv')
public_leaderboard_start_date = ""2020-03-12""
last_public_leaderboard_train_date = ""2020-03-11""
public_leaderboard_end_date  = ""2020-03-26""

cases  = df[df[""Date""]==last_public_leaderboard_train_date][""ConfirmedCases""].values[0] * (rate)
df_test.insert(1, ""ConfirmedCases"", 0)
for i in range(15):
    df_test.loc[i, ""ConfirmedCases""] = cases
    cases = cases * rate  
df_test.head()"
3043,30919301,10,"# Reference: https://plot.ly/python/time-series/
fig = go.Figure(
    [
        go.Scatter(x=df['Date'], y=df['ConfirmedCases'], name=""actual""),
        go.Scatter(x=df_test['Date'].iloc[:15], y=df_test['ConfirmedCases'], name=""predicted""),
    ],
    layout_title_text=""Confirmed Cases in California""
)
fig.update_layout(
    yaxis_type=""log"",
    margin=dict(l=20, r=20, t=50, b=20),
    template=""plotly_white"")
fig.show()"
3044,30919301,11,"# Reference: https://plot.ly/python/time-series/
fig = go.Figure(
    [go.Scatter(x=df['Date'], y=df['Fatalities'])],
    layout_title_text=""Fatalities in California""
)
fig.update_layout(
    yaxis_type=""log"",
    margin=dict(l=20, r=20, t=50, b=20),
    template=""plotly_white"")
fig.show()"
3045,30919301,12,"df_growth = pd.DataFrame({
    ""Date"": df[""Date""].iloc[1:].values,
    ""Rate"": df[""Fatalities""].iloc[1:].values / df[""Fatalities""].iloc[:-1].values * 100
})
# Reference: https://plot.ly/python/bar-charts/
fig = px.bar(df_growth, x='Date', y='Rate', width=600, height=400)
fig.update_layout(
    margin=dict(l=20, r=20, t=50, b=20),
    template=""plotly_white"",
    title=""Empirical Growth Rate"",
    yaxis_title=""Rate (%)""
)
fig.update_yaxes(range=[100, 180])
fig.show()"
3046,30919301,13,"rate = df_growth[""Rate""].iloc[:7].median() / 100
print(f""Rate used: {rate:.4f}"")
df_test = pd.read_csv('../input/covid19-local-us-ca-forecasting-week-1/ca_test.csv')
public_leaderboard_start_date = ""2020-03-12""
last_public_leaderboard_train_date = ""2020-03-11""
public_leaderboard_end_date  = ""2020-03-26""

cases  = df[df[""Date""]==last_public_leaderboard_train_date][""Fatalities""].values[0] * (rate)
df_test.insert(1, ""Fatalities"", 0)
for i in range(15):
    df_test.loc[i, ""Fatalities""] = cases
    cases = cases * rate  
df_test.head()"
3047,30919301,14,"# Reference: https://plot.ly/python/time-series/
fig = go.Figure(
    [
        go.Scatter(x=df['Date'], y=df['Fatalities'], name=""actual""),
        go.Scatter(x=df_test['Date'].iloc[:15], y=df_test['Fatalities'], name=""predicted""),
    ],
    layout_title_text=""Fatalities in California""
)
fig.update_layout(
    yaxis_type=""log"",
    margin=dict(l=20, r=20, t=50, b=20),
    template=""plotly_white"")
fig.show()"
3048,30842974,0,"import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))"
3049,30842974,1,"import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

import tensorflow as tf
from tensorflow import keras
from keras.models import Sequential
from keras.layers import Dense, LSTM, Dropout"
3050,30842974,2,"train = pd.read_csv('/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_train.csv')
train.Date = pd.to_datetime(train.Date)
train.info()"
3051,30842974,3,train
3052,30842974,4,"cc = train.ConfirmedCases.values
f = train.Fatalities.values
plt.plot(cc, color = 'blue', label = 'Confirmed Cases')
plt.plot(f, color = 'orange', label = 'Fatalities')
plt.legend()
plt.show()"
3053,30842974,5,"plot_acf(cc)
plot_pacf(cc)
plt.show()"
3054,30842974,6,"plot_acf(f)
plot_pacf(f)
plt.show()"
3055,30842974,7,"ccf = train[['ConfirmedCases', 'Fatalities']]
ccf = ccf[ccf.ConfirmedCases > 0].values
ccf_max, ccf_min = np.max(ccf), np.min(ccf)
ccf_norm = (ccf - ccf_min) / (ccf_max - ccf_min)

X, y = [], []
for i in range(len(ccf_norm)):
    end = i+2
    if end > len(ccf_norm)-1:
        break
    X.append(ccf_norm[i:end])
    y.append(ccf_norm[end])
    
X, y = np.array(X).reshape(-1, 2, 2), np.array(y)
print(X.shape, y.shape)"
3056,30842974,8,"tf.random.set_seed(1)

m = Sequential()
m.add(LSTM(100, input_shape = (X.shape[1], X.shape[2]), activation = 'relu'))
m.add(Dense(2))
m.compile(loss = 'mse', optimizer = 'adam')
h = m.fit(X, y, epochs = 100, verbose = 0)"
3057,30842974,9,"plt.plot(h.history['loss'], label = 'Loss')
plt.legend()
plt.show()"
3058,30842974,10,"plt.figure(figsize = (10, 4))
plt.subplot(121)
plt.title('Confirmed Cases')
plt.plot(np.pad((m.predict(X) * (ccf_max - ccf_min) + ccf_min)[:,0], (2,0)), 'r--', label = 'Predict')
plt.plot(ccf[:,0], label = 'Actual')
plt.legend()

plt.subplot(122)
plt.title('Fatalities')
plt.plot(np.pad((m.predict(X) * (ccf_max - ccf_min) + ccf_min)[:,1], (2,0)), 'r--', label = 'Predict')
plt.plot(ccf[:,1], label = 'Actual')
plt.legend()
plt.show()"
3059,30842974,11,"test = pd.read_csv('/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_test.csv')
pd.concat([test.head(1), test.tail(1)])"
3060,30842974,12,"test_pred = y[-2:]
n = len(test) - len(train[train.Date >= '2020-03-12'])
for i in range(n):
    p = m.predict(test_pred[-2:].reshape(-1, 2, 2))
    test_pred = np.append(test_pred, p).reshape(-1, 2)
    i += 1"
3061,30842974,13,"test_pred_round = np.round(test_pred * (ccf_max - ccf_min) + ccf_min, 0)[:-2]
plt.plot(test_pred_round[:,0], label = 'Conf Cases Pred')
plt.plot(test_pred_round[:,1], label = 'Fatal Pred')
plt.legend()
plt.show()"
3062,30842974,14,sub = pd.read_csv('/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_submission.csv')
3063,30842974,15,"cc_sub = np.append(train[train.Date >= '2020-03-12'].ConfirmedCases.values, test_pred_round[:, 0])
f_sub = np.append(train[train.Date >= '2020-03-12'].Fatalities.values, test_pred_round[:, 1])"
3064,30842974,16,"sub = sub.assign(ConfirmedCases = cc_sub, Fatalities = f_sub)
sub.to_csv('submission.csv', index = False)"
3065,30776229,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the ""../input/"" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# Any results you write to the current directory are saved as output."
3066,30776229,1,"import pandas as pd
import numpy as np"
3067,30776229,2,"train = pd.read_csv('/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_train.csv')
test = pd.read_csv('/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_test.csv')"
3068,30776229,3,train.info()
3069,30776229,4,test.info()
3070,30776229,5,"train['Date'] = pd.to_datetime(train['Date'])
test['Date'] = pd.to_datetime(test['Date'])"
3071,30776229,6,train.head(5)
3072,30776229,7,train.tail(5)
3073,30776229,8,"train['Date'] = train['Date'].astype('int64')
test['Date'] = test['Date'].astype('int64')"
3074,30776229,9,train.tail(5)
3075,30776229,10,train.info()
3076,30776229,11,"train.iloc[:,-3].sample(3)"
3077,30776229,12,"X = train.iloc[:,-3]
print(X.shape)
X = np.array(X).reshape(-1,1)
print(X.shape)"
3078,30776229,13,"Y = train.iloc[:,-2:]
print(Y.shape)
Y.sample(3)"
3079,30776229,14,"from sklearn.model_selection import train_test_split 
trainX , valX, trainY, valY = train_test_split(X, Y, random_state=1)"
3080,30776229,15,"y1Train = trainY.iloc[:,0]
print(y1Train.shape)
y1Train.sample(3)"
3081,30776229,16,"y2Train = trainY.iloc[:,1]
y2Train.sample(3)"
3082,30776229,17,"y1Val = valY.iloc[:,0]
y1Val.sample(3)"
3083,30776229,18,"y2Val = valY.iloc[:,1]
y2Val.sample(3)"
3084,30776229,19,print(trainX.shape)
3085,30776229,20,"from sklearn.tree import DecisionTreeRegressor
lrModel1 = DecisionTreeRegressor(random_state = 27)
%time lrModel1.fit(trainX, y1Train)"
3086,30776229,21,"%time y1Pred = lrModel1.predict(valX)
print(y1Pred[:,])"
3087,30776229,22,"from sklearn.metrics import mean_absolute_error

print(""Accuracy in train set : "", lrModel1.score(trainX, y1Train))
print(""RMSE : "", mean_absolute_error(y1Val, y1Pred)**(0.5))"
3088,30776229,23,"lrModel2 = DecisionTreeRegressor(random_state = 27)
%time lrModel2.fit(trainX.reshape(-1, 1), y2Train)

%time y2Pred = lrModel2.predict(valX)

print(""Accuracy in train set : "", lrModel2.score(trainX, y2Train))
print(""RMSE : "", mean_absolute_error(y2Val, y2Pred)**(0.5))"
3089,30776229,24,"print(test.shape)
test.sample(3)"
3090,30776229,25,"forecastID = test.iloc[:,0]"
3091,30776229,26,"test.iloc[:,-1].sample(3)"
3092,30776229,27,"test = np.array(test.iloc[:,-1]).reshape(-1,1)"
3093,30776229,28,"%time finalPred1 = lrModel1.predict(test)
print(finalPred1[:,])"
3094,30776229,29,"%time finalPred2 = lrModel2.predict(test)
print(finalPred2[:,])"
3095,30776229,30,"outputFile = pd.DataFrame({""ForecastId"": forecastID,
                           ""ConfirmedCases"": (finalPred1+0.5).astype('int'),
                           ""Fatalities"": (finalPred2+0.5).astype('int')})"
3096,30776229,31,outputFile.sample(3)
3097,30776229,32,"outputFile.to_csv(""submission.csv"", index=False)"
3098,30772485,0,"library(data.table)
library(dplyr)
library(ggplot2)
library(leaflet)

dt<-fread(""../input/novel-corona-virus-2019-dataset/covid_19_data.csv"",stringsAsFactors = F)
dti<-fread(""../input/covid19-in-italy/covid19_italy_region.csv"",stringsAsFactors = F)
colnames(dt)<-c(""SNo"",""Date"", ""Province"",""Country"",""Update"",""Confirmed"",""Deaths"",""Recovered"")
dt<-select(dt,-SNo,-Update)
dt$Province[dt$Province==""France""|dt$Province==""United Kingdom""|dt$Province==""UK""|dt$Province==""Netherlands""|dt$Province==""Denmark""]<-""""
dt$Date<-as.Date(dt$Date, ""%m/%d/%Y"")"
3099,30772485,1,"dt$day_cont<-as.numeric(as.Date(dt$Date)-as.Date(min(dt$Date)))
"
3100,30772485,2,"dt$province_country<-paste(dt$Province,dt$Country,sep = "", "")
dt$province_country[is.na(dt$Province)|dt$Province==""""]<-dt$Country[is.na(dt$Province)|dt$Province==""""]
dti$Date<-as.Date(dti$Date, ""%Y-%m-%d"")
dti$day_cont<-as.numeric(as.Date(dti$Date)-as.Date(min(dt$Date)))
dti<-dti[order(dti$day_cont,decreasing = T),]
dti_last<-dti[!duplicated(dti$RegionName),]
dti_last<-select(dti_last,Latitude,Longitude,TotalPositiveCases)

dti<-select(dti,RegionName,TotalPositiveCases,Deaths,Recovered,day_cont)
dti$province_country<-paste0(dti$RegionName,"", Italy"")
dti<-select(dti,-RegionName)
dt<-dt[!dt$Country==""Italy""]
dt<-select(dt,-Date,-Province,-Country)
setnames(dti, ""TotalPositiveCases"",""Confirmed"")

dt<-rbind(dt,dti)

last<-dt[order(dt$day_cont,decreasing = T),]
last<-last[!duplicated(last$province_country),]
last<-last[order(last$Confirmed,decreasing = T),]"
3101,30772485,3,"print(last[1:20,])"
3102,30772485,4,"regions_to_compare<-c(""Hubei, Mainland China"",""Lombardia, Italy"",""Spain"",""Emilia Romagna, Italy"",
                     ""New York, US"",""Washington, US"",""California, US"" )

dd<-dt[dt$province_country %in% regions_to_compare]
"
3103,30772485,5,"dd[,day_100_cases:=min(day_cont[Confirmed>100]), by=province_country]
dd$day_100<-dd$day_cont-dd$day_100_cases
dd<-dd[dd$day_100>0]

dd_100<-select(dd,province_country,day_100,Confirmed)

ggplot(dd_100, aes(x = day_100, y = Confirmed, colour = province_country)) + 
  geom_line() + 
  ylab(label=""Cummulative cases"") + 
  xlab(""Days since 100th reported case"") + 
  scale_colour_manual(values=c(""red"", ""blue"",""green"",""orange"",""black"",""yellow"",""brown"",""pink""))"
3104,30772485,6,"dd_log<-dd_100
dd_log$Confirmed<-log10(dd_log$Confirmed)
ggplot(dd_log, aes(x = day_100, y = Confirmed, colour = province_country)) + 
  geom_line() + 
  ylab(label=""log of Cummulative cases"") + 
  xlab(""Days since 100th reported case"") + 
  scale_colour_manual(values=c(""red"", ""blue"",""green"",""orange"",""black"",""yellow"",""brown"",""pink""))"
3105,30772485,7,"dd$deaths_to_cases<-dd$Deaths/dd$Confirmed*100
ggplot(dd, aes(x = day_100, y = deaths_to_cases, colour = province_country)) + 
  geom_line() + 
  ylab(label=""deaths per 100 cases"") + 
  xlab(""Days since 100th reported case"") + 
  scale_colour_manual(values=c(""red"", ""blue"",""green"",""orange"",""black"",""yellow"",""brown"",""pink""))"
3106,30772485,8,"dd$recovered_to_confirmed<-dd$Recovered/dd$Confirmed
ggplot(dd, aes(x = day_100, y = recovered_to_confirmed, colour = province_country)) + 
  geom_line() + 
  ylab(label=""revovered to confirmed per 100 cases"") + 
  xlab(""Days since 100th reported case"") + 
  scale_colour_manual(values=c(""red"", ""blue"",""green"",""orange"",""black"",""yellow"",""brown"",""pink""))
"
3107,30902241,0,!pip install hy > /dev/null
3108,30902241,1,"# Hy Magic
"
3109,30902241,2,"import IPython
"
3110,30902241,3,"def hy_eval(*args):import hy;return hy.eval(hy.read_str(""(do\n""+"""".join(map(lambda s:s or """",args))+""\n)\n""),globals())
"
3111,30902241,4,"@IPython.core.magic.register_line_cell_magic
"
3112,30902241,5,"def h(*args):hy_eval(*args) # Silent: Does not print result.
"
3113,30902241,6,"@IPython.core.magic.register_line_cell_magic
"
3114,30902241,7,"def hh(*args): return hy_eval(*args) # Verbose: Prints result.
"
3115,30902241,8,"del h, hh"
3116,30902241,9,"%%h
"
3117,30902241,10,"(import  [useful [*]])
"
3118,30902241,11,(require [useful [*]])
3119,30902241,12,"%%h
"
3120,30902241,13,"; Figure out location of data
"
3121,30902241,14,"(s covid-root-kaggle ""/kaggle/input"")
"
3122,30902241,15,"(s covid-root-laptop ""$HOME/d"")
"
3123,30902241,16,"(s covid-root 
"
3124,30902241,17,"   (-> ""/kaggle"" 
"
3125,30902241,18,"       (os.path.exists) 
"
3126,30902241,19,"       (if covid-root-kaggle covid-root-laptop)
"
3127,30902241,20,"       (os.path.expandvars)))
"
3128,30902241,21,"(s covid-prefix (+ covid-root ""/covid19-local-us-ca-forecasting-week-1/ca_""))
"
3129,30902241,22,"(s covid-train  (+ covid-prefix ""train.csv""))
"
3130,30902241,23,"(s covid-test   (+ covid-prefix ""test.csv""))
"
3131,30902241,24,"(s covid-submit (+ covid-prefix ""submission.csv""))"
3132,30902241,25,"%%hh
"
3133,30902241,26,"; Find out day of week
"
3134,30902241,27,"(-> covid-train
"
3135,30902241,28,"  (pd.read-csv) 
"
3136,30902241,29,"  (pd-keep [""Date"" ""ConfirmedCases"" ""Fatalities""]) 
"
3137,30902241,30,"  (.assign :Date (fn [x] (pd.to-datetime :yearfirst True x.Date)))
"
3138,30902241,31,"  (.assign :Day  (fn [x] (-> (x.Date.dt.day_name))))
"
3139,30902241,32,"  (.head)
"
3140,30902241,33,)
3141,30902241,34,"%%h
"
3142,30902241,35,"; Plot the data
"
3143,30902241,36,"
"
3144,30902241,37,"(import math)
"
3145,30902241,38,"(-> 39.94 (* 1000) (* 1000) (math.log1p) (s1 ca-pop-log))
"
3146,30902241,39,"
"
3147,30902241,40,"(-> covid-train
"
3148,30902241,41,"  (pd.read-csv) 
"
3149,30902241,42,"  (pd-keep [""Date"" ""ConfirmedCases"" ""Fatalities""]) 
"
3150,30902241,43,"  (.where (fn1-> (. ConfirmedCases) (> 0.0))) (.dropna)
"
3151,30902241,44,"  (.assign :ConfirmedCases (fn1-> (. ConfirmedCases) (np.log1p))) 
"
3152,30902241,45,"  (.assign :Fatalities     (fn1-> (. Fatalities)     (np.log1p)))
"
3153,30902241,46,"  (.assign :Date           (fn1-> (. Date)           (pd.to-datetime :yearfirst True)))
"
3154,30902241,47,"  (.assign :DayOfYear      (fn1-> (. Date) (. dt) (. dayofyear)))
"
3155,30902241,48,"  (.assign :Day            (fn1-> (. Date) (. dt) ( .day_name))) 
"
3156,30902241,49,"  (.dropna)
"
3157,30902241,50,"  (s1 df1))
"
3158,30902241,51,"
"
3159,30902241,52,"(-> df1
"
3160,30902241,53,"  (pd-keep [""Date"" ""ConfirmedCases"" ""Fatalities""])
"
3161,30902241,54,"  (.set-index ""Date"")
"
3162,30902241,55,"  (pd-plot ""Log1p"")
"
3163,30902241,56,  (display))
3164,30902241,57,"%%h
"
3165,30902241,58,"; Define data frames.
"
3166,30902241,59,"
"
3167,30902241,60,"(s MILLION    (-> 1000 (* 1000)))
"
3168,30902241,61,"(s population (-> 39.94 (* MILLION)))
"
3169,30902241,62,"
"
3170,30902241,63,"(-> covid-train
"
3171,30902241,64,"  (pd.read-csv) 
"
3172,30902241,65,"  (pd-keep [""Date"" ""ConfirmedCases"" ""Fatalities""]) 
"
3173,30902241,66,"  (.rename :columns {""ConfirmedCases"" ""Conf"" ""Fatalities"" ""Dead"" })
"
3174,30902241,67,"  (.assign :Conf (fn1-> (. Conf) (/ population))) 
"
3175,30902241,68,"  (.assign :Dead (fn1-> (. Dead) (/ population)))
"
3176,30902241,69,"  (.dropna)
"
3177,30902241,70,"  (s1 df))
"
3178,30902241,71,"
"
3179,30902241,72,"(setv conf-actual (df.Conf.rename ""Actual""))
"
3180,30902241,73,"(setv dead-actual (df.Dead.rename ""Actual""))"
3181,30902241,74,"%%h
"
3182,30902241,75,"(import [scipy.optimize [minimize]])
"
3183,30902241,76,"(import [math [exp]])
"
3184,30902241,77,"
"
3185,30902241,78,"; Define conf model then run it.
"
3186,30902241,79,"
"
3187,30902241,80,"(defn conf-model [a alpha t0 t]
"
3188,30902241,81,"  (setv t-delta (- t t0))
"
3189,30902241,82,"  (if (< t-delta 0)
"
3190,30902241,83,"    0.0
"
3191,30902241,84,"    (** (- 1 (exp (* (- a) t-delta))) alpha)))
"
3192,30902241,85,"
"
3193,30902241,86,"(defn conf-model-loss [x df]
"
3194,30902241,87,"  (setv (, a alpha t0) x)
"
3195,30902241,88,"  (setv r 0)
"
3196,30902241,89,"  (for [t (range (len df))]
"
3197,30902241,90,"    (+= r (-> (conf-model a alpha t0 t) (- (get df t)) (** 2))))
"
3198,30902241,91,"  r)
"
3199,30902241,92,"
"
3200,30902241,93,"(-> conf-model-loss 
"
3201,30902241,94,"    (minimize :x0 (np.array [0.1 1.0 5]) 
"
3202,30902241,95,"              :args conf-actual 
"
3203,30902241,96,"              :method ""Nelder-Mead"" :tol 1e-6)
"
3204,30902241,97,    (s1 conf-opt))
3205,30902241,98,"%%h
"
3206,30902241,99,"
"
3207,30902241,100,"; Define dead model then run it.
"
3208,30902241,101,"
"
3209,30902241,102,"(defn dead-model [death-rate lag t]
"
3210,30902241,103,"  (s (, a alpha t0) conf-opt.x)
"
3211,30902241,104,"  (s t (- t lag))
"
3212,30902241,105,"  (s conf (conf-model a alpha t0 t))
"
3213,30902241,106,"  (s dead (* conf death-rate)))
"
3214,30902241,107,"
"
3215,30902241,108,"(defn dead-model-loss [x df]
"
3216,30902241,109,"  (s (, death-rate lag) x)
"
3217,30902241,110,"  (s (, a alpha t0) conf-opt.x)
"
3218,30902241,111,"  (s r 0)
"
3219,30902241,112,"  (for [t (range (len df))]
"
3220,30902241,113,"    (+= r (-> (dead-model death-rate lag t) (- (get df t)) (** 2))))
"
3221,30902241,114,"  r)
"
3222,30902241,115,"
"
3223,30902241,116,"(-> dead-model-loss 
"
3224,30902241,117,"    (minimize :x0 (np.array [0.01 15]) 
"
3225,30902241,118,"              :args dead-actual
"
3226,30902241,119,"              :method ""Nelder-Mead"" :tol 1e-6)
"
3227,30902241,120,"    (s1 dead-opt))
"
3228,30902241,121,"
"
3229,30902241,122,[conf-opt dead-opt]
3230,30902241,123,"%%h
"
3231,30902241,124,"(defn model-to-fn [model opt] 
"
3232,30902241,125,"  (fn [&rest args]
"
3233,30902241,126,"    (setv params (-> opt (. x) (list) (+ (list args))))
"
3234,30902241,127,"    (-> model (apply params))))
"
3235,30902241,128,"
"
3236,30902241,129,"(-> conf-model (model-to-fn conf-opt) (s1 conf-fn))
"
3237,30902241,130,(-> dead-model (model-to-fn dead-opt) (s1 dead-fn))
3238,30902241,131,"%%h
"
3239,30902241,132,"; Compare actual vs predictions
"
3240,30902241,133,"(-> conf-actual (len) (range) (map1 conf-fn) (pd.Series :name ""Predict"") (s1 conf-predict))
"
3241,30902241,134,"(-> (pd.concat [conf-actual conf-predict] :axis 1) (s1 conf-eval))
"
3242,30902241,135,"(-> conf-eval (* population) (.plot :title ""Confirmed Cases""))
"
3243,30902241,136,"
"
3244,30902241,137,"(-> dead-actual (len) (range) (map1 dead-fn) (pd.Series :name ""Predict"") (s1 dead-predict))
"
3245,30902241,138,"(-> (pd.concat [dead-actual dead-predict] :axis 1) (s1 dead-eval))
"
3246,30902241,139,"(-> dead-eval (* population) (.plot :title ""Fatalities""))"
3247,30902241,140,"%%h
"
3248,30902241,141,"(import [sklearn [metrics]])
"
3249,30902241,142,"
"
3250,30902241,143,"; Calculate conf errors.
"
3251,30902241,144,"(print ""Confirmed Cases Errors"")
"
3252,30902241,145,"(print ""Conf MSE ="" (metrics.mean-squared-error (conf-eval.Actual.to-numpy) (conf-eval.Predict.to-numpy)))
"
3253,30902241,146,"(print ""Conf MAE ="" (metrics.mean-absolute-error (conf-eval.Actual.to-numpy) (conf-eval.Predict.to-numpy)))
"
3254,30902241,147,"(print ""Conf RMSE ="" (np.sqrt (metrics.mean-squared-error (conf-eval.Actual.to-numpy) (conf-eval.Predict.to-numpy))))
"
3255,30902241,148,"
"
3256,30902241,149,"; Calculate dead errors.
"
3257,30902241,150,"(print ""Fatalities Errors"")
"
3258,30902241,151,"(print ""Dead MSE ="" (metrics.mean-squared-error (dead-eval.Actual.to-numpy) (dead-eval.Predict.to-numpy)))
"
3259,30902241,152,"(print ""Dead MAE ="" (metrics.mean-absolute-error (dead-eval.Actual.to-numpy) (dead-eval.Predict.to-numpy)))
"
3260,30902241,153,"(print ""Dead RMSE ="" (np.sqrt (metrics.mean-squared-error (dead-eval.Actual.to-numpy) (dead-eval.Predict.to-numpy))))"
3261,30902241,154,"%%h
"
3262,30902241,155,"; Next lets build out the test
"
3263,30902241,156,"(defn pd-head-tail [df]
"
3264,30902241,157,"  (print ""Rows = ""(-> df (len)))
"
3265,30902241,158,"  (-> df (.head 1) (display))
"
3266,30902241,159,"  (-> df (.tail 1) (display)))
"
3267,30902241,160,"
"
3268,30902241,161,"(-> covid-train  (pd.read-csv) (s1 df-train))
"
3269,30902241,162,"(-> covid-test   (pd.read-csv) (s1 df-test))
"
3270,30902241,163,"
"
3271,30902241,164,";(pd-head-tail df-train)
"
3272,30902241,165,;(pd-head-tail df-test)
3273,30902241,166,"%%h
"
3274,30902241,167,"; Compute date0 of training data.
"
3275,30902241,168,"(-> df-train (. Date) (get 0) (dateparser.parse) (s1 date0-train))
"
3276,30902241,169,"
"
3277,30902241,170,"; Useful functions.
"
3278,30902241,171,"(defn date-string->t [d] 
"
3279,30902241,172,"  (-> d (dateparser.parse) (- date0-train) (. days)))
"
3280,30902241,173,"(defn date-string->confirmed-cases [d]
"
3281,30902241,174,"  (-> d (date-string->t) (conf-fn) (* population) (int)))
"
3282,30902241,175,"(defn date-string->fatalities [d]
"
3283,30902241,176,"  (-> d (date-string->t) (dead-fn) (* population) (int)))
"
3284,30902241,177,"
"
3285,30902241,178,"; Submission.
"
3286,30902241,179,"(-> df-test 
"
3287,30902241,180,"    (.to-dict ""records"") 
"
3288,30902241,181,"    (map1 (fn [r] 
"
3289,30902241,182,"            (-> r
"
3290,30902241,183,"              (assoc1 ""ConfirmedCases"" (-> r (get ""Date"") (date-string->confirmed-cases)))
"
3291,30902241,184,"              (assoc1 ""Fatalities""     (-> r (get ""Date"") (date-string->fatalities))))))
"
3292,30902241,185,"    (pd.DataFrame)
"
3293,30902241,186,"    (pd-keep [""ForecastId"" ""ConfirmedCases"" ""Fatalities""])
"
3294,30902241,187,"    (.to-csv ""submission.csv"" :index False))"
3295,30633398,0,"import numpy as np
import pandas as pd
train_data = pd.read_csv('/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_train.csv')
# we don't need the test data
# test_data = pd.read_csv('/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_test.csv')"
3296,30633398,1,print(train_data)
3297,30633398,2,"useless_cols = []
for col in train_data.columns:
    entries_num = len(set(train_data[col]))
    if entries_num < 2:
        useless_cols.append(col)
    print(f'{col} -- has {entries_num} unic entries')"
3298,30633398,3,"train_data.drop(useless_cols, axis=1, inplace=True)
train_data.drop(['Id', 'Date'], axis=1, inplace=True)"
3299,30633398,4,"X_train = train_data[48:]
print(X_train)"
3300,30633398,5,"# Approximation of the confirmed cases first and second derivatives
diff_conf, conf_old = [], 0 
dd_conf, dc_old = [], 0

# Approximation of the fatalities first and second derivatives
diff_fat, fat_old = [], 0
dd_fat, df_old = [], 0

# Approximation of exponential grow coefficient
exp_conf, exp_fat = [], []

# Approximation of the ration between fatalities and confirmed cases
ratio = []

# Calc the features' arrays
for row in X_train.values:
    diff_conf.append(row[0]-conf_old)
    conf_old=row[0]
    
    diff_fat.append(row[1]-fat_old)
    fat_old=row[1]
    
    exp_conf.append(diff_conf[-1]/row[0])
    exp_fat.append(diff_fat[-1]/row[1])
    
    ratio.append(row[1]/row[0])
    
    dd_conf.append(diff_conf[-1]-dc_old)
    dc_old=diff_conf[-1]
    
    dd_fat.append(diff_fat[-1]-df_old)
    df_old=diff_fat[-1]

# Insert features into training set
X_train['diff_confirmed'] = diff_conf
X_train['diff_fatalities'] = diff_fat
X_train['exp_confirmed'] = exp_conf
X_train['exp_fatalities'] = exp_fat
X_train['fatalities_to_confirmed'] = ratio
X_train['dd_confirmed'] = dd_conf
X_train['dd_fatalities'] = dd_conf
print(X_train)"
3301,30633398,6,"exp_c = X_train.exp_confirmed.drop(48).mean()
print(f'exp_c: {exp_c}')
exp_f = X_train.exp_fatalities.drop(48).mean()
print(f'exp_f: {exp_f}')
ratio = X_train.fatalities_to_confirmed.drop(48).mean()
print(f'ratio: {ratio}')
d_c = X_train.diff_confirmed.drop(48).mean()
print(f'd_c: {d_c}')
dd_c = X_train.dd_confirmed.drop(48).drop(49).mean()
print(f'dd_c: {dd_c}')
d_f = X_train.diff_fatalities.drop(48).mean()
print(f'd_f: {d_f}')
dd_f = X_train.dd_fatalities.drop(48).drop(49).mean()
print(f'dd_f: {dd_f}')"
3302,30633398,7,"pred_c, pred_f = list(X_train.ConfirmedCases.loc[50:56]), list(X_train.Fatalities.loc[50:56])"
3303,30633398,8,"for i in range(1, 44 - 7):
    # use taylor series to predict confirmed cases
    pred_c.append((X_train.ConfirmedCases[56] + d_c*i + 0.5*dd_c*(i**2)))
    # use taylor series to predict fatalities
    # pred_f.append((X_train.Fatalities[56] + d_f*i + 0.5*dd_f*(i**2)))
    
    # We can also try to use ratio of fatalities to cases instead
    pred_f.append(pred_c[-1] * ratio)"
3304,30633398,9,"# for i in range(1, 44 - 7):
#     pred_c.append(X_train.ConfirmedCases[56] * ((1 + exp_c) ** i ))
#     pred_f.append(pred_c[-1] * ratio)"
3305,30633398,10,"my_submission = pd.DataFrame({'ForecastId': list(range(1,44)), 'ConfirmedCases': pred_c, 'Fatalities': pred_f})
my_submission.to_csv('submission.csv', index=False)
print(my_submission)"
3306,30633398,11,
3307,41203950,0,"# Input data files are available in the ""../input/"" directory.

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt
%matplotlib inline

# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# Any results you write to the current directory are saved as output.

"
3308,41203950,1,"global_data = pd.read_csv(""../input/novel-corona-virus-2019-dataset/covid_19_data.csv"")
global_data.head()"
3309,41203950,2,"#We will use it to smooth the data for growth factor..
def smoother(inputdata,w,imax):
    data = 1.0*inputdata
    data = data.replace(np.nan,1)
    data = data.replace(np.inf,1)
    #print(data)
    smoothed = 1.0*data
    normalization = 1
    for i in range(-imax,imax+1):
        if i==0:
            continue
        smoothed += (w**abs(i))*data.shift(i,axis=0)
        normalization += w**abs(i)
    smoothed /= normalization
    return smoothed
# function to compute growth factor
def growth_factor(confirmed):
    confirmed_iminus1 = confirmed.shift(1, axis=0)
    confirmed_iminus2 = confirmed.shift(2, axis=0)
    return (confirmed-confirmed_iminus1)/(confirmed_iminus1-confirmed_iminus2)
#function to compute growth ratio
def growth_ratio(confirmed):
    confirmed_iminus1 = confirmed.shift(1, axis=0)
    return (confirmed/confirmed_iminus1)

# We don't need a function for growth rate since we can use the np.gradient() function.

# This is a function which plots (for in input country) the active, confirmed, and recovered cases, deaths, and the growth factor.
def plot_country_active_confirmed_recovered_growth_metrics(country):
    
    # Plots Active, Confirmed, and Recovered Cases. Also plots deaths.
    country_data = global_data[global_data['Country/Region']==country]
    table = country_data.drop(['SNo','Province/State', 'Last Update'], axis=1)
    table['ActiveCases'] = table['Confirmed'] - table['Recovered'] - table['Deaths']
    table2 = pd.pivot_table(table, values=['ActiveCases','Confirmed', 'Recovered','Deaths'], index=['ObservationDate'], aggfunc=np.sum)
    table3 = table2.drop(['Deaths'], axis=1)
   
    # Growth Factor
    w = 0.5
    table2['GrowthFactor'] = growth_factor(table2['Confirmed'])
    table2['GrowthFactor'] = smoother(table2['GrowthFactor'],w,5)

    # 2nd Derivative
    table2['2nd_Derivative'] = np.gradient(np.gradient(table2['Confirmed'])) #2nd derivative
    table2['2nd_Derivative'] = smoother(table2['2nd_Derivative'],w,7)


    #Plot confirmed[i]/confirmed[i-1], this is called the growth ratio
    table2['GrowthRatio'] = growth_ratio(table2['Confirmed'])
    table2['GrowthRatio'] = smoother(table2['GrowthRatio'],w,5)
    
    #Plot the growth rate, we will define this as k in the logistic function presented at the beginning of this notebook.
    table2['GrowthRate']=np.gradient(np.log(table2['Confirmed']))
    table2['GrowthRate'] = smoother(table2['GrowthRate'],0.5,3)
    
    # horizontal line at growth rate 1.0 for reference
    x_coordinates = [1, 100]
    y_coordinates = [1, 1]
    #plots
    table2['Deaths'].plot(title='Deaths')
    plt.show()
    table3.plot() 
    plt.show()
    table2['GrowthFactor'].plot(title='Growth Factor')
    plt.plot(x_coordinates, y_coordinates) 
    plt.show()
    table2['2nd_Derivative'].plot(title='2nd_Derivative')
    plt.show()
    table2['GrowthRatio'].plot(title='Growth Ratio')
    plt.plot(x_coordinates, y_coordinates)
    plt.show()
    table2['GrowthRate'].plot(title='Growth Rate')
    plt.show()

    
   # import plotly.express as px
   # table3 = table3.melt(id_vars=""ObservationDate"", value_vars=['ActiveCases','Confirmed','Recovered'],
   #              var_name='case', value_name='count')
   # table3.reindex()
   # fig = px.area(table3, x=""ObservationDate"", y=""count"", color='case',
   #              title='Confirmed Cases', color_discrete_sequence = ['cyan', 'red', 'orange'])
   # fig.show()

    return 
"
3310,41203950,3,plot_country_active_confirmed_recovered_growth_metrics('Mainland China')
3311,41203950,4,plot_country_active_confirmed_recovered_growth_metrics('South Korea')
3312,41203950,5,"plot_country_active_confirmed_recovered_growth_metrics('US')
"
3313,41203950,6,plot_country_active_confirmed_recovered_growth_metrics('Germany')
3314,41203950,7,plot_country_active_confirmed_recovered_growth_metrics('Italy')
3315,41203950,8,plot_country_active_confirmed_recovered_growth_metrics('Netherlands')
3316,41203950,9,plot_country_active_confirmed_recovered_growth_metrics('Russia')
3317,41203950,10,plot_country_active_confirmed_recovered_growth_metrics('New Zealand')
3318,41203950,11,"restofworld_data = global_data
for country in restofworld_data['Country/Region']:
    if country != 'US': 
        restofworld_data['Country/Region'] = restofworld_data['Country/Region'].replace(country, ""RestOfWorld"")

plot_country_active_confirmed_recovered_growth_metrics('RestOfWorld')"
3319,41203950,12,"from scipy.optimize import curve_fit
"
3320,41203950,13,"# We want number of confirmed for each date for each country
#country_data = global_data[global_data['Country/Region']=='Mainland China']
global_data2 = pd.read_csv(""../input/novel-corona-virus-2019-dataset/covid_19_data.csv"")
country_data = global_data2[global_data2['Country/Region']=='US']
#country_data = country_data[country_data['ObservationDate']<""07/05/2020""]
#country_data = country_data.drop(['SNo','Province/State', 'Last Update.'], axis=1)
country_data = pd.pivot_table(country_data, values=['Confirmed', 'Recovered','Deaths'], index=['ObservationDate'], aggfunc=np.sum)
country_data.tail()"
3321,41203950,14,"#country_data['GrowthFactor'] = growth_factor(country_data['Confirmed'])

# we will want x_data to be the number of days since first confirmed and the y_data to be the confirmed data. This will be the data we use to fit a logistic curve
x_data = range(len(country_data.index))
y_data = country_data['Confirmed']

def log_curve(x, k, x_0, ymax):
    return ymax / (1 + np.exp(-k*(x-x_0)))

# Fit the curve
popt, pcov = curve_fit(log_curve, x_data, y_data, bounds=([0,0,0],np.inf), maxfev=50000)
estimated_k, estimated_x_0, ymax= popt


# Plot the fitted curve
k = estimated_k
x_0 = estimated_x_0
y_fitted = log_curve(range(0,350), k, x_0, ymax)
print(k, x_0, ymax)
#print(y_fitted)
y_data.tail()"
3322,41203950,15,"# Plot everything for illustration
fig = plt.figure()
ax = fig.add_subplot(111)
ax.plot(range(0,350), y_fitted, '--', label='fitted')
ax.plot(x_data, y_data, 'o', label='Confirmed Data')
"
3323,30799422,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
from fbprophet import Prophet

# Input data files are available in the ""../input/"" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# Any results you write to the current directory are saved as output.

# Read Data

cal_df = pd.read_csv(""/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_train.csv"")
cal_test= pd.read_csv(""/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_test.csv"")

## prepare data for FB-Prophet 
## Confirm data
cal_dat_fb = pd.DataFrame({'ds':cal_df['Date'],'y':cal_df['ConfirmedCases']})
cal_dat_fb['ds'] = cal_dat_fb['ds'].apply(pd.to_datetime)

## Fatalities
cal_dat_fat = pd.DataFrame({'ds':cal_df['Date'],'y':cal_df['Fatalities']})
cal_dat_fat['ds'] = cal_dat_fat['ds'].apply(pd.to_datetime)
    
#future data
future = pd.DataFrame({'ds':cal_test['Date']})

## FB prophet code

### Confirmed cases
m1 = Prophet(mcmc_samples=1000,interval_width=0.01,yearly_seasonality=0.4)
m2 = Prophet(mcmc_samples=1000,interval_width=0.01,yearly_seasonality=0.4)
Confirm_forecast = m1.fit(cal_dat_fb[48:]).predict(future)
Fatality_forecast = m2.fit(cal_dat_fat[48:]).predict(future)

#cf= pd.DataFrame(columns=['ConfirmedCases'])
#ff=pd.DataFrame(columns=['Fatalities'])
cf = Confirm_forecast[['yhat']].apply(np.ceil)
ff=Fatality_forecast[['yhat']].apply(np.ceil)
cf= cf.rename(columns={""yhat"": ""ConfirmedCases""})
ff= ff.rename(columns={""yhat"": ""Fatalities""})

print(Fatality_forecast[['yhat']])

## submission code 
dlist = [cal_test['ForecastId'],cf['ConfirmedCases'],ff['Fatalities']]
#names = ['ForecastId','ConfirmedCases','Fatalities']
submission = pd.concat(dlist,axis=1)

print(submission)
submission.to_csv('submission.csv',index=False)"
3324,30755960,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import seaborn as sns

# Input data files are available in the ""../input/"" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# Any results you write to the current directory are saved as output."
3325,30755960,1,"df_train = pd.read_csv('/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_train.csv')
df_test = pd.read_csv('/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_test.csv')
df_submit = pd.read_csv('/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_submission.csv')
df_train.head()"
3326,30755960,2,df_train.info()
3327,30755960,3,"print(df_train['Province/State'].unique())
print(df_train['Country/Region'].unique())"
3328,30755960,4,"import matplotlib.pyplot as plt
plt.figure(figsize=(14,6))
plt.hist(df_train['ConfirmedCases'],bins=10,color='green')
plt.xlabel('Confirmed Cases')
plt.ylabel('Count')
plt.title('Count of Confirmed Cases')"
3329,30755960,5,"import matplotlib.pyplot as plt
plt.figure(figsize=(15,10))
plt.plot(df_train['Fatalities'])
plt.xlabel('Fatalities')
plt.ylabel('Count')
plt.title('Graph of Fatalities ')"
3330,30755960,6,"df_train = df_train[['Date','ConfirmedCases','Fatalities']]
df_train.head()"
3331,30755960,7,"plt.figure(figsize=(15,10))
sns.barplot(x=df_train['Date'] , y = df_train['ConfirmedCases'])
plt.xticks(rotation=90)
"
3332,30755960,8,"plt.figure(figsize=(15,10))
sns.barplot(x=df_train['Date'] , y = df_train['Fatalities'])
plt.xticks(rotation=90)"
3333,30755960,9,"df_train_new = df_train.query('ConfirmedCases > 0')
df_train_new"
3334,30755960,10,"plt.figure(figsize=(15,10))
#sns.barplot(x=df_train_new['Date'] , y = df_train_new['Fatalities'])
sns.barplot(x=df_train_new['Date'] , y = df_train_new['ConfirmedCases'])
plt.xticks(rotation=45)
plt.title('ConfirmedCases as per Date')"
3335,30755960,11,"df_train['Date'] = pd.to_datetime(df_train['Date'])
df_train.insert(1,'Week',df_train['Date'].dt.week)
df_train.insert(2,'Day',df_train['Date'].dt.day)
df_train.insert(3,'DayofWeek',df_train['Date'].dt.dayofweek)
df_train.insert(4,'DayofYear',df_train['Date'].dt.dayofyear)"
3336,30755960,12,df_train
3337,30755960,13,"from sklearn.linear_model import LinearRegression
from sklearn.linear_model import BayesianRidge 
from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import r2_score
from sklearn.model_selection import train_test_split"
3338,30755960,14,"X = df_train.drop(['Date', 'ConfirmedCases', 'Fatalities'], axis=1)
y = df_train[['ConfirmedCases', 'Fatalities']]"
3339,30755960,15,"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
3340,30755960,16,y_train.head()
3341,30755960,17,"#Function that predicts the scores of models.
def predict_scores(reg_alg):
    m = reg_alg()
    m.fit(X_train, y_train['ConfirmedCases'])
    y_pred = m.predict(X_test)
    m_r = r2_score(y_test['ConfirmedCases'], y_pred)
    sc_Cases.append(m_r)
    
    m.fit(X_train, y_train['Fatalities'])
    y_pred = m.predict(X_test)
    m_r2 = r2_score(y_test['Fatalities'], y_pred)
    sc_Fatalities.append(m_r2)


    
reg_models = [KNeighborsRegressor, LinearRegression, RandomForestRegressor, GradientBoostingRegressor, DecisionTreeRegressor,BayesianRidge]

sc_Cases = []
sc_Fatalities = []

for x in reg_models:
    predict_scores(x)"
3342,30755960,18,sc_Cases
3343,30755960,19,sc_Fatalities
3344,30755960,20,"models = pd.DataFrame({
    'Model': ['KNeighborsRegressor', 'LinearRegression', 'RandomForestRegressor', 'GradientBoostingRegressor', 'DecisionTreeRegressor','BayesianRidge' ],
    'ConfirmedCase_r2': sc_Cases,
    'Fatalities_r2' : sc_Fatalities
})

models"
3345,30755960,21,df_test.head()
3346,30755960,22,df_test.info()
3347,30755960,23,"df_test = df_test[['ForecastId', 'Date']]

df_test['Date'] = pd.to_datetime(df_test['Date'])
df_test.insert(1,'Week',df_test['Date'].dt.week)
df_test.insert(2,'Day',df_test['Date'].dt.day)
df_test.insert(3,'DayofWeek',df_test['Date'].dt.dayofweek)
df_test.insert(4,'DayofYear',df_test['Date'].dt.dayofyear)

df_test.head()"
3348,30755960,24,"model1 = RandomForestRegressor()
model1.fit(X_train, y_train['ConfirmedCases'])

model2 = RandomForestRegressor()
model2.fit(X_train, y_train['Fatalities'])

df_test['ConfirmedCases'] = model1.predict(df_test.drop(['Date', 'ForecastId'], axis=1))
df_test['Fatalities'] = model2.predict(df_test.drop(['Date', 'ForecastId', 'ConfirmedCases'], axis=1))"
3349,30755960,25,"import warnings
warnings.filterwarnings('ignore')
df_results = df_test[['ForecastId', 'ConfirmedCases', 'Fatalities']] 
df_results['ConfirmedCases'] = df_results['ConfirmedCases'].astype(int)
df_results['Fatalities'] = df_results['Fatalities'].astype(int)

df_results.head()"
3350,30755960,26,"df_results.to_csv('submission.csv', index=False)"
3351,30592516,0,"import pandas as pd
import matplotlib.pyplot as plt
from statsmodels.tsa.arima_model import ARIMA"
3352,30592516,1,"df = pd.read_csv('../input/covid19-local-us-ca-forecasting-week-1/ca_train.csv')
df.head()"
3353,30592516,2,"data_confirm = df['ConfirmedCases'].to_list()
print(""Length of initial data_confim = "",len(data_confirm)) 
data_Fat = df['Fatalities'].to_list()
print(""Length of initial data_Fat = "",len(data_Fat)) "
3354,30592516,3,"def ARIMA_FIT(data,num_prediction):
    temp_list = data
    for i in range(num_prediction):
        model = ARIMA(temp_list, order=(0, 2, 1))
        model_fit = model.fit(disp=False)
        yhat = model_fit.predict(len(temp_list), len(temp_list), typ='levels')
        temp_list.append(round(yhat[0]))
    return temp_list"
3355,30592516,4,"ConfirmedCases = ARIMA_FIT(data_confirm,36)
print(""Length of predict data_confim = "",len(ConfirmedCases))

Fatalities = ARIMA_FIT(data_Fat,36)
print(""Length of predict data_Fat = "",len(Fatalities))"
3356,30592516,5,"plt.plot(ConfirmedCases)
plt.plot(Fatalities)"
3357,30592516,6,"df_submission = pd.read_csv('../input/covid19-local-us-ca-forecasting-week-1/ca_submission.csv')
submission = pd.DataFrame({""ForecastId"": df_submission['ForecastId'], ""ConfirmedCases"": ConfirmedCases[50:],""Fatalities"":Fatalities[50:]})
submission.to_csv('submission.csv', index=False)"
3358,30592516,7,
3359,30715869,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the ""../input/"" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# Any results you write to the current directory are saved as output."
3360,30715869,1,"# Load data into Pandas dataframes
df_train = pd.read_csv('/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_train.csv')
df_test = pd.read_csv('/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_test.csv')
df_submission = pd.read_csv('/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_submission.csv')"
3361,30715869,2,"# Check a preview of the data
df_train.tail()"
3362,30715869,3,"# Check the properties of the data

print(df_train['Province/State'].unique())
print(df_train['Country/Region'].unique())
print(df_train['Lat'].unique())
print(df_train['Long'].unique())
print(df_train.dtypes)"
3363,30715869,4,df_train.describe()
3364,30715869,5,"# Check the distribution of the confirmed cases

df_train.hist(column='ConfirmedCases')"
3365,30715869,6,"# Check the distribution of the fatalities

df_train.hist(column='Fatalities')"
3366,30715869,7,"# Take only what we need: date, confirmed cases and fatalities

df_train = df_train[['Date', 'ConfirmedCases', 'Fatalities']]"
3367,30715869,8,"# Convert Date column to Pandas date and orther to get chronological data

df_train['Date'] = pd.to_datetime(df_train['Date'])
df_train = df_train.sort_values(by=['Date'])"
3368,30715869,9,"# Check the trend in a chart

df_train.plot.bar(x='Date', y=['ConfirmedCases','Fatalities'])"
3369,30715869,10,"# As the confirmed cases are far away from the start we will focus in that time

df_train2 = df_train.query('ConfirmedCases != 0.0')

df_train2.plot.bar(x='Date', y=['ConfirmedCases', 'Fatalities'])"
3370,30715869,11,"df_train['Week'] = df_train['Date'].dt.week
df_train['Day'] = df_train['Date'].dt.day
df_train['WeekDay'] = df_train['Date'].dt.dayofweek
df_train['YearDay'] = df_train['Date'].dt.dayofyear"
3371,30715869,12,"from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import Lasso, ElasticNet
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import r2_score

from sklearn.model_selection import train_test_split

predictors = df_train.drop(['Date', 'ConfirmedCases', 'Fatalities'], axis=1)
target = df_train[['ConfirmedCases', 'Fatalities']]
x_train, x_test, y_train, y_test = train_test_split(predictors, target, test_size=0.2, random_state=1)

def scores(alg):
    lin = alg()
    lin.fit(x_train, y_train['ConfirmedCases'])
    y_pred = lin.predict(x_test)
    lin_r = r2_score(y_test['ConfirmedCases'], y_pred)
    s.append(lin_r)
    
    lin.fit(x_train, y_train['Fatalities'])
    y_pred = lin.predict(x_test)
    lin_r = r2_score(y_test['Fatalities'], y_pred)
    s2.append(lin_r)
    
algos = [KNeighborsRegressor, LinearRegression, RandomForestRegressor, GradientBoostingRegressor, Lasso, ElasticNet, DecisionTreeRegressor]

s = []
s2 = []

for algo in algos:
    scores(algo)
    
models = pd.DataFrame({
    'Method': ['KNeighborsRegressor', 'LinearRegression', 'RandomForestRegressor', 'GradientBoostingRegressor', 'Lasso', 'ElasticNet', 'DecisionTreeRegressor'],
    'ScoreCC': s,
    'ScoreF' : s2
})

models.sort_values(by=['ScoreCC', 'ScoreF'], ascending=False)"
3372,30715869,13,"# Now let's try for last an ARIMA model

# First we see that data is not stationary, so we need to check the autocorrelation of the time series

from pandas.plotting import autocorrelation_plot

autocorrelation_plot(df_train['ConfirmedCases'])"
3373,30715869,14,"from statsmodels.tsa.arima_model import ARIMA
from matplotlib import*

arima_model = ARIMA(df_train['ConfirmedCases'], order=(4,1,0)).fit(disp=0, transparams=True, trend='c')
print(arima_model.summary())

residuals = pd.DataFrame(arima_model.resid)
residuals.plot()
pyplot.show()
residuals.plot(kind='kde')
pyplot.show()
print(residuals.describe())

arima_model2 = ARIMA(df_train['Fatalities'], order=(4,1,0)).fit(disp=0, transparams=True, trend='c')
print(arima_model2.summary())

residuals2 = pd.DataFrame(arima_model2.resid)
residuals2.plot()
pyplot.show()
residuals2.plot(kind='kde')
pyplot.show()
print(residuals2.describe())"
3374,30715869,15,"predictions_arima = list(arima_model.predict())
predictions_arima.append(arima_model.forecast()[0][0])
predictions_arima.append(arima_model.forecast()[0][0])

df_train['arima'] = predictions_arima

predictions_arima2 = list(arima_model2.predict())
predictions_arima2.append(arima_model2.forecast()[0][0])
predictions_arima2.append(arima_model2.forecast()[0][0])

df_train['arima2'] = predictions_arima2

df_train.plot.bar(x='Date', y=['ConfirmedCases', 'arima'])
df_train.plot.bar(x='Date', y=['Fatalities', 'arima2'])"
3375,30715869,16,df_submission.head()
3376,30715869,17,"print(df_test['Date'].values)
print(len(df_test['Date']))"
3377,30715869,18,"df_test = df_test[['ForecastId', 'Date']]

df_test['Date'] = pd.to_datetime(df_test['Date'])
df_test['Week'] = df_test['Date'].dt.week
df_test['Day'] = df_test['Date'].dt.day
df_test['WeekDay'] = df_test['Date'].dt.dayofweek
df_test['YearDay'] = df_test['Date'].dt.dayofyear

df_test.head()"
3378,30715869,19,"model = RandomForestRegressor()
model.fit(x_train, y_train['ConfirmedCases'])

model2 = RandomForestRegressor()
model2.fit(x_train, y_train['Fatalities'])


df_test['ConfirmedCases'] = model.predict(df_test.drop(['Date', 'ForecastId'], axis=1))
df_test['Fatalities'] = model2.predict(df_test.drop(['Date', 'ForecastId', 'ConfirmedCases'], axis=1))"
3379,30715869,20,"df_final = df_test[['ForecastId', 'ConfirmedCases', 'Fatalities']] 
df_final['ConfirmedCases'] = df_final['ConfirmedCases'].astype(int)
df_final['Fatalities'] = df_final['Fatalities'].astype(int)

df_final.head()"
3380,30715869,21,"df_final.plot.bar(x='ForecastId', y=['ConfirmedCases', 'Fatalities'])"
3381,30715869,22,"df_final.to_csv('submission.csv', index=False)"
3382,30715869,23,
3383,30827911,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the ""../input/"" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# Any results you write to the current directory are saved as output."
3384,30827911,1,"#import IPython
#IPython.display.IFrame(<iframe width=""650"" height=""400"" frameborder=""0"" scrolling=""no"" marginheight=""0"" marginwidth=""0"" title=""2019-nCoV"" src=""/gisanddata.maps.arcgis.com/apps/Embed/index.html?webmap=14aa9e5660cf42b5b4b546dec6ceec7c&extent=77.3846,11.535,163.5174,52.8632&zoom=true&previewImage=false&scale=true&disable_scroll=true&theme=light""></iframe>)"
3385,30827911,2,"from IPython.display import HTML

HTML('<div style=""position:relative;height:0;padding-bottom:56.25%""><iframe src=""https://www.youtube.com/embed/jmHbS8z57yI?ecver=2"" width=""640"" height=""360"" frameborder=""0"" style=""position:absolute;width:100%;height:100%;left:0"" allowfullscreen></iframe></div>')"
3386,30827911,3,"## install calmap
#! pip install calmap"
3387,30827911,4,"# essential libraries
import json
import random
from urllib.request import urlopen

# storing and anaysis
import numpy as np
import pandas as pd

# visualization
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objs as go
import plotly.figure_factory as ff
#import calmap
import folium
import plotly.io as pio
pio.templates.default = ""plotly_dark""
from plotly.subplots import make_subplots

# color pallette
cnf = '#393e46' # confirmed - grey
dth = '#ff2e63' # death - red
rec = '#21bf73' # recovered - cyan
act = '#fe9801' # active case - yellow

# converter
from pandas.plotting import register_matplotlib_converters
register_matplotlib_converters()   

# hide warnings
import warnings
warnings.filterwarnings('ignore')

# html embedding
from IPython.display import Javascript
from IPython.core.display import display
from IPython.core.display import HTML"
3388,30827911,5,"# list files
#!ls ../input/corona-virus-report
# https://www.kaggle.com/imdevskp/corona-virus-report"
3389,30827911,6,"# importing datasets


full_table = pd.read_csv('../input/corona-virus-report/covid_19_clean_complete.csv', parse_dates=['Date'])
train = pd.read_csv('/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_train.csv')
sub = pd.read_csv('/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_submission.csv')
test = pd.read_csv('/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_test.csv')"
3390,30827911,7,"full_table[full_table['Province/State']=='California']
"
3391,30827911,8,train
3392,30827911,9,"ca_by_state = train.copy()

train.columns
ca_by_state.columns =['Id', 'Province/State', 'Country/Region', 'Lat', 'Long', 'Date',
       'Confirmed', 'Deaths']
ca_by_state = ca_by_state[ca_by_state.Date >'2020-03-09']
"
3393,30827911,10,"
ca_by_state['Active'] = ca_by_state.Confirmed - ca_by_state.Deaths
ca_by_state"
3394,30827911,11,"print ('Last update of this dataset was ' + str(train.loc[len(train)-1]['Date']))
print ('Last update of the studay dataset was ' + str(full_table.loc[len(full_table)-1]['Date']))"
3395,30827911,12,"#rates
"
3396,30827911,13,"dict = {
        'California':ca_by_state,
        #'United States': us_by_date,
}"
3397,30827911,14,"def plots_by_country (country, country_name):

    temp = country

    # adding two more columns
    temp['No. of Deaths to 100 Confirmed Cases'] = round(temp['Deaths']/temp['Confirmed'], 3)*100
    # temp['No. of Recovered to 1 Death Case'] = round(temp['Recovered']/temp['Deaths'], 3)
    #print (temp)

    
    #print (temp.iloc[13]['Date'])
    last_date = temp.iloc[len(temp)-1]['Date']
    death_rate = temp[temp.Date ==last_date]['No. of Deaths to 100 Confirmed Cases']
    temp = temp.melt(id_vars='Date', value_vars=['No. of Deaths to 100 Confirmed Cases', ], 
                     var_name='Ratio', value_name='Value')

    #str(full_table.loc[len(full_table)-1]['Date'])

    fig = px.line(temp, x=""Date"", y=""Value"", color='Ratio', log_y=True, width=1000, height=700,
                  title=country_name + ' Recovery and Mortality Rate Over Time', color_discrete_sequence=[dth, rec])
    fig.show()
    return death_rate, 0
        
rates = []
for key, value in dict.items():
    death_rate, recovered_rate  = plots_by_country (value,key)
    rates.append ([key,np.float(death_rate),np.float(recovered_rate)]) "
3398,30827911,15,"import pylab
from scipy.optimize import curve_fit

def sigmoid(x, x0, k):
     y = 1 / (1 + np.exp(-k*(x-x0)))
     return y

def exp (x,a,b):
    y = a* np.exp(x*b)
    return y

def gaussian(x, a, x0, sigma):
    return a*np.exp(-(x-x0)**2/(2*sigma**2))

def growth_rate_over_time (f, country, attribute, title):
    ydata = country[attribute]
    

    xdata = list(range(len(ydata)))

    rates = []
    for i, x in enumerate(xdata):
        if i > 2:
#            print (xdata[:x+1])
#            print (ydata[:x+1])

            popt, pcov = curve_fit(f, xdata[:x+1], ydata[:x+1])
            rates.append (popt[1])
    rates = np.array(rates) 
    pylab.style.use('dark_background')
    pylab.figure(figsize=(12,8))
    xdata = np.array(xdata)
    #pylab.grid(True, linestyle='-', color='0.75')
    pylab.plot(xdata[3:]+1, 100*rates, 'o', linestyle='solid', label=attribute)
    #if fit_good:
    #    pylab.plot(x,y, label='fit')
    #pylab.ylim(0, ymax*1.05)
    #pylab.legend(loc='best')
    pylab.xlabel('Days Since Start')
    pylab.ylabel('Growth rate percentage ' + attribute)
    pylab.title(title + attribute, size = 15)
    pylab.show()
    
        
    

def plot_curve_fit (f, country, attribute, title, normalize = False, curve = 'Exp'):
    #print (country)
    #country = country[10:]
    fit_good = True
    ydata = country[attribute]
    #ydata = np.array(ydata)
    xdata = range(len(ydata))
    mu = np.mean(ydata)
    sigma = np.std(ydata)
    ymax = np.max(ydata)    
    if normalize:
        ydata_norm = ydata/ymax
    else:
        ydata_norm = ydata
    #f = sigmoid
    try:
        if curve == 'Gauss': # pass the mean and stddev
            popt, pcov = curve_fit(f, xdata, ydata_norm, p0 = [1, mu, sigma])
        else:    
            popt, pcov = curve_fit(f, xdata, ydata_norm,)
    except RuntimeError:
        print ('Exception - RuntimeError - could not fit curve')
        fit_good = False
    else:

        fit_good = True
        
    if fit_good:
        if curve == 'Exp':   
            print (key + ' -- Coefficients for y = a * e^(x*b)  are ' + str(popt))
            print ('Growth rate is now ' + str(round(popt[1],2)))
        elif curve == 'Gauss':
            print (key + ' -- Coefficients are ' + str(popt))
        else:   # sigmoid 
            print (key + ' -- Coefficients for y 1/(1 + e^(-k*(x-x0)))  are ' + str(popt))
            
        print ('Mean error for each coefficient: ' + str(np.sqrt(np.diag(pcov))/popt))
    else:
        print (key + ' -- Could not resolve coefficients ---')
    x = np.linspace(-1, len(ydata), 100)
    if fit_good:
        y = f(x, *popt)
        if normalize:
            y = y * ymax
    plt.style.use('dark_background')
    pylab.figure(figsize=(15,12)) 
    #pylab.grid(True, linestyle='-', color='0.75')
    pylab.plot(xdata, ydata, 'o', label=attribute)
    if fit_good:
        pylab.plot(x,y, label='fit')
    pylab.ylim(0, ymax*1.05)
    pylab.legend(loc='best')
    pylab.xlabel('Days Since Start')
    pylab.ylabel('Number of ' + attribute)
    pylab.title(title + attribute, size = 15)
    pylab.show()
"
3399,30827911,16,"for key, value in dict.items():
    if key in [""China"",'Rest of China w/o Hubei']:
        pass
    else:
        #growth_rate_over_time (exp, value, 'Confirmed', ""Growth Rate Percentage - "")
        growth_rate_over_time (exp, value, 'Confirmed', key + ' - Growth Rate Percentage for ',)
        #growth_rate_over_time (exp, value, 'Deaths', key + ' - Growth Curve for ',)
        #growth_rate_over_time (exp, value, 'Recovered', key + ' - Growth Curve for ',False)"
3400,30827911,17,"round (72/35,2)"
3401,30827911,18,"for key, value in dict.items():
    if key in [""China"",'Rest of China w/o Hubei']:
        pass
    else:
        plot_curve_fit (exp, value, 'Confirmed', key + ' - Growth Curve for ',False,'Exp')
        plot_curve_fit (exp, value, 'Deaths', key + ' - Growth Curve for ',False,'Exp')
        #plot_curve_fit (exp, value, 'Recovered', key + ' - Growth Curve for ',False,'Exp')"
3402,30827911,19,"#    plot_curve_fit (sigmoid, value, 'Recovered', key + ' - Logistic Growth Curve for ',True,'Logistic')"
3403,30827911,20,"plot_curve_fit (gaussian, ca_by_state, 'Active', 'California' + ' - Curve for Cases ',False,'Gauss')"
3404,30827911,21,"x0 = 33
k = 0.27
kd = 0.3
x0_death = 35
results = [] 
total_estimated = 500000
total_deaths = total_estimated * 0.15
for x in range(1,44):
    conf = int(total_estimated * sigmoid(x, x0, k))
    deaths = int(total_deaths * sigmoid(x, x0_death, kd))
    print ('Confirmed estimate for day ' + str(x) + ' - ' + str(conf))
    print ('Death estimate for day ' + str(x) + ' - ' + str(deaths))
    results.append([x,conf,deaths])"
3405,30827911,22,ca_by_state
3406,30827911,23,"r = pd.DataFrame(results)
r.columns = sub.columns
sub = r.copy()
sub
"
3407,30827911,24,"sub.to_csv(""submission.csv"", index=False)"
3408,30725465,0,"#Libraried
import pandas as pd
pd.set_option('display.max_columns', None)
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings(""ignore"")

import datetime
from time import time
from scipy import stats

from sklearn.model_selection import GroupKFold
import lightgbm as lgb
import xgboost as xgb
from catboost import CatBoostRegressor, CatBoostClassifier
import plotly.express as px
import plotly.graph_objects as go
import plotly.figure_factory as ff
from plotly.subplots import make_subplots
import os
import glob
import copy

import numpy as np
from scipy.integrate import odeint"
3409,30725465,1,"ca_train = pd.read_csv('/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_train.csv')
ca_test = pd.read_csv('/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_test.csv')
ca_submission = pd.read_csv('/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_submission.csv')

train_df = ca_train
test_df =  ca_test
submission_df =  ca_submission"
3410,30725465,2,train_df.head()
3411,30725465,3,"x_1 = train_df['Date']
y_1 = train_df['ConfirmedCases']
y_2 = train_df['Fatalities']

fig = make_subplots(rows=1, cols=1)

fig.add_trace(
    go.Scatter(x=x_1, mode='lines+markers', y=y_1, marker=dict(color=""mediumaquamarine""), showlegend=False,
               name=""Original signal""),
    row=1, col=1
)

fig.add_trace(
    go.Scatter(x=x_1, mode='lines+markers', y=y_2, marker=dict(color=""darkgreen""), showlegend=False,
               name=""Original signal""),
    row=1, col=1
)

fig.update_layout(height=400, width=800, title_text=""ConfirmedCases (pale) vs. Fatalities (dark) "")
fig.show()"
3412,30725465,4,"# SI model
N = 1000000          # Total population
I = np.zeros(200)  # Infected
S = np.zeros(200)   # Susceptible

r = 10             # This value defines how quickly the disease spreads
B = 0.01            # Probability of being infected

I[0] = 1           # On day 0, there's only one infected person
S[0] = N-I[0]      # So the suspecptible people is equal = N - I[0]

for idx in range(199):
    S[idx+1] = S[idx] - r*B*I[idx]*S[idx]/N
    I[idx+1] = I[idx] + r*B*I[idx]*S[idx]/N
"
3413,30725465,5,"sns.lineplot(x=np.arange(200), y=S, label='Susceptible')
sns.lineplot(x=np.arange(200), y=I, label='Infected')"
3414,30725465,6,"N = 36000000        # Total population
days = 200          # Period
E = np.zeros(days)  # Exposed          
E[0] = 0            # Day 0 exposed
I = np.zeros(days)  # Infected
I[0] = 144          # Day 0 infected                                                                
S = np.zeros(days)  # Susceptible
S[0] = N - I[0]     # Day 0 susceptible
R = np.zeros(days)  # Recovered
R[0] = 0

r = 20              # Number of susceptible could be contactes by an infected
B = 0.03            # Probability of spread for infected
a = 0.1             # Probability of converted from exposed to infected
r2 = 20             # Number of susceptible could be contactes by an exposed
B2 = 0.03           # Probability of spread for exposed
y = 0.1             # Probability of recovered


for idx in range(days-1):
    S[idx+1] = S[idx] - r*B*S[idx]*I[idx]/N - r2*B2*S[idx]*E[idx]/N
    E[idx+1] = E[idx] + r*B*S[idx]*I[idx]/N -a*E[idx] + r2*B2*S[idx]*E[idx]/N
    I[idx+1] = I[idx] + a*E[idx] - y*I[idx]
    R[idx+1] = R[idx] + y*I[idx]

    
plt.figure(figsize=(16,9))
sns.lineplot(x=np.arange(200), y=S, label='Susceptible')
sns.lineplot(x=np.arange(200), y=I, label='Infected')
sns.lineplot(x=np.arange(200), y=E, label='Exposed')
sns.lineplot(x=np.arange(200), y=R, label='Recovered')



I_origin = copy.copy(I)"
3415,30725465,7,"N = 36000000        # Total population
days = 200          # Period
E = np.zeros(days)  # Exposed          
E[0] = 0            # Day 0 exposed
I = np.zeros(days)  # Infected
I[0] = 144            # Day 0 infected                                                                
S = np.zeros(days)  # Susceptible
S[0] = N - I[0]     # Day 0 susceptible
R = np.zeros(days)  # Recovered
R[0] = 0

r = 20              # Number of susceptible could be contactes by an infected
B = 0.03            # Probability of spread for infected
a = 0.1             # Probability of converted from exposed to infected
r2 = 20             # Number of susceptible could be contactes by an exposed
B2 = 0.03           # Probability of spread for exposed
y = 0.1             # Probability of recovered


for idx in range(days-1):
    if idx>10:
        r = 5
        r2 = 5
    S[idx+1] = S[idx] - r*B*S[idx]*I[idx]/N - r2*B2*S[idx]*E[idx]/N
    E[idx+1] = E[idx] + r*B*S[idx]*I[idx]/N -a*E[idx] + r2*B2*S[idx]*E[idx]/N
    I[idx+1] = I[idx] + a*E[idx] - y*I[idx]
    R[idx+1] = R[idx] + y*I[idx]

plt.figure(figsize=(16,9))
sns.lineplot(x=np.arange(200), y=S, label='Secestible')
sns.lineplot(x=np.arange(200), y=I, label='Infected')
sns.lineplot(x=np.arange(200), y=E, label='Exposed')
sns.lineplot(x=np.arange(200), y=R, label='Recovered')

I_sd = copy.copy(I)"
3416,30725465,8,"plt.figure(figsize=(16,9))
sns.lineplot(x=np.arange(200), y=I_origin, label='Infected w/o social distancing')
sns.lineplot(x=np.arange(200), y=I_sd, label='Infected w/ social distancing')"
3417,30725465,9,"def base_seir_model(init_vals, params, t):
    S_0, E_0, I_0, R_0 = init_vals
    S, E, I, R = [S_0], [E_0], [I_0], [R_0]
    alpha, beta, gamma = params
    dt = t[1] - t[0]
    for _ in t[1:]:
        next_S = S[-1] - (beta*S[-1]*I[-1])*dt
        next_E = E[-1] + (beta*S[-1]*I[-1] - alpha*E[-1])*dt
        next_I = I[-1] + (alpha*E[-1] - gamma*I[-1])*dt
        next_R = R[-1] + (gamma*I[-1])*dt
        S.append(next_S)
        E.append(next_E)
        I.append(next_I)
        R.append(next_R)
    return np.stack([S, E, I, R]).T"
3418,30725465,10,"# Define parameters
t_max = 100
dt = .1
t = np.linspace(0, t_max, int(t_max/dt) + 1)
N = 36000000
init_infected = 144
init_exposed = 200
init_vals = 1 - (init_infected + init_exposed)/N,  init_exposed/N, init_infected/N, 0
alpha = 0.2
beta = 1.75
gamma = 0.5
params = alpha, beta, gamma
# Run simulation
results = base_seir_model(init_vals, params, t)
results_df = pd.DataFrame(results*N, columns=['susceptible', 'exposed',
                               'infected', 'recovered'])
results_df.head()
plt.figure(figsize=(16,9))
sns.lineplot(x=results_df.index, y=results_df['infected'], label='infected')
sns.lineplot(x=results_df.index, y=results_df['susceptible'], label='susceptible')
sns.lineplot(x=results_df.index, y=results_df['exposed'], label='exposed')
sns.lineplot(x=results_df.index, y=results_df['recovered'], label='recovered')
"
3419,30725465,11,"def seir_model_with_soc_dist(init_vals, params, t):
    S_0, E_0, I_0, R_0 = init_vals
    S, E, I, R = [S_0], [E_0], [I_0], [R_0]
    alpha, beta, gamma, rho = params
    dt = t[1] - t[0]
    for _ in t[1:]:
        next_S = S[-1] - (rho*beta*S[-1]*I[-1])*dt
        next_E = E[-1] + (rho*beta*S[-1]*I[-1] - alpha*E[-1])*dt
        next_I = I[-1] + (alpha*E[-1] - gamma*I[-1])*dt
        next_R = R[-1] + (gamma*I[-1])*dt
        S.append(next_S)
        E.append(next_E)
        I.append(next_I)
        R.append(next_R)
    return np.stack([S, E, I, R]).T"
3420,30725465,12,"# Define parameters
t_max = 100
dt = .1
t = np.linspace(0, t_max, int(t_max/dt) + 1)
N = 36000000
init_infected = 144
init_exposed = 5000
init_vals = 1 - (init_infected + init_exposed)/N,  init_exposed/N, init_infected/N, 0
alpha = 0.2
beta = 1.75
gamma = 0.5
rho = 0.8
params = alpha, beta, gamma, rho
# Run simulation
results = seir_model_with_soc_dist(init_vals, params, t)
results_df = pd.DataFrame(results*N, columns=['susceptible', 'exposed',
                               'infected', 'recovered'])
results_df.head()

plt.figure(figsize=(16,9))
sns.lineplot(x=results_df.index, y=results_df['infected'], label='infected')
sns.lineplot(x=results_df.index, y=results_df['susceptible'], label='susceptible')
sns.lineplot(x=results_df.index, y=results_df['exposed'], label='exposed')
sns.lineplot(x=results_df.index, y=results_df['recovered'], label='recovered')
"
3421,30725465,13,"# # Define parameters
# t_max = 100
# dt = .1
# t = np.linspace(0, t_max, int(t_max/dt) + 1)
# N = 36000000
# init_infected = 144
# init_exposed = 300
# init_vals = 1 - (init_infected + init_exposed)/N,  init_exposed/N, init_infected/N, 0
# alpha = 0.2
# beta = 1.75
# gamma = 0.5
# rho = 1
# params = alpha, beta, gamma, rho
# # Run simulation
# results = seir_model_with_soc_dist(init_vals, params, t)
# results_df = pd.DataFrame(results*N, columns=['susceptible', 'exposed',
#                                'infected', 'recovered'])
# results_df.head()
# plt.figure(figsize=(16,9))
# sns.lineplot(x=results_df.index, y=results_df['infected'], label='infected')
# sns.lineplot(x=results_df.index, y=results_df['susceptible'], label='susceptible')
# sns.lineplot(x=results_df.index, y=results_df['exposed'], label='exposed')
# sns.lineplot(x=results_df.index, y=results_df['recovered'], label='recovered')
"
3422,30725465,14,"ca_covid_df = ca_train[(ca_train['Province/State']=='California') & (ca_train['Date']>='2020-03-10')]
ca_covid_df['pred_confirmed'] = results_df[:ca_covid_df.shape[0]]['infected'].values
ca_covid_df"
3423,30725465,15,"results_df['Date'] = pd.date_range('2020-03-10', periods=results_df.shape[0]).values
submission_df['Date'] =  pd.date_range(start=ca_test['Date'].min(), periods=len(submission_df))
death_rate = 0.012
submission_df = pd.merge(submission_df, results_df, how='left', on='Date')
submission_df['ConfirmedCases'] = submission_df['infected']
submission_df['Fatalities'] = submission_df['ConfirmedCases'] * death_rate
submission_df[['ForecastId', 'ConfirmedCases', 'Fatalities']].to_csv('submission.csv', index=False)"
3424,30725465,16,submission_df
3425,30696294,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the ""../input/"" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# Any results you write to the current directory are saved as output."
3426,30696294,1,"train = pd.read_csv(""/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_train.csv"", parse_dates=['Date'])
"
3427,30696294,2,train.head()
3428,30696294,3,train.tail()
3429,30696294,4,"test = pd.read_csv(""/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_test.csv"", parse_dates=['Date'])
"
3430,30696294,5,test.head()
3431,30696294,6,test.tail()
3432,30696294,7,"#Are rows unique?
(len(train)==train.index.nunique()) & (len(test)==test.Date.nunique())"
3433,30696294,8,"from fbprophet import Prophet
"
3434,30696294,9,"df_c = pd.DataFrame()
df_c['ds'] = pd.to_datetime(train.Date)
df_c['y'] = train.ConfirmedCases
df_c.set_index('ds', inplace=True)
df_c = df_c.loc[""2020-03-09"":""2020-03-11""]
df_c.reset_index(inplace=True)

df_f = pd.DataFrame()
df_f['ds'] = pd.to_datetime(train.Date)
df_f['y'] = train.Fatalities
df_f.set_index('ds', inplace=True)
df_f = df_f.loc[""2020-03-09"":""2020-03-11""]
df_f.reset_index(inplace=True)"
3435,30696294,10,"m_c, m_f = Prophet(), Prophet()
m_c.fit(df_c)
m_f.fit(df_f)

future = m_c.make_future_dataframe(periods=len(test))

forecast_c = m_c.predict(future)
forecast_f = m_f.predict(future)"
3436,30696294,11,"forecast_c = forecast_c[['ds','yhat']]
forecast_f = forecast_f[['ds','yhat']]

forecast_c= forecast_c.rename(columns={""yhat"":""ConfirmedCases""})
forecast_f = forecast_f.rename(columns={""yhat"":""Fatalities""})
"
3437,30696294,12,"conf_cases = forecast_c.ConfirmedCases.iloc[3:].reset_index(drop=True)
fatal_cases = forecast_f.Fatalities.iloc[3:].reset_index(drop=True)

"
3438,30696294,13,"test.reset_index(inplace=True,drop=True)"
3439,30696294,14,"submissions = pd.concat([test.ForecastId, conf_cases,fatal_cases], axis=1)

"
3440,30696294,15,submissions.head()
3441,30696294,16,"submissions.ConfirmedCases = submissions.ConfirmedCases.astype(int)
submissions.Fatalities = submissions.Fatalities.astype(int)"
3442,30696294,17,submissions.head()
3443,30696294,18,"submissions.to_csv('submission.csv', index=False)"
3444,30696294,19,
3445,30846479,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the ""../input/"" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# Any results you write to the current directory are saved as output."
3446,30846479,1,"import datetime as dt
import matplotlib.pyplot as plt
%matplotlib inline
import matplotlib.patches as mpatches"
3447,30846479,2,"ca_test_covid = pd.read_csv('/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_test.csv')
ca_test_covid['Date']=pd.to_numeric(ca_test_covid.Date.str.replace('-',''))
ca_test_covid.head()
"
3448,30846479,3,"ca_train_covid = pd.read_csv('/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_train.csv')
ca_train_covid.head()"
3449,30846479,4,"#plt.close('all')
plt.figure()
ca_train_covid_confirm_cases=ca_train_covid[ca_train_covid['ConfirmedCases']>0]
ca_train_covid_confirm_cases['Date']=pd.to_numeric(ca_train_covid_confirm_cases.Date.str.replace('-',''))

"
3450,30846479,5,"def calculateDateRank(date_values):
    listRank=()
    arrayVal=np.array(date_values)
    rank=1
    for i in np.arange(len(arrayVal)):
        listRank=np.append(listRank,rank)
        rank=rank+1   
    
    return listRank"
3451,30846479,6,"ca_train_covid_confirm_cases['DateRank']=calculateDateRank(ca_train_covid_confirm_cases.Date)
ca_train_covid_confirm_cases.head()"
3452,30846479,7,"ca_test_covid['DateRank']=calculateDateRank(ca_test_covid.Date)
ca_train_covid_confirm_cases.plot.scatter(x='Date',y='ConfirmedCases')
ca_train_covid_confirm_cases.plot.scatter(x='Date',y='Fatalities')"
3453,30846479,8,ca_train_covid_confirm_cases.corr()
3454,30846479,9,"import matplotlib.pyplot as plt
import numpy as np
from sklearn import datasets, linear_model
from sklearn.metrics import mean_squared_error, r2_score,mean_squared_log_error
from sklearn.preprocessing import PolynomialFeatures"
3455,30846479,10,"# Create linear regression model
#regModelConfirmedCases = linear_model.LinearRegression()
regModelConfirmedCases = linear_model.RidgeCV(alphas=np.logspace(-6, 6, 13))
trainX=np.array(ca_train_covid_confirm_cases['DateRank']).reshape(-1,1)
trainY=np.array(ca_train_covid_confirm_cases['ConfirmedCases']).reshape(-1,1)"
3456,30846479,11,"# Train the model using the training sets
poly_features = PolynomialFeatures(degree=4)

X_train_poly = poly_features.fit_transform(trainX)    
    
regModelConfirmedCases.fit(X_train_poly, trainY)"
3457,30846479,12,"# Make predictions using the testing set
ca_test_covid_date_gt_26Mar=ca_test_covid
testX_Date=np.array(ca_test_covid.DateRank).reshape(-1,1)
pred_ConfirmedCase = regModelConfirmedCases.predict(poly_features.fit_transform(testX_Date))"
3458,30846479,13,"#print(""Graph of a Train Date point vs ConfirmedCase prediction regression line"")
# Plot outputs
plt.scatter(trainX, trainY,  color='green')
plt.plot(testX_Date, pred_ConfirmedCase, color='blue', linewidth=1)
plt.scatter(testX_Date, pred_ConfirmedCase,  color='yellow')

plt.title('training / predicted values across regression line for ')
plt.xticks(())
plt.yticks(())
plt.xlabel(""Date Rank"")
plt.ylabel(""ConfirmedCase"")

green_patch = mpatches.Patch(color='green', label='Train Values')
yellow_patch = mpatches.Patch(color='yellow', label='Predicted Values')

plt.legend(handles=[green_patch,yellow_patch])

plt.show()"
3459,30846479,14,"# Train the model using the training sets
trainY_Fatalities=np.array(ca_train_covid_confirm_cases['Fatalities']).reshape(-1,1)
regrModelFatalities = linear_model.RidgeCV(alphas=np.logspace(-6, 6, 13))
regrModelFatalities.fit(X_train_poly, trainY_Fatalities)

predY_Fatalities=regrModelFatalities.predict(poly_features.fit_transform(testX_Date))
"
3460,30846479,15,"# Plot outputs

plt.scatter(trainX, trainY_Fatalities,  color='green')
plt.plot(testX_Date, predY_Fatalities, color='blue', linewidth=1)
plt.scatter(testX_Date, predY_Fatalities,  color='yellow')

plt.xticks(())
plt.yticks(())
plt.xlabel(""Date Rank"")
plt.ylabel(""Fatalities"")
plt.title('training data (DateRank)/Predicted Data(Fatalities) and regression line for ')

green_patch = mpatches.Patch(color='green', label='Train Values')
yellow_patch = mpatches.Patch(color='yellow', label='Predicted Values')

plt.legend(handles=[green_patch,yellow_patch])
plt.show()
#print(ca_test_covid.Date)"
3461,30846479,16,"ca_test_covid['ConfirmedCases']=pred_ConfirmedCase
ca_test_covid['Fatalities']=predY_Fatalities
ca_test_covid_submission=ca_test_covid[['ForecastId','ConfirmedCases','Fatalities']]
ca_test_covid_submission.to_csv('submission.csv', index=False)
"
3462,30846479,17,ca_test_covid_submission
3463,30544538,0,"import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import integrate, optimize
from sklearn.linear_model import LinearRegression"
3464,30544538,1,"ca_train = pd.read_csv('../input/covid19-local-us-ca-forecasting-week-1/ca_train.csv')
ca_test = pd.read_csv('../input/covid19-local-us-ca-forecasting-week-1/ca_test.csv')
ca_submission = pd.read_csv('../input/covid19-local-us-ca-forecasting-week-1/ca_submission.csv')

train_df = ca_train
test_df =  ca_test
submission_df =  ca_submission"
3465,30544538,2,train_df.head()
3466,30544538,3,"reported = train_df[train_df['Date']>= '2020-03-10'].reset_index()
reported['day_count'] = list(range(1,len(reported)+1))
reported.head()"
3467,30544538,4,"ydata = [i for i in reported.ConfirmedCases.values]
xdata = reported.day_count
ydata = np.array(ydata, dtype=float)
xdata = np.array(xdata, dtype=float)"
3468,30544538,5,"N = 36000000 #population of California
inf0 = ydata[0] #Infectious
sus0 = N - inf0 #Susceptible
exp0 = 0.0 #Exposed
rec0 = 0.0 #Recovered
init_state = [sus0, exp0, inf0, rec0]
#beta = 1.0 #constant.
#gamma = 1.0 / 7.0 #constant."
3469,30544538,6,"# Define differential equation of SEIR model

'''
dS/dt = -beta * S * I / N
dE/dt = beta* S * I / N - epsilon * E
dI/dt = epsilon * E - gamma * I
dR/dt = gamma * I

[v[0], v[1], v[2], v[3]]=[S, E, I, R]

dv[0]/dt = -beta * v[0] * v[2] / N
dv[1]/dt = beta * v[0] * v[2] / N - epsilon * v[1]
dv[2]/dt = epsilon * v[1] - gamma * v[2]
dv[3]/dt = gamma * v[2]

'''

def seir_model(v, x, beta, epsilon, gamma, N):
    return [-beta * v[0] * v[2] / N ,beta * v[0] * v[2] / N - epsilon * v[1],
            epsilon * v[1] - gamma * v[2],gamma * v[2]]

def fit_odeint(x, beta, epsilon, gamma):
    return integrate.odeint(seir_model, init_state, x, args=(beta, epsilon, gamma, N))[:,2]"
3470,30544538,7,"popt, pcov = optimize.curve_fit(fit_odeint, xdata, ydata)
fitted = fit_odeint(xdata, *popt)"
3471,30544538,8,"print(""Optimal parameters: beta = "", popt[0], ""epsilon = "", popt[1], "", gamma = "", popt[2])"
3472,30544538,9,"N = 36000000 #population of California
inf0 = ydata[0] #Infectious
sus0 = N - inf0 #Susceptible
exp0 = 0.0 #Exposed
rec0 = 0.0 #Recovered
init_state = [sus0, exp0, inf0, rec0]
beta = 1.0 #constant.
#gamma = 1.0 / 7.0 #constant."
3473,30544538,10,"# Define differential equation of SEIR model
def seir_model(v, x, beta, epsilon, gamma, N):
    return [-beta * v[0] * v[2] / N ,beta * v[0] * v[2] / N - epsilon * v[1],
            epsilon * v[1] - gamma * v[2],gamma * v[2]]

def fit_odeint(x, epsilon, gamma):
    return integrate.odeint(seir_model, init_state, x, args=(beta, epsilon, gamma, N))[:,2]"
3474,30544538,11,"popt, pcov = optimize.curve_fit(fit_odeint, xdata, ydata)
fitted = fit_odeint(xdata, *popt)"
3475,30544538,12,"print(""Optimal parameters: epsilon = "", popt[0], "", gamma = "", popt[1])"
3476,30544538,13,"N = 36000000 #population of California
inf0 = ydata[0] #Infectious
sus0 = N - inf0 #Susceptible
exp0 = 0.0 #Exposed
rec0 = 0.0 #Recovered
init_state = [sus0, exp0, inf0, rec0]
beta = 1.0 #constant.
gamma = 1.0 / 7.0 #constant."
3477,30544538,14,"# Define differential equation of SEIR model
def seir_model(v, x, beta, epsilon, gamma, N):
    return [-beta * v[0] * v[2] / N ,beta * v[0] * v[2] / N - epsilon * v[1],
            epsilon * v[1] - gamma * v[2],gamma * v[2]]

def fit_odeint(x, epsilon):
    return integrate.odeint(seir_model, init_state, x, args=(beta, epsilon, gamma, N))[:,2]"
3478,30544538,15,"popt, pcov = optimize.curve_fit(fit_odeint, xdata, ydata)
fitted = fit_odeint(xdata, *popt)"
3479,30544538,16,"inf_period = 1.0/gamma
lat_period = 1.0/popt[0]
print(""Optimal parameters: gamma ="", gamma, "", epsilon = "", popt[0], ""\ninfectious period(day) = "", inf_period, "", latency period(day) = "", lat_period)"
3480,30544538,17,"plt.plot(xdata, ydata, 'o')
plt.plot(xdata, fitted)
plt.title(""Fit of SEIR model to global infected cases"")
plt.ylabel(""Population infected"")
plt.xlabel(""Days"")
plt.show()"
3481,30544538,18,"# parameters
t_max = 100 #days
dt = 1

N = 36000000 #population of California
inf0 = ydata[0] #Infectious
sus0 = N - inf0 #Susceptible
exp0 = 0.0 #Exposed
rec0 = 0.0 #Recovered
init_state = [sus0, exp0, inf0, rec0]
beta_const = 1.0 #Assumption: Infection rate is constant.
epsilon_const = popt[0]
gamma_const = 1.0 / 7.0 #Assumption: Recovery rate is constant."
3482,30544538,19,"# numerical integration
times = np.arange(0, t_max, dt)
args = (beta_const, epsilon_const, gamma_const, N)

# Numerical Solution using scipy.integrate
# Solver SEIR model
result = integrate.odeint(seir_model, init_state, times, args)
# plot
plt.plot(times, result)
plt.legend(['Susceptible', 'Exposed', 'Infectious', 'Removed'])
plt.title(""SEIR model  COVID-19"")
plt.xlabel('time(days)')
plt.ylabel('population')
plt.grid()

plt.show()"
3483,30544538,20,"result_df = pd.DataFrame(data=result, columns=['Susceptible', 'Exposed', 'Infectious', 'Removed'])
result_df.shape"
3484,30544538,21,"lr = LinearRegression()
X_train = reported[['ConfirmedCases']].values
Y_train = reported[['Fatalities']].values
lr.fit(X_train, Y_train)
print('coefficient = ', lr.coef_[0], '(which means Fatality rate)')
print('intercept = ', lr.intercept_)"
3485,30544538,22,"X_pred = result_df[['Infectious']].values
Y_pred = lr.predict(X_pred)
plt.scatter(X_train, Y_train, c='blue')
plt.plot(X_pred, Y_pred, c='red')
plt.title(""Regression Line"")
plt.xlabel('ConfirmedCases')
plt.ylabel('Fatalities')
plt.grid()

plt.xlim([100,800])
plt.ylim([0,20])

plt.show()"
3486,30544538,23,"Y_pred_df = pd.DataFrame(Y_pred)
result_df['Fatalities'] = Y_pred_df
result_df.head()"
3487,30544538,24,"submission = result_df[0:len(submission_df)].reset_index()
submission_df['ConfirmedCases'] = submission['Infectious']
submission_df['Fatalities'] = submission['Fatalities']
submission_df.head()"
3488,30544538,25,"submission_df.to_csv(""submission.csv"", index=False)"
3489,30544538,26,
3490,30636382,0,"#!/usr/bin/env python3
# -*- coding: utf-8 -*-
""""""
Created on Sat Mar 21 15:36:47 2020

@author: rahul
""""""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import datetime

df_train=pd.read_csv('../input/covid19-local-us-ca-forecasting-week-1/ca_train.csv')
df_test=pd.read_csv('../input/covid19-local-us-ca-forecasting-week-1/ca_test.csv')
df_sub=pd.read_csv('../input/covid19-local-us-ca-forecasting-week-1/ca_submission.csv')

""""""
df_train=pd.read_csv('ca_train.csv')
df_test=pd.read_csv('ca_test.csv')
df_sub=pd.read_csv('ca_submission.csv')
""""""

reported=df_train[df_train['Date']>='2020-03-10'].reset_index()
reported['day_count']=list(range(1, len(reported)+1))

X_train=reported.iloc[:, 9:10].values
y_train=reported.iloc[:, 7:8].values

X_test=list(range(1, 53))

from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
poly_reg=PolynomialFeatures(degree=2)
X_poly=poly_reg.fit_transform(X_train)
lin_reg=LinearRegression()
lin_reg.fit(X_poly, y_train)

X_grid = np.arange(min(X_train), max(X_train), 0.1)
X_grid = X_grid.reshape((len(X_grid), 1))

plt.scatter(X_train, y_train, color = 'red')
plt.scatter(X_train, y_train, color='red')
plt.plot(X_grid, lin_reg.predict(poly_reg.fit_transform(X_grid)), color='blue')
plt.title('Covid 19')
plt.xlabel('Date')
plt.ylabel('Confirmed Cases-Training Dataset')
plt.show()

y_train_pred=lin_reg.predict(poly_reg.fit_transform(X_train))

X_test=np.arange(1, 53, 1)
X_test = X_test.reshape((len(X_test), 1))

y_pred=lin_reg.predict(poly_reg.fit_transform(X_test))

plt.plot(X_test, y_pred, color='blue')
plt.title('Covid 19')
plt.xlabel('Date')
plt.ylabel('Confirmed Cases-Test Dataset')
plt.show()

regressor=LinearRegression()
X_train_2=reported[['ConfirmedCases']].values
y_train_2=reported[['Fatalities']].values
regressor.fit(X_train_2, y_train_2)

X_test_2=y_pred
y_pred_2 = regressor.predict(X_test_2)

plt.scatter(X_train_2, y_train_2, c='blue')
plt.plot(X_test_2, y_pred_2, c='red')
plt.title(""Regression Line"")
plt.xlabel('ConfirmedCases')
plt.ylabel('Fatalities')
plt.grid()
plt.xlim([100,800])
plt.ylim([0,20])
plt.show()

result=pd.DataFrame(y_pred)
result[1]=y_pred_2
submission=result[0:len(df_sub)].reset_index()
df_sub['ConfirmedCases'] = submission[0]
df_sub['Fatalities'] = submission[1]

df_sub.to_csv(""submission.csv"", index=False)"
3491,30636382,1,
3492,30487343,0,"#Libraried
import pandas as pd
pd.set_option('display.max_columns', None)
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings(""ignore"")

import datetime
from time import time
from scipy import stats

from sklearn.model_selection import GroupKFold
import lightgbm as lgb
import xgboost as xgb
from catboost import CatBoostRegressor, CatBoostClassifier
import plotly.express as px
import plotly.graph_objects as go
import plotly.figure_factory as ff
from plotly.subplots import make_subplots
import os
import glob

import numpy as np
from scipy.integrate import odeint"
3493,30487343,1,"ca_train = pd.read_csv('/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_train.csv')
ca_test = pd.read_csv('/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_test.csv')
ca_submission = pd.read_csv('/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_submission.csv')

train_df = ca_train
test_df =  ca_test
submission_df =  ca_submission"
3494,30487343,2,train_df.head()
3495,30487343,3,"x_1 = train_df['Date']
y_1 = train_df['ConfirmedCases']
y_2 = train_df['Fatalities']

fig = make_subplots(rows=1, cols=1)

fig.add_trace(
    go.Scatter(x=x_1, mode='lines+markers', y=y_1, marker=dict(color=""mediumaquamarine""), showlegend=False,
               name=""Original signal""),
    row=1, col=1
)

fig.add_trace(
    go.Scatter(x=x_1, mode='lines+markers', y=y_2, marker=dict(color=""darkgreen""), showlegend=False,
               name=""Original signal""),
    row=1, col=1
)

fig.update_layout(height=400, width=800, title_text=""ConfirmedCases (pale) vs. Fatalities (dark) "")
fig.show()"
3496,30487343,4,"x_1 = train_df['Date']
y_1 = train_df['Lat']
y_2 = train_df['Long']

fig = make_subplots(rows=1, cols=1)

fig.add_trace(
    go.Scatter(x=x_1, mode='lines+markers', y=y_1, marker=dict(color=""mediumaquamarine""), showlegend=False,
               name=""Original signal""),
    row=1, col=1
)

fig.add_trace(
    go.Scatter(x=x_1, mode='lines+markers', y=y_2, marker=dict(color=""darkgreen""), showlegend=False,
               name=""Original signal""),
    row=1, col=1
)

fig.update_layout(height=400, width=800, title_text=""Lat (pale) vs. Long (dark) "")
fig.show()"
3497,30487343,5,"#define seir

def SEIR_EQ(v, t, beta, epsilon, gamma, N ):
    return [-beta * v[0] * v[2] / N ,beta * v[0] * v[2] / N - epsilon * v[1],
            epsilon * v[1] - gamma * v[2],gamma * v[2]]"
3498,30487343,6,"# parameters
t_max = 100 #days
dt = 1

# initial_state
S_0 = 36000000 #population of California
E_0 = 0  #number of Exposed on Mar 10: it can't be 0 actually
I_0 = 144 #number of Infectious on Mar 10
R_0 = 0
N_pop = S_0 + E_0 + I_0 + R_0
ini_state = [S_0, E_0, I_0, R_0]  # [S[0],E,[0], I[0], R[0]]


#infection rate
beta_const = 1 #infection rate

#infection rateafter expose
latency_period = 2 #days
epsilon_const = 1/latency_period


infectious_period = 7.4 #days
gamma_const = 1/infectious_period

#case fatality rate
death_rate = 0.01"
3499,30487343,7,"# numerical integration
times = np.arange(0, t_max, dt)
args = (beta_const, epsilon_const, gamma_const, N_pop)

# Numerical Solution using scipy.integrate
# Solver SEIR model
result = odeint(SEIR_EQ, ini_state, times, args)
# plot
plt.plot(times, result)
plt.legend(['Susceptible', 'Exposed', 'Infectious', 'Removed'])
plt.title(""SEIR model  COVID-19"")
plt.xlabel('time(days)')
plt.ylabel('population')
plt.grid()

plt.show()"
3500,30487343,8,"predicted = pd.DataFrame(result)
predicted.columns = ['Susceptible', 'Exposed', 'Infectious', 'Removed']
predicted['death'] = predicted['Infectious']*death_rate"
3501,30487343,9,"reported = train_df[train_df['Date']>= '2020-03-10'].reset_index()
reported.head()"
3502,30487343,10,"tmp_predicted = predicted[0:len(reported)]

reported_rate = reported['ConfirmedCases']/tmp_predicted['Infectious']
reported_rate_c = np.average(a= reported_rate, weights=reported['ConfirmedCases'])
reported_rate_c"
3503,30487343,11,"tmp_predicted = predicted[0:len(reported)]

reported_rate = reported['Fatalities']/tmp_predicted['death']
reported_rate_d = np.average(a= reported_rate, weights=reported['Fatalities'])
reported_rate_d"
3504,30487343,12,"predicted_s = predicted[0:len(submission_df)]
submission_df['ConfirmedCases'] = predicted_s['Infectious']*reported_rate_c
submission_df['Fatalities'] = predicted_s['death']*reported_rate_d
submission_df.head()"
3505,30487343,13,"x_1 = test_df['Date']
y_1 = submission_df['ConfirmedCases']
y_2 = submission_df['Fatalities']

fig = make_subplots(rows=1, cols=1)

fig.add_trace(
    go.Scatter(x=x_1, mode='lines+markers', y=y_1, marker=dict(color=""mediumaquamarine""), showlegend=False,
               name=""Original signal""),
    row=1, col=1
)

fig.add_trace(
    go.Scatter(x=x_1, mode='lines+markers', y=y_2, marker=dict(color=""darkgreen""), showlegend=False,
               name=""Original signal""),
    row=1, col=1
)

fig.update_layout(height=400, width=800, title_text=""Predicted ConfirmedCases (pale) vs. Fatalities (dark) "")
fig.show()"
3506,30487343,14,"submission_df.to_csv(""submission.csv"", index=False)"
3507,30834836,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the ""../input/"" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# Any results you write to the current directory are saved as output."
3508,30834836,1,"from scipy.integrate import odeint
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
from matplotlib.dates import DateFormatter, DayLocator, WeekdayLocator
import datetime as dt
from matplotlib.dates import MO, TU, WE, TH, FR, SA, SU
from matplotlib import gridspec
from matplotlib import dates
from matplotlib.ticker import (MultipleLocator, FormatStrFormatter,
                               AutoMinorLocator)

train_data = pd.read_csv(""/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_train.csv"")
test_data = pd.read_csv(""/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_test.csv"")"
3509,30834836,2,train_data.head()
3510,30834836,3,test_data.head()
3511,30834836,4,"train_data.shape, test_data.shape"
3512,30834836,5,"fig, (ax1, ax2) = plt.subplots(1,2, figsize = (18,8))
fig.suptitle('Number of Confirmed Cases and Fatalities in CA')
plt .xticks(np.arange(0, 60,  step = 10)) 
#Left plot
ax1.plot(train_data['Date'], train_data['ConfirmedCases'], color = 'purple', marker = 'o',linewidth = 1)
ax1.set(xlabel = 'Date',
        ylabel = 'Number of ConfirmedCases in CA')
ax1.set_xticks(np.arange(0, 60,  step = 12))
ax1.grid()
#Right plot

ax2.plot(train_data['Date'], train_data['Fatalities'], color = 'orange', marker = 'o', linewidth = 1)
ax2.set(xlabel = 'Date',
        ylabel = 'Number of Fatalities in CA')
ax2.set_xticks(np.arange(0, 60,  step = 12))
ax2.grid()

plt.rcParams[""font.family""] = ""Times New Roman""
plt.rcParams[""font.size""] = ""12""

plt.show()"
3513,30834836,6,"train_initial = train_data[48:]
train_initial = train_initial.reset_index()
y1_train= train_initial['ConfirmedCases']
y2_train = train_initial['Fatalities']"
3514,30834836,7,"#SIR Model 
def SIR_DEQ(y, time, beta, k, N):
    DS = -beta * y[0] * y[1]/N
    DI = (beta * y[0] * y[1] - k * y[1])/N
    DR = k * y[1]/N
    return [DS, DI, DR]

#initial conditions for training data
N = 39560000  #Population of California 
I0 = 144
S0 = N  # initial population of susceptible individual
R0 = 2 # initial number of fatalities 
init_state = [S0, I0, R0]

# Parameters
t0 = 0 
tmax = 15
dt = 1
# Rate of infection
beta = 0.2
# Rate of recovery
k = 1/10

time = np.arange(t0, tmax, dt)
args = (beta, k, N)

solution = odeint(SIR_DEQ, init_state, time, args)

plt.plot(time, solution[:, 1], 'g', marker = 'x', label  = 'Infected SIR model')
plt.plot(time, y1_train, 'r', marker = 'o', label  = 'Infected Data')
plt.legend(['Infected SIR Model', 'Infected Input Data'])"
3515,30834836,8,"#initial conditions for ouput
N = 39560000  #Population of California 
I0 = 221
S0 = N  # initial population of susceptible individual
R0 = 4 # initial number of fatalities 
init_state_out = [S0, I0, R0]

time = np.arange(t0, 43, dt)
args = (beta, k, N)
solution_out = odeint(SIR_DEQ, init_state_out, time, args)
"
3516,30834836,9,"submission_file = pd.read_csv(""/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_submission.csv"")
"
3517,30834836,10,"submission_file['ConfirmedCases']= solution_out[:,1]
submission_file['Fatalities'] = solution_out[:,2]"
3518,30834836,11,"submission_file.to_csv(""submission.csv"", index=False)"
3519,30835008,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the ""../input/"" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# Any results you write to the current directory are saved as output."
3520,30835008,1,"%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
sns.set()

import warnings
warnings.filterwarnings('ignore')"
3521,30835008,2,"ca_train = pd.read_csv(""/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_train.csv"",parse_dates=[5],index_col=0)
ca_test = pd.read_csv(""/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_test.csv"",parse_dates=[5],index_col=0)
ca_submission = pd.read_csv(""/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_test.csv"",parse_dates=[5])"
3522,30835008,3,"ca_train.info()
#ca_test.info()"
3523,30835008,4,"#any null values?
ca_train.isnull().sum()"
3524,30835008,5,"#remove unnecessary columns
ca_train.nunique()"
3525,30835008,6,"for i in range(ca_train.shape[1]):
    if (ca_train.iloc[:,i].nunique() == 1):
        print(ca_train.columns[i],'\t',ca_train.iloc[:,i].value_counts())    "
3526,30835008,7,"#necessary data
ca_filtered_train = ca_train.copy()
ca_filtered_train.tail(10)"
3527,30835008,8,"ca_filtered_train['weekday'] = ca_filtered_train.Date.dt.day_name()
ca_filtered_train['Total_affected_people'] = ca_filtered_train['ConfirmedCases']+ca_filtered_train['Fatalities']
ca_filtered_train.tail()"
3528,30835008,9,"print('Total CA population is 39.56M as of year 2018')
print('confirmed cases population percentage',round((sum(ca_train['ConfirmedCases'])/39.56e6)*100,2),'%')
print('fatalities population percentage',round((sum(ca_train['Fatalities'])/39.56e6)*100,4),'%')"
3529,30835008,10,"affected_people = pd.DataFrame(ca_filtered_train[47:].groupby('ConfirmedCases')['Fatalities'].sum())
affected_people['Cumulative_deaths_percentage'] = round(affected_people['Fatalities']/sum(affected_people['Fatalities'])*100,2)
sns.scatterplot(x=affected_people.index,y=affected_people.Cumulative_deaths_percentage)"
3530,30835008,11,"cats = [ 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
ca_filtered_train.groupby('weekday')['ConfirmedCases','Fatalities'].sum().reindex(cats).plot(kind='bar')
plt.title('daywise analysis')"
3531,30835008,12,"plt.figure(figsize=(20,12))
ax = sns.lineplot(x='Date',y='ConfirmedCases',data=ca_filtered_train[40:],label='Confirmed_cases')
ax = sns.lineplot(x='Date',y='Fatalities',data=ca_filtered_train[40:],label='Deaths')
ax = sns.lineplot(x='Date',y='Total_affected_people',data=ca_filtered_train[47:],label='Total_affected_people')
plt.xticks(ca_filtered_train['Date'][40:],rotation='vertical')
ax.annotate('Lockdown', xy=('2020-03-20',1177), xytext=('2020-03-17', 1300),arrowprops=dict(facecolor='black',shrink=0.05),fontsize=15)
ax.annotate('Declared State of Emergency', xy=('2020-03-04',0), xytext=('2020-03-04', 500),arrowprops=dict(facecolor='black',shrink=0.05),fontsize=15)
ax.annotate('Increased health care capacity', xy=('2020-03-21',1364), xytext=('2020-03-14', 1600),arrowprops=dict(facecolor='black',shrink=0.05),fontsize=15)
ax.legend(loc='upper left',fontsize='x-large',fancybox=True,shadow=True,borderpad=1)
plt.ylabel('Confirmed_cases and Fatalities')
plt.xticks(rotation='vertical')
plt.title('Trend over a MARCH month',fontsize=20)"
3532,30835008,13,"ca_train.index = ca_train['Date']
ca_train.drop('Date',axis=1,inplace=True)"
3533,30835008,14,"ca_train = ca_train[['ConfirmedCases','Fatalities']]
train_confirmedcases = ca_train[['ConfirmedCases']]
train_confirmedcases= train_confirmedcases.iloc[47:]"
3534,30835008,15,"train_fatalities = ca_train[['Fatalities']]
train_fatalities= train_fatalities.iloc[47:]"
3535,30835008,16,"#ts_diff_1 = train_confirmedcases - train_confirmedcases.shift()
#ts_diff_1.dropna(inplace=True)"
3536,30835008,17,"from statsmodels.tsa.arima_model import ARIMA
from pandas import datetime

#fit model on confirmedcases
model = ARIMA(train_confirmedcases, order=(1,1,1)) # (ARIMA) = (1,1,0)
model_fit = model.fit(disp=0)"
3537,30835008,18,"# predict
start_index = datetime(2020, 3, 12)
end_index = datetime(2020, 4, 23)
forecast_confirmedcases = model_fit.predict(start=start_index, end=end_index)"
3538,30835008,19,"#fit model on fatalities
model_F = ARIMA(train_fatalities, order=(1,1,0)) # (ARIMA) = (1,1,0)
model_fit_F = model_F.fit(disp=0)"
3539,30835008,20,"# predict
start_index = datetime(2020, 3, 12)
end_index = datetime(2020, 4, 23)
forecast_fatalities = model_fit_F.predict(start=start_index, end=end_index)"
3540,30835008,21,"df=pd.concat([forecast_confirmedcases.astype(int),forecast_fatalities.astype(int)],axis=1)"
3541,30835008,22,ca_submission.head()
3542,30835008,23,"ca_submission['ConfirmedCases'] = list(df[0])
ca_submission['Fatalities'] = list(df[1])
ca_submission.head()"
3543,30835008,24,"ca_submission = ca_submission[['ForecastId','ConfirmedCases','Fatalities']]
ca_submission.head()"
3544,30835008,25,"# visualization
plt.figure(figsize=(22,10))
plt.plot(train_confirmedcases.index,train_confirmedcases.ConfirmedCases,label = ""original"")
plt.plot(forecast_confirmedcases,label = ""predicted"")
plt.legend(loc='upper left',fontsize='x-large',fancybox=True,shadow=True,borderpad=1)
plt.title('For ConfirmedCases')
plt.show()"
3545,30835008,26,"# visualization
plt.figure(figsize=(22,10))
plt.plot(train_fatalities.index,train_fatalities.Fatalities,label = ""original"")
plt.plot(forecast_fatalities,label = ""predicted"")
plt.legend(loc='upper left',fontsize='x-large',fancybox=True,shadow=True,borderpad=1)
plt.title('For Fatalities')
plt.show()"
3546,30835008,27,"ca_submission.to_csv('submission.csv',index=False)
ca_submission.head()"
3547,30700486,0,"import pandas as pd
import numpy as np"
3548,30700486,1,"df_train = pd.read_csv(""../input/covid19-local-us-ca-forecasting-week-1/ca_train.csv"")
df_train"
3549,30700486,2,"diff_cases = [df_train['ConfirmedCases'][i+1] - df_train['ConfirmedCases'][i] for i in range(len(df_train)-1)]
diff_fatalities = [df_train['Fatalities'][i+1] - df_train['Fatalities'][i] for i in range(len(df_train)-1)]"
3550,30700486,3,diff_cases
3551,30700486,4,diff_fatalities
3552,30700486,5,"diff_fatal_case = [f/c if c != 0 else 0 for f, c in zip(diff_fatalities, diff_cases)]"
3553,30700486,6,diff_fatal_case
3554,30700486,7,np.mean([x for x in diff_fatal_case if x != 0])
3555,30700486,8,increase = [df_train['ConfirmedCases'][i+1]/diff_cases[i] if diff_cases[i] != 0 else 0 for i in range(len(diff_cases))]
3556,30700486,9,increase
3557,30700486,10,"increase_rate = np.mean([x for x in increase if x != 0])
increase_rate"
3558,30700486,11,"fatality_rate = np.mean([x for x in diff_fatal_case if x != 0])
fatality_rate"
3559,30700486,12,"def n_step_pred(cases, n):
    new_cases = []
    new_fatalities = []
    for i in range(n):
        if i == 0:
            new_cases.append(cases + int(cases/increase_rate))
        else:
            new_cases.append(new_cases[i-1] + int(new_cases[i-1]/increase_rate))
        new_fatalities.append(int(new_cases[i] * fatality_rate))
    
    pred_df = pd.DataFrame(list(zip(forecast_id ,new_cases, new_fatalities)), columns=[""ForecastId"", ""ConfirmedCases"", ""Fatalities""])
    pred_df = pred_df.set_index(""ForecastId"")
    return pred_df"
3560,30700486,13,"test_df = pd.read_csv(""../input/covid19-local-us-ca-forecasting-week-1/ca_test.csv"")
forecast_id = list(test_df[""ForecastId""])
test_df.head()"
3561,30700486,14,"predicted = n_step_pred(df_train['ConfirmedCases'].iloc[-1], len(test_df))"
3562,30700486,15,"predicted.to_csv(""submission.csv"")"
3563,30700486,16,
3564,31316040,0,"import numpy as np
import pandas as pd
import os

from PIL import Image

import matplotlib.pyplot as plt
%matplotlib inline

import torch
import torchvision
import torchvision.transforms as transforms

import torch.nn as nn
import torch.nn.functional as F

import torch.optim as optim

# for creating validation set
from sklearn.model_selection import train_test_split

# for evaluating the model
from sklearn.metrics import accuracy_score
from tqdm import tqdm

# PyTorch libraries and modules
import torch
from torch.autograd import Variable
from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout
from torch.optim import Adam, SGD"
3565,31316040,1,!pip install -U -q kaggle --force
3566,31316040,2,"from google.colab import files
f=files.upload()"
3567,31316040,3,!mkdir -p ~/.kaggle
3568,31316040,4,!cp kaggle.json ~/.kaggle/
3569,31316040,5,!chmod 600 /root/.kaggle/kaggle.json
3570,31316040,6,!kaggle competitions download -c nnfl-cnn-lab2
3571,31316040,7,"%%bash
cd /content
unzip nnfl-cnn-lab2.zip"
3572,31316040,8,"import numpy as np
import pandas as pd 
from keras.preprocessing.image import ImageDataGenerator, load_img
from keras.utils import to_categorical
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import random
import os
print(os.listdir(""../content""))
"
3573,31316040,9,"FAST_RUN = False
IMAGE_WIDTH=150
IMAGE_HEIGHT=150
IMAGE_SIZE=(IMAGE_WIDTH, IMAGE_HEIGHT)
IMAGE_CHANNELS=3"
3574,31316040,10,cd upload
3575,31316040,11,ls
3576,31316040,12,"df = pd.read_csv(""train_set.csv"") "
3577,31316040,13,df.head()
3578,31316040,14,df.tail()
3579,31316040,15,df['label'].value_counts().plot.bar()
3580,31316040,16,df['label']=df['label'].astype(str)
3581,31316040,17,"transform = transforms.Compose([transforms.Resize(150),
                                transforms.ToTensor()                               
                                ])

class TrainingDataset(torch.utils.data.Dataset):

    def __init__(self, csv_file, root_dir, transform=transform):
        """"""
        Args:
            csv_file(string): path to csv file
            root_dir(string): directory with all train images
        """"""
        self.name_frame = pd.read_csv(csv_file, usecols=range(1))
        self.label_frame = pd.read_csv(csv_file, usecols=range(1,2))
        self.root_dir = root_dir
        self.transform = transform

    def __len__(self):
        return len(self.name_frame)

    def __getitem__(self, idx):
        img_name = os.path.join(self.root_dir, self.name_frame.iloc[idx, 0])
        image = Image.open(img_name)
        image = self.transform(image)
        labels = self.label_frame.iloc[idx, 0]
        #sample = {'image': image, 'labels': labels}

        return image, labels

TrainSet = TrainingDataset(csv_file = './train_set.csv', root_dir = './train_images/train_images')

TrainLoader = torch.utils.data.DataLoader(TrainSet,batch_size=1, shuffle=True, num_workers=2)"
3582,31316040,18,"def imshow(img):
    npimg = img.numpy()
    plt.imshow(np.transpose(npimg, (1, 2, 0))) # transform to size x size x #channels
    plt.show()

# get some random training images
dataiter = iter(TrainLoader)
image, label = dataiter.next()

print(image.shape)

# show images
imshow(torchvision.utils.make_grid(image))
# print labels
print(label.item())"
3583,31316040,19,"import pandas as pd
import numpy as np 
import matplotlib.pyplot as plt
import os
from urllib.request import urlopen,urlretrieve
from PIL import Image
from tqdm import tqdm_notebook
%matplotlib inline
from sklearn.utils import shuffle
import cv2


from tensorflow.keras.models import load_model
from sklearn.datasets import load_files   
from keras.utils import np_utils
from glob import glob
from tensorflow.keras import applications
from tensorflow.keras.preprocessing.image import ImageDataGenerator 
from tensorflow.keras import optimizers
from tensorflow.keras.models import Sequential,Model,load_model
from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D,GlobalMaxPooling2D
from tensorflow.keras.callbacks import TensorBoard,ReduceLROnPlateau,ModelCheckpoint

img_height,img_width = 150,150
num_classes = 6
base_model = applications.resnet_v2.ResNet50V2(weights= None, include_top=False, input_shape= (img_height,img_width,3))

x = base_model.output
x = GlobalMaxPooling2D()(x)

#x = Dense(32, activation='relu')(x)
#x = Dropout(0.7)(x)

predictions = Dense(num_classes, activation= 'softmax')(x)
model = Model(inputs = base_model.input, outputs = predictions)
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

model.summary()"
3584,31316040,20,"from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau"
3585,31316040,21,earlystop = EarlyStopping(patience=10)
3586,31316040,22,"learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', 
                                            patience=2, 
                                            verbose=1, 
                                            factor=0.5, 
                                            min_lr=0.00001)"
3587,31316040,23,"from tensorflow.keras.callbacks import ModelCheckpoint

callbacks = [earlystop, learning_rate_reduction,ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]"
3588,31316040,24,df['label'].head()
3589,31316040,25,"train_df, validate_df = train_test_split(df, test_size=0.20, random_state=42)
train_df = train_df.reset_index(drop=True)
validate_df = validate_df.reset_index(drop=True)"
3590,31316040,26,train_df['label'].value_counts().plot.bar()
3591,31316040,27,validate_df['label'].value_counts().plot.bar()
3592,31316040,28,"total_train = train_df.shape[0]
total_validate = validate_df.shape[0]
batch_size=15"
3593,31316040,29,"print(total_train)
print(total_validate)"
3594,31316040,30,"train_datagen = ImageDataGenerator(
    rotation_range=-10,
    rescale=1./255,
    shear_range=0.1,
    zoom_range=0.1,
    horizontal_flip=True,
    width_shift_range=0.1,
    height_shift_range=0.1
)

train_generator = train_datagen.flow_from_dataframe(
    train_df, 
    ""./train_images/train_images"", 
    x_col='image_name',
    y_col='label',
    target_size=IMAGE_SIZE,
    class_mode='categorical',
    batch_size=batch_size,
)"
3595,31316040,31,train_df['label']=train_df['label'].astype(str)
3596,31316040,32,validate_df['label']=validate_df['label'].astype(str)
3597,31316040,33,"validation_datagen = ImageDataGenerator(rescale=1./255)
validation_generator = validation_datagen.flow_from_dataframe(
    validate_df, 
    ""./train_images/train_images"", 
    x_col='image_name',
    y_col='label',
    target_size=IMAGE_SIZE,
    class_mode='categorical',
    batch_size=batch_size
)"
3598,31316040,34,train_df['label']=train_df['label'].astype(str)
3599,31316040,35,"example_df = train_df.sample(n=1).reset_index(drop=True)
example_generator = train_datagen.flow_from_dataframe(
    example_df, 
    ""./train_images/train_images"", 
    x_col='image_name',
    y_col='label',
    target_size=IMAGE_SIZE,
    class_mode='categorical'
)"
3600,31316040,36,"plt.figure(figsize=(12, 12))
for i in range(0, 15):
    plt.subplot(5, 3, i+1)
    for X_batch, Y_batch in example_generator:
        image = X_batch[0]
        plt.imshow(image)
        break
plt.tight_layout()
plt.show()"
3601,31316040,37,"epochs=3 if FAST_RUN else 40
history = model.fit(
    train_generator, 
    epochs=epochs,
    validation_data=validation_generator,
    validation_steps=total_validate//batch_size,
    steps_per_epoch=total_train//batch_size,
    callbacks=callbacks
)"
3602,31316040,38,"model.save_weights(""model.h5"")"
3603,31316040,39,"fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12))
ax1.plot(history.history['loss'], color='b', label=""Training loss"")
ax1.plot(history.history['val_loss'], color='r', label=""validation loss"")
ax1.set_xticks(np.arange(1, epochs, 1))
ax1.set_yticks(np.arange(0, 1, 0.1))

ax2.plot(history.history['accuracy'], color='b', label=""Training accuracy"")
ax2.plot(history.history['val_accuracy'], color='r',label=""Validation accuracy"")
ax2.set_xticks(np.arange(1, epochs, 1))

legend = plt.legend(loc='best', shadow=True)
plt.tight_layout()
plt.show()"
3604,31316040,40,"test_filenames = os.listdir(""./test_images/test_images"")
test_df = pd.DataFrame({
    'filename': test_filenames
})
nb_samples = test_df.shape[0]"
3605,31316040,41,"test_gen = ImageDataGenerator(rescale=1./255)
test_generator = test_gen.flow_from_dataframe(
    test_df, 
    ""./test_images/test_images"", 
    x_col='filename',
    y_col=None,
    class_mode=None,
    target_size=IMAGE_SIZE,
    batch_size=batch_size,
    shuffle=False
)"
3606,31316040,42,"predict = model.predict_generator(test_generator, steps=np.ceil(nb_samples/batch_size))"
3607,31316040,43,"test_df['category'] = np.argmax(predict, axis=-1)"
3608,31316040,44,"label_map = dict((v,k) for k,v in train_generator.class_indices.items())
test_df['category'] = test_df['category'].replace(label_map)"
3609,31316040,45,test_df['category'].value_counts().plot.bar()
3610,31316040,46,"sample_test = test_df.head(18)
sample_test.head()
plt.figure(figsize=(12, 24))
for index, row in sample_test.iterrows():
    filename = row['filename']
    category = row['category']
    img = load_img(""./test_images/test_images/""+filename, target_size=IMAGE_SIZE)
    plt.subplot(6, 3, index+1)
    plt.imshow(img)
    plt.xlabel(filename + '(' + ""{}"".format(category) + ')' )
plt.tight_layout()
plt.show()"
3611,31316040,47,"submission_df = test_df.copy()
submission_df['image_name'] = submission_df['filename']
submission_df['label'] = submission_df['category']
submission_df.drop(['filename', 'category'], axis=1, inplace=True)
submission_df.to_csv('submission.csv', index=False)"
3612,31316040,48,"from google.colab import files
files.download(""submission.csv"")"
3613,31316040,49,"files.download(""model.h5"")"
3614,31316040,50,"from google.colab import drive
drive.mount('/content/gdrive')"
3615,31316040,51,"mv best_model.h5 /content/gdrive/'My Drive'
"
3616,31316040,52,mv model.h5 /content/gdrive/'My Drive'
3617,31316040,53,
4615,32592428,0,"import os
import numpy as np 
import pandas as pd 
import json"
4616,32592428,1,os.listdir('../input/imet-2020-fgvc7')
4617,32592428,2,"submission = pd.read_csv('../input/imet-2020-fgvc7/sample_submission.csv')
submission.head()"
4618,32592428,3,"# ====================================================
# Library
# ====================================================

import sys

import gc
import os
import random
import time
from contextlib import contextmanager
from pathlib import Path
from collections import defaultdict, Counter

import cv2
from PIL import Image
import numpy as np
import pandas as pd
import scipy as sp

import sklearn.metrics
from sklearn.metrics import accuracy_score
from sklearn.model_selection import StratifiedKFold

from functools import partial
from tqdm import tqdm

import torch
import torch.nn as nn
from torch.optim import Adam, SGD
from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau
from torch.utils.data import DataLoader, Dataset
import torchvision.models as models

from albumentations import Compose, Normalize, Resize, RandomResizedCrop, RandomCrop, HorizontalFlip
from albumentations.pytorch import ToTensorV2

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device"
4619,32592428,4,"# ====================================================
# Utils
# ====================================================

@contextmanager
def timer(name):
    t0 = time.time()
    LOGGER.info(f'[{name}] start')
    yield
    LOGGER.info(f'[{name}] done in {time.time() - t0:.0f} s.')

    
def init_logger(log_file='train.log'):
    from logging import getLogger, DEBUG, FileHandler,  Formatter,  StreamHandler
    
    log_format = '%(asctime)s %(levelname)s %(message)s'
    
    stream_handler = StreamHandler()
    stream_handler.setLevel(DEBUG)
    stream_handler.setFormatter(Formatter(log_format))
    
    file_handler = FileHandler(log_file)
    file_handler.setFormatter(Formatter(log_format))
    
    logger = getLogger('Herbarium')
    logger.setLevel(DEBUG)
    logger.addHandler(stream_handler)
    logger.addHandler(file_handler)
    
    return logger

LOG_FILE = 'train.log'
LOGGER = init_logger(LOG_FILE)


def seed_torch(seed=777):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True

SEED = 777
seed_torch(SEED)"
4620,32592428,5,"N_CLASSES = 3474


class TrainDataset(Dataset):
    def __init__(self, df, labels, transform=None):
        self.df = df
        self.labels = labels
        self.transform = transform
        
    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        file_name = self.df['id'].values[idx]
        file_path = f'../input/imet-2020-fgvc7/train/{file_name}.png'
        image = cv2.imread(file_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        if self.transform:
            augmented = self.transform(image=image)
            image = augmented['image']
            
        label = self.labels.values[idx]
        target = torch.zeros(N_CLASSES)
        for cls in label.split():
            target[int(cls)] = 1
        
        return image, target
    

class TestDataset(Dataset):
    def __init__(self, df, transform=None):
        self.df = df
        self.transform = transform
        
    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        file_name = self.df['id'].values[idx]
        file_path = f'../input/imet-2020-fgvc7/test/{file_name}.png'
        image = cv2.imread(file_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        if self.transform:
            augmented = self.transform(image=image)
            image = augmented['image']
        
        return image"
4621,32592428,6,"HEIGHT = 128
WIDTH = 128


def get_transforms(*, data):
    
    assert data in ('train', 'valid')
    
    if data == 'train':
        return Compose([
            #Resize(HEIGHT, WIDTH),
            RandomResizedCrop(HEIGHT, WIDTH),
            Normalize(
                mean=[0.485, 0.456, 0.406],
                std=[0.229, 0.224, 0.225],
            ),
            ToTensorV2(),
        ])
    
    elif data == 'valid':
        return Compose([
            #Resize(HEIGHT, WIDTH),
            RandomCrop(256, 256),
            HorizontalFlip(p=0.5),
            Normalize(
                mean=[0.485, 0.456, 0.406],
                std=[0.229, 0.224, 0.225],
            ),
            ToTensorV2(),
            
        ])"
4622,32592428,7,"batch_size = 128

test_dataset = TestDataset(submission, transform=get_transforms(data='valid'))
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
4623,32592428,8,"from functools import partial

import torch
from torch import nn
from torch.nn import functional as F
import torchvision.models as M

class AvgPool(nn.Module):
    def forward(self, x):
        return F.avg_pool2d(x, x.shape[2:])

def create_net(net_cls, pretrained: bool):
    if True and pretrained:
        net = net_cls()
        model_name = net_cls.__name__
        weights_path = f'../input/{model_name}/{model_name}.pth'
        net.load_state_dict(torch.load(weights_path))
    else:
        net = net_cls(pretrained=pretrained)
    return net


class ResNet(nn.Module):
    def __init__(self, num_classes,
                 pretrained=False, net_cls=M.resnet50, dropout=False):
        super().__init__()
        self.net = create_net(net_cls, pretrained=pretrained)
        self.net.avgpool = AvgPool()
        if dropout:
            self.net.fc = nn.Sequential(
                nn.Dropout(),
                nn.Linear(self.net.fc.in_features, num_classes),
            )
        else:
            self.net.fc = nn.Linear(self.net.fc.in_features, num_classes)

    def fresh_params(self):
        return self.net.fc.parameters()

    def forward(self, x):
        return self.net(x)


class DenseNet(nn.Module):
    def __init__(self, num_classes,
                 pretrained=False, net_cls=M.densenet121):
        super().__init__()
        self.net = create_net(net_cls, pretrained=pretrained)
        self.avg_pool = AvgPool()
        self.net.classifier = nn.Linear(
            self.net.classifier.in_features, num_classes)

    def fresh_params(self):
        return self.net.classifier.parameters()

    def forward(self, x):
        out = self.net.features(x)
        out = F.relu(out, inplace=True)
        out = self.avg_pool(out).view(out.size(0), -1)
        out = self.net.classifier(out)
        return out


resnet18 = partial(ResNet, net_cls=M.resnet18)
resnet34 = partial(ResNet, net_cls=M.resnet34)
resnet50 = partial(ResNet, net_cls=M.resnet50)
resnet101 = partial(ResNet, net_cls=M.resnet101)
resnet152 = partial(ResNet, net_cls=M.resnet152)

densenet121 = partial(DenseNet, net_cls=M.densenet121)
densenet169 = partial(DenseNet, net_cls=M.densenet169)
densenet201 = partial(DenseNet, net_cls=M.densenet201)
densenet161 = partial(DenseNet, net_cls=M.densenet161)"
4624,32592428,9,"criterion = nn.BCEWithLogitsLoss(reduction='none')
model = resnet50(num_classes=N_CLASSES, pretrained=True)"
4625,32592428,10,from typing import Dict
4626,32592428,11,"def load_model(model: nn.Module, path: Path) -> Dict:
    state = torch.load(str(path))
    model.load_state_dict(state['model'])
    print('Loaded model from epoch {epoch}, step {step:,}'.format(**state))
    return state"
4627,32592428,12,"load_model(model, '../input/imet2020/best-model.pt')"
4628,32592428,13,"with timer('inference'):
    
    model.to(device) 
    
    preds = []
    tk0 = tqdm(enumerate(test_loader), total=len(test_loader))

    for i, images in tk0:
            
        images = images.to(device)
            
        with torch.no_grad():
            y_preds = model(images)
            
        preds.append(torch.sigmoid(y_preds).to('cpu').numpy())"
4629,32592428,14,"threshold = 0.10
predictions = np.concatenate(preds) > threshold

for i, row in enumerate(predictions):
    ids = np.nonzero(row)[0]
    submission.iloc[i].attribute_ids = ' '.join([str(x) for x in ids])
    
submission.to_csv('submission.csv', index=False)
submission.head()"
4630,32592428,15,
4872,34360968,0,"from datetime import date
from lightgbm import LGBMRegressor
from sklearn.ensemble import VotingRegressor
from sklearn.metrics import mean_squared_error, mean_squared_log_error
from sklearn.model_selection import train_test_split

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import plotly.express as px
import seaborn as sns
sns.set()

seed = 42"
4873,34360968,1,"PATH_TO_DATA = ""/kaggle/input/ieor242hw4""
train = pd.read_csv(PATH_TO_DATA + ""/train.csv"", parse_dates=[""pickup_datetime""])
test = pd.read_csv(PATH_TO_DATA + ""/test.csv"", parse_dates=[""pickup_datetime""])
sub = pd.read_csv(PATH_TO_DATA + ""/submission.csv"")

print(""Train sample:"")
display(train.sample(5))
print(""Test sample:"")
display(test.sample(5))"
4874,34360968,2,"PATH_TO_LOOKUP = ""/kaggle/input/nyc-yellow-taxi-zone-lookup-table""
lookup = pd.read_csv(PATH_TO_LOOKUP + ""/taxi_zone_lookup.csv"")

loc_to_borough = dict(zip(lookup[""LocationID""], lookup[""Borough""].apply(lambda x: str(x).lower())))
loc_to_zone = dict(zip(lookup[""LocationID""], lookup[""Zone""].apply(lambda x: str(x).lower())))

zone_to_loc = {value: key for key, value in loc_to_zone.items()}
zone_to_loc[""unknown""] = 265

borough_to_label = {'Bronx': 1, 'Brooklyn': 2, 'EWR': 3, 'Manhattan': 4, 'Queens': 5, 'Staten Island': 6, 'Unknown': 7}
borough_to_label = {key.lower(): value for key, value in borough_to_label.items()}"
4875,34360968,3,train.isnull().mean()
4876,34360968,4,"train[
    train[""VendorID""].isnull() |
    train[""passenger_count""].isnull()
][[""VendorID"", ""passenger_count""]].isnull().mean()"
4877,34360968,5,test.isnull().mean()
4878,34360968,6,"out_val = 999

def preprocess_data(data):
    # VendorID and passenger count
    data[""VendorID""] = data[""VendorID""].replace({np.nan: 0}).apply(int)
    data[""passenger_count""] = data[""passenger_count""].replace({np.nan: 0}).apply(int)

    # Pickup and dropoff boroughs and zones
    data[""pickup_borough""] = data[""pickup_borough""].apply(lambda x: borough_to_label[x.lower()])
    data[""dropoff_borough""] = data[""dropoff_borough""].apply(lambda x: borough_to_label[x.lower()])
    data[""pickup_zone""] = data[""pickup_zone""].replace({np.nan: ""Unknown""}).apply(lambda x: zone_to_loc[x.lower()])
    data[""dropoff_zone""] = data[""dropoff_zone""].replace({np.nan: ""Unknown""}).apply(lambda x: zone_to_loc[x.lower()])

    # Pickup datetime
    data[""pickup_datetime""] = data[""pickup_datetime""].replace({np.nan: out_val})
    data[""pickup_year""] = data[""pickup_datetime""].apply(lambda x: int(x.year) if x != out_val else out_val)
    data[""pickup_month""] = data[""pickup_datetime""].apply(lambda x: int(x.month) if x != out_val else out_val)
    data[""pickup_day""] = data[""pickup_datetime""].apply(lambda x: int(x.day) if x != out_val else out_val)
    data[""pickup_hour""] = data[""pickup_datetime""].apply(lambda x: x.hour if x != out_val else out_val)
    data[""pickup_minute""] = data[""pickup_datetime""].apply(lambda x: x.minute if x != out_val else out_val)
   
    # Further extraction: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Timestamp.html
    data[""pickup_dayofweek""] = data[""pickup_datetime""].apply(lambda x: int(x.dayofweek) if x != out_val else out_val)
    data[""pickup_dayofyear""] = data[""pickup_datetime""].apply(lambda x: int(x.dayofyear) if x != out_val else out_val)
    # data[""pickup_weekofyear""] = data[""pickup_datetime""].apply(lambda x: int(x.weekofyear) if x != out_val else out_val)
    
    # Drop useless columns
    data = data.drop(columns=[""row_id""])
    
    return data.reset_index(drop=True)


train = preprocess_data(train)
test = preprocess_data(test)"
4879,34360968,7,"categorical_columns = [
    ""VendorID"", ""passenger_count"", ""pickup_year"", ""pickup_month"", ""pickup_day"", ""pickup_hour"", ""pickup_minute"",
    ""pickup_dayofweek"", ""pickup_borough"", ""dropoff_borough""
]
numerical_columns = [""trip_distance"", ""pickup_dayofyear""]


def display_data(data):
    nb_rows, nb_cols = max((len(categorical_columns + numerical_columns) - 3) // 4 + 1, 2), 4
    fig, ax = plt.subplots(figsize=(14.5, 4 * nb_rows), nrows=nb_rows, ncols=nb_cols)
    for ind, column in enumerate(categorical_columns + numerical_columns):
        if column in categorical_columns:
            sns.countplot(data[data[column] != out_val][column], ax=ax[ind // nb_cols, ind % nb_cols])
        elif column in numerical_columns:
            sns.distplot(data[data[column] != out_val][column], ax=ax[ind // nb_cols, ind % nb_cols])
            ax[ind // nb_cols, ind % nb_cols].set_ylim((0, 0.05))

    plt.show()


print(""Train EDA:"")
display_data(train)
print(""Test EDA:"")
display_data(test)"
4880,34360968,8,"fig, ax = plt.subplots(figsize=(15, 4 * 2), nrows=2, ncols=2)

ax[0, 0].scatter(train[""trip_distance""], train[""duration""], s=1)
ax[0, 0].set_title(""Trip duration vs. distance"")
max_duration = 3600 * 3
# Some outlier removal performed here, be careful
train = train[(train[""duration""] <= max_duration) & (train[""trip_distance""] <= test[""trip_distance""].max())]
ax[0, 1].scatter(train[""trip_distance""], train[""duration""], s=1)
ax[0, 1].set_title(""Trip duration vs. distance (duration < 3 hrs)"")

negative_dist = train[(train[""trip_distance""] < 0)]
ax[1, 0].scatter(negative_dist[""trip_distance""], negative_dist[""duration""], s=1)
ax[1, 0].set_title(""Trip duration vs. distance (distance < 0)"")

positive_dist = train[(train[""trip_distance""] >= 0)]
ax[1, 1].scatter(positive_dist[""trip_distance""], positive_dist[""duration""], s=1)
ax[1, 1].scatter(negative_dist[""trip_distance""].apply(abs), negative_dist[""duration""], s=1)
ax[1, 1].set_xlim(0, 40)
ax[1, 1].set_ylim(0, max_duration)
ax[1, 1].set_title(""Trip duration vs. distance (absolute distance)"")

plt.show()"
4881,34360968,9,"fig = px.scatter(
    x=train[""trip_distance""], y=train[""duration""], range_x=[0, 40], range_y=[0, 3600 * 2]
)
fig.update_traces(marker=dict(size=3))
fig.show()"
4882,34360968,10,"fig, ax = plt.subplots(figsize=(15, 4), nrows=1, ncols=2)
ax[0].scatter(train[""trip_distance""], train[""duration""], s=1)
slow_distance, slow_duration = 2.61, 29828
fast_distance, fast_duration = 35.7, 2124
ax[0].plot(
    [0.5, 0.5, fast_distance, 125],
    [0, 0.5 * fast_duration / fast_distance, fast_duration, 125 * fast_duration / fast_distance],
    color=""green""
)
ax[0].plot(
    [0, slow_distance * 3000 / slow_duration, slow_distance * 2, slow_distance * 86400 / slow_duration],
    [3000, 3000, slow_duration * 2, 86400],
    color=""green""
)
ax[0].set_xlim(0, 40)
ax[0].set_ylim(0, 3600 * 2)
ax[0].set_title(""Trip duration vs. distance (with outliers)"")

outliers = (
    (train[""trip_distance""] == 0) |
    (train[""duration""] == 0) |
    (train[""trip_distance""] / train[""duration""] >= fast_distance / fast_duration) & (train[""trip_distance""] >= 0.5) |
    (train[""trip_distance""] / train[""duration""] <= slow_distance / slow_duration) & (train[""duration""] >= 3000)
)
ax[1].scatter(train[~outliers][""trip_distance""], train[~outliers][""duration""], s=1)
ax[1].set_xlim(0, 40)
ax[1].set_ylim(0, 3600 * 2)
ax[1].set_title(""Trip duration vs. distance (without outliers)"")

plt.show()"
4883,34360968,11,"def correct_outliers(data):
    data[""trip_distance""] = data[""trip_distance""].apply(abs)
    
    return data.reset_index(drop=True)


train = correct_outliers(train)
test = correct_outliers(test)"
4884,34360968,12,"def remove_outliers(data):
    data = data[(data[""duration""] <= max_duration)]  # Already removed before, be careful
    outliers = (
        (data[""trip_distance""] == 0) |
        (data[""duration""] == 0) |
        (data[""trip_distance""] / data[""duration""] >= fast_distance / fast_duration) & (data[""trip_distance""] >= 0.5) |
        (data[""trip_distance""] / data[""duration""] <= slow_distance / slow_duration) & (data[""duration""] >= 3000)
    )
    data = data[~outliers]

    return data.reset_index(drop=True)


train = remove_outliers(train)"
4885,34360968,13,"plt.figure(figsize=(15, 4))
train[""duration""].apply(np.log).hist(bins=160)
plt.xlim(4, 9)
plt.show()"
4886,34360968,14,"def feature_engineering(data):
    data[""pickup_quarterhour""] = data[""pickup_datetime""].apply(
        lambda x: (x - pd.Timestamp(int(x.year), 1, 1)).seconds // (60 * 15) if x != out_val else out_val
    )
    data = data.drop(columns=[
        ""pickup_datetime"", ""pickup_year"", ""pickup_month"", ""pickup_day"", ""pickup_hour"", ""pickup_minute""
    ])
    
    return data.reset_index(drop=True)


train = feature_engineering(train)
test = feature_engineering(test)"
4887,34360968,15,"def one_hot_encoding(data):
    for i in range(1, 8):
        data[""pickup_borough_{}"".format(i)] = data[""pickup_zone""].apply(
            lambda x: x if borough_to_label[loc_to_borough[x]] == i else 0
        )
        data[""dropoff_borough_{}"".format(i)] = data[""dropoff_zone""].apply(
            lambda x: x if borough_to_label[loc_to_borough[x]] == i else 0
        )

    data = data.drop(columns=[""pickup_zone"", ""dropoff_zone"", ""pickup_borough"", ""dropoff_borough""])
    
    return data


train = one_hot_encoding(train)
test = one_hot_encoding(test)

train.shape"
4888,34360968,16,(train >= 0).mean()
4889,34360968,17,"def downcast_data(data):
    data[""trip_distance""] = (100 * data[""trip_distance""]).astype(int)

    for column in data.columns:
        data[column] = pd.to_numeric(data[column], downcast='unsigned')
    
    return data.reset_index(drop=True)


train = downcast_data(train)
test = downcast_data(test)

train.dtypes"
4890,34360968,18,"plt.figure(figsize=(15, 15))
sns.heatmap(np.abs(np.round(train.corr(), 2)), square=True, annot=True, cmap=plt.cm.Blues)
plt.show()"
4891,34360968,19,"X, y = train.drop(columns=[""duration""]), train[""duration""]
X_test = test

validate = True

if validate:
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=seed)"
4892,34360968,20,"commargs = {""learning_rate"": 0.03, ""colsample_bytree"": 0.9, ""reg_lambda"": 0.2, ""random_state"": seed, ""n_jobs"": -1}

lgbm_63 = LGBMRegressor(n_estimators=12000, num_leaves=63, **commargs)
lgbm_127 = LGBMRegressor(n_estimators=6000, num_leaves=127, **commargs)
lgbm_255 = LGBMRegressor(n_estimators=4000, num_leaves=255, **commargs)
lgbm_511 = LGBMRegressor(n_estimators=2000, num_leaves=511, **commargs)
lgbm_1023 = LGBMRegressor(n_estimators=1000, num_leaves=1023, **commargs)

lgbm_estimators = [
    (""LGBM_63"", lgbm_63), (""LGBM_127"", lgbm_127), (""LGBM_255"", lgbm_255),
    (""LGBM_511"", lgbm_511), (""LGBM_1023"", lgbm_1023),
]

lgbm_voting = VotingRegressor(lgbm_estimators)"
4893,34360968,21,"def plot_classifiers_validation(regs, X_train, y_train, X_val, y_val):
    fitted_regs = []
    nb_rows, nb_cols = 2, 3
    fig, ax = plt.subplots(figsize=(15, 10), nrows=nb_rows, ncols=nb_cols)
    for ind, reg in enumerate(regs):
        reg.fit(X_train, np.log(y_train))
        fitted_regs.append(reg)
        y_pred = reg.predict(X_val)
        ax[ind // nb_cols, ind % nb_cols].scatter(y_val, np.exp(y_pred), s=1)
        max_plot_value = max(y_val.max(), np.exp(y_pred).max())
        ax[ind // nb_cols, ind % nb_cols].plot([0, max_plot_value], [0, max_plot_value], color=""orange"")
        ax[ind // nb_cols, ind % nb_cols].set_title(""IN-RMSE = {0:.2f}, IN-RMSLE = {1:.5f}"".format(
            np.sqrt(mean_squared_error(y_val[y_val <= 7000], np.exp(y_pred)[y_val <= 7000])),
            np.sqrt(mean_squared_log_error(y_val[y_val <= 7000], np.exp(y_pred)[y_val <= 7000]))
        ))
        ax[ind // nb_cols, ind % nb_cols].set_xlim(0, max_plot_value)
        ax[ind // nb_cols, ind % nb_cols].set_ylim(0, max_plot_value)
    plt.show()
    return fitted_regs


if validate:
    lgbm_regs = plot_classifiers_validation(
        [lgbm_63, lgbm_127, lgbm_255, lgbm_511, lgbm_1023, lgbm_voting], X_train, y_train, X_val, y_val
    )"
4894,34360968,22,"categorical_columns = [""VendorID"", ""passenger_count"", ""pickup_dayofweek"", ""pickup_borough"", ""dropoff_borough""]
numerical_columns = [""trip_distance"", ""pickup_dayofyear"", ""pickup_quarterhour""]


def plot_lgbm_feature_importance(clf, ax):
    ft_imp_dummies = dict(zip(X_train.columns, clf.feature_importances_))
    ft_imp = {
        column: sum([value for key, value in ft_imp_dummies.items() if column in key])
        if column in categorical_columns else ft_imp_dummies[column]
        for column in categorical_columns + numerical_columns
    }
    ft_imp = {key: value for key, value in sorted(ft_imp.items(), key=lambda item: item[1])}

    labels, values = list(ft_imp.keys()), list(ft_imp.values())
    ylocs = np.arange(len(values))
    ax.barh(ylocs, values, align='center', height=0.2)
    for x, y in zip(values, ylocs):
        ax.text(x + 1, y, x, va='center')
    ax.set_yticks(ylocs)
    ax.set_yticklabels(labels)
    ax.set_title(""Feature importance for LGBM"")


if validate:
    fig, ax = plt.subplots(figsize=(13, 8), nrows=3, ncols=2)
    for ind, reg in enumerate(lgbm_regs[:-1]):
        plot_lgbm_feature_importance(reg, ax=ax[ind // 2, ind % 2])
    plt.subplots_adjust(wspace=0.5, hspace=0.4)
    plt.show()"
4895,34360968,23,"fit_predict = True
predict_on_train = True

if fit_predict:
    plt.figure(figsize=(15, 4))
    lgbm_voting.fit(X, np.log(y))

    if predict_on_train:
        y_pred = np.exp(lgbm_voting.predict(X))
        print(""Train RMSE: "", np.sqrt(mean_squared_error(y, y_pred)))
        plt.hist(y, density=True, bins=[50 * i for i in range(160)])

    y_sub = np.exp(lgbm_voting.predict(X_test))
    plt.hist(y_sub, density=True, bins=[50 * i for i in range(160)], alpha=0.5)
    plt.xlim((0, 3500))
    plt.show()

    sub[""duration""] = y_sub
    display(sub)
    sub.to_csv(""lgbm-voting-final.csv"", index=False)"
5076,32617166,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the ""../input/"" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# Any results you write to the current directory are saved as output."
5077,32617166,1,"train_df = pd.read_csv(""/kaggle/input/explicit-content-detection/train.csv"")
test_df = pd.read_csv(""/kaggle/input/explicit-content-detection/test.csv"")

titles = train_df[""title""].values
urls = train_df[""url""].values
y = train_df[""target""].astype(int).values"
5078,32617166,2,train_df.head()
5079,32617166,3,"from string import punctuation
import nltk"
5080,32617166,4,"# Define symbols & words we don't need
translator = {ord(c): ' ' for c in punctuation + '0123456789'}
nltk.download('stopwords')
stop_words = nltk.corpus.stopwords.words('russian') + nltk.corpus.stopwords.words('english')
print(stop_words[:10])"
5081,32617166,5,"'''
Remove stop words
'''
def remove_stops(collection):
    return [w for w in collection if w not in stop_words]


'''
Split titles array into tokens
'''
def split_into_tokens(X):
    n_samples = X.size
    tokens = []

    print('Splitting...')
    for i in range(n_samples):
        tokens.append(remove_stops(X[i].translate(translator).lower().split()))

        if i % 10000 == 0:
            print(f'Done: {i}/{n_samples}')

    print(f'Done: {n_samples}/{n_samples}\n')
    return tokens


from nltk.stem.snowball import SnowballStemmer 

'''
Do stemming with splitted word tokens
'''
def do_stemming(X):
    n_samples = len(X)
    stemmer = SnowballStemmer(""russian"")
    stemmed = []

    print('Stemming...')
    for i in range(n_samples):
        stemmed.append(list(map(stemmer.stem, X[i])))

        if i % 10000 == 0:
            print(f'Done: {i}/{n_samples}')

    print(f'Done: {n_samples}/{n_samples}\n')
    return stemmed"
5082,32617166,6,from sklearn.model_selection import train_test_split
5083,32617166,7,"# Split data into test and train sets
train_titles, test_titles, train_urls, test_urls, train_y, test_y = train_test_split(titles, urls, y, test_size=0.33, stratify=y)
n_samples = len(train_y)"
5084,32617166,8,X_stemmed_tokens = do_stemming(split_into_tokens(train_titles))
5085,32617166,9,from collections import Counter
5086,32617166,10,"'''
Create corpus of all words
'''
def make_corpus(X, y):
    corpus_all = []
    corpus_porn = []

    for sample, is_porn in zip(X, y):
        corpus_all.extend(set(sample))

        if is_porn:
            corpus_porn.extend(set(sample))
            
    return corpus_all, corpus_porn"
5087,32617166,11,"# Count words
# We need to do this to calculate importance of each word
corpus_titles_all, corpus_titles_porn = make_corpus(X_stemmed_tokens, train_y)
count_titles_all = Counter(corpus_titles_all)
count_titles_porn = Counter(corpus_titles_porn)

# Count urls
corpus_urls_porn = [train_urls[i] for i in range(n_samples) if train_y[i]]
count_urls_all = Counter(train_urls)
count_urls_porn = Counter(corpus_urls_porn)"
5088,32617166,12,"'''
Entropy of word
We have to estimate, how valuable deviation of given porn share from overall mean 
'''
def word_porn_rate(porn_share, mean_share, penalize):
    if porn_share > mean_share:
        return (porn_share - mean_share) / (1 - mean_share)
    
    fine = mean_share / (1 - mean_share) if penalize else 1
    return (porn_share - mean_share) * fine / mean_share"
5089,32617166,13,"'''
Define word score based on it's frequency and entropy
'''
def word_score(porn_rate, frequency, f_weight = 0.3):
    return ((frequency * f_weight) + np.abs(porn_rate) * (1 - f_weight)) / 2

'''
Get n best words based on word_score
'''
def get_n_best_words(n, x_size, mean_share, counter_porn, counter_all, min_freq=0.001, f_weight=0.3, penalize=True):
    word_scores = {}
    word_weights = {}
    
    # Evaluate scores for each popular word
    for word, count in counter_all.items():
        if count/x_size >= min_freq:
            porn_count = counter_porn[word]
            porn_share = porn_count / count
            frequency = count / x_size
            porn_rate = word_porn_rate(porn_share, mean_share, penalize)
            word_weights[word] = porn_rate
            word_scores[word] = word_score(porn_rate, frequency, f_weight)
            
    # Get n best words (or all words we have, if n is too big)
    best_words, best_weights = [], []
    
    for word in sorted(word_scores.keys(), key=lambda w: word_scores[w], reverse=True)[:n]:
        best_words.append(word)
        best_weights.append(word_weights[word])
            
    return best_words, np.array(best_weights)"
5090,32617166,14,"mean = np.mean(train_y)
best_words, best_word_weights = get_n_best_words(100, n_samples, mean, count_titles_porn, count_titles_all, min_freq=0.001, f_weight=0.3)
best_urls, best_url_weights = get_n_best_words(100000, n_samples, mean, count_urls_porn, count_urls_all, min_freq=0.0001, f_weight=0.3, penalize=False)"
5091,32617166,15,"print('Best words:')
for i in range(70, 100):
    word = best_words[i]
    print(f'{word} : {best_word_weights[i]}, {count_titles_all[word]}')
    
print('\nBest urls:')
for i in range(30):
    url = best_urls[i]
    print(f'{url} : {best_url_weights[i]}, {count_urls_all[url]}')"
5092,32617166,16,"'''
Transform the tokens array into vectors
'''
def tokens_to_vecs(tokens_X, n_samples, words, n_words):
    vectors_X = np.empty((n_samples, n_words))
    
    print('Vectorising...')
    for i in range(n_samples):
        tokens = tokens_X[i]
        vectors_X[i] = [int(word in tokens) for word in words]
        
        if i % 10000 == 0:
            print(f'Done: {i}/{n_samples}')
        
    print(f'Done: {n_samples}/{n_samples}')
    return vectors_X"
5093,32617166,17,"'''
Make target prediction based on valuable words
'''
def vec_to_predict(tokens_vec, best_weights):
    return int(np.mean(tokens_vec * best_weights) > 0)"
5094,32617166,18,"from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt"
5095,32617166,19,"# test_X_vecs = tokens_to_vecs(test_X, len(test_X), best_words, len(best_words))
# train_X_vecs = tokens_to_vecs(train_X, len(train_X), best_words, len(best_words))

# test_accuracies = []
# train_accuracies = []
# always_false = [0 for _ in test_X_vecs]
# xs = np.arange(10, 210, 10)
# const_accuracies = [1 - mean for i in xs]

# for i in xs:
#     test_predictions = [vec_to_predict(vec[:i], best_weights[:i]) for vec in test_X_vecs]
#     test_accuracies.append(accuracy_score(test_y, test_predictions))
    
# plt.plot(xs, test_accuracies, 'r', xs, const_accuracies, 'g')
# plt.plot(max(xs, key=lambda x: test_accuracies[int(x/10 - 1)]), max(test_accuracies), 'r^')
# plt.show()"
5096,32617166,20,"# test_predictions = [vec_to_predict(vec[:70], best_weights[:70]) for vec in test_X_vecs]
# print(f'Accuracy: {accuracy_score(test_y, test_predictions)}')"
5097,32617166,21,"X_urls = train_df[""url""].values"
5098,32617166,22,"# # Split data into test and train sets
train_X_urls, test_X_urls, train_y_urls, test_y_urls = train_test_split(X_urls, y_train, test_size=0.33, stratify=y_train)
porn_urls = [url for i, url in enumerate(train_X_urls) if train_y_urls[i]]

# Count urls
count_urls = Counter(train_X_urls)
count_urls_porn = Counter(porn_urls)"
5099,32617166,23,"# Get valuable urls
mean = np.mean(train_y_urls)
x_size = len(train_X_urls)
best_urls, best_urls_weights = get_n_best_words(25000, x_size, mean, count_urls_porn, count_urls, min_freq=0.00001, f_weight=0.3, penalize=False)"
5100,32617166,24,len(best_urls)
5101,32617166,25,"for i in range(100):
    url = best_urls[i]
    print(f'{url}  count: {count_urls[url]}  share: {count_urls_porn[url]/count_urls[url]}  weight: {best_urls_weights[i]}')"
5102,32617166,26,"def url_to_predict(url, urls, weights):
    try:
        return int(weights[urls.index(url)] > 0)
    except ValueError:
        return 0"
5103,32617166,27,"# test_accuracies_urls = []
# xs = np.arange(10000, 25500, 500)

# for x in xs:
#     urls, weights = best_urls[:x], best_urls_weights[:x]
#     predictions = [url_to_predict(url, urls, weights) for url in test_X_urls]
#     test_accuracies_urls.append(accuracy_score(test_y_urls, predictions))
    
# plt.plot(xs, test_accuracies_urls, 'r')
# plt.show()"
5104,32617166,28,"predictions = [url_to_predict(url, best_urls, best_urls_weights) for url in test_X_urls]
accuracy_score(test_y_urls, predictions)"
5105,32617166,29,"# X_test_urls = test_df[""url""].values
# n_samples = X_test_urls.size
# validate_predictions = [bool(url_to_predict(url, best_urls, best_urls_weights)) for url in X_test_urls]

# data = {
#     'id': [i for i in range(135309, 135309 + n_samples)],
#     'target': validate_predictions
# }

# validate_df = pd.DataFrame(data)
# validate_df.to_csv('simple_urls.csv', index=False)"
5944,32985411,0,"from tqdm import tqdm
import numpy as np

from collections import defaultdict
import itertools
from functools import lru_cache

import pickle

import warnings
warnings.filterwarnings('ignore')"
5945,32985411,1,np.random.seed(42)
5946,32985411,2,"def cost(photo1, photo2):
    intersect = len(photo1.intersection(photo2))
    return min(len(photo1) - intersect, len(photo2) - intersect, intersect)

def sequence_cost(sequence):
    total_cost = 0
    for i in range(len(sequence) - 1):
        if sequence[i + 1] == -1:
            break
            
        if isinstance(sequence[i], tuple):
            old_tags = photos[sequence[i][0]][1].union(photos[sequence[i][1]][1])
        else:
            old_tags = photos[sequence[i]][1]
            
        if isinstance(sequence[i + 1], tuple):
            new_tags = photos[sequence[i + 1][0]][1].union(photos[sequence[i + 1][1]][1])
        else:
            new_tags = photos[sequence[i + 1]][1]
            
        total_cost += cost(old_tags, new_tags)
    return total_cost

# Read our input
with open('../input/hashcode-photo-slideshow/d_pet_pictures.txt', 'r') as ifp:
    lines = ifp.readlines()

photos = []
all_tags = list()
photos_per_tag = defaultdict(list)
for i, line in enumerate(lines[1:]):
    orient, _, *tags = line.strip().split()
    photos.append((orient, set(tags)))
    for tag in tags:
        photos_per_tag[tag].append(i)

# Create some variables to store the solution in
sequence = [-1] * len(photos)
total_cost = 0

# Sample our first slide (must be horizontal)
sequence[0] = np.random.choice([i for i in range(len(photos)) if photos[i][0] == 'H'])
tags = photos[sequence[0]][1]
for tag in photos[sequence[0]][1]:
    photos_per_tag[tag].remove(sequence[0])
    
remaining_pics = list(set(range(len(photos))) - set(sequence))
remaining_horizontal_pics = [p for p in remaining_pics if photos[p][0] == 'H']
remaining_vertical_pics = [p for p in remaining_pics if photos[p][0] == 'V']

# Iteratively add a slide to the sequence
for i in tqdm(range(1, len(sequence))):
    # Fallback: In case we do not find any candidates, we just take 1 random horizontal or 2 random vertical pics
    if len(remaining_horizontal_pics) > 0:
        best_j = np.random.choice(remaining_horizontal_pics)
    elif len(remaining_vertical_pics) > 1:
        best_j = tuple(np.random.choice(remaining_vertical_pics, size=2, replace=False))
    else:
        break
        
    best_cost = total_cost
    
    # Get a list of K possible good candidates
    K = 2500
    k = 0.5
    vertical_candidates = set()
    horizontal_candidates = set()
    in_common_tags = defaultdict(int)
    for tag in tags:
        for p in photos_per_tag[tag]:
            in_common_tags[p] += 1
            
    if len(in_common_tags) > 0:
            
        max_tags = max(in_common_tags.values())
        for p in in_common_tags:
            if in_common_tags[p] == max_tags:
                if photos[p][0] == 'H':
                    horizontal_candidates.add(p)
                else:
                    vertical_candidates.add(p)
                    
        for p in in_common_tags:
            if len(horizontal_candidates) + len(vertical_candidates) > K:
                break

            if in_common_tags[p] >= k * max_tags:
                if photos[p][0] == 'H':
                    horizontal_candidates.add(p)
                else:
                    vertical_candidates.add(p)

        # Candidates consist of all possible horizontal candidates and all combinations of 2 vertical candidates
        candidates = list(horizontal_candidates) + list(itertools.combinations(vertical_candidates, 2))

        # Iterate over candidates and pick the one that increases the score the most.
        curr_best = 0
        old_cost = best_cost
        for j in candidates:
            if isinstance(j, tuple):
                new_tags = photos[j[0]][1].union(photos[j[1]][1])
            else:
                new_tags = photos[j][1]

            if len(new_tags) <= 2*curr_best:
                continue

            new_cost = total_cost + cost(tags, new_tags)

            if new_cost >= best_cost:
                best_cost = new_cost
                curr_best = new_cost - old_cost
                best_j = j

    # Assign a new picture to the next slide
    total_cost = best_cost
    sequence[i] = best_j
    
    if isinstance(best_j, tuple):
        tags = photos[sequence[i][0]][1].union(photos[sequence[i][1]][1])
        remaining_pics.remove(best_j[0])
        remaining_vertical_pics.remove(best_j[0])
        for tag in photos[sequence[i][0]][1]:
            photos_per_tag[tag].remove(sequence[i][0])
        remaining_pics.remove(best_j[1])
        remaining_vertical_pics.remove(best_j[1])
        for tag in photos[sequence[i][1]][1]:
            photos_per_tag[tag].remove(sequence[i][1])
    else:
        remaining_horizontal_pics.remove(best_j)
        remaining_pics.remove(best_j)
        tags = photos[sequence[i]][1]
        for tag in photos[sequence[i]][1]:
            photos_per_tag[tag].remove(sequence[i])"
5947,32985411,3,print('Score = {}'.format(sequence_cost(sequence)))
5948,32985411,4,"with open('submission.txt', 'w+') as ofp:
    ofp.write('{}\n'.format(sum(np.array(sequence) != -1)))
    for p in sequence:
        if p == -1:
            break
            
        if isinstance(p, tuple):
            ofp.write('{} {}\n'.format(p[0], p[1]))
        else:
            ofp.write('{}\n'.format(p))"
5949,32985411,5,"# CHECKS:
# 1) We dont want duplicates
# 2) We want vertical pictures to always be paired with another vertical picture
# 3) We don't want horizontal pictures to be paired
# 4) Preferably, we assign all of the pictures to slides
# 5) We cannot assign a picture to two different slides
done = set()
for i, p in enumerate(sequence):
    if p == -1:
        break
    if isinstance(p, tuple):
        assert p[0] != p[1]
        assert p[0] not in done
        assert photos[p[0]][0] == 'V'
        done.add(p[0])
        
        assert p[1] not in done
        assert photos[p[1]][0] == 'V'
        done.add(p[1])
    else:
        assert p not in done
        assert photos[p][0] == 'H'
        done.add(p)
print(i, len(done))
print(done - set(range(len(photos))))"
5950,32985411,6,!wc -l submission.txt
5951,32985411,7,!tail submission.txt
5952,32985411,8,!head submission.txt
5953,32985411,9,
6968,33450719,0,"%matplotlib inline
import pandas as pd
from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = ""all""
pd.set_option('display.max_columns', 99)
pd.set_option('display.max_rows', 99)
import os
import numpy as np
from matplotlib import pyplot as plt
from tqdm import tqdm
import datetime as dt"
6969,33450719,1,"import matplotlib.pyplot as plt
plt.rcParams['figure.figsize'] = [16, 10]
plt.rcParams['font.size'] = 14
import seaborn as sns
sns.set_palette(sns.color_palette('tab20', 20))

import plotly.express as px
import plotly.graph_objects as go"
6970,33450719,2,"COMP = '../input/covid19-global-forecasting-week-4'
DATEFORMAT = '%Y-%m-%d'


def get_comp_data(COMP):
    train = pd.read_csv(f'{COMP}/train.csv')
    test = pd.read_csv(f'{COMP}/test.csv')
    submission = pd.read_csv(f'{COMP}/submission.csv')
    print(train.shape, test.shape, submission.shape)
    train['Country_Region'] = train['Country_Region'].str.replace(',', '')
    test['Country_Region'] = test['Country_Region'].str.replace(',', '')

    train['Location'] = train['Country_Region'] + '-' + train['Province_State'].fillna('')

    test['Location'] = test['Country_Region'] + '-' + test['Province_State'].fillna('')

    train['LogConfirmed'] = to_log(train.ConfirmedCases)
    train['LogFatalities'] = to_log(train.Fatalities)
    train = train.drop(columns=['Province_State'])
    test = test.drop(columns=['Province_State'])

    country_codes = pd.read_csv('../input/covid19-metadata/country_codes.csv', keep_default_na=False)
    train = train.merge(country_codes, on='Country_Region', how='left')
    test = test.merge(country_codes, on='Country_Region', how='left')

    train['DateTime'] = pd.to_datetime(train['Date'])
    test['DateTime'] = pd.to_datetime(test['Date'])
    
    return train, test, submission


def process_each_location(df):
    dfs = []
    for loc, df in tqdm(df.groupby('Location')):
        df = df.sort_values(by='Date')
        df['Fatalities'] = df['Fatalities'].cummax()
        df['ConfirmedCases'] = df['ConfirmedCases'].cummax()
        df['LogFatalities'] = df['LogFatalities'].cummax()
        df['LogConfirmed'] = df['LogConfirmed'].cummax()
        df['LogConfirmedNextDay'] = df['LogConfirmed'].shift(-1)
        df['ConfirmedNextDay'] = df['ConfirmedCases'].shift(-1)
        df['DateNextDay'] = df['Date'].shift(-1)
        df['LogFatalitiesNextDay'] = df['LogFatalities'].shift(-1)
        df['FatalitiesNextDay'] = df['Fatalities'].shift(-1)
        df['LogConfirmedDelta'] = df['LogConfirmedNextDay'] - df['LogConfirmed']
        df['ConfirmedDelta'] = df['ConfirmedNextDay'] - df['ConfirmedCases']
        df['LogFatalitiesDelta'] = df['LogFatalitiesNextDay'] - df['LogFatalities']
        df['FatalitiesDelta'] = df['FatalitiesNextDay'] - df['Fatalities']
        dfs.append(df)
    return pd.concat(dfs)


def add_days(d, k):
    return dt.datetime.strptime(d, DATEFORMAT) + dt.timedelta(days=k)


def to_log(x):
    return np.log(x + 1)


def to_exp(x):
    return np.exp(x) - 1
"
6971,33450719,3,"start = dt.datetime.now()
train, test, submission = get_comp_data(COMP)
train.shape, test.shape, submission.shape
train.head(2)
test.head(2)"
6972,33450719,4,"train[train.geo_region.isna()].Country_Region.unique()
train = train.fillna('#N/A')
test = test.fillna('#N/A')

train[train.duplicated(['Date', 'Location'])]
train.count()"
6973,33450719,5,"train.describe()
train.nunique()
train.dtypes
train.count()

TRAIN_START = train.Date.min()
TEST_START = test.Date.min()
TRAIN_END = train.Date.max()
TEST_END = test.Date.max()
TRAIN_START, TRAIN_END, TEST_START, TEST_END"
6974,33450719,6,"train = train.sort_values(by='Date')
countries_latest_state = train[train['Date'] == TRAIN_END].groupby([
    'Country_Region', 'continent', 'geo_region', 'country_iso_code_3']).sum()[[
    'ConfirmedCases', 'Fatalities']].reset_index()
countries_latest_state['Log10Confirmed'] = np.log10(countries_latest_state.ConfirmedCases + 1)
countries_latest_state['Log10Fatalities'] = np.log10(countries_latest_state.Fatalities + 1)
countries_latest_state = countries_latest_state.sort_values(by='Fatalities', ascending=False)
countries_latest_state.to_csv('countries_latest_state.csv', index=False)

countries_latest_state.shape
countries_latest_state.head()"
6975,33450719,7,"fig = go.Figure(data=go.Choropleth(
    locations = countries_latest_state['country_iso_code_3'],
    z = countries_latest_state['Log10Confirmed'],
    text = countries_latest_state['Country_Region'],
    colorscale = 'viridis_r',
    autocolorscale=False,
    reversescale=False,
    marker_line_color='darkgray',
    marker_line_width=0.5,
    colorbar_tickprefix = '10^',
    colorbar_title = 'Confirmed cases <br>(log10 scale)',
))

_ = fig.update_layout(
    title_text=f'COVID-19 Global Cases [Updated: {TRAIN_END}]',
    geo=dict(
        showframe=False,
        showcoastlines=False,
        projection_type='equirectangular'
    )
)

fig.show()"
6976,33450719,8,"fig = go.Figure(data=go.Choropleth(
    locations = countries_latest_state['country_iso_code_3'],
    z = countries_latest_state['Log10Fatalities'],
    text = countries_latest_state['Country_Region'],
    colorscale = 'viridis_r',
    autocolorscale=False,
    reversescale=False,
    marker_line_color='darkgray',
    marker_line_width=0.5,
    colorbar_tickprefix = '10^',
    colorbar_title = 'Deaths <br>(log10 scale)',
))

_ = fig.update_layout(
    title_text=f'COVID-19 Global Deaths [Updated: {TRAIN_END}]',
    geo=dict(
        showframe=False,
        showcoastlines=False,
        projection_type='equirectangular'
    )
)

fig.show()"
6977,33450719,9,"countries_latest_state['DeathConfirmedRatio'] = (countries_latest_state.Fatalities + 1) / (countries_latest_state.ConfirmedCases + 1)
countries_latest_state['DeathConfirmedRatio'] = countries_latest_state['DeathConfirmedRatio'].clip(0, 0.15) 
fig = px.scatter(countries_latest_state,
                 x='ConfirmedCases',
                 y='Fatalities',
                 color='DeathConfirmedRatio',
                 size='Log10Fatalities',
                 size_max=20,
                 hover_name='Country_Region',
                 color_continuous_scale='viridis_r'
)
_ = fig.update_layout(
    title_text=f'COVID-19 Deaths vs Confirmed Cases by Country [Updated: {TRAIN_END}]',
    xaxis_type=""log"",
    yaxis_type=""log"",
    width = 1600,
    height = 900,
)
fig.show()"
6978,33450719,10,"# The source dataset is not necessary cumulative we will force it
latest_loc = train[train['Date'] == TRAIN_END][['Location', 'ConfirmedCases', 'Fatalities']]
max_loc = train.groupby(['Location'])[['ConfirmedCases', 'Fatalities']].max().reset_index()
check = pd.merge(latest_loc, max_loc, on='Location')
np.mean(check.ConfirmedCases_x == check.ConfirmedCases_y)
np.mean(check.Fatalities_x == check.Fatalities_y)
check[check.Fatalities_x != check.Fatalities_y]
check[check.ConfirmedCases_x != check.ConfirmedCases_y]"
6979,33450719,11,"train_clean = process_each_location(train)

train_clean.shape
train_clean.tail()"
6980,33450719,12,"regional_progress = train_clean.groupby(['DateTime', 'continent']).sum()[['ConfirmedCases', 'Fatalities']].reset_index()
regional_progress['Log10Confirmed'] = np.log10(regional_progress.ConfirmedCases + 1)
regional_progress['Log10Fatalities'] = np.log10(regional_progress.Fatalities + 1)
regional_progress = regional_progress[regional_progress.continent != '#N/A']
regional_progress = regional_progress.sort_values(by=['continent', 'DateTime'])

regional_progress['ConfirmedCasesDiff'] = regional_progress.groupby('continent').ConfirmedCases.diff().rolling(3).mean()
regional_progress['FatalitiesDiff'] = regional_progress.groupby('continent').Fatalities.diff().rolling(3).mean()"
6981,33450719,13,"fig = px.area(regional_progress, x=""DateTime"", y=""ConfirmedCases"", color=""continent"")
_ = fig.update_layout(
    title_text=f'COVID-19 Cumulative Confirmed Cases by Continent [Updated: {TRAIN_END}]',
    width=1600,
    height=900
)
fig.show()
fig2 = px.line(regional_progress, x='DateTime', y='ConfirmedCases', color='continent')
_ = fig2.update_layout(
    yaxis_type=""log"",
    title_text=f'COVID-19 Cumulative Confirmed Cases by Continent [Updated: {TRAIN_END}]'
)
fig2.show()

fig3 = px.line(regional_progress, x='DateTime', y='ConfirmedCasesDiff', color='continent')
_ = fig3.update_layout(
    title_text=f'COVID-19 Daily New Confirmed Cases by Continent [Updated: {TRAIN_END}]'
)
fig3.show()

"
6982,33450719,14,"fig = px.area(regional_progress, x=""DateTime"", y=""Fatalities"", color=""continent"")
_ = fig.update_layout(
    title_text=f'COVID-19 Cumulative Confirmed Deaths by Continent [Updated: {TRAIN_END}]'
)
fig.show()
fig2 = px.line(regional_progress, x='DateTime', y='Fatalities', color='continent')
_ = fig2.update_layout(
    yaxis_type=""log"",
    title_text=f'COVID-19 Cumulative Confirmed Deaths by Continent [Updated: {TRAIN_END}]'
)
fig2.show()
fig3 = px.line(regional_progress, x='DateTime', y='FatalitiesDiff', color='continent')
_ = fig3.update_layout(
    title_text=f'COVID-19 Daily New Fatalities by Continent [Updated: {TRAIN_END}]'
)
fig3.show()"
6983,33450719,15,"china = train_clean[train_clean.Location.str.startswith('China')]
top10_locations = china.groupby('Location')[['ConfirmedCases']].max().sort_values(
    by='ConfirmedCases', ascending=False).reset_index().Location.values[:10]
fig2 = px.line(china[china.Location.isin(top10_locations)], x='DateTime', y='ConfirmedCases', color='Location')
_ = fig2.update_layout(
    yaxis_type=""log"",
    title_text=f'COVID-19 Cumulative Confirmed Cases in China [Updated: {TRAIN_END}]'
)
fig2.show()"
6984,33450719,16,"europe = train_clean[train_clean.continent == 'Europe']
top10_locations = europe.groupby('Location')[['ConfirmedCases']].max().sort_values(
    by='ConfirmedCases', ascending=False).reset_index().Location.values[:10]
fig2 = px.line(europe[europe.Location.isin(top10_locations)], x='DateTime', y='ConfirmedCases', color='Location')
_ = fig2.update_layout(
    yaxis_type=""log"",
    title_text=f'COVID-19 Cumulative Confirmed Cases in Europe [Updated: {TRAIN_END}]'
)
fig2.show()"
6985,33450719,17,"us = train_clean[train_clean.Country_Region == 'US']
top10_locations = us.groupby('Location')[['ConfirmedCases']].max().sort_values(
    by='ConfirmedCases', ascending=False).reset_index().Location.values[:10]
fig2 = px.line(us[us.Location.isin(top10_locations)], x='DateTime', y='ConfirmedCases', color='Location')
_ = fig2.update_layout(
    yaxis_type=""log"",
    title_text=f'COVID-19 Cumulative Confirmed Cases in the USA [Updated: {TRAIN_END}]'
)
fig2.show()"
6986,33450719,18,"africa = train_clean[train_clean.continent == 'Africa']
top10_locations = africa.groupby('Location')[['ConfirmedCases']].max().sort_values(
    by='ConfirmedCases', ascending=False).reset_index().Location.values[:10]
fig2 = px.line(africa[africa.Location.isin(top10_locations)], x='DateTime', y='ConfirmedCases', color='Location')
_ = fig2.update_layout(
    yaxis_type=""log"",
    title_text=f'COVID-19 Cumulative Confirmed Cases in Africa [Updated: {TRAIN_END}]'
)
fig2.show()"
6987,33450719,19,"country_progress = train_clean.groupby(['Date', 'DateTime', 'Country_Region']).sum()[[
    'ConfirmedCases', 'Fatalities', 'ConfirmedDelta', 'FatalitiesDelta']].reset_index()
top10_countries = country_progress.groupby('Country_Region')[['Fatalities']].max().sort_values(
    by='Fatalities', ascending=False).reset_index().Country_Region.values[:10]

fig2 = px.line(country_progress[country_progress.Country_Region.isin(top10_countries)],
               x='DateTime', y='ConfirmedCases', color='Country_Region')
_ = fig2.update_layout(
    yaxis_type=""log"",
    title_text=f'COVID-19 Cumulative Confirmed Cases by Country [Updated: {TRAIN_END}]'
)
fig2.show()
fig3 = px.line(country_progress[country_progress.Country_Region.isin(top10_countries)],
               x='DateTime', y='Fatalities', color='Country_Region')
_ = fig3.update_layout(
    yaxis_type=""log"",
    title_text=f'COVID-19 Cumulative Deaths by Country [Updated: {TRAIN_END}]'
)
fig3.show()"
6988,33450719,20,"countries_0301 = country_progress[country_progress.Date == '2020-03-01'][[
    'Country_Region', 'ConfirmedCases', 'Fatalities']]
countries_0331 = country_progress[country_progress.Date == '2020-03-31'][[
    'Country_Region', 'ConfirmedCases', 'Fatalities']]
countries_in_march = pd.merge(countries_0301, countries_0331, on='Country_Region', suffixes=['_0301', '_0331'])
countries_in_march['IncreaseInMarch'] = countries_in_march.ConfirmedCases_0331 / (countries_in_march.ConfirmedCases_0301 + 1)
countries_in_march = countries_in_march[countries_in_march.ConfirmedCases_0331 > 200].sort_values(
    by='IncreaseInMarch', ascending=False)
countries_in_march.tail(15)"
6989,33450719,21,"selected_countries = [
    'Italy', 'Vietnam', 'Bahrain', 'Singapore', 'Taiwan*', 'Japan', 'Kuwait', 'Korea, South', 'China']
fig2 = px.line(country_progress[country_progress.Country_Region.isin(selected_countries)],
               x='DateTime', y='ConfirmedCases', color='Country_Region')
_ = fig2.update_layout(
    yaxis_type=""log"",
    title_text=f'COVID-19 Cumulative Confirmed Cases by Country [Updated: {TRAIN_END}]'
)
fig2.show()
fig3 = px.line(country_progress[country_progress.Country_Region.isin(selected_countries)],
               x='DateTime', y='Fatalities', color='Country_Region')
_ = fig3.update_layout(
    yaxis_type=""log"",
    title_text=f'COVID-19 Cumulative Deaths by Country [Updated: {TRAIN_END}]'
)
fig3.show()"
6990,33450719,22,"train_clean['Geo#Country#Contintent'] = train_clean.Location + '#' + train_clean.Country_Region + '#' + train_clean.continent
latest = train_clean[train_clean.Date == TRAIN_END][[
    'Geo#Country#Contintent', 'ConfirmedCases', 'Fatalities', 'LogConfirmed', 'LogFatalities']]
daily_confirmed_deltas = train_clean[train_clean.Date >= '2020-03-17'].pivot(
    'Geo#Country#Contintent', 'Date', 'LogConfirmedDelta').round(3).reset_index()
daily_confirmed_deltas = latest.merge(daily_confirmed_deltas, on='Geo#Country#Contintent')
daily_confirmed_deltas.shape
daily_confirmed_deltas.head()
daily_confirmed_deltas.to_csv('daily_confirmed_deltas.csv', index=False)"
6991,33450719,23,"deltas = train_clean[np.logical_and(
        train_clean.LogConfirmed > 2,
        ~train_clean.Location.str.startswith('China')
)].dropna().sort_values(by='LogConfirmedDelta', ascending=False)

deltas['start'] = deltas['LogConfirmed'].round(0)
confirmed_deltas = pd.concat([
    deltas.groupby('start')[['LogConfirmedDelta']].mean(),
    deltas.groupby('start')[['LogConfirmedDelta']].std(),
    deltas.groupby('start')[['LogConfirmedDelta']].count()
], axis=1)

deltas.mean()

confirmed_deltas.columns = ['avg', 'std', 'cnt']
confirmed_deltas
confirmed_deltas.to_csv('confirmed_deltas.csv')"
6992,33450719,24,"fig = px.box(deltas,  x=""start"", y=""LogConfirmedDelta"", range_y=[0, 0.35])
fig.show()"
6993,33450719,25,"fig = px.box(deltas[deltas.Date >= '2020-03-01'],  x=""DateTime"", y=""LogConfirmedDelta"", range_y=[0, 0.6])
fig.update_layout(
    width = 1600,
    height = 800,
)
fig.show()"
6994,33450719,26,"deltas = train_clean[np.logical_and(
        train_clean.LogConfirmed > 0,
        ~train_clean.Location.str.startswith('China')
)].dropna().sort_values(by='LogConfirmedDelta', ascending=False)
deltas = deltas[deltas['Date'] >= '2020-03-12']

confirmed_deltas = pd.concat([
    deltas.groupby('Location')[['LogConfirmedDelta']].mean(),
    deltas.groupby('Location')[['LogConfirmedDelta']].std(),
    deltas.groupby('Location')[['LogConfirmedDelta']].count(),
    deltas.groupby('Location')[['LogConfirmed']].max()
], axis=1)
confirmed_deltas.columns = ['avg', 'std', 'cnt', 'max']

confirmed_deltas.sort_values(by='avg').head(10)
confirmed_deltas.sort_values(by='avg').tail(10)
confirmed_deltas.to_csv('confirmed_deltas.csv')"
6995,33450719,27,"end = dt.datetime.now()
print('Finished', end, (end - start).seconds, 's')"
7689,38265550,0,"import os
import pandas as pd
import tensorflow as tf
import tensorflow_hub as hub
import matplotlib.pyplot as plt
from six import BytesIO
import numpy as np
import xml.etree.ElementTree as et
import ast
import tqdm
from itertools import chain
from xml.dom import minidom
from PIL import Image
from PIL import ImageColor
from PIL import ImageDraw
from PIL import ImageFont
from PIL import ImageOps
import cv2
import glob
import time"
7690,38265550,1,path1='/kaggle/input/open-images-object-detection-rvc-2020/test/'
7691,38265550,2,"sample = pd.read_csv(""/kaggle/input/open-images-object-detection-rvc-2020/sample_submission.csv"")
sample.head()"
7692,38265550,3,sample.shape
7693,38265550,4,"ids = []
for i in range(len(sample)):
    ids.append(sample['ImageId'][i])"
7694,38265550,5,ids[0:5]
7695,38265550,6,"img_data=[]
for i in range(len(sample)):
    img_data.append(glob.glob('/kaggle/input/open-images-object-detection-rvc-2020/test/{0}.jpg'.format(ids[i])))"
7696,38265550,7,img_data[0:5]
7697,38265550,8,img_data=list(chain.from_iterable(img_data))
7698,38265550,9,img_data[0:5]
7699,38265550,10,"def get_prediction_string(result):
    with tf.device('/device:GPU:0'):
        df = pd.DataFrame(columns=['Ymin','Xmin','Ymax', 'Xmax','Score','Label','Class_label','Class_name'])
        min_score=0.01
        for i in range(result['detection_boxes'].shape[0]):
           if (result[""detection_scores""][i]) >= min_score:
              df.loc[i]= tuple(result['detection_boxes'][i])+(result[""detection_scores""][i],)+(result[""detection_class_labels""][i],)+(result[""detection_class_names""][i],)+(result[""detection_class_entities""][i],)
        return df"
7700,38265550,11,"import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()"
7701,38265550,12,"module_handle = ""https://tfhub.dev/google/faster_rcnn/openimages_v4/inception_resnet_v2/1""
with tf.device('/device:GPU:0'):
    with tf.Graph().as_default():
        detector = hub.Module(module_handle)
        image_string_placeholder = tf.placeholder(tf.string)
        decoded_image = tf.image.decode_jpeg(image_string_placeholder)
        decoded_image_float = tf.image.convert_image_dtype(
            image=decoded_image, dtype=tf.float32)
        module_input = tf.expand_dims(decoded_image_float, 0)
        result = detector(module_input, as_dict=True)
        init_ops = [tf.global_variables_initializer(), tf.tables_initializer()]

        session = tf.Session()
        session.run(init_ops)"
7702,38265550,13,"def nms(dets, thresh):
    x1 = dets[:, 0]
    y1 = dets[:, 1]
    x2 = dets[:, 2]
    y2 = dets[:, 3]
    scores = dets[:, 4]

    areas = (x2 - x1 + 1) * (y2 - y1 + 1)
    order = scores.argsort()[::-1]

    keep = []
    while order.size > 0:
        i = order[0]
        keep.append(i)
        xx1 = np.maximum(x1[i], x1[order[1:]])
        yy1 = np.maximum(y1[i], y1[order[1:]])
        xx2 = np.minimum(x2[i], x2[order[1:]])
        yy2 = np.minimum(y2[i], y2[order[1:]])

        w = np.maximum(0.0, xx2 - xx1 + 1)
        h = np.maximum(0.0, yy2 - yy1 + 1)
        inter = w * h
        ovr = inter / (areas[i] + areas[order[1:]] - inter)

        inds = np.where(ovr <= thresh)[0]
        order = order[inds + 1]

    return keep"
7703,38265550,14,image_paths = img_data[0:20]
7704,38265550,15,"images = []
for f in image_paths:
    images.append(np.asarray(Image.open(f)))"
7705,38265550,16,!mkdir deepak
7706,38265550,17,"image_id = sample['ImageId']
def format_prediction_string(image_id, result):
    prediction_strings = []
    
    for i in range(len(result['Score'])):
        class_name = result['Class_label'][i].decode(""utf-8"")
        YMin,XMin,YMax,XMax = result['Ymin'][i],result['Xmin'][i],result['Ymax'][i],result['Xmax'][i]
        score = result['Score'][i]
        
        prediction_strings.append(
            f""{class_name} {score} {XMin} {YMin} {XMax} {YMax}""
        )
        
    prediction_string = "" "".join(prediction_strings)

    return {
        ""PredictionString"": prediction_string
    }"
7707,38265550,18,"k =-1
predictions = []
with tf.device('/device:GPU:0'):
    for image_path in image_paths:
        k=k+1
        img_path = img_data[k]
        img = cv2.imread(img_path)
        with tf.gfile.Open(image_path, ""rb"") as binfile:
            image_string = binfile.read()

        inference_start_time = time.time()
        result_out, image_out = session.run(
            [result, decoded_image],
            feed_dict={image_string_placeholder: image_string})
        df1=get_prediction_string(result_out)
        z1=nms(df1.values,0.68)
        z=df1.iloc[z1]
        z=z.reset_index()
        predictions.append(format_prediction_string(image_id, z))
        data1=z
        COLORS = np.random.uniform(0, 255, size=(len(z['Class_name']), 3))
        for m in range(len(data1)):
            if data1['Score'][m] >=0.01:
                img_class=data1.iloc[m].Class_name
                img_xmax, img_ymax =images[k].shape[1],images[k].shape[0]
                bbox_x_max, bbox_x_min = data1.Xmax[m] * img_xmax, data1.Xmin[m] * img_xmax
                bbox_y_max ,bbox_y_min = data1.Ymax[m] * img_ymax, data1.Ymin[m] * img_ymax
                xmin = int(bbox_x_min)
                ymin = int(bbox_y_min)
                xmax = int(bbox_x_max)
                ymax = int(bbox_y_max)
                width = xmax - xmin
                height = ymax - ymin
                label = str(data1['Class_name'][m])
                color = COLORS[m]
                cv2.rectangle(img, (xmin, ymax), (xmax, ymin), color, 2)
                path1 = '/kaggle/working/deepak/'+str(k)+'.jpg'
                img_path = path1
                cv2.imwrite(path1, img)
                cv2.putText(img, label, (xmax,ymin), cv2.FONT_HERSHEY_SIMPLEX, 0.9,color, 2)"
7708,38265550,19,"def load_images(folder):
    images = []
    for filename in os.listdir(folder):
        img = Image.open(os.path.join(folder, filename))
        if img is not None:
            images.append(img)
    return images"
7709,38265550,20,"z = load_images(""/kaggle/working/deepak"")"
7710,38265550,21,z[0]
7711,38265550,22,z[3]
7712,38265550,23,z[4]
7713,38265550,24,z[6]
7714,38265550,25,z[9]
7715,38265550,26,z[10]
7716,38265550,27,z[11]
7717,38265550,28,z[15]
7718,38265550,29,z[18]
7719,38265550,30,"pred_df = pd.DataFrame(predictions)
pred_df.head()"
7720,38265550,31,sample['PredictionString']= pred_df['PredictionString']
7721,38265550,32,sample.head()
7776,36234681,0,"import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import numpy as np #
import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))
        if filename.find(""rain"")>0:
            print('reading train')            
            train=pd.read_csv(os.path.join(dirname, filename) )
        if filename.find(""est"")>0:
            print('reading test')
            test=pd.read_csv(os.path.join(dirname, filename) )
"
7777,36234681,1,"trainr=train
train"
7778,36234681,2,"from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer
cv = TfidfVectorizer()#CountVectorizer()
train_tf=cv.fit_transform(train.ItemId.fillna(''))
count_df=pd.DataFrame(cv.transform(train['ItemId']).toarray(), columns=cv.get_feature_names())
words=count_df.sum()
words=pd.DataFrame(words,columns=['pos'])
words"
7779,36234681,3,"from sklearn.metrics.pairwise import cosine_similarity

item_item=cosine_similarity(train_tf.T,train_tf.T)
item_item"
7780,36234681,4,np.asarray(train.iloc[3]['ItemId'].split(' '))
7781,36234681,5,"recommended=pd.DataFrame(item_item.T*cv.transform(train.iloc[0:1].ItemId).T,index=cv.get_feature_names()).sort_values(0,ascending=False)[:100]
#1938 490 128 1197 2893 2983 1861 1307 2547 231...
recommended=[xi for xi in recommended.index if xi not in np.asarray(train.iloc[0]['ItemId'].split(' '))]
' '.join(recommended)"
7782,36234681,6,"for xi in range(len(train)):
    if xi/100==int(xi/100):
        print(xi)
    recommended=pd.DataFrame(item_item.T*cv.transform(train.iloc[xi:xi+1].ItemId).T,index=cv.get_feature_names()).sort_values(0,ascending=False)[:100]
    recommended=[ri for ri in recommended.index if ri not in np.asarray(train.loc[xi].ItemId.split(' '))]
    predi=' '.join(recommended)
    train.iat[xi,1]=predi
    
train"
7783,36234681,7,"train.to_csv('submit.csv',index=False)"
7784,36234681,8,"user_user=cosine_similarity(train_tf,train_tf)
user_user"
7785,36234681,9,"train_tf[:,1]"
7786,36234681,10,"    sameuser=pd.DataFrame(user_user*train_tf[:,1]).sort_values(0,ascending=False)[:3]
    sameuser"
7787,36234681,11,user_user[0][1:].max()
7788,36234681,12,"comarket=(user_user*train_tf)+(item_item.T*train_tf.T).T
"
7789,36234681,13,comarket.shape
7790,36234681,14,"for xi in range(train_tf.shape[0]):
    if xi/100==int(xi/100):
        print(xi)
    
    recommended=pd.DataFrame(comarket[xi],index=cv.get_feature_names()).sort_values(0,ascending=False)[:100]
    recommended=[ri for ri in recommended.index if ri not in np.asarray(trainr.loc[xi]['ItemId'].split(' '))]
    predi=' '.join(recommended)
    train.iat[xi,1]=predi
    "
7791,36234681,15,"train.to_csv('submit2.csv',index=False)"
7792,35883312,0,"# import json
# import pandas as pd

# def convert_to_df(json_filepath, filename):
#     with open(json_filepath) as json_file:
#         data = json.load(json_file)
        
    
#     train_annotations = pd.DataFrame(data['annotations'])
#     train_images = pd.DataFrame(data['images'])
#     train_categories = pd.DataFrame(data['categories'])

#     categories_dict = pd.Series(train_categories.name.values,index=train_categories.id).to_dict() # for mapping
#     train_images.columns = ['file_name', 'image_id'] # for merging
    
#     df = pd.merge(train_annotations, train_images, on='image_id', how='left')
    
#     df['file_name'] = df['file_name'].apply(lambda x: '/kaggle/input/til2020/train/train/' + x)
    
#     df['xmin'] = df['bbox'].apply(lambda x: int(x[0]))
#     df['ymin'] = df['bbox'].apply(lambda x: int(x[1]))

#     df['xmax'] = df['bbox'].apply(lambda x: int(x[0] +  x[2]))
#     df['ymax'] = df['bbox'].apply(lambda x: int(x[1] + x[3]))
#     df['class'] = df['category_id'].apply(lambda x: categories_dict[x])
#     df = df.iloc[:, 6:]
#     df.to_csv(filename, index=False)
    
#     return df"
7793,35883312,1,"# convert_to_df('/kaggle/input/til2020/train.json', 'train_df.csv').head()"
7794,35883312,2,"# def convert_to_df(json_filepath, filename):
#     with open(json_filepath) as json_file:
#         data = json.load(json_file)
        
    
#     train_annotations = pd.DataFrame(data['annotations'])
#     train_images = pd.DataFrame(data['images'])
#     train_categories = pd.DataFrame(data['categories'])

#     categories_dict = pd.Series(train_categories.name.values,index=train_categories.id).to_dict() # for mapping
#     train_images.columns = ['file_name', 'image_id'] # for merging
    
#     df = pd.merge(train_annotations, train_images, on='image_id', how='left')
    
#     df['file_name'] = df['file_name'].apply(lambda x: '/kaggle/input/til2020/val/val/' + x)
    
#     df['xmin'] = df['bbox'].apply(lambda x: int(x[0]))
#     df['ymin'] = df['bbox'].apply(lambda x: int(x[1]))

#     df['xmax'] = df['bbox'].apply(lambda x: int(x[0] +  x[2]))
#     df['ymax'] = df['bbox'].apply(lambda x: int(x[1] + x[3]))
    
#     df['class'] = df['category_id'].apply(lambda x: categories_dict[x])
#     df = df.iloc[:, 6:]
#     df.to_csv(filename, index=False)
    
#     return df"
7795,35883312,3,"# convert_to_df('/kaggle/input/til2020/val.json', 'val_df.csv').head()"
7796,35883312,4,# data['categories']
7797,35883312,5,# !git clone https://github.com/xuannianz/EfficientDet.git
7798,35883312,6,"# import os

# os.chdir('/kaggle/input/efficientdet-til/EfficientDet')"
7799,35883312,7,!cp -r /kaggle/input/efficientdet-til/EfficientDet /kaggle/working
7800,35883312,8,"import os
os.chdir('/kaggle/working/EfficientDet')

!ls"
7801,35883312,9,!pip install numpy --user
7802,35883312,10,!pip install . --user
7803,35883312,11,!python setup.py build_ext --inplace
7804,35883312,12,# !pip install -r requirements.txt
7805,35883312,13,!pip install progressbar2 
7806,35883312,14,!python train.py --snapshot imagenet --phi 0 --gpu 0 --weighted-bifpn --epochs 10 --random-transform --compute-val-loss --batch-size 2 --steps 4113 csv /kaggle/input/til-df/train_df.csv /kaggle/input/til-df/class.csv --val /kaggle/input/til-df/val_df.csv
7807,35883312,15,"import tensorflow
tensorflow.test.is_gpu_available()
print(tensorflow.__version__)"
7808,35883312,16,
8705,41034990,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only ""../input/"" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
8706,41034990,1,"trainData=pd.read_csv(""/kaggle/input/mobile-price-range-prediction-is2020-v2/train_data.csv"")
trainData"
8707,41034990,2,"testData=pd.read_csv(""/kaggle/input/mobile-price-range-prediction-is2020-v2/test_data.csv"")
testData"
8708,41034990,3,"submissionData=pd.read_csv(""/kaggle/input/mobile-price-range-prediction-is2020-v2/sample_submission.csv"")
submissionData"
8709,41034990,4,"x_train=trainData.drop(columns=['price_range','id'])
y_train=trainData['price_range']
x_test=testData.drop(columns=['id'])"
8710,41034990,5,"print(x_train,y_train,x_test,sep=""\n"")"
8711,41034990,6,"from sklearn.preprocessing import StandardScaler as ss
x_trainscale =ss().fit_transform(x_train)
x_testscale =ss().fit_transform(x_test)"
8712,41034990,7,"from sklearn.linear_model import LogisticRegression as lr
from sklearn.ensemble import RandomForestClassifier as rfc
from sklearn.model_selection import cross_val_score as cvs
r = rfc().fit(x_trainscale,y_train)
y_pred=r.predict(x_testscale)
v=cvs(rfc(),x_trainscale,y_train,cv=3)
v"
8713,41034990,8,"res=pd.DataFrame({'id':testData['id'],'price_range':y_pred})"
8714,41034990,9,"res.to_csv('/kaggle/working/result_dtc.csv',index=False)"
8715,41034990,10,"reg=lr(C=1000,penalty='l2')
reg.fit(x_trainscale,y_train)
y_pred=reg.predict(x_testscale)
value=cvs(lr(),x_trainscale,y_train,cv=3)
print(value)"
8716,41034990,11,"res2=pd.DataFrame({'id':testData['id'],'price_range':y_pred})
res2.to_csv('/kaggle/working/result_lr.csv',index=False)"
8717,41034990,12,
8718,40629572,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only ""../input/"" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
8719,40629572,1,"trainData=pd.read_csv(""/kaggle/input/mobile-price-range-prediction-is2020-v2/train_data.csv"")
trainData"
8720,40629572,2,"testData=pd.read_csv(""/kaggle/input/mobile-price-range-prediction-is2020-v2/test_data.csv"")
testData"
8721,40629572,3,"submissionData=pd.read_csv(""/kaggle/input/mobile-price-range-prediction-is2020-v2/sample_submission.csv"")
submissionData"
8722,40629572,4,"x_train=trainData.drop(columns=['price_range','id'])
y_train=trainData['price_range']
x_test=testData.drop(columns=['id'])"
8723,40629572,5,"print(x_train,y_train,x_test,sep=""\n"")"
8724,40629572,6,"from sklearn.preprocessing import StandardScaler as ss
x_trainscale =ss().fit_transform(x_train)
x_testscale =ss().fit_transform(x_test)"
8725,40629572,7,"from sklearn.linear_model import LogisticRegression as lr
from sklearn.ensemble import RandomForestClassifier as rfc
from sklearn.model_selection import cross_val_score as cvs
r = rfc().fit(x_trainscale,y_train)
y_pred=r.predict(x_testscale)
v=cvs(rfc(),x_trainscale,y_train,cv=3)
v"
8726,40629572,8,"res=pd.DataFrame({'id':testData['id'],'price_range':y_pred})"
8727,40629572,9,"reg=lr(C=1000,penalty='l2')
reg.fit(x_trainscale,y_train)
y_pred=reg.predict(x_testscale)
value=cvs(lr(),x_trainscale,y_train,cv=3)
print(value)"
8728,40629572,10,"res2=pd.DataFrame({'id':testData['id'],'price_range':y_pred})
res2.to_csv('/kaggle/working/result_lr.csv',index=False)"
8729,40629982,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only ""../input/"" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
8730,40629982,1,"trainData=pd.read_csv(""/kaggle/input/mobile-price-range-prediction-is2020-v2/train_data.csv"")
trainData"
8731,40629982,2,"testData=pd.read_csv(""/kaggle/input/mobile-price-range-prediction-is2020-v2/test_data.csv"")
testData"
8732,40629982,3,"submissionData=pd.read_csv(""/kaggle/input/mobile-price-range-prediction-is2020-v2/sample_submission.csv"")
submissionData"
8733,40629982,4,"x_train=trainData.drop(columns=['price_range','id'])
y_train=trainData['price_range']
x_test=testData.drop(columns=['id'])"
8734,40629982,5,"print(x_train,y_train,x_test,sep=""\n"")"
8735,40629982,6,"from sklearn.preprocessing import StandardScaler as ss
x_trainscale =ss().fit_transform(x_train)
x_testscale =ss().fit_transform(x_test)"
8736,40629982,7,"from sklearn.linear_model import LogisticRegression as lr
from sklearn.ensemble import RandomForestClassifier as rfc
from sklearn.model_selection import cross_val_score as cvs
r = rfc().fit(x_trainscale,y_train)
y_pred=r.predict(x_testscale)
v=cvs(rfc(),x_trainscale,y_train,cv=3)
v"
8737,40629982,8,"res=pd.DataFrame({'id':testData['id'],'price_range':y_pred})"
8738,40629982,9,"reg=lr(C=1000,penalty='l2')
reg.fit(x_trainscale,y_train)
y_pred=reg.predict(x_testscale)
value=cvs(lr(),x_trainscale,y_train,cv=3)
print(value)"
8739,40629982,10,"res2=pd.DataFrame({'id':testData['id'],'price_range':y_pred})
res2.to_csv('/kaggle/working/result_lr.csv',index=False)"
8740,40630451,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only ""../input/"" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
8741,40630451,1,"trainData=pd.read_csv(""/kaggle/input/mobile-price-range-prediction-is2020-v2/train_data.csv"")
trainData"
8742,40630451,2,"testData=pd.read_csv(""/kaggle/input/mobile-price-range-prediction-is2020-v2/test_data.csv"")
testData"
8743,40630451,3,"submissionData=pd.read_csv(""/kaggle/input/mobile-price-range-prediction-is2020-v2/sample_submission.csv"")
submissionData"
8744,40630451,4,"x_train=trainData.drop(columns=['price_range','id'])
y_train=trainData['price_range']
x_test=testData.drop(columns=['id'])"
8745,40630451,5,"print(x_train,y_train,x_test,sep=""\n"")"
8746,40630451,6,"from sklearn.preprocessing import StandardScaler as ss
x_trainscale =ss().fit_transform(x_train)
x_testscale =ss().fit_transform(x_test)"
8747,40630451,7,"from sklearn.linear_model import LogisticRegression as lr
from sklearn.ensemble import RandomForestClassifier as rfc
from sklearn.model_selection import cross_val_score as cvs
r = rfc().fit(x_trainscale,y_train)
y_pred=r.predict(x_testscale)
v=cvs(rfc(),x_trainscale,y_train,cv=3)
v"
8748,40630451,8,"res=pd.DataFrame({'id':testData['id'],'price_range':y_pred})"
8749,40630451,9,"reg=lr(C=1000,penalty='l2')
reg.fit(x_trainscale,y_train)
y_pred=reg.predict(x_testscale)
value=cvs(lr(),x_trainscale,y_train,cv=3)
print(value)"
8750,40630451,10,"res2=pd.DataFrame({'id':testData['id'],'price_range':y_pred})
res2.to_csv('/kaggle/working/result_lr.csv',index=False)"
8751,41060706,0,"import pandas as pd
df=pd.read_csv('/kaggle/input/mobile-price-range-prediction-is2020-v2/train_data.csv')
test_data=pd.read_csv('/kaggle/input/mobile-price-range-prediction-is2020-v2/test_data.csv')
sample_submission=pd.read_csv('../input/mobile-price-range-prediction-is2020-v2/sample_submission.csv')
#how does ram effected by price
import seaborn as sns
sns.jointplot(x='ram',y='price_range',data=df,color='red',kind='kde');

# Internal Memory vs PriceRange
sns.pointplot(y=""int_memory"", x=""price_range"", data=df)

# % of phones with support 3G
import matplotlib.pyplot as plt
labels = [""3G-supported"",'Not supported']
values=df['three_g'].value_counts().values
fig1, ax1 = plt.subplots()
ax1.pie(values, labels=labels, autopct='%1.1f%%',shadow=True,startangle=90)
plt.show()

# % of phones with support 4G
labels = [""4G-supported"",'Not supported']
values=df['four_g'].value_counts().values
fig1, ax1 = plt.subplots()
ax1.pie(values, labels=labels, autopct='%1.1f%%',shadow=True,startangle=90)
plt.show()

# Battery power vs Price_range
sns.boxplot(x=""price_range"", y=""battery_power"", data=df)

# No of Phones vs Camera megapixels of front and primary camera
plt.figure(figsize=(10,6))
df['fc'].hist(alpha=0.5,color='blue',label='Front camera')
df['pc'].hist(alpha=0.5,color='red',label='Primary camera')
plt.legend()
plt.xlabel('MegaPixels')

# MobileWeight vs PriceRange
sns.jointplot(x='mobile_wt',y='price_range',data=df,kind='kde');

# Talktime vs PriceRange
sns.pointplot(y=""talk_time"", x=""price_range"", data=df)

#Algorithms
x_train=df.drop(columns=['price_range','id'])
y_train=df['price_range']
x_test=test_data.drop(columns=['id'])
print(x_train,y_train,x_test,sep=""\n"")
from sklearn.preprocessing import StandardScaler as ss
x_trainscale=ss().fit_transform(x_train)
x_testscale=ss().fit_transform(x_test)


from sklearn.linear_model import LogisticRegression as lr
from sklearn.ensemble import RandomForestClassifier as rfc
from sklearn.tree import DecisionTreeClassifier as dtc
from sklearn.neighbors import KNeighborsClassifier as knn
from sklearn.svm import SVC as svm
from sklearn.naive_bayes import GaussianNB as nvb
from sklearn.model_selection import cross_val_score as cvs

r=rfc().fit(x_trainscale,y_train)
y_pred=r.predict(x_testscale)
v=cvs(rfc(),x_trainscale,y_train,cv=3)
v

res=pd.DataFrame({'id':test_data['id'],'price_range':y_pred})
res.to_csv('/kaggle/working/res_rfc.csv',index=False)

reg=lr(C=1000,penalty='l2')
reg.fit(x_trainscale,y_train)
y_pred=reg.predict(x_testscale)
value=cvs(lr(),x_trainscale,y_train,cv=3)
print(value)

res2=pd.DataFrame({'id':test_data['id'],'price_range':y_pred})
res2.to_csv('/kaggle/working/res_lr.csv',index=False)

#decision tree algorithm


r=dtc().fit(x_trainscale,y_train)
y_pred=r.predict(x_testscale)
v=cvs(dtc(),x_trainscale,y_train,cv=3)
v


res3=pd.DataFrame({'id':test_data['id'],'price_range':y_pred})
res3.to_csv('/kaggle/working/res_dtc.csv',index=False)

#KNeighborsClassifier algorithm


r=rfc().fit(x_trainscale,y_train)
y_pred=r.predict(x_testscale)
v=cvs(rfc(),x_trainscale,y_train,cv=3)
v

res4=pd.DataFrame({'id':test_data['id'],'price_range':y_pred})
res4.to_csv('/kaggle/working/res_knn.csv',index=False)

#support vector machine algorithm


r=rfc().fit(x_trainscale,y_train)
y_pred=r.predict(x_testscale)
v=cvs(rfc(),x_trainscale,y_train,cv=3)
v

res5=pd.DataFrame({'id':test_data['id'],'price_range':y_pred})
res5.to_csv('/kaggle/working/res_svm.csv',index=False)

#naive bayes algorithm


r=rfc().fit(x_trainscale,y_train)
y_pred=r.predict(x_testscale)
v=cvs(rfc(),x_trainscale,y_train,cv=3)
v

res6=pd.DataFrame({'id':test_data['id'],'price_range':y_pred})
res6.to_csv('/kaggle/working/res_nvb.csv',index=False)
"
8752,41062064,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only ""../input/"" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
8753,41062064,1,"import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline"
8754,41062064,2,dataset=pd.read_csv('../input/mobile-price-range-prediction-is2020-v2/train_data.csv')
8755,41062064,3,dataset.head()
8756,41062064,4,dataset.info()
8757,41062064,5,dataset.describe()
8758,41062064,6,"sns.jointplot(x='ram',y='price_range',data=dataset,color='red',kind='kde');"
8759,41062064,7,"sns.pointplot(y=""int_memory"", x=""price_range"", data=dataset)"
8760,41062064,8,"labels = [""3G-supported"",'Not supported']
values=dataset['three_g'].value_counts().values"
8761,41062064,9,"fig1, ax1 = plt.subplots()
ax1.pie(values, labels=labels, autopct='%1.1f%%',shadow=True,startangle=90)
plt.show()
"
8762,41062064,10,"labels4g = [""4G-supported"",'Not supported']
values4g = dataset['four_g'].value_counts().values
fig1, ax1 = plt.subplots()
ax1.pie(values4g, labels=labels4g, autopct='%1.1f%%',shadow=True,startangle=90)
plt.show()"
8763,41062064,11,"sns.boxplot(x=""price_range"", y=""battery_power"", data=dataset)"
8764,41062064,12,"plt.figure(figsize=(10,6))
dataset['fc'].hist(alpha=0.5,color='blue',label='Front camera')
dataset['pc'].hist(alpha=0.5,color='red',label='Primary camera')
plt.legend()
plt.xlabel('MegaPixels')"
8765,41062064,13,"sns.jointplot(x='mobile_wt',y='price_range',data=dataset,kind='kde');"
8766,41062064,14,"sns.pointplot(y=""talk_time"", x=""price_range"", data=dataset)"
8767,41062064,15,"X=dataset.drop('price_range',axis=1)"
8768,41062064,16,y=dataset['price_range']
8769,41062064,17,from sklearn.model_selection import train_test_split
8770,41062064,18,"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=101)"
8771,41062064,19,"from sklearn.linear_model import LinearRegression
lm = LinearRegression()"
8772,41062064,20,"lm.fit(X_train,y_train)"
8773,41062064,21,"lm.score(X_test,y_test)"
8774,41062064,22,"from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=10)
knn.fit(X_train,y_train)"
8775,41062064,23,"knn.score(X_test,y_test)"
8776,41062064,24,"error_rate = []
for i in range(1,20):
    
    knn = KNeighborsClassifier(n_neighbors=i)
    knn.fit(X_train,y_train)
    pred_i = knn.predict(X_test)
    error_rate.append(np.mean(pred_i != y_test))"
8777,41062064,25,"plt.figure(figsize=(10,6))
plt.plot(range(1,20),error_rate,color='blue', linestyle='dashed', marker='o',
         markerfacecolor='red', markersize=5)
plt.title('Error Rate vs. K Value')
plt.xlabel('K')
plt.ylabel('Error Rate')"
8778,41062064,26,"from sklearn.linear_model import LogisticRegression
logmodel = LogisticRegression()
"
8779,41062064,27,"logmodel.fit(X_train,y_train)"
8780,41062064,28,"logmodel.score(X_test,y_test)"
8781,41062064,29,"from sklearn.tree import DecisionTreeClassifier
dtree = DecisionTreeClassifier()"
8782,41062064,30,"dtree.fit(X_train,y_train)"
8783,41062064,31,"dtree.score(X_test,y_test)"
8784,41062064,32,"feature_names=['battery_power', 'blue', 'clock_speed', 'dual_sim', 'fc', 'four_g',
       'int_memory', 'm_dep', 'mobile_wt', 'n_cores', 'pc', 'px_height',
       'px_width', 'ram', 'sc_h', 'sc_w', 'talk_time', 'three_g',
       'touch_screen', 'wifi']"
8785,41062064,33,"#For tree Visualization as kaggle does't support pydotplus just install the pydotplus in your systems's conda terminal
'''
import pydotplus as pydot

from IPython.display import Image

from sklearn.externals.six import StringIO

dot_data = StringIO()

tree.export_graphviz(dtree, out_file=dot_data,feature_names=feature_names)

graph = pydot.graph_from_dot_data(dot_data.getvalue())

Image(graph.create_png())'''"
8786,41062064,34,"#Another way
'''from IPython.display import Image  
from sklearn.externals.six import StringIO  
from sklearn.tree import export_graphviz
import pydot 
import os
os.environ[""PATH""] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'
dot_data = StringIO()  
export_graphviz(dtree, out_file=dot_data,feature_names=feature_names,filled=True)

graph = pydot.graph_from_dot_data(dot_data.getvalue())  
Image(graph[0].create_png())'''  "
8787,41062064,35,"from sklearn.ensemble import RandomForestClassifier
rfc = RandomForestClassifier(n_estimators=200)
rfc.fit(X_train, y_train)"
8788,41062064,36,"rfc.score(X_test,y_test)"
8789,41062064,37,"y_pred=lm.predict(X_test)
"
8790,41062064,38,"plt.scatter(y_test,y_pred)
"
8791,41062064,39,"plt.plot(y_test,y_pred)"
8792,41062064,40,"from sklearn.metrics import classification_report,confusion_matrix"
8793,41062064,41,pred = knn.predict(X_test)
8794,41062064,42,"print(classification_report(y_test,pred))"
8795,41062064,43,"matrix=confusion_matrix(y_test,pred)
print(matrix)"
8796,41062064,44,"plt.figure(figsize = (10,7))
sns.heatmap(matrix,annot=True)
"
8797,41062064,45,data_test=pd.read_csv('../input/mobile-price-range-prediction-is2020-v2/test_data.csv')
8798,41062064,46,data_test.head()
8799,41062064,47,"data_test=data_test.drop('id',axis=1)"
8800,41062064,48,data_test.head()
8801,41062064,49,predicted_price=knn.predict(data_test)
8802,41062064,50,predicted_price
8803,41062064,51,data_test['price_range']=predicted_price
8804,41062064,52,data_test
8805,39999420,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only ""../input/"" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session
import warnings
warnings.filterwarnings('ignore')"
8806,39999420,1,"train=pd.read_csv(""/kaggle/input/mobile-price-range-prediction-is2020-v2/train_data.csv"")
test=pd.read_csv(""/kaggle/input/mobile-price-range-prediction-is2020-v2/test_data.csv"")
submissiond=pd.read_csv(""/kaggle/input/mobile-price-range-prediction-is2020-v2/sample_submission.csv"")"
8807,39999420,2,train.info()
8808,39999420,3,test.info()
8809,39999420,4,"x_train=train.drop(columns=['price_range','id'])
x_test=test.drop(columns=['id'])
y_train=train['price_range']"
8810,39999420,5,"print("" x train:{} y train{}"".format(x_train,y_train))

"
8811,39999420,6,"y_train.value_counts()

"
8812,39999420,7,print(x_test)
8813,39999420,8,"import sklearn 
from sklearn.preprocessing import StandardScaler 
x_trains=StandardScaler().fit_transform(x_train)
x_tests=StandardScaler().fit_transform(x_test)
"
8814,39999420,9,"print(""X trains {}"".format(x_trains))
print(""X tests {}"".format(x_tests))
"
8815,39999420,10,"pd.DataFrame(x_trains).head()
"
8816,39999420,11,pd.DataFrame(x_tests).head()
8817,39999420,12,"from sklearn.linear_model import LogisticRegression as l
#from sklearn.ensemble import RandomForestClassifier as l
LR=l().fit(x_trains,y_train)
y_pred=LR.predict(x_tests)
"
8818,39999420,13,"from sklearn.model_selection import cross_val_score as c
#from sklearn.metrics import classification_report
val=c(l(),x_trains,y_train,cv=3,scoring='accuracy')
print(val)"
8819,39999420,14,print(val.mean())
8820,39999420,15,"result=pd.DataFrame({'id':test['id'],'price_range':y_pred})
result.to_csv('/kaggle/working/result_rf4.csv',index=False)"
8821,41079129,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only ""../input/"" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session
import warnings
warnings.filterwarnings(""ignore"")"
8822,41079129,1,"train=pd.read_csv(""/kaggle/input/mobile-price-range-prediction-is2020-v2/train_data.csv"")
test=pd.read_csv(""/kaggle/input/mobile-price-range-prediction-is2020-v2/test_data.csv"")
sample_submission=pd.read_csv(""/kaggle/input/mobile-price-range-prediction-is2020-v2/sample_submission.csv"")"
8823,41079129,2,train.head()
8824,41079129,3,test.head()
8825,41079129,4,"print(train.shape,test.shape)"
8826,41079129,5,train.info()
8827,41079129,6,test.info()
8828,41079129,7,train.isnull().sum()
8829,41079129,8,test.isnull().sum()
8830,41079129,9,"train_clean =train.drop(columns=[""id""])
test_clean=test.drop(columns=[""id""])"
8831,41079129,10,train_clean
8832,41079129,11,test_clean
8833,41079129,12,"x_train=train_clean.drop(columns=[""price_range""])
y_train=train_clean[""price_range""]
print(y_train.shape)
print(type(y_train))"
8834,41079129,13,"import sklearn
from sklearn.preprocessing import StandardScaler
x_train_std=StandardScaler().fit_transform(x_train)
pd.DataFrame(x_train_std).head()"
8835,41079129,14,"from sklearn.linear_model import LogisticRegression
lr=LogisticRegression().fit(x_train_std,y_train)"
8836,41079129,15,y_pred=lr.predict(test_clean)
8837,41079129,16,"from sklearn.metrics import classification_report
print(classification_report(y_pred,sample_submission[""price_range""]))"
8838,41079129,17,"from sklearn.model_selection import cross_val_score
scores=cross_val_score(LogisticRegression(C=1),x_train_std,y_train,cv=5)
print(scores)
print(scores.mean())"
8839,41079129,18,"from sklearn.model_selection import GridSearchCV
grid={""C"":[0.6,0.7,0.8,0.9,1],""penalty"":[""l1"",""l2""]}
grid"
8840,41079129,19,"score=GridSearchCV(LogisticRegression(solver='liblinear'),grid).fit(x_train_std,y_train)
print(score.best_params_)
print(score.best_score_)"
8841,41079129,20,test_clean_std= StandardScaler().fit_transform(test_clean)
8842,41079129,21,"lr=LogisticRegression(solver=""liblinear"",penalty='l1',C=0.9).fit(x_train_std,y_train)
y_pred=lr.predict(test_clean_std)"
8843,41079129,22,lr.predict_proba(test_clean_std)
8844,41079129,23,"final_output = test.assign(price_range = y_pred)[['id','price_range']]"
8845,41079129,24,final_output
8846,41079129,25,"result=pd.DataFrame(final_output)
result.to_csv(""/kaggle/working/result.lr.csv"",index=False)"
8847,41079129,26,"from sklearn.ensemble import RandomForestClassifier
RF=RandomForestClassifier(criterion='gini',n_estimators=25,random_state=0)
RF=RF.fit(x_train_std,y_train)
y_pred_ref=RF.predict(test_clean)
from sklearn.metrics import classification_report
scores=cross_val_score(RandomForestClassifier(),x_train,y_train,cv=5)
print(scores)
print(scores.mean())"
8848,41079129,27,"model=RandomForestClassifier().fit(x_train,y_train)
y_pred_RF=model.predict(test_clean)
final_output_RF = test.assign(price_range = y_pred_RF)[['id','price_range']]
final_output_RF"
8849,41079129,28,"result1=pd.DataFrame(final_output_RF)
result1.to_csv(""/kaggle/working/result.rf.csv"",index=False)"
8850,41079129,29,"from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import cross_val_score
dtc=DecisionTreeClassifier(criterion=""gini"",max_depth=4,random_state=0)
scores=cross_val_score(dtc,x_train,y_train,cv=5)
print(scores)
print(scores.mean())"
8851,41079129,30,"model1=dtc.fit(x_train,y_train)
y_pred_dtc=model1.predict(test_clean)
final_output_dtc = test.assign(price_range = y_pred_dtc)[['id','price_range']]
final_output_dtc"
8852,41079129,31,"result2=pd.DataFrame(final_output_dtc)
result2.to_csv(""/kaggle/working/result.dtc.csv"",index=False)"
8853,41079129,32,"from sklearn.svm import SVC
from sklearn.model_selection import cross_val_score
svc=SVC(kernel='linear',C=1,random_state=0)
scores=cross_val_score(svc,x_train_std,y_train,cv=5)
print(scores)
print(scores.mean())"
8854,41079129,33,"model2=svc.fit(x_train_std,y_train)
y_pred_svm=model2.predict(test_clean_std)
final_output_svm = test.assign(price_range = y_pred_svm)[['id','price_range']]
final_output_svm"
8855,41079129,34,"result3=pd.DataFrame(final_output_svm)
result3.to_csv(""/kaggle/working/result.svm.csv"",index=False)"
8856,41079129,35,"from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import cross_val_score
knn = KNeighborsClassifier(n_neighbors=5,metric='minkowski',p=2)
knn=knn.fit(x_train_std,y_train)
scores=cross_val_score(knn,x_train_std,y_train,cv=5)
print(scores)
print(scores.mean())"
8857,41079129,36,"model3=knn.fit(x_train_std,y_train)
y_pred_knn=model3.predict(test_clean_std)
final_output_knn = test.assign(price_range = y_pred_knn)[['id','price_range']]
final_output_knn"
8858,41079129,37,"result4=pd.DataFrame(final_output_knn)
result4.to_csv(""/kaggle/working/result.knn.csv"",index=False)"
8859,41079129,38,"from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
nb=GaussianNB()
nb=nb.fit(x_train_std,y_train)
scores=cross_val_score(svc,x_train_std,y_train,cv=5)
print(scores)
print(scores.mean())"
8860,41079129,39,"model4=nb.fit(x_train_std,y_train)
y_pred_nb=model4.predict(test_clean_std)
final_output_nb = test.assign(price_range = y_pred_nb)[['id','price_range']]
final_output_nb"
8861,41079129,40,"result5=pd.DataFrame(final_output_nb)
result5.to_csv(""/kaggle/working/result.nb.csv"",index=False)"
8862,41952353,0,"train = pd.read_csv('/kaggle/input/mobile-price-range-prediction-is2020-v2/train_data.csv')
test = pd.read_csv('/kaggle/input/mobile-price-range-prediction-is2020-v2/test_data.csv')
submissions = pd.read_csv('/kaggle/input/mobile-price-range-prediction-is2020-v2/sample_submission.csv')

train_clean = train.drop(columns=['id'])
test_clean = test.drop(columns=['id'])

x_train = train_clean.drop(columns=['price_range'])
y_train = train_clean[['price_range']]

x_test = test_clean

from sklearn.preprocessing import StandardScaler
x_train_scale = StandardScaler().fit_transform(x_train)

x_test_scale = StandardScaler().fit_transform(x_test)


from sklearn.model_selection import cross_val_score

y = np.array(y_train)
y = y.ravel()

from sklearn.linear_model import LogisticRegression
model = LogisticRegression()

from sklearn.model_selection import StratifiedKFold

accuracy = []
skf = StratifiedKFold(n_splits=15, random_state=None)
skf.get_n_splits(x_train_scale, y)

for train_index, test_index in skf.split(x_train_scale, y):
  x1_train, x1_test = x_train_scale[train_index], x_train_scale[test_index]
  y1_train, y1_test = y[train_index], y[test_index]

  model.fit(x1_train, y1_train)  
  score = model.score(x1_test, y1_test)
  accuracy.append(score)

print(accuracy)


y_pred = model.predict(x_test_scale)

from sklearn.metrics import classification_report
print(classification_report(y_pred, submissions['price_range']))

data = {'id':submissions['id'],
       'price_range':y_pred}
results = pd.DataFrame(data)
results.to_csv('/kaggle/working/results_lr.csv', index=False)"
8863,41952353,1,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only ""../input/"" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
8864,44285437,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only ""../input/"" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
8865,44285437,1,"train=pd.read_csv('/kaggle/input/mobile-price-range-prediction-is2020-v2/train_data.csv')
test=pd.read_csv('/kaggle/input/mobile-price-range-prediction-is2020-v2/test_data.csv')
price_sub=pd.read_csv('/kaggle/input/mobile-price-range-prediction-is2020-v2/sample_submission.csv')"
8866,44285437,2,train.head()
8867,44285437,3,test.head()
8868,44285437,4,train.info()
8869,44285437,5,test.info()
8870,44285437,6,train.isnull().sum()
8871,44285437,7,test.isnull().sum()
8872,44285437,8,train.describe()
8873,44285437,9,"import seaborn as sns
sns.pointplot(y=""talk_time"",x=""price_range"",data=train)"
8874,44285437,10,"sns.pointplot(y=""battery_power"",x=""price_range"",data=train)"
8875,44285437,11,"sns.pointplot(y=""ram"",x=""price_range"",data=train)"
8876,44285437,12,"sns.pointplot(y=""int_memory"",x=""price_range"",data=train)"
8877,44285437,13,"sns.pointplot(y=""mobile_wt"",x=""price_range"",data=train)"
8878,44285437,14,"sns.pointplot(y=""dual_sim"",x=""price_range"",data=train)"
8879,44285437,15,"sns.pointplot(y=""four_g"",x=""price_range"",data=train)"
8880,44285437,16,"sns.pointplot(y=""three_g"",x=""price_range"",data=train)"
8881,44285437,17,train_clean=train.drop(columns=['id'])
8882,44285437,18,"x_train=train_clean.drop(columns=['price_range'])
y_train=train_clean['price_range']"
8883,44285437,19,data_test=test.drop(columns=['id'])
8884,44285437,20,data_test.head()
8885,44285437,21,"import matplotlib.pyplot as plt
plt.figure(figsize=(10,6))
train['fc'].hist(alpha=0.5,color='blue',label='Front camera')

plt.legend()
plt.xlabel('MegaPixels')"
8886,44285437,22,"import matplotlib.pyplot as plt
plt.figure(figsize=(10,6))

train['pc'].hist(alpha=0.5,color='red',label='Primary camera')
plt.legend()
plt.xlabel('MegaPixels')"
8887,44285437,23,"plt.figure(figsize=(10,6))
train['fc'].hist(alpha=0.5,color='blue',label='Front camera')
train['pc'].hist(alpha=0.5,color='red',label='Primary camera')
plt.legend()
plt.xlabel('MegaPixels')"
8888,44285437,24,"sns.pointplot(y=""dual_sim"",x=""price_range"",data=train)"
8889,44285437,25,"sns.pointplot(y=""four_g"",x=""price_range"",data=train)"
8890,44285437,26,train_clean=train.drop(columns=['id'])
8891,44285437,27,data_test.head()
8892,44285437,28,"from sklearn.preprocessing import StandardScaler
x_train_scales=StandardScaler().fit_transform(x_train)
pd.DataFrame(x_train_scales).head()
"
8893,44285437,29,x_train.head()
8894,44285437,30,y_train.head()
8895,44285437,31,"from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import cross_val_score
knn=KNeighborsClassifier(n_neighbors=10).fit(x_train,y_train)
scores=cross_val_score(knn,x_train_scales,y_train,cv=5)
print(scores)
print(scores.mean())"
8896,44285437,32,"from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import cross_val_score
dtc=DecisionTreeClassifier().fit(x_train,y_train)
scores=cross_val_score(dtc,x_train_scales,y_train,cv=5)
print(scores)
print(scores.mean())"
8897,44285437,33,"price_pred_knn=knn.predict(data_test)
print(price_pred_knn)"
8898,44285437,34,"price_pred_dtc=dtc.predict(data_test)
print(price_pred_dtc)"
8899,44285437,35,"data = {'id':price_sub['id'],'price_range':price_pred_knn}
results_knn = pd.DataFrame(data)
results_knn.to_csv('/kaggle/working/results_knn.csv', index=False)"
8900,44285437,36,"data = {'id':price_sub['id'],'price_range':price_pred_dtc}
results_dtc = pd.DataFrame(data)
results_dtc.to_csv('/kaggle/working/results_dtc.csv', index=False)"
8901,41038003,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only ""../input/"" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
8902,41038003,1,"import pandas as pd
import numpy as np"
8903,41038003,2,"train=pd.read_csv(""/kaggle/input/mobile-price-range-prediction-is2020-v2/train_data.csv"")
test=pd.read_csv(""/kaggle/input/mobile-price-range-prediction-is2020-v2/test_data.csv"")
submission=pd.read_csv(""/kaggle/input/mobile-price-range-prediction-is2020-v2/sample_submission.csv"")"
8904,41038003,3,train.head()
8905,41038003,4,train.columns
8906,41038003,5,train.info()
8907,41038003,6,"y = train['price_range']
x = train.drop('price_range', axis = 1)"
8908,41038003,7,y.unique()
8909,41038003,8,"from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test= train_test_split(x, y, test_size = 0.2, random_state = 101)"
8910,41038003,9,"print(x_train.shape)
print(x_test.shape)"
8911,41038003,10,"import matplotlib.pyplot as plt
import seaborn as sns"
8912,41038003,11,"plt.figure(figsize=(12,12))
sns.heatmap(train.corr(),annot=True, cmap=""GnBu"")
plt.show()"
8913,41038003,12,from sklearn.linear_model import LogisticRegression
8914,41038003,13,"lr =LogisticRegression()
lr.fit(x_train,y_train)
y_pred_lr=lr.predict(x_test)
y_pred_lr"
8915,41038003,14,"from sklearn import metrics
from sklearn.metrics import confusion_matrix, classification_report 
from sklearn.metrics import accuracy_score"
8916,41038003,15,"confusion_matrix = metrics.confusion_matrix(y_test, y_pred_lr)
confusion_matrix"
8917,41038003,16,"print(metrics.classification_report(y_test, y_pred_lr))"
8918,41038003,17,"acc_lr = metrics.accuracy_score(y_test, y_pred_lr)
acc_lr"
8919,41038003,18,from sklearn.tree import DecisionTreeClassifier
8920,41038003,19,"dt = DecisionTreeClassifier(random_state=101)
dt.fit(x_train, y_train)
y_pred_dt = dt.predict(x_test)"
8921,41038003,20,"confusion_matrix = metrics.confusion_matrix(y_test, y_pred_dt)
confusion_matrix"
8922,41038003,21,"print(metrics.classification_report(y_test, y_pred_dt))"
8923,41038003,22,"acc_dt = metrics.accuracy_score(y_test, y_pred_dt)
acc_dt"
8924,41038003,23,from sklearn.ensemble import RandomForestClassifier
8925,41038003,24,"rf = DecisionTreeClassifier(random_state=0)
rf.fit(x_train, y_train)
y_pred_rf = rf.predict(x_test)"
8926,41038003,25,"confusion_matrix = metrics.confusion_matrix(y_test, y_pred_rf)
confusion_matrix"
8927,41038003,26,"print(metrics.classification_report(y_test, y_pred_rf))"
8928,41038003,27,"acc_rf = metrics.accuracy_score(y_test, y_pred_rf)
acc_rf"
8929,41038003,28,from sklearn.svm import SVC
8930,41038003,29,"svc = SVC()
svc.fit(x_train, y_train)
y_pred_svc = svc.predict(x_test)"
8931,41038003,30,"confusion_matrix = metrics.confusion_matrix(y_test, y_pred_svc)
confusion_matrix"
8932,41038003,31,"print(metrics.classification_report(y_test, y_pred_svc))"
8933,41038003,32,"acc_svc = metrics.accuracy_score(y_test, y_pred_svc)
acc_svc"
8934,41038003,33,"from sklearn.neighbors import KNeighborsClassifier
model_knn = KNeighborsClassifier(n_neighbors=3)  
model_knn.fit(x_train, y_train)"
8935,41038003,34,y_pred_knn = model_knn.predict(x_test)
8936,41038003,35,"print(metrics.confusion_matrix(y_test, y_pred_knn))"
8937,41038003,36,"print(accuracy_score(y_test, y_pred_knn))"
8938,41038003,37,"from sklearn.model_selection import GridSearchCV
parameters = {'n_neighbors':np.arange(1,30)}
knn = KNeighborsClassifier()

model = GridSearchCV(knn, parameters, cv=5)
model.fit(x_train, y_train)
model.best_params_"
8939,41038003,38,"knn = KNeighborsClassifier(n_neighbors=26)
knn.fit(x_train, y_train)
y_pred_knn = knn.predict(x_test)"
8940,41038003,39,"confusion_matrix = metrics.confusion_matrix(y_test, y_pred_knn)
confusion_matrix"
8941,41038003,40,"print(metrics.classification_report(y_test, y_pred_knn))"
8942,41038003,41,"acc_knn = metrics.accuracy_score(y_test, y_pred_knn)
acc_knn"
8943,41038003,42,"models = ['logistic regression', 'decision tree', 'random forest', 'support vector machine','knn']
acc_scores = [acc_lr,acc_dt,acc_rf,acc_svc,acc_knn]
print(acc_scores)
plt.bar(models, acc_scores, color=['lightblue', 'pink', 'lightgrey', 'cyan','lightgreen'])
plt.ylabel(""accuracy scores"")
plt.title(""Which model is the most accurate?"")
plt.show()"
8944,41038003,43,"predicted_price=svc.predict(test)
predicted_price"
8945,41038003,44,test['price_range']=predicted_price
8946,41038003,45,"data={'Id':test['id'],'price_range':predicted_price}
result=pd.DataFrame(data)
result.to_csv('/kaggle/working/prediction.csv',index=False)"
8947,44284275,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only ""../input/"" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
8948,44284275,1,"train=pd.read_csv('/kaggle/input/mobile-price-range-prediction-is2020-v2/train_data.csv')
test=pd.read_csv('/kaggle/input/mobile-price-range-prediction-is2020-v2/test_data.csv')
price_sub=pd.read_csv('/kaggle/input/mobile-price-range-prediction-is2020-v2/sample_submission.csv')"
8949,44284275,2,train.head()
8950,44284275,3,test.head()
8951,44284275,4,train.info()
8952,44284275,5,test.info()
8953,44284275,6,test.isnull().sum()
8954,44284275,7,train.describe()
8955,44284275,8,"import seaborn as sns
sns.pointplot(y=""talk_time"",x=""price_range"",data=train)"
8956,44284275,9,"sns.pointplot(y=""battery_power"",x=""price_range"",data=train)"
8957,44284275,10,"sns.pointplot(y=""ram"",x=""price_range"",data=train)"
8958,44284275,11,"sns.pointplot(y=""int_memory"",x=""price_range"",data=train)"
8959,44284275,12,"sns.pointplot(y=""mobile_wt"",x=""price_range"",data=train)"
8960,44284275,13,"sns.pointplot(y=""dual_sim"",x=""price_range"",data=train)"
8961,44284275,14,"sns.pointplot(y=""four_g"",x=""price_range"",data=train)"
8962,44284275,15,"sns.pointplot(y=""three_g"",x=""price_range"",data=train)"
8963,44284275,16,train_clean=train.drop(columns=['id'])
8964,44284275,17,"x_train=train_clean.drop(columns=['price_range'])
y_train=train_clean['price_range']"
8965,44284275,18,data_test=test.drop(columns=['id'])
8966,44284275,19,data_test.head()
8967,44284275,20,"import matplotlib.pyplot as plt
plt.figure(figsize=(10,6))
train['fc'].hist(alpha=0.5,color='blue',label='Front camera')

plt.legend()
plt.xlabel('MegaPixels')"
8968,44284275,21,"import matplotlib.pyplot as plt
plt.figure(figsize=(10,6))

train['pc'].hist(alpha=0.5,color='red',label='Primary camera')
plt.legend()
plt.xlabel('MegaPixels')"
8969,44284275,22,"plt.figure(figsize=(10,6))
train['fc'].hist(alpha=0.5,color='blue',label='Front camera')
train['pc'].hist(alpha=0.5,color='red',label='Primary camera')
plt.legend()
plt.xlabel('MegaPixels')

"
8970,44284275,23,"sns.pointplot(y=""dual_sim"",x=""price_range"",data=train)"
8971,44284275,24,"sns.pointplot(y=""four_g"",x=""price_range"",data=train)"
8972,44284275,25,data_test.head()
8973,44284275,26,"from sklearn.preprocessing import StandardScaler
x_train_scales=StandardScaler().fit_transform(x_train)
pd.DataFrame(x_train_scales).head()"
8974,44284275,27,x_train.head()
8975,44284275,28,"from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import cross_val_score
knn = KNeighborsClassifier(n_neighbors=10).fit(x_train,y_train)
scores=cross_val_score(knn,x_train_scales,y_train,cv=5)
print(scores)
print(scores.mean())"
8976,44284275,29,"from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import cross_val_score
dtc = DecisionTreeClassifier().fit(x_train,y_train)
scores=cross_val_score(dtc,x_train_scales,y_train,cv=5)
print(scores)
print(scores.mean())"
8977,44284275,30,"price_pred_dtc=dtc.predict(data_test)
print(price_pred_dtc)"
8978,44284275,31,"price_pred_knn=knn.predict(data_test)
print(price_pred_knn)"
8979,44284275,32,"data={'id':price_sub['id'],'price_range':price_pred_knn}
result_knn=pd.DataFrame(data)
result_knn.to_csv('/kaggle/working/result_knn.csv',index=False)
"
8980,44284275,33,"data={'id':price_sub['id'],'price_range':price_pred_dtc}
result_dtc=pd.DataFrame(data)
result_dtc.to_csv('/kaggle/working/result_dtc.csv',index=False)
"
8981,41954265,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only ""../input/"" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
8982,41954265,1,"import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline"
8983,41954265,2,"dataset=pd.read_csv('../input/mobile-price-range-prediction-is2020-v2/train_data.csv')
dataset=dataset.drop('id',axis=1)
dataset.info()
dataset.describe()"
8984,41954265,3,"sns.jointplot(x='ram',y='price_range',data=dataset,kind='kde');"
8985,41954265,4,"sns.pointplot(y=""int_memory"", x=""price_range"", data=dataset)"
8986,41954265,5,"labels = [""3G-supported"",'Not supported']
values=dataset['three_g'].value_counts().values"
8987,41954265,6,"fig1, ax1 = plt.subplots()
ax1.pie(values, labels=labels, autopct='%1.1f%%',shadow=True,startangle=90)
plt.show()"
8988,41954265,7,"labels4g = [""4G-supported"",'Not supported']
values4g = dataset['four_g'].value_counts().values
fig1, ax1 = plt.subplots()
ax1.pie(values4g, labels=labels4g, autopct='%1.1f%%',shadow=True,startangle=90)
plt.show()"
8989,41954265,8,"sns.boxplot(x=""price_range"", y=""battery_power"", data=dataset)"
8990,41954265,9,"plt.figure(figsize=(10,6))
dataset['fc'].hist(alpha=0.5,color='blue',label='Front camera')
dataset['pc'].hist(alpha=0.5,color='red',label='Primary camera')
plt.legend()
plt.xlabel('MegaPixels')"
8991,41954265,10,"sns.jointplot(x='mobile_wt',y='price_range',data=dataset,kind='kde');"
8992,41954265,11,"sns.pointplot(y=""talk_time"", x=""price_range"", data=dataset)"
8993,41954265,12,"X=dataset.drop('price_range',axis=1)"
8994,41954265,13,y=dataset['price_range']
8995,41954265,14,"from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=101)"
8996,41954265,15,"from sklearn.linear_model import LinearRegression
lm = LinearRegression()
lm.fit(X_train,y_train)
lm.score(X_test,y_test)"
8997,41954265,16,"from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=10)
knn.fit(X_train,y_train)"
8998,41954265,17,"knn.score(X_test,y_test)"
8999,41954265,18,"error_rate = []
for i in range(1,20):
    
    knn = KNeighborsClassifier(n_neighbors=i)
    knn.fit(X_train,y_train)
    pred_i = knn.predict(X_test)
    error_rate.append(np.mean(pred_i != y_test))
"
9000,41954265,19,"plt.figure(figsize=(10,6))
plt.plot(range(1,20),error_rate,color='blue', linestyle='dashed', marker='o',
         markerfacecolor='red', markersize=5)
plt.title('Error Rate vs. K Value')
plt.xlabel('K')
plt.ylabel('Error Rate')"
9001,41954265,20,"from sklearn.linear_model import LogisticRegression
logmodel = LogisticRegression()
"
9002,41954265,21,"logmodel.fit(X_train,y_train)"
9003,41954265,22,"logmodel.score(X_test,y_test)"
9004,41954265,23,"from sklearn.tree import DecisionTreeClassifier
dtree = DecisionTreeClassifier()"
9005,41954265,24,"dtree.fit(X_train,y_train)"
9006,41954265,25,"dtree.score(X_test,y_test)"
9007,41954265,26,"feature_names=['battery_power', 'blue', 'clock_speed', 'dual_sim', 'fc', 'four_g',
       'int_memory', 'm_dep', 'mobile_wt', 'n_cores', 'pc', 'px_height',
       'px_width', 'ram', 'sc_h', 'sc_w', 'talk_time', 'three_g',
       'touch_screen', 'wifi']"
9008,41954265,27,"from sklearn.ensemble import RandomForestClassifier
rfc = RandomForestClassifier(n_estimators=200)
rfc.fit(X_train, y_train)"
9009,41954265,28,"rfc.score(X_test,y_test)"
9010,41954265,29,y_pred=lm.predict(X_test)
9011,41954265,30,"plt.scatter(y_test,y_pred)"
9012,41954265,31,"plt.plot(y_test,y_pred)"
9013,41954265,32,"from sklearn.metrics import classification_report,confusion_matrix"
9014,41954265,33,pred = knn.predict(X_test)
9015,41954265,34,"matrix=confusion_matrix(y_test,pred)
plt.figure(figsize = (10,7))
sns.heatmap(matrix,annot=True)
"
9016,41954265,35,"data_test=pd.read_csv('../input/mobile-price-range-prediction-is2020-v2/test_data.csv')
data_test.head()"
9017,41954265,36,"data_test=data_test.drop('id',axis=1)"
9018,41954265,37,predicted_price_range=knn.predict(data_test)
9019,41954265,38,predicted_price_range
9020,41954265,39,data_test['price_range']=predicted_price
9021,41954265,40,data_test
9022,41954265,41,
9023,41954265,42,
9024,39858539,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only ""../input/"" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
9025,39858539,1,"test=pd.read_csv('/kaggle/input//mobile-price-range-prediction-is2020-v2/test_data.csv')
train=pd.read_csv('/kaggle/input//mobile-price-range-prediction-is2020-v2/train_data.csv')
sample_submission=pd.read_csv('/kaggle/input//mobile-price-range-prediction-is2020-v2/sample_submission.csv')"
9026,39858539,2,test.head()
9027,39858539,3,train.head()
9028,39858539,4,train.shape
9029,39858539,5,train.describe()
9030,39858539,6,"y=train['price_range']
x=train.drop('price_range',axis=1)
"
9031,39858539,7,y.unique()
9032,39858539,8,"import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns"
9033,39858539,9,"from sklearn.model_selection import train_test_split

x_train, x_valid, y_train, y_valid = train_test_split(x, y, test_size = 0.01, random_state = 101, stratify = y)

#x_train, x_valid, y_train, y_valid = train_test_split(x, y, test_size = 0.2, random_state = 101, stratify = y)"
9034,39858539,10,"print(x_train.shape)
print(x_valid.shape)"
9035,39858539,11,"fig = plt.subplots (figsize = (12, 12))
sns.heatmap(train.corr (), square = True, cbar = True, annot = True, cmap=""GnBu"", annot_kws = {'size': 8})
plt.title('Correlations between Attributes')
plt.show ()"
9036,39858539,12,"from sklearn.linear_model import LogisticRegression

lr = LogisticRegression(multi_class = 'multinomial', solver = 'sag',  max_iter = 10000)
lr.fit(x_train, y_train)"
9037,39858539,13,y_pred_lr = lr.predict(x_valid)
9038,39858539,14,"from sklearn import metrics
from sklearn.metrics import accuracy_score
confusion_matrix = metrics.confusion_matrix(y_valid, y_pred_lr)
confusion_matrix"
9039,39858539,15,"acc_lr = metrics.accuracy_score(y_valid, y_pred_lr)
acc_lr"
9040,39858539,16,"from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import mean_squared_error
import math
dt = DecisionTreeClassifier(random_state=101)
dt_model = dt.fit(x_train, y_train)"
9041,39858539,17,y_pred_dt = dt.predict(x_valid)
9042,39858539,18,"
dt_model"
9043,39858539,19,"
print(metrics.confusion_matrix(y_valid, y_pred_dt))"
9044,39858539,20,"print(metrics.classification_report(y_valid, y_pred_dt))"
9045,39858539,21,"acc_dt = metrics.accuracy_score(y_valid, y_pred_dt)
acc_dt"
9046,39858539,22,"from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
rf = RandomForestClassifier(n_estimators = 100, random_state=101, criterion = 'entropy', oob_score = True) 
model_rf = rf.fit(x_train, y_train)"
9047,39858539,23,y_pred_rf = rf.predict(x_valid)
9048,39858539,24,"print(metrics.confusion_matrix(y_valid, y_pred_rf))"
9049,39858539,25,"pd.crosstab(y_valid, y_pred_rf, rownames=['Actual Class'], colnames=['Predicted Class'])"
9050,39858539,26,"acc_rf = metrics.accuracy_score(y_valid, y_pred_rf)
acc_rf"
9051,39858539,27,"model_knn = KNeighborsClassifier(n_neighbors=3)  
model_knn.fit(x_train, y_train)"
9052,39858539,28,y_pred_knn = model_knn.predict(x_valid)
9053,39858539,29,"print(metrics.confusion_matrix(y_valid, y_pred_knn))"
9054,39858539,30,"print(accuracy_score(y_valid, y_pred_knn))"
9055,39858539,31,"from sklearn.model_selection import GridSearchCV
parameters = {'n_neighbors':np.arange(1,30)}
knn = KNeighborsClassifier()

model = GridSearchCV(knn, parameters, cv=5)
model.fit(x_train, y_train)
model.best_params_"
9056,39858539,32,"model_knn = KNeighborsClassifier(n_neighbors=9)  
model_knn.fit(x_train, y_train)"
9057,39858539,33,y_pred_knn = model_knn.predict(x_valid)
9058,39858539,34,"print(metrics.confusion_matrix(y_valid, y_pred_knn))"
9059,39858539,35,"acc_knn = accuracy_score(y_valid, y_pred_knn)
acc_knn"
9060,39858539,36,"models = ['logistic regression', 'decision tree', 'random forest', 'knn']
acc_scores = [0.73, 0.83, 0.90, 0.95]

plt.bar(models, acc_scores, color=['lightblue', 'pink', 'lightgrey', 'cyan'])
plt.ylabel(""accuracy scores"")
plt.title(""Which model is the most accurate?"")
plt.show()"
9061,39858539,37,test.head()
9062,39858539,38,predicted_price_range = model_knn.predict(test)
9063,39858539,39,predicted_price_range 
9064,39858539,40,train.head()
9065,39858539,41,"data={'id':sample_submission['id'],
     'price_range':predicted_price_range}
result=pd.DataFrame(data)
result.to_csv(""/kaggle/working/result_12.csv"",index=False)
output=pd.read_csv('/kaggle/working/result_12.csv')"
9066,43372368,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only ""../input/"" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session
import warnings
warnings.filterwarnings('ignore')"
9067,43372368,1,"train_data=pd.read_csv('../input/mobile-price-range-prediction-is2020-v2/train_data.csv')
train_data.head()"
9068,43372368,2,"X=train_data.drop('price_range',axis=1).values
Y=train_data['price_range']"
9069,43372368,3,"from sklearn.model_selection import train_test_split
df_test=pd.read_csv('../input/mobile-price-prediction-is2020-v2/test_data.csv')
id_test=df_test['id']
X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.5,random_state=101)"
9070,43372368,4,"from sklearn.neighbors import KNeighborsClassifier
kn=KNeighborsClassifier(n_neighbors=10)"
9071,43372368,5,"import numpy as np
error_rate=[]
for i in range(1,20):
    kn=KNeighborsClassifier(n_neighbors=i)
    kn.fit(X_train,Y_train)
    pred_i=kn.predict(X_test)
    error_rate.append(np.mean(pred_i!=Y_test))"
9072,43372368,6,"from sklearn.linear_model import LogisticRegression
logmodel=LogisticRegression()
logmodel.fit(X_train,Y_train)
Y_pred=logmodel.predict(X_test)"
9073,43372368,7,"from sklearn.metrics import classification_report,confusion_matrix
pred=kn.predict(X_test)

data_test=pd.read_csv('../input/mobile-price-range-prediction-is2020-v2/test_data.csv')
predicted_price=kn.predict(data_test)
data_test['price_range']=predicted_price

data_test=pd.DataFrame({'id':id_test,'price_range':predicted_price})
data_test.to_csv('output.csv',index=False)
data_test

"
9074,43430221,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only ""../input/"" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session
import warnings
warnings.filterwarnings('ignore')
"
9075,43430221,1,"train_data=pd.read_csv('../input/mobile-price-range-prediction-is2020-v2/train_data.csv')
train_data.head()
"
9076,43430221,2,train_data.info()
9077,43430221,3,"X=train_data.drop('price_range',axis=1).values
Y=train_data['price_range']"
9078,43430221,4,"from sklearn.model_selection import train_test_split
df_test=pd.read_csv('../input/mobile-price-range-prediction-is2020-v2/test_data.csv')
id_test=df_test['id']
X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.5,random_state=101)"
9079,43430221,5,"from sklearn.neighbors import KNeighborsClassifier
kn=KNeighborsClassifier(n_neighbors=10)"
9080,43430221,6,"import numpy as np
error_rate=[]
for i in range(1,20):
    kn=KNeighborsClassifier(n_neighbors=i)
    kn.fit(X_train,Y_train)
    pred_i=kn.predict(X_test)
    error_rate.append(np.mean(pred_i!=Y_test))"
9081,43430221,7,"from sklearn.linear_model import LogisticRegression
logmodel=LogisticRegression()
logmodel.fit(X_train,Y_train)
Y_pred=logmodel.predict(X_test)"
9082,43430221,8,"from sklearn.metrics import classification_report,confusion_matrix
pred=kn.predict(X_test)

data_test=pd.read_csv('../input/mobile-price-range-prediction-is2020-v2/test_data.csv')
predicted_price=kn.predict(data_test)
data_test['price_range']=predicted_price

data_test=pd.DataFrame({'id':id_test,'price_range':predicted_price})
data_test.to_csv('output.csv',index=False)
data_test
"
9083,43431036,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only ""../input/"" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session
import warnings
warnings.filterwarnings('ignore')"
9084,43431036,1,"train_data=pd.read_csv('../input/mobile-price-range-prediction-is2020-v2/train_data.csv')
train_data.head()"
9085,43431036,2,train_data.info()
9086,43431036,3,"X=train_data.drop('price_range',axis=1).values
Y=train_data['price_range']"
9087,43431036,4,"from sklearn.model_selection import train_test_split
df_test=pd.read_csv('../input/mobile-price-range-prediction-is2020-v2/test_data.csv')
id_test=df_test['id']
X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.5,random_state=101)"
9088,43431036,5,"from sklearn.neighbors import KNeighborsClassifier
kn=KNeighborsClassifier(n_neighbors=10)"
9089,43431036,6,"import numpy as np
error_rate=[]
for i in range(1,20):
    kn=KNeighborsClassifier(n_neighbors=i)
    kn.fit(X_train,Y_train)
    pred_i=kn.predict(X_test)
    error_rate.append(np.mean(pred_i!=Y_test))
    "
9090,43431036,7,"from sklearn.linear_model import LogisticRegression
logmodel=LogisticRegression()
logmodel.fit(X_train,Y_train)
Y_pred=logmodel.predict(X_test)"
9091,43431036,8,"from sklearn.metrics import classification_report,confusion_matrix
pred=kn.predict(X_test)
data_test=pd.read_csv('../input/mobile-price-range-prediction-is2020-v2/test_data.csv')
predicted_price=kn.predict(data_test)
data_test['price_range']=predicted_price

data_test=pd.DataFrame({'id':id_test,'price_range':predicted_price})
data_test.to_csv('output.csv',index=False)
data_test"
9092,43474743,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only ""../input/"" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
9093,43474743,1,"train_data=pd.read_csv('../input/mobile-price-range-prediction-is2020-v2/train_data.csv')
train_data.head()"
9094,43474743,2,"X=train_data.drop('price_range',axis=1).values
Y=train_data['price_range']"
9095,43474743,3,"from sklearn.model_selection import train_test_split
df_test=pd.read_csv('../input/mobile-price-range-prediction-is2020-v2/test_data.csv')
id_test=df_test['id']
X_train,X_test,Y_train,Y_test=train_test_split(X,Y, test_size=0.5,random_state=101)"
9096,43474743,4,"from sklearn.neighbors import KNeighborsClassifier
kn=KNeighborsClassifier(n_neighbors=10)"
9097,43474743,5,"import numpy as np
error_rate=[]
for i in range(1,20):
    kn=KNeighborsClassifier(n_neighbors=i)
    kn.fit(X_train,Y_train)
    pred_i=kn.predict(X_test)
    error_rate.append(np.mean(pred_i!=Y_test))"
9098,43474743,6,"from sklearn.linear_model import LogisticRegression
logmodel=LogisticRegression()
logmodel.fit(X_train,Y_train)"
9099,43474743,7,"from sklearn.metrics import classification_report,confusion_matrix
pred=kn.predict(X_test)

data_test=pd.read_csv('../input/mobile-price-range-prediction-is2020-v2/test_data.csv')
predicted_price=kn.predict(data_test)
data_test['price_range']=predicted_price

data_test=pd.DataFrame({'id':id_test,'price_range':predicted_price})
data_test.to_csv('output.csv',index=False)
data_test"
9100,43443958,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only ""../input/"" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
9101,43443958,1,"import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)



import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))"
9102,43443958,2,"train=pd.read_csv('/kaggle/input/mobile-price-range-prediction-is2020-v2/train_data.csv')
test=pd.read_csv('/kaggle/input/mobile-price-range-prediction-is2020-v2/test_data.csv')
sample_submission=pd.read_csv('/kaggle/input/mobile-price-range-prediction-is2020-v2/sample_submission.csv')"
9103,43443958,3,train.head()
9104,43443958,4,test.head()
9105,43443958,5,"print(train.shape, test.shape)"
9106,43443958,6,train.info()
9107,43443958,7,test.info()
9108,43443958,8,train.isnull().sum()
9109,43443958,9,"import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns"
9110,43443958,10,"from sklearn.svm import svm as svm
regr = SVR(C=1.0, epsilon=0.2)
regr.fit(X,y)"
9111,43443958,11,SVR(epsilon=0.2)
9112,43443958,12,"X=train.drop('price_range',axis=1).values
Y=train['price_range']"
9113,43443958,13,"from sklearn.model_selection import train_test_split
df_test=pd.read_csv('../input/mobile-price-range-prediction-is2020-v2/test_data.csv')
id_test=df_test['id']
X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.5,random_state=101)
"
9114,43443958,14,"from sklearn.neighbors import KNeighborsClassifier
kn=KNeighborsClassifier(n_neighbors=10)
"
9115,43443958,15,"import numpy as np
error_rate=[]
for i in range(1,20):
    kn=KNeighborsClassifier(n_neighbors=i)
    kn.fit(X_train,Y_train)
    pred_i=kn.predict(X_test)
    error_rate.append(np.mean(pred_i!=Y_test))
    "
9116,43443958,16,"from sklearn.linear_model import LogisticRegression
logmodel=LogisticRegression()
logmodel.fit(X_train,Y_train)
Y_pred=logmodel.predict(X_test)
"
9117,43443958,17,"from sklearn.metrics import classification_report,confusion_matrix
pred=kn.predict(X_test)
data_test=pd.read_csv('../input/mobile-price-range-prediction-is2020-v2/test_data.csv')
predicted_price=kn.predict(data_test)
data_test['price_range']=predicted_price
"
9118,43443958,18,"data_test=pd.DataFrame({'price_range':predicted_price})
data_test.to_csv('output.csv',index=False)
data_test"
9119,44229684,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only ""../input/"" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
9120,44229684,1,"trainData=pd.read_csv(""/kaggle/input/mobile-price-range-prediction-is2020-v2/train_data.csv"")
trainData"
9121,44229684,2,"testData=pd.read_csv(""/kaggle/input/mobile-price-range-prediction-is2020-v2/test_data.csv"")
testData"
9122,44229684,3,"submissionData=pd.read_csv(""/kaggle/input/mobile-price-range-prediction-is2020-v2/sample_submission.csv"")
submissionData"
9123,44229684,4,"x_train=trainData.drop(columns=['price_range','id'])
y_train=trainData['price_range']
x_test=testData.drop(columns=['id'])
print(x_train,y_train,x_test,sep=""\n"")"
9124,44229684,5,"from sklearn.preprocessing import StandardScaler as ss
x_trainscale =ss().fit_transform(x_train)
x_testscale =ss().fit_transform(x_test)"
9125,44229684,6,"from sklearn.ensemble import RandomForestClassifier as rfc
from sklearn.linear_model import LogisticRegression as lr

from sklearn.model_selection import cross_val_score as cvs
r = rfc().fit(x_trainscale,y_train)
y_pred=r.predict(x_testscale)
v=cvs(rfc(),x_trainscale,y_train,cv=3)
res=pd.DataFrame({'id':testData['id'],'price_range':y_pred})
res.to_csv('/kaggle/working/result_dtc.csv',index=False)
reg=lr(C=1000,penalty='l2')
reg.fit(x_trainscale,y_train)
y_pred=reg.predict(x_testscale)
value=cvs(lr(),x_trainscale,y_train,cv=3)
print(value)"
9126,44229684,7,
9127,44221495,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only ""../input/"" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
9128,44221495,1,"trainData=pd.read_csv(""/kaggle/input/mobile-price-range-prediction-is2020-v2/train_data.csv"")
trainData"
9129,44221495,2,"testData=pd.read_csv(""/kaggle/input/mobile-price-range-prediction-is2020-v2/test_data.csv"")
testData"
9130,44221495,3,"submissionData=pd.read_csv(""/kaggle/input/mobile-price-range-prediction-is2020-v2/sample_submission.csv"")
submissionData"
9131,44221495,4,"x_train=trainData.drop(columns=['price_range','id'])
y_train=trainData['price_range']
x_test=testData.drop(columns=['id'])
print(x_train,y_train,x_test,sep=""\n"")"
9132,44221495,5,"from sklearn.preprocessing import StandardScaler as ss
x_trainscale =ss().fit_transform(x_train)
x_testscale =ss().fit_transform(x_test)"
9133,44221495,6,"from sklearn.linear_model import LogisticRegression as lr
from sklearn.ensemble import RandomForestClassifier as rfc
from sklearn.model_selection import cross_val_score as cvs
r = rfc().fit(x_trainscale,y_train)
y_pred=r.predict(x_testscale)
v=cvs(rfc(),x_trainscale,y_train,cv=3)
res=pd.DataFrame({'id':testData['id'],'price_range':y_pred})
res.to_csv('/kaggle/working/result_dtc.csv',index=False)
reg=lr(C=1000,penalty='l2')
reg.fit(x_trainscale,y_train)
y_pred=reg.predict(x_testscale)
value=cvs(lr(),x_trainscale,y_train,cv=3)
print(value)"
9134,44221495,7,
9135,44242087,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only ""../input/"" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
9136,44242087,1,"import pandas as pd
trainData=pd.read_csv(""/kaggle/input/mobile-price-range-prediction-is2020-v2/train_data.csv"")
trainData
"
9137,44242087,2,"testData=pd.read_csv(""/kaggle/input/mobile-price-range-prediction-is2020-v2/test_data.csv"")
testData"
9138,44242087,3,"submissionData=pd.read_csv(""/kaggle/input/mobile-price-range-prediction-is2020-v2/sample_submission.csv"")
submissionData"
9139,44242087,4,"x_train=trainData.drop(columns=['price_range','id'])
y_train=trainData['price_range']
x_test=testData.drop(columns=['id'])
print(x_train,y_train,x_test,sep=""\n"")"
9140,44242087,5,"from sklearn.preprocessing import StandardScaler as ss
x_trainscale =ss().fit_transform(x_train)
x_testscale =ss().fit_transform(x_test)
"
9141,44242087,6,"from sklearn.linear_model import LogisticRegression as lr
from sklearn.ensemble import RandomForestClassifier as rfc
from sklearn.model_selection import cross_val_score as cvs
r = rfc().fit(x_trainscale,y_train)
y_pred=r.predict(x_testscale)
v=cvs(rfc(),x_trainscale,y_train,cv=3)
res=pd.DataFrame({'id':testData['id'],'price_range':y_pred})
res.to_csv('/kaggle/working/result_dtc.csv',index=False)
reg=lr(C=1000,penalty='l2')
reg.fit(x_trainscale,y_train)
y_pred=reg.predict(x_testscale)
value=cvs(lr(),x_trainscale,y_train,cv=3)
print(value)"
9142,44230862,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only ""../input/"" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
9143,44230862,1,"trainData=pd.read_csv(""/kaggle/input/mobile-price-range-prediction-is2020-v2/train_data.csv"")
trainData"
9144,44230862,2,"testData=pd.read_csv(""/kaggle/input/mobile-price-range-prediction-is2020-v2/test_data.csv"")
testData"
9145,44230862,3,"submissionData=pd.read_csv(""/kaggle/input/mobile-price-range-prediction-is2020-v2/sample_submission.csv"")
submissionData"
9146,44230862,4,"x_train=trainData.drop(columns=['price_range','id'])
y_train=trainData['price_range']
x_test=testData.drop(columns=['id'])
print(x_train,y_train,x_test,sep=""\n"")"
9147,44230862,5,"from sklearn.preprocessing import StandardScaler as ss
x_trainscale =ss().fit_transform(x_train)
x_testscale =ss().fit_transform(x_test)"
9148,44230862,6,"from sklearn.linear_model import LogisticRegression as lr
from sklearn.ensemble import RandomForestClassifier as rfc
from sklearn.model_selection import cross_val_score as cvs
r = rfc().fit(x_trainscale,y_train)
y_pred=r.predict(x_testscale)
v=cvs(rfc(),x_trainscale,y_train,cv=3)
res=pd.DataFrame({'id':testData['id'],'price_range':y_pred})
res.to_csv('/kaggle/working/result_dtc.csv',index=False)
reg=lr(C=1000,penalty='l2')
reg.fit(x_trainscale,y_train)
y_pred=reg.predict(x_testscale)
value=cvs(lr(),x_trainscale,y_train,cv=3)
print(value)"
9149,44270080,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only ""../input/"" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
9150,44270080,1,"trainData=pd.read_csv(""/kaggle/input/mobile-price-range-prediction-is2020-v2/train_data.csv"")
trainData"
9151,44270080,2,"testData=pd.read_csv(""/kaggle/input/mobile-price-range-prediction-is2020-v2/test_data.csv"")
testData"
9152,44270080,3,"submissionData=pd.read_csv(""/kaggle/input/mobile-price-range-prediction-is2020-v2/sample_submission.csv"")
submissionData"
9153,44270080,4,"x_train=trainData.drop(columns=['price_range','id'])
y_train=trainData['price_range']
x_test=testData.drop(columns=['id'])
print(x_train,y_train,x_test,sep=""\n"")"
9154,44270080,5,
9155,44270080,6,"from sklearn.preprocessing import StandardScaler as ss
x_trainscale =ss().fit_transform(x_train)
x_testscale =ss().fit_transform(x_test)"
9156,44270080,7,"from sklearn.linear_model import LogisticRegression as lr
from sklearn.ensemble import RandomForestClassifier as rfc
from sklearn.model_selection import cross_val_score as cvs
r = rfc().fit(x_trainscale,y_train)
y_pred=r.predict(x_testscale)
v=cvs(rfc(),x_trainscale,y_train,cv=3)
res=pd.DataFrame({'id':testData['id'],'price_range':y_pred})
res.to_csv('/kaggle/working/result_dtc.csv',index=False)
reg=lr(C=1000,penalty='l2')
reg.fit(x_trainscale,y_train)
y_pred=reg.predict(x_testscale)
value=cvs(lr(),x_trainscale,y_train,cv=3)
print(value)"
9157,44270080,8,
9158,44272600,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only ""../input/"" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
9159,44272600,1,"trainData=pd.read_csv(""/kaggle/input/mobile-price-range-prediction-is2020-v2/train_data.csv"")
trainData"
9160,44272600,2,"testData=pd.read_csv(""/kaggle/input/mobile-price-range-prediction-is2020-v2/test_data.csv"")
testData"
9161,44272600,3,"submissionData=pd.read_csv(""/kaggle/input/mobile-price-range-prediction-is2020-v2/sample_submission.csv"")
submissionData"
9162,44272600,4,"x_train=trainData.drop(columns=['price_range','id'])
y_train=trainData['price_range']
x_test=testData.drop(columns=['id'])
print(x_train,y_train,x_test,sep=""\n"")
"
9163,44272600,5,"from sklearn.preprocessing import StandardScaler as ss
x_trainscale =ss().fit_transform(x_train)
x_testscale =ss().fit_transform(x_test)"
9164,44272600,6,"from sklearn.linear_model import LogisticRegression as lr
from sklearn.ensemble import RandomForestClassifier as rfc
from sklearn.model_selection import cross_val_score as cvs
r = rfc().fit(x_trainscale,y_train)
y_pred=r.predict(x_testscale)
v=cvs(rfc(),x_trainscale,y_train,cv=3)
res=pd.DataFrame({'id':testData['id'],'price_range':y_pred})
res.to_csv('/kaggle/working/result_dtc.csv',index=False)
reg=lr(C=1000,penalty='l2')
reg.fit(x_trainscale,y_train)
y_pred=reg.predict(x_testscale)
value=cvs(lr(),x_trainscale,y_train,cv=3)
print(value)"
9165,42014625,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only ""../input/"" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
9166,42014625,1,"import warnings
warnings.filterwarnings('ignore')"
9167,42014625,2,"train=pd.read_csv('../input/mobile-price-range-prediction-is2020-v2/train_data.csv')
test=pd.read_csv('../input/mobile-price-range-prediction-is2020-v2/test_data.csv')
sample_submission= pd.read_csv('../input/mobile-price-range-prediction-is2020-v2/sample_submission.csv')"
9168,42014625,3,train.head()
9169,42014625,4,test.head()
9170,42014625,5,"print(train.shape,test.shape)"
9171,42014625,6,"x_train=train.drop(['price_range','id'],axis=1)
y_train=train['price_range']
x_test=test.drop(['id'],axis=1)
x_test.head()"
9172,42014625,7,"from sklearn.linear_model import LogisticRegression
logisticRegr = LogisticRegression()
logisticRegr.fit(x_train, y_train)
y_pred = logisticRegr.predict(x_test)"
9173,42014625,8,"from sklearn.model_selection import cross_val_score
scores=cross_val_score(logisticRegr,x_train,y_train,cv=5)
print(scores)
print(scores.mean())"
9174,42014625,9,"from sklearn import svm
clf = svm.SVC()
clf.fit(x_train, y_train)
y_pred = clf.predict(x_test)
from sklearn.model_selection import cross_val_score
scores=cross_val_score(logisticRegr,x_train,y_train,cv=5)
print(scores)
print(scores.mean())"
9175,42014625,10,"data={'id':sample_submission['id'],'price_range':y_pred}
result_dtr=pd.DataFrame(data)
result_dtr.to_csv('/kaggle/working/result_svm.csv',index=False)"
9176,42014625,11,"from sklearn.tree import DecisionTreeClassifier
Dec_tree=DecisionTreeClassifier()
Dec_tree=Dec_tree.fit(x_train,y_train)
y_pred=Dec_tree.predict(x_test)
from sklearn.model_selection import cross_val_score
scores=cross_val_score(logisticRegr,x_train,y_train,cv=5)
print(scores)
print(scores.mean())"
9177,42014625,12,"data={'id':sample_submission['id'],'price_range': y_pred}
result_dtr=pd.DataFrame(data)
result_dtr.to_csv('/kaggle/working/result_dtr.csv',index=False)"
9178,42014625,13,"from sklearn.ensemble import RandomForestClassifier
Ran_forest=RandomForestClassifier()
Ran_forest=Ran_forest.fit(x_train,y_train)
y_pred=Ran_forest.predict(x_test)
from sklearn.model_selection import cross_val_score
scores=cross_val_score(logisticRegr,x_train,y_train,cv=5)
print(scores)
print(scores.mean())"
9179,42014625,14,"data={'id':sample_submission['id'],'price_range':y_pred}
result_rf=pd.DataFrame(data)
result_rf.to_csv('/kaggle/working/result_rf.csv',index=False)"
9180,39816524,0,"# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only ""../input/"" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
9181,39816524,1,"Train_data=pd.read_csv(""/kaggle/input/mobile-price-range-prediction-is2020-v2/train_data.csv"")
Train_data.info()"
9182,39816524,2,"Test_data=pd.read_csv(""/kaggle/input/mobile-price-range-prediction-is2020-v2/test_data.csv"")
Test_data.info()"
9183,39816524,3,"Submission_data=pd.read_csv(""/kaggle/input/mobile-price-range-prediction-is2020-v2/sample_submission.csv"")
print(Submission_data)
print(Submission_data.head(10))"
9184,39816524,4,"print(Train_data.head(10))
print(Test_data.head(10))"
9185,39816524,5,"x_train=Train_data.drop(columns=['price_range','id'])
y_train=Train_data['price_range']"
9186,39816524,6,"print(x_train,y_train,sep=""\n"")"
9187,39816524,7,"x_test=Test_data.drop(columns=['id'])
print(x_test)"
9188,39816524,8,y_train.value_counts()
9189,39816524,9,"from sklearn.preprocessing import StandardScaler as ss
x_trainscale=ss().fit_transform(x_train)
x_testscale=ss().fit_transform(x_test)"
9190,39816524,10,pd.DataFrame(x_trainscale).head()
9191,39816524,11,pd.DataFrame(x_testscale).head()
9192,39816524,12,"from sklearn.linear_model import LogisticRegression as lr
lr?"
9193,39816524,13,"from sklearn.ensemble import RandomForestClassifier as rfc
from sklearn.model_selection import cross_val_score as cvs
ranfor=rfc().fit(x_trainscale,y_train)
y_prediction=ranfor.predict(x_testscale)
value=cvs(rfc(),x_trainscale,y_train,cv=3)
print(value)"
9194,39816524,14,print(value.mean())
9195,39816524,15,"res=pd.DataFrame({'id':Test_data['id'],'price_range':y_prediction})"
9196,39816524,16,"res.to_csv('/kaggle/working/result_rf.csv',index=False)"
9197,39816524,17,"from sklearn.svm import SVC
svc=SVC(kernel='linear',C=1)
y_prediction_svc=svc.fit(x_trainscale,y_train).predict(x_testscale)
value=cvs(svc,x_trainscale,y_train,cv=3)
print(value)"
9198,39816524,18,print(value.mean())
9199,39816524,19,"res1=pd.DataFrame({'id':Test_data['id'],'price_range':y_prediction_svc})
res1.to_csv('/kaggle/working/result_dtc.csv',index=False)"
9200,39816524,20,"from sklearn.model_selection import GridSearchCV
gsc={'C':np.logspace(-3,3,7),'penalty':['l1','l2']}
value=GridSearchCV(lr(),gsc).fit(x_trainscale,y_train)
import warnings
warnings.filterwarnings(""ignore"")
print(value.best_params_)
print(value.best_score_)"
9201,39816524,21,"reg=lr(C=1000,penalty='l2')
reg.fit(x_trainscale,y_train)
y_pred=reg.predict(x_testscale)
value=cvs(lr(),x_trainscale,y_train,cv=3)
print(value)"
9202,39816524,22,"res2=pd.DataFrame({'id':Test_data['id'],'price_range':y_pred})
res2.to_csv('/kaggle/working/result_lr.csv',index=False)"
9445,43694573,0,!pip install pytorch-tabnet
9446,43694573,1,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only ""../input/"" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
9447,43694573,2,"import pandas as pd
import numpy as np
from sklearn.metrics import f1_score,classification_report
from catboost import CatBoostClassifier,CatBoostRegressor
from sklearn.model_selection import train_test_split

from sklearn.base import BaseEstimator, ClassifierMixin

from sklearn.model_selection import cross_val_predict

import itertools
from tqdm import tqdm_notebook
import gc

from sklearn.naive_bayes import BernoulliNB
from sklearn.neighbors import KNeighborsClassifier
import lightgbm as lgb

from sklearn.linear_model import Lasso

from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler


from pytorch_tabnet.tab_model import TabNetRegressor
import torch
import warnings
from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import RandomForestRegressor"
9448,43694573,3,"data = pd.read_csv('/kaggle/input/mf-accelerator/contest_train.csv')
target = data.TARGET
data = data.fillna(0)"
9449,43694573,4,"features = data.drop(columns=[""TARGET"",""ID""])
features.head()"
9450,43694573,5,"data_test = pd.read_csv('/kaggle/input/mf-accelerator/contest_test.csv')
features_test = data_test.copy().drop(columns=[""ID""])
features_test.head()"
9451,43694573,6,"features_train,features_val,labels_train,labels_val = train_test_split(features,target, test_size = 0.3,\
                                                                   shuffle=True,random_state=1,\
                                                                   stratify = target)
features_train.head()"
9452,43694573,7,"class Stacking(BaseEstimator, ClassifierMixin):  
    """"""  
       . 
    """"""
    

    def __init__(self, models, metamodel,merge=False):
        """"""
        
        models -    
        metamodel - 
        """"""
        self.models = models
        self.metamodel = metamodel
        self.n = len(models)
        self.meta = None
        self.merge = merge


    def fit(self, X, y=None, p=0.25, random_state=0):
        """"""
         

        p -       
             
        random_state -  
        merge -            
        """"""
        #      
        base, meta, y_base, y_meta = train_test_split(X, y, test_size=p, random_state=random_state,stratify = y)
            
        #     
        self.meta = np.zeros((meta.shape[0], self.n))
        for t, base_model in enumerate(self.models):
            base_model.fit(np.array(base), np.array(y_base))
                
            self.meta[:, t] = base_model.predict(meta).reshape((1,-1))#reshape     
            print(f""Ok {t}"")

        #  
        if self.merge:#          
            data_meta_ext = np.concatenate((meta,self.meta),axis=1)
            self.metamodel.fit(data_meta_ext, y_meta)
        else:
            self.metamodel.fit(self.meta, y_meta)
        print(""------"")
        print(""Ok"")


        return self
    


    def predict(self, X, y=None):
        """"""
         
        """"""
        #    -
        X_meta = np.zeros((X.shape[0], self.n))
        
        print(""------"")
        print(""Prediction"")  



        for t, base_model in enumerate(self.models):
            
            X_meta[:, t] = base_model.predict(X).reshape((1,-1))
          
            print(f""Ok{t}"")  
          

        if self.merge:#     
            data_meta_test = np.concatenate((X,X_meta),axis=1)
            res = self.metamodel.predict(data_meta_test)

        else:
            res = self.metamodel.predict(X_meta)
        
        return (res)"
9453,43694573,8,"class NNWrapper(BaseEstimator, ClassifierMixin):  
    """"""      """"""

    def __init__(self, model,scaler,X_valid=None,
                 y_valid=None,max_epochs=10, patience=150):
        
        self.model = model
        self.X_valid = X_valid
        self.y_valid = y_valid
        self.max_epochs = max_epochs
        self.patience = patience
        self.scaler = scaler

    def fit(self, X, y=None):
        X_sc = self.scaler.fit_transform(X)
        self.model.fit(X_train=np.array(X_sc), y_train=np.array(y), X_valid=self.X_valid,y_valid=self.y_valid,
                  max_epochs=self.max_epochs, patience=self.patience) 

        return self

    def predict(self, X_test, y=None):
        X_test_sc = self.scaler.transform(np.array(X_test))
        prediction = self.model.predict(np.array(X_test_sc)).reshape((1,-1))# 
        
        
        return prediction

class LMWrapper(BaseEstimator, ClassifierMixin):  
    """"""       
      """"""

    def __init__(self, model,scaler):
        
        self.model = model
        self.scaler = scaler
        
    def fit(self, X, y=None):
        
        X_sc = self.scaler.fit_transform(X)
        self.model.fit(X_sc, y) 

        return self

    def predict(self, X_test, y=None):
        X_test_sc = self.scaler.transform(X_test)
        prediction = self.model.predict(X_test_sc)
        
        
        return prediction"
9454,43694573,9,"ls0 = Lasso(alpha=0.01,random_state=0)
knn1 = LMWrapper(KNeighborsRegressor(n_neighbors=3,),StandardScaler())
knn2 = LMWrapper(KNeighborsRegressor(n_neighbors=10),StandardScaler())
rf2 = RandomForestRegressor(n_estimators=100, max_depth=10,random_state=100)
gbm1 = lgb.LGBMRegressor(boosting_type='gbdt', learning_rate=0.05, max_depth=7, n_estimators=200, nthread=-1,
                        objective='regression',random_state=0) 
cb_reg1 = CatBoostRegressor(task_type='GPU',random_state=0,
                             iterations=1000,verbose=False)
reg_tabnet = NNWrapper(TabNetRegressor(verbose=0,seed=0),StandardScaler(),max_epochs=100, patience=150,
                       X_valid=np.array(features_val), y_valid = np.array(labels_val).reshape(-1, 1))
clf_cb_1 = CatBoostClassifier(task_type='GPU',random_state=0, loss_function='MultiClass',
                                auto_class_weights=""Balanced"",iterations=1000,verbose=False)
clf_lr = LMWrapper(LogisticRegression(multi_class=""multinomial"",class_weight=""balanced"",
                                        C=1e-1,max_iter=300,random_state=0),StandardScaler())
clf_nb1 =  BernoulliNB(alpha=1,binarize=0.3)"
9455,43694573,10,"%%time
warnings.filterwarnings(""ignore"")
models = [ls0,knn1, knn2,rf2,gbm1,cb_reg1,
           reg_tabnet,clf_cb_1,clf_nb1,clf_lr]

meta_model = CatBoostClassifier(task_type='GPU',random_state=0, loss_function='MultiClassOneVsAll',
                                auto_class_weights=""Balanced"",iterations=1000,verbose=False)

stack = Stacking(models, meta_model,merge=True)
stack.fit(features_train,np.array(labels_train).reshape(-1, 1),p=0.2,random_state=0)#  - 
preds = stack.predict(features_val)
print(classification_report(labels_val,preds))
print(""------"")
print(f""Macro f1 score: {f1_score(labels_val,preds,average='macro')}"")"
9456,43694573,11,
9553,47064716,0,"import pandas as pd
import numpy as np
import os
import requests
import json
import datetime
import time
"
9554,47064716,1,"MIN_FINAL_RATING = 1500 # top submission in a match must have reached this score
num_api_calls_today = 0
"
9555,47064716,2,"all_files = []
for root, dirs, files in os.walk('../input/', topdown=False):
    all_files.extend(files)
seen_episodes = [int(f.split('.')[0]) for f in all_files 
                      if '.' in f and f.split('.')[0].isdigit() and f.split('.')[1] == 'json']
print('{} games in existing library'.format(len(seen_episodes)))
"
9556,47064716,3,"NUM_TEAMS = 1
EPISODES = 600 

BUFFER = 1

base_url = ""https://www.kaggle.com/requests/EpisodeService/""
get_url = base_url + ""GetEpisodeReplay""
list_url = base_url + ""ListEpisodes"""
9557,47064716,4,"# inital team list

r = requests.post(list_url, json = {""teamId"":  18039598}) # arbitrary ID, change to leading ID during challenge

rj = r.json()

teams_df = pd.DataFrame(rj['result']['teams'])"
9558,47064716,5,"teams_df.sort_values('publicLeaderboardRank', inplace = True)
teams_df.head(6)"
9559,47064716,6,"def getTeamEpisodes(team_id):
    # request
    r = requests.post(list_url, json = {""teamId"":  int(team_id)})
    rj = r.json()

    # update teams list
    global teams_df
    teams_df_new = pd.DataFrame(rj['result']['teams'])
    
    if len(teams_df.columns) == len(teams_df_new.columns) and (teams_df.columns == teams_df_new.columns).all():
        teams_df = pd.concat( (teams_df, teams_df_new.loc[[c for c in teams_df_new.index if c not in teams_df.index]] ) )
        teams_df.sort_values('publicLeaderboardRank', inplace = True)
    else:
        print('teams dataframe did not match')
    
    # make df
    team_episodes = pd.DataFrame(rj['result']['episodes'])
    team_episodes['avg_score'] = -1;
    
    for i in range(len(team_episodes)):
        agents = team_episodes['agents'].loc[i]
        agent_scores = [a['updatedScore'] for a in agents if a['updatedScore'] is not None]
        team_episodes.loc[i, 'submissionId'] = [a['submissionId'] for a in agents if a['submission']['teamId'] == team_id][0]
        team_episodes.loc[i, 'updatedScore'] = [a['updatedScore'] for a in agents if a['submission']['teamId'] == team_id][0]
        
        if len(agent_scores) > 0:
            team_episodes.loc[i, 'avg_score'] = np.mean(agent_scores)

    for sub_id in team_episodes['submissionId'].unique():
        sub_rows = team_episodes[ team_episodes['submissionId'] == sub_id ]
        max_time = max( [r['seconds'] for r in sub_rows['endTime']] )
        final_score = max( [r['updatedScore'] for r_idx, (r_index, r) in enumerate(sub_rows.iterrows())
                                if r['endTime']['seconds'] == max_time] )

        team_episodes.loc[sub_rows.index, 'final_score'] = final_score
        
    team_episodes.sort_values('avg_score', ascending = False, inplace=True)
    return rj, team_episodes"
9560,47064716,7,"def saveEpisode(epid, rj):
    # request
    re = requests.post(get_url, json = {""EpisodeId"": int(epid)})
        
    # save replay
    with open('{}.json'.format(epid), 'w') as f:
        f.write(re.json()['result']['replay'])

    # save episode info
    with open('{}_info.json'.format(epid), 'w') as f:
        json.dump([r for r in rj['result']['episodes'] if r['id']==epid][0], f)"
9561,47064716,8,"global num_api_calls_today

pulled_teams = {}
pulled_episodes = []
start_time = datetime.datetime.now()
r = BUFFER;

while num_api_calls_today < EPISODES:
    # pull team
    top_teams = [i for i in teams_df.id if i not in pulled_teams]
    if len(top_teams) > 0:
        team_id = top_teams[0]
    else:
        break;
        
    # get team data
    team_json, team_df = getTeamEpisodes(team_id); r+=1;
    num_api_calls_today+=1
    print('{} games for {}'.format(len(team_df), teams_df.loc[teams_df.id == team_id].iloc[0].teamName))

    
    team_df = team_df[  (MIN_FINAL_RATING is None or (team_df.final_score > MIN_FINAL_RATING))]
    
    print('   {} in score range from {} submissions'.format(len(team_df), len(team_df.submissionId.unique() ) ) )
    
    team_df = team_df[~team_df.id.isin(pulled_episodes + seen_episodes)]        
    print('      {} remain to be downloaded\n'.format(len(team_df)))
        
    # pull games
    target_team_games = int(np.ceil(EPISODES / NUM_TEAMS))
    if target_team_games + len(pulled_episodes) > EPISODES:
        target_team_games = EPISODES - len(pulled_episodes)
     
    pulled_teams[team_id] = 0
    
    i = 0
    while i < len(team_df) and pulled_teams[team_id] < target_team_games:
        epid = team_df.id.iloc[i]
        if not (epid in pulled_episodes or epid in seen_episodes):
            try:
                saveEpisode(epid, team_json); r+=1;
                num_api_calls_today+=1
            except:
                time.sleep(20)
                i+=1;
                continue;
                
            pulled_episodes.append(epid)
            pulled_teams[team_id] += 1
            try:
                size = os.path.getsize('{}.json'.format(epid)) / 1e6
                print(str(num_api_calls_today) + ': Saved Episode #{} @ {:.1f}MB'.format(epid, size))
            except:
                print('  file {}.json did not seem to save'.format(epid))    
            if r > (datetime.datetime.now() - start_time).seconds:
                time.sleep( r - (datetime.datetime.now() - start_time).seconds)
                

        i+=1;
    print(); print()"
9562,47064716,9,
9563,47064716,10,
9564,48346260,0,"import numpy as np
from numpy import linalg as LA

import matplotlib.pyplot as plt
import pandas as pd
import json
import glob
import seaborn as sns

from tqdm import tqdm

import math

from collections import defaultdict
import collections"
9565,48346260,1,"TEAMNAME = ""WeKick""
replay_dir = ""../input/wekick-small/wekick_small/"""
9566,48346260,2,"action_set_v1=[
""idle"",""left"",""top_left"",""top"",""top_right"",""right"",""bottom_right"",""bottom"",""bottom_left"",""long_pass"",""high_pass"",""short_pass"",""shot"",""sprint"",""release_direction"",""release_sprint"",""sliding"",""dribble"",""release_dribble""
]"
9567,48346260,3,"json_paths=[]
for path in glob.glob(replay_dir+""*""): 
    json_paths.append(path)
        
print(""replay num: {}"".format(len(json_paths)))"
9568,48346260,4,"# return action list
def create_episode_dict(json_path):
    act_lis =[]
    
    json_open = open(json_path, 'r')
    json_load = json.load(json_open)
    
    sub_id = int(json_path.split(""/"")[-1].split(""_"")[0])
    
    for frame in range(len(json_load[""steps""])-1):
        if TEAMNAME in json_load[""info""][""TeamNames""][0]:
            team=0
        elif TEAMNAME in json_load[""info""][""TeamNames""][1]:
            team=1
        else:
            raise BaseException(""teamname{} not found!"".format(TEAMNAME))

        raw = json_load[""steps""][frame][team][""observation""][""players_raw""][0]
        action = json_load[""steps""][frame+1][team][""action""][0]
        
        act_lis.append(action)

    
    return act_lis,sub_id"
9569,48346260,5,"from joblib import Parallel, delayed
sub = Parallel(n_jobs=-1, verbose=10)( [delayed(create_episode_dict)(j) for j in json_paths] )"
9570,48346260,6,"subs = defaultdict(list)

for act_lis,sub_id in sub:
    subs[sub_id].extend(act_lis)"
9571,48346260,7,"sub_action_prob= defaultdict(list)

for subid, actions in subs.items():
    c = collections.Counter(actions)
    
    for act in range(19):
        sub_action_prob[subid].append(c[act]/len(actions))"
9572,48346260,8,"act_df = pd.DataFrame(sub_action_prob)
act_df.index = action_set_v1
act_df = act_df.reindex(sorted(act_df.columns), axis=1)
act_df"
9573,48346260,9,"plt.figure(figsize = (15,5))
plt.plot(act_df)
plt.xticks(rotation=45)
plt.grid()"
9574,48346260,10,"df_corr = act_df.corr()
plt.figure(figsize = (8,8))
sns.heatmap(df_corr, square=True,vmax=1, vmin=-1, center=0)"
9575,48346260,11,"from scipy.spatial.distance import squareform
from scipy.cluster.hierarchy import dendrogram, linkage

corr_condensed =squareform(1-df_corr)

z = linkage(corr_condensed, method='average')
dn = dendrogram(z,  leaf_rotation=70)"
9576,43704621,0,"import pandas as pd
import numpy as np
import os
import requests
import json
import datetime
import time
"
9577,43704621,1,"MIN_FINAL_RATING = 500 # top submission in a match must have reached this score
num_api_calls_today = 0
"
9578,43704621,2,"all_files = []
for root, dirs, files in os.walk('../input/', topdown=False):
    all_files.extend(files)
seen_episodes = [int(f.split('.')[0]) for f in all_files 
                      if '.' in f and f.split('.')[0].isdigit() and f.split('.')[1] == 'json']
print('{} games in existing library'.format(len(seen_episodes)))
"
9579,43704621,3,"NUM_TEAMS = 1
EPISODES = 600 

BUFFER = 1

base_url = ""https://www.kaggle.com/requests/EpisodeService/""
get_url = base_url + ""GetEpisodeReplay""
list_url = base_url + ""ListEpisodes"""
9580,43704621,4,"# inital team list

r = requests.post(list_url, json = {""teamId"":  5586412}) # arbitrary ID, change to leading ID during challenge

rj = r.json()

teams_df = pd.DataFrame(rj['result']['teams'])"
9581,43704621,5,"teams_df.sort_values('publicLeaderboardRank', inplace = True)
teams_df.head(6)"
9582,43704621,6,"def getTeamEpisodes(team_id):
    # request
    r = requests.post(list_url, json = {""teamId"":  int(team_id)})
    rj = r.json()

    # update teams list
    global teams_df
    teams_df_new = pd.DataFrame(rj['result']['teams'])
    
    if len(teams_df.columns) == len(teams_df_new.columns) and (teams_df.columns == teams_df_new.columns).all():
        teams_df = pd.concat( (teams_df, teams_df_new.loc[[c for c in teams_df_new.index if c not in teams_df.index]] ) )
        teams_df.sort_values('publicLeaderboardRank', inplace = True)
    else:
        print('teams dataframe did not match')
    
    # make df
    team_episodes = pd.DataFrame(rj['result']['episodes'])
    team_episodes['avg_score'] = -1;
    
    for i in range(len(team_episodes)):
        agents = team_episodes['agents'].loc[i]
        agent_scores = [a['updatedScore'] for a in agents if a['updatedScore'] is not None]
        team_episodes.loc[i, 'submissionId'] = [a['submissionId'] for a in agents if a['submission']['teamId'] == team_id][0]
        team_episodes.loc[i, 'updatedScore'] = [a['updatedScore'] for a in agents if a['submission']['teamId'] == team_id][0]
        
        if len(agent_scores) > 0:
            team_episodes.loc[i, 'avg_score'] = np.mean(agent_scores)

    for sub_id in team_episodes['submissionId'].unique():
        sub_rows = team_episodes[ team_episodes['submissionId'] == sub_id ]
        max_time = max( [r['seconds'] for r in sub_rows['endTime']] )
        final_score = max( [r['updatedScore'] for r_idx, (r_index, r) in enumerate(sub_rows.iterrows())
                                if r['endTime']['seconds'] == max_time] )

        team_episodes.loc[sub_rows.index, 'final_score'] = final_score
        
    team_episodes.sort_values('avg_score', ascending = False, inplace=True)
    return rj, team_episodes"
9583,43704621,7,"def saveEpisode(epid, rj):
    # request
    re = requests.post(get_url, json = {""EpisodeId"": int(epid)})
        
    # save replay
    with open('{}.json'.format(epid), 'w') as f:
        f.write(re.json()['result']['replay'])

    # save episode info
    with open('{}_info.json'.format(epid), 'w') as f:
        json.dump([r for r in rj['result']['episodes'] if r['id']==epid][0], f)"
9584,43704621,8,"global num_api_calls_today

pulled_teams = {}
pulled_episodes = []
start_time = datetime.datetime.now()
r = BUFFER;

while num_api_calls_today < EPISODES:
    # pull team
    top_teams = [i for i in teams_df.id if i not in pulled_teams]
    if len(top_teams) > 0:
        team_id = top_teams[0]
    else:
        break;
        
    # get team data
    team_json, team_df = getTeamEpisodes(team_id); r+=1;
    num_api_calls_today+=1
    print('{} games for {}'.format(len(team_df), teams_df.loc[teams_df.id == team_id].iloc[0].teamName))

    
    team_df = team_df[  (MIN_FINAL_RATING is None or (team_df.final_score > MIN_FINAL_RATING))]
    
    print('   {} in score range from {} submissions'.format(len(team_df), len(team_df.submissionId.unique() ) ) )
    
    team_df = team_df[~team_df.id.isin(pulled_episodes + seen_episodes)]        
    print('      {} remain to be downloaded\n'.format(len(team_df)))
        
    # pull games
    target_team_games = int(np.ceil(EPISODES / NUM_TEAMS))
    if target_team_games + len(pulled_episodes) > EPISODES:
        target_team_games = EPISODES - len(pulled_episodes)
     
    pulled_teams[team_id] = 0
    
    i = 0
    while i < len(team_df) and pulled_teams[team_id] < target_team_games:
        epid = team_df.id.iloc[i]
        if not (epid in pulled_episodes or epid in seen_episodes):
            try:
                saveEpisode(epid, team_json); r+=1;
                num_api_calls_today+=1
            except:
                time.sleep(20)
                i+=1;
                continue;
                
            pulled_episodes.append(epid)
            pulled_teams[team_id] += 1
            try:
                size = os.path.getsize('{}.json'.format(epid)) / 1e6
                print(str(num_api_calls_today) + ': Saved Episode #{} @ {:.1f}MB'.format(epid, size))
            except:
                print('  file {}.json did not seem to save'.format(epid))    
            if r > (datetime.datetime.now() - start_time).seconds:
                time.sleep( r - (datetime.datetime.now() - start_time).seconds)
                

        i+=1;
    print(); print()"
9585,43704621,9,
9586,43704621,10,
9587,44678142,0,"# Install:
# GFootball environment (https://github.com/google-research/football/)

!apt-get update
!apt-get install -y libsdl2-gfx-dev libsdl2-ttf-dev

# Update kaggle-environments to the newest version.
!pip3 install kaggle-environments -U

# Make sure that the Branch in git clone and in wget call matches !!
!git clone -b v2.3 https://github.com/google-research/football.git
!mkdir -p football/third_party/gfootball_engine/lib

!wget https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_v2.3.so -O football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so
!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install ."
9588,44678142,1,"from gfootball.env.wrappers import Simple115StateWrapper
from kaggle_environments import make
env = make(""football"", 
           configuration={""save_video"": False, 
                          ""scenario_name"": ""11_vs_11_kaggle"", 
                          ""running_in_notebook"": True,
                         })
obs = env.reset()"
9589,44678142,2,"# all game information
obs"
9590,44678142,3,"# get raw obs for the first player we control.
obs[0]['observation']['players_raw']"
9591,44678142,4,"from gfootball.env import observation_preprocessing
raw_obs = obs[0]['observation']['players_raw']
obs_smm = observation_preprocessing.generate_smm(raw_obs)[0]
print(obs_smm)
print(obs_smm.shape)"
9592,44678142,5,"from gfootball.env.wrappers import Simple115StateWrapper
raw_obs = obs[0]['observation']['players_raw']
# Note: simple115v2 enables fixed_positions option.
# Source code in https://github.com/google-research/football/blob/3603de77d2bf25e53a1fbd52bc439f1377397b3b/gfootball/env/wrappers.py#L119
obs_115 = Simple115StateWrapper.convert_observation(raw_obs, fixed_positions=True)[0]
print(obs_115)
print(obs_115.shape)"
9593,44678142,6,"%%writefile ./test.py

from gfootball.env import observation_preprocessing
from gfootball.env.wrappers import Simple115StateWrapper
import random

def agent(obs):
    
    # error:
    # raw_obs = obs[0]['observation']['players_raw']
    # obs115 = Simple115StateWrapper.convert_observation(raw_obs, True)[0]
    # obs_smm = observation_preprocessing.generate_smm([raw_obs])[0]
    
    # correct:
    raw_obs = obs['players_raw'][0]
    obs_115 = Simple115StateWrapper.convert_observation([raw_obs], True)[0]
    obs_smm = observation_preprocessing.generate_smm([raw_obs])[0]
    
    agent_output = random.randint(1, 18)
    
    # you need return a list contains your single action(a int type number from [1, 18])
    # be ware of your model output might be a float number, so make sure return a int type number.
    return [int(agent_output)]"
9594,44678142,7,"from kaggle_environments import make

log = []

# you can set debug=True or/and logs to get more information for debug.
env = make(""football"", 
           configuration={""save_video"": True, 
                          ""scenario_name"": ""11_vs_11_kaggle"", 
                          ""running_in_notebook"": True,
                         }, debug=True, logs=log)
output = env.run([""./test.py"", ""./test.py""])[-1]
print('Left player: reward = %s, status = %s, info = %s' % (output[0]['reward'], output[0]['status'], output[0]['info']))
print('Right player: reward = %s, status = %s, info = %s' % (output[1]['reward'], output[1]['status'], output[1]['info']))

# you can print detailed log
# print(log)

env.render(mode=""human"", width=800, height=600)"
9595,44678142,8,
9596,47965519,0,"import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))
import requests
import json
import time"
9597,47965519,1,"base_url = ""https://www.kaggle.com/requests/EpisodeService/""
get_url = base_url + ""GetEpisodeReplay""
list_url = base_url + ""ListEpisodes""
# inital team list

r = requests.post(list_url, json = {""teamId"":  5696217}) # arbitrary ID, change to leading ID during challenge

rj = r.json()

teams_df = pd.DataFrame(rj['result']['teams'])
teams_df.sort_values('publicLeaderboardRank', inplace = True)
teams_df.head(6)"
9598,47965519,2,"# target_sub_id = int(teams_df[teams_df['teamName'] == 'SaltyFish']['publicLeaderboardSubmissionId'])
target_sub_id = 18239675"
9599,47965519,3,
9600,47965519,4,"# rr = requests.post(list_url, json = {""teamId"":  int(5696217)})
# rrj = rr.json()
# tteam_episodes = pd.DataFrame(rrj['result']['episodes'])"
9601,47965519,5,
9602,47965519,6,"rs = requests.post(list_url, json = {""SubmissionId"":int(target_sub_id)})
rsj = rs.json()
steam_episodes = pd.DataFrame(rsj['result']['episodes'])"
9603,47965519,7,steam_episodes.head()
9604,47965519,8,steam_episodes['agents'][0]
9605,47965519,9,!mkdir episodes_log
9606,47965519,10,"def saveEpisode(epid, rj):
    # request
    re = requests.post(get_url, json = {""EpisodeId"": int(epid)})
        
    # save replay
    with open('./episodes_log/{}.json'.format(epid), 'w') as f:
        f.write(re.json()['result']['replay'])

    # save episode info
    with open('./episodes_log/{}_info.json'.format(epid), 'w') as f:
        json.dump([r for r in rj['result']['episodes'] if r['id']==epid][0], f)"
9607,47965519,11,len(steam_episodes)
9608,47965519,12,"max_episodes = 150

if max_episodes > len(steam_episodes):
    max_episodes = len(steam_episodes)"
9609,47965519,13,max_episodes
9610,47965519,14,"i = 0
# while i < 5:
while i < max_episodes:
    epid = steam_episodes['id'].iloc[i]
    try:
        saveEpisode(epid, rsj); r+=1;
        num_api_calls_today+=1
    except:
        time.sleep(20)
        i+=1;
        continue;

    i+=1;"
9611,47965519,15,"# rre = requests.post(get_url, json = {""EpisodeId"": int(4790921)})"
9612,47965519,16,# data = json.load(open('./episodes_log/4790921.json'))
9613,47965519,17,# d_info = json.load(open('./episodes_log/4790921_info.json'))
9614,47965519,18,# d_info
9615,47965519,19,"# if d_info['agents'][0]['submissionId'] == '18010220':
#     team_index = 0
# else:
#     team_index = 1
"
9616,47965519,20,# data['steps'][5][team_index]['observation']['players_raw'][0]
9617,47965519,21,
9618,45619348,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only ""../input/"" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
9619,45619348,1,"# Install:
# Kaggle environments.
!git clone https://github.com/Kaggle/kaggle-environments.git
!cd kaggle-environments && pip install .

# GFootball environment.
!apt-get update -y
!apt-get install -y libsdl2-gfx-dev libsdl2-ttf-dev

# Make sure that the Branch in git clone and in wget call matches !!
!git clone -b v2.7 https://github.com/google-research/football.git
!mkdir -p football/third_party/gfootball_engine/lib

!wget https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_v2.7.so -O football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so
!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install ."
9620,45619348,2,"import pandas as pd
import numpy as np
import os
import requests
import json
import datetime
import time"
9621,45619348,3,"MIN_FINAL_RATING = 1200 # top submission in a match must have reached this score
num_api_calls_today = 0"
9622,45619348,4,"all_files = []
for root, dirs, files in os.walk('./', topdown=False):
    all_files.extend(files)
seen_episodes = [int(f.split('.')[0]) for f in all_files 
                      if '.' in f and f.split('.')[0].isdigit() and f.split('.')[1] == 'json']
print('{} games in existing library'.format(len(seen_episodes)))"
9623,45619348,5,"NUM_TEAMS = 3
EPISODES = 90 

BUFFER = 1

base_url = ""https://www.kaggle.com/requests/EpisodeService/""
get_url = base_url + ""GetEpisodeReplay""
list_url = base_url + ""ListEpisodes""
# inital team list

r = requests.post(list_url, json = {""teamId"":  5696217}) # arbitrary ID, change to leading ID during challenge

rj = r.json()

teams_df = pd.DataFrame(rj['result']['teams'])
teams_df.sort_values('publicLeaderboardRank', inplace = True)
teams_df.head(6)"
9624,45619348,6,"def getTeamEpisodes(team_id):
    r = requests.post(list_url, json = {""teamId"":  int(team_id)})
    rj = r.json()

    # update teams list
    global teams_df
    teams_df_new = pd.DataFrame(rj['result']['teams'])
    
    if len(teams_df.columns) == len(teams_df_new.columns) and (teams_df.columns == teams_df_new.columns).all():
        teams_df = pd.concat( (teams_df, teams_df_new.loc[[c for c in teams_df_new.index if c not in teams_df.index]] ) )
        teams_df.sort_values('publicLeaderboardRank', inplace = True)
    else:
        print('teams dataframe did not match')
    
    # make df
    team_episodes = pd.DataFrame(rj['result']['episodes'])
    team_episodes['avg_score'] = -1;
    
    for i in range(len(team_episodes)):
        agents = team_episodes['agents'].loc[i]
        agent_scores = [a['updatedScore'] for a in agents if a['updatedScore'] is not None]
        team_episodes.loc[i, 'submissionId'] = [a['submissionId'] for a in agents if a['submission']['teamId'] == team_id][0]
        team_episodes.loc[i, 'updatedScore'] = [a['updatedScore'] for a in agents if a['submission']['teamId'] == team_id][0]
        
        if len(agent_scores) > 0:
            team_episodes.loc[i, 'avg_score'] = np.mean(agent_scores)

    for sub_id in team_episodes['submissionId'].unique():
        sub_rows = team_episodes[ team_episodes['submissionId'] == sub_id ]
        max_time = max( [r['seconds'] for r in sub_rows['endTime']] )
        final_score = max( [r['updatedScore'] for r_idx, (r_index, r) in enumerate(sub_rows.iterrows())
                                if r['endTime']['seconds'] == max_time] )

        team_episodes.loc[sub_rows.index, 'final_score'] = final_score
        
    team_episodes.sort_values('avg_score', ascending = False, inplace=True)
    return rj, team_episodes
def saveEpisode(epid, rj):
    # request
    re = requests.post(get_url, json = {""EpisodeId"": int(epid)})
        
    # save replay
    with open('{}.json'.format(epid), 'w') as f:
        f.write(re.json()['result']['replay'])

    # save episode info
    with open('{}_info.json'.format(epid), 'w') as f:
        json.dump([r for r in rj['result']['episodes'] if r['id']==epid][0], f)
global num_api_calls_today

pulled_teams = {}
pulled_episodes = []
start_time = datetime.datetime.now()
r = BUFFER;

while num_api_calls_today < EPISODES:
    # pull team
    top_teams = [i for i in teams_df.id if i not in pulled_teams]
    if len(top_teams) > 0:
        team_id = top_teams[0]
    else:
        break;
        
    # get team data
    team_json, team_df = getTeamEpisodes(team_id); r+=1;
    num_api_calls_today+=1
    print('{} games for {}'.format(len(team_df), teams_df.loc[teams_df.id == team_id].iloc[0].teamName))

    
    team_df = team_df[  (MIN_FINAL_RATING is None or (team_df.final_score > MIN_FINAL_RATING))]
    
    print('   {} in score range from {} submissions'.format(len(team_df), len(team_df.submissionId.unique() ) ) )
    
    team_df = team_df[~team_df.id.isin(pulled_episodes + seen_episodes)]        
    print('      {} remain to be downloaded\n'.format(len(team_df)))
        
    # pull games
    target_team_games = int(np.ceil(EPISODES / NUM_TEAMS))
    if target_team_games + len(pulled_episodes) > EPISODES:
        target_team_games = EPISODES - len(pulled_episodes)
     
    pulled_teams[team_id] = 0
    
    i = 0
    while i < len(team_df) and pulled_teams[team_id] < target_team_games:
        epid = team_df.id.iloc[i]
        if not (epid in pulled_episodes or epid in seen_episodes):
            try:
                saveEpisode(epid, team_json); r+=1;
                num_api_calls_today+=1
            except:
                time.sleep(20)
                i+=1;
                continue;
                
            pulled_episodes.append(epid)
            pulled_teams[team_id] += 1
            try:
                size = os.path.getsize('{}.json'.format(epid)) / 1e6
                print(str(num_api_calls_today) + ': Saved Episode #{} @ {:.1f}MB'.format(epid, size))
            except:
                print('  file {}.json did not seem to save'.format(epid))    
            if r > (datetime.datetime.now() - start_time).seconds:
                time.sleep( r - (datetime.datetime.now() - start_time).seconds)
                

        i+=1;
    print(); print()"
9625,45619348,7,"
import json
import os
from tqdm import tqdm, notebook"
9626,45619348,8,"def convert_observation(observation, fixed_positions=False):

    def do_flatten(obj):
        if type(obj) == list:
            return np.array(obj).flatten()
        return obj.flatten()

    final_obs = []
    
    for obs in observation:
        

        o = []
        if fixed_positions:
            for i, name in enumerate(['left_team', 'left_team_direction',
                                    'right_team', 'right_team_direction']):
                o.extend(do_flatten(obs[name]))
            # If there were less than 11vs11 players we backfill missing values
            # with -1.
            if len(o) < (i + 1) * 22:
                o.extend([-1] * ((i + 1) * 22 - len(o)))
        else:
            o.extend(do_flatten(obs['left_team']))
            o.extend(do_flatten(obs['left_team_direction']))
            o.extend(do_flatten(obs['right_team']))
            o.extend(do_flatten(obs['right_team_direction']))

        # If there were less than 11vs11 players we backfill missing values with
        # -1.
        # 88 = 11 (players) * 2 (teams) * 2 (positions & directions) * 2 (x & y)
        if len(o) < 88:
            o.extend([-1] * (88 - len(o)))

        # ball position
        o.extend(obs['ball'])
        # ball direction
        o.extend(obs['ball_direction'])
        # one hot encoding of which team owns the ball
        if obs['ball_owned_team'] == -1:
            o.extend([1, 0, 0])
        if obs['ball_owned_team'] == 0:
            o.extend([0, 1, 0])
        if obs['ball_owned_team'] == 1:
            o.extend([0, 0, 1])

        active = [0] * 11
        if obs['active'] != -1:
            active[obs['active']] = 1
        o.extend(active)

        game_mode = [0] * 7
        game_mode[obs['game_mode']] = 1
        o.extend(game_mode)
        final_obs.append(o)

        return np.array(final_obs, dtype=np.float32).flatten()
"
9627,45619348,9,"y=[]
x=[]
filenames = [p for p in os.listdir('.') if 'info' not in p and 'json' in p]

for f in notebook.tqdm(filenames):
    filename = ""./"" + f

    try:
        with open(filename) as json_file:
            data = json.load(json_file)
    except:
        continue
        
    counter=0


    for team in [0,1]:
        final_score = data['steps'][-2][team]['observation']['players_raw'][0]['score'][0]

        goal=False

        for i in range(2,len(data['steps'])-2,):

            action=data['steps'][i][team]['action']
            y.append(action[0])
            
            obs=data['steps'][i][team]['observation']['players_raw'][0]
            x.append(convert_observation([obs]))
                "
9628,45619348,10,"import lightgbm as lgb
from sklearn.metrics import accuracy_score
def evaluate_model(model,X_test,Y_test):
    preds = model.predict(X_test)
    best_preds = np.asarray([np.argmax(line) for line in preds])

    print(""Accuracy = {}"".format(accuracy_score(Y_test, best_preds)))
    "
9629,45619348,11,x[0].shape
9630,45619348,12,"d_train = lgb.Dataset(np.array(x).reshape((-1,115)), label=y)
params = {}
params['objective'] = 'multiclass'
params['num_classes'] = 19


mod = lgb.train(params, d_train, 100,)

mod.save_model('model.txt')    
evaluate_model(mod,x[:1000],y[:1000])"
9631,45619348,13,"def tree_agent(obs):
    try:
        obs1=obs['observation']['players_raw'][0]
    except:
        obs1=obs['players_raw'][0]

    obs1=convert_observation([obs1])

#     action=np.random.choice(np.arange(19),p=(mod.predict([obs1])).flatten())
    action=np.argmax(mod.predict([obs1]).flatten())
    return [int(action)]"
9632,45619348,14,"from kaggle_environments import make

env = make(""football"", configuration={""save_video"": True, ""scenario_name"": ""11_vs_11_kaggle"", ""running_in_notebook"": True})
output = env.run([tree_agent,tree_agent])
print('Left player: reward = %s, status = %s, info = %s' % (output[-1][0]['reward'],
                                                            output[-1][0]['status'], output[-1][0]['info']))
env.render(mode='human',width=800, height=600)"
9633,45619348,15,
9634,44215692,0,"# Install:
# Kaggle environments.
!git clone https://github.com/Kaggle/kaggle-environments.git
!cd kaggle-environments && pip install .

# GFootball environment.
!apt-get update -y
!apt-get install -y libsdl2-gfx-dev libsdl2-ttf-dev

# Make sure that the Branch in git clone and in wget call matches !!
!git clone -b v2.3 https://github.com/google-research/football.git
!mkdir -p football/third_party/gfootball_engine/lib

!wget https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_v2.3.so -O football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so
!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install .
"
9635,44215692,1,"%%writefile submission.py
from kaggle_environments.envs.football.helpers import *
from math import sqrt

directions = [
[Action.TopLeft, Action.Top, Action.TopRight],
[Action.Left, Action.Idle, Action.Right],
[Action.BottomLeft, Action.Bottom, Action.BottomRight]]

dirsign = lambda x: 1 if abs(x) < 0.01 else (0 if x < 0 else 2)

enemyGoal = [1, 0]
perfectRange = [[0.6, 1], [-0.2, 0.2]]

def inside(pos, area):
    return area[0][0] <= pos[0] <= area[0][1] and area[1][0] <= pos[1] <= area[1][1]

@human_readable_agent
def agent(obs):
    controlled_player_pos = obs['left_team'][obs['active']]

    if obs[""game_mode""] == GameMode.Penalty:
        return Action.Shot
    if obs[""game_mode""] == GameMode.Corner:
        if controlled_player_pos[0] > 0:
            return Action.Shot
    if obs[""game_mode""] == GameMode.FreeKick:
        return Action.Shot
    # Make sure player is running.
    if  0 < controlled_player_pos[0] < 0.6 and Action.Sprint not in obs['sticky_actions']:
        return Action.Sprint
    elif 0.6 < controlled_player_pos[0] and Action.Sprint in obs['sticky_actions']:
        return Action.ReleaseSprint

    # Does the player we control have the ball?
    if obs['ball_owned_player'] == obs['active'] and obs['ball_owned_team'] == 0:
        goalkeeper = 0
        if inside(controlled_player_pos, perfectRange) and controlled_player_pos[0] < obs['ball'][0]:
            return Action.Shot
        elif abs(obs['right_team'][goalkeeper][0] - 1) > 0.2 and controlled_player_pos[0] > 0.4 and abs(controlled_player_pos[1]) < 0.2:
            return Action.Shot
        else:
            xdir = dirsign(enemyGoal[0] - controlled_player_pos[0])
            ydir = dirsign(enemyGoal[1] - controlled_player_pos[1])
            return directions[ydir][xdir]
    else:
        # Run towards the ball.
        xdir = dirsign(obs['ball'][0] - controlled_player_pos[0])
        ydir = dirsign(obs['ball'][1] - controlled_player_pos[1])
        return directions[ydir][xdir]
"
9636,44215692,2,"# Set up the Environment.
from kaggle_environments import make
env = make(""football"", configuration={""save_video"": True, ""scenario_name"": ""11_vs_11_kaggle"", ""running_in_notebook"": True}, debug=True)
output = env.run([""/kaggle/working/submission.py"", ""do_nothing""])[-1]
print('Left player: reward = %s, status = %s, info = %s' % (output[0]['reward'], output[0]['status'], output[0]['info']))
print('Right player: reward = %s, status = %s, info = %s' % (output[1]['reward'], output[1]['status'], output[1]['info']))
env.render(mode=""human"", width=800, height=600)"
9637,44665747,0,"# Install:
# Kaggle environments.
!git clone https://github.com/Kaggle/kaggle-environments.git
!cd kaggle-environments && pip install .

# GFootball environment.
!apt-get update -y
!apt-get install -y libsdl2-gfx-dev libsdl2-ttf-dev

# Make sure that the Branch in git clone and in wget call matches !!
!git clone -b v2.3 https://github.com/google-research/football.git
!mkdir -p football/third_party/gfootball_engine/lib

!wget https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_v2.3.so -O football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so
!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install ."
9638,44665747,1,!pip install pfrl==0.1.0
9639,44665747,2,"import os
import cv2
import sys
import glob 
import random
import imageio
import pathlib
import collections
from collections import deque
import numpy as np
import argparse
import matplotlib.pyplot as plt
import seaborn as sns
sns.set()
%matplotlib inline

from gym import spaces
from tqdm import tqdm
from logging import getLogger, StreamHandler, FileHandler, DEBUG, INFO
from typing import Union, Callable, List, Tuple, Iterable, Any, Dict
from dataclasses import dataclass
from IPython.display import Image, display
sns.set()


# PyTorch
import pfrl
from pfrl.agents import CategoricalDoubleDQN
from pfrl import experiments
from pfrl import explorers
from pfrl import nn as pnn
from pfrl import utils
from pfrl import replay_buffers
from pfrl.wrappers import atari_wrappers
from pfrl.q_functions import DistributionalDuelingDQN

import torch
from torch import nn

# Env
import gym
import gfootball
import gfootball.env as football_env
from gfootball.env import observation_preprocessing"
9640,44665747,3,"# Check we can use GPU
print(torch.cuda.is_available())

# set gpu id
if torch.cuda.is_available(): 
    # NOTE: it is not number of gpu but id which start from 0
    gpu = 0
else:
    # cpu=>-1
    gpu = -1"
9641,44665747,4,"# set logger
def logger_config():
    logger = getLogger(__name__)
    handler = StreamHandler()
    handler.setLevel(""DEBUG"")
    logger.setLevel(""DEBUG"")
    logger.addHandler(handler)
    logger.propagate = False

    filepath = './result.log'
    file_handler = FileHandler(filepath)
    logger.addHandler(file_handler)
    return logger

logger = logger_config()"
9642,44665747,5,"# fixed random seed
# but this is NOT enough to fix the result of rewards.Please tell me the reason.
def seed_everything(seed=1234):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True
    utils.set_random_seed(seed)  # for PFRL
    
# Set a random seed used in PFRL.
seed = 5046
seed_everything(seed)

# Set different random seeds for train and test envs.
train_seed = seed
test_seed = 2 ** 31 - 1 - seed"
9643,44665747,6,"# wrapper for env(resize and transpose channel order)
class TransEnv(gym.ObservationWrapper):
    def __init__(self, env, channel_order=""hwc""):

        gym.ObservationWrapper.__init__(self, env)
        self.height = 84
        self.width = 84
        self.ch = env.observation_space.shape[2]
        shape = {
            ""hwc"": (self.height, self.width, self.ch),
            ""chw"": (self.ch, self.height, self.width),
        }
        self.observation_space = spaces.Box(
            low=0, high=255, shape=shape[channel_order], dtype=np.uint8
        )
        

    def observation(self, frame):
        frame = cv2.resize(frame, (self.width, self.height), interpolation=cv2.INTER_AREA)
        return frame.reshape(self.observation_space.low.shape)"
9644,44665747,7,"def make_env(test):
    # Use different random seeds for train and test envs
    env_seed = test_seed if test else train_seed
    
    # env = gym.make('GFootball-11_vs_11_kaggle-SMM-v0')
    env = football_env.create_environment(
      env_name='11_vs_11_easy_stochastic',  # easy mode
      stacked=False,
      representation='extracted',  # SMM
      rewards='scoring, checkpoints',
      write_goal_dumps=False,
      write_full_episode_dumps=False,
      render=False,
      write_video=False,
      dump_frequency=1,
      logdir='./',
      extra_players=None,
      number_of_left_players_agent_controls=1,
      number_of_right_players_agent_controls=0
    )
    env = TransEnv(env, channel_order=""chw"")

    env.seed(int(env_seed))
    if test:
        # Randomize actions like epsilon-greedy in evaluation as well
        env = pfrl.wrappers.RandomizeAction(env, random_fraction=0.0)
    return env

env = make_env(test=False)
eval_env = make_env(test=True)"
9645,44665747,8,"print('observation space:', env.observation_space.low.shape)
print('action space:', env.action_space)"
9646,44665747,9,"env.reset()
action = env.action_space.sample()
obs, r, done, info = env.step(action)
print('next observation:', obs.shape)
print('reward:', r)
print('done:', done)
print('info:', info)"
9647,44665747,10,"obs_n_channels = env.observation_space.low.shape[0]
n_actions = env.action_space.n
print(""obs_n_channels: "", obs_n_channels)
print(""n_actions: "", n_actions)

# params based the original paper
n_atoms = 51
v_max = 10
v_min = -10
q_func = DistributionalDuelingDQN(n_actions, n_atoms, v_min, v_max, obs_n_channels)
print(q_func)"
9648,44665747,11,"# Noisy nets
pnn.to_factorized_noisy(q_func, sigma_scale=0.5)

# Turn off explorer
explorer = explorers.Greedy()

# Use the same hyper parameters as https://arxiv.org/abs/1710.02298
opt = torch.optim.Adam(q_func.parameters(), 6.25e-5, eps=1.5 * 10 ** -4)

# Prioritized Replay
# Anneal beta from beta0 to 1 throughout training
update_interval = 4
betasteps = 5 * 10 ** 7 / update_interval
rbuf = replay_buffers.PrioritizedReplayBuffer(
        10 ** 5,  # Default value is 10 ** 6 but it is too large in this notebook. I chose 10 ** 5.
        alpha=0.5,
        beta0=0.4,
        betasteps=betasteps,
        num_steps=3,
        normalize_by_max=""memory"",
    )


def phi(x):
    # Feature extractor
    return np.asarray(x, dtype=np.float32) / 255"
9649,44665747,12,"agent = CategoricalDoubleDQN(
        q_func,
        opt,
        rbuf,
        gpu=gpu,  
        gamma=0.99,
        explorer=explorer,
        minibatch_size=32,
        replay_start_size=2 * 10 ** 4,
        target_update_interval=32000,
        update_interval=update_interval,
        batch_accumulator=""mean"",
        phi=phi,
    )"
9650,44665747,13,"# if you have a pretrained model, agent can load pretrained weight. 
use_pretrained = False
pretrained_path = None
if use_pretrained:
    agent.load(pretrained_path)"
9651,44665747,14,"%%time
experiments.train_agent_with_evaluation(
    agent=agent,
    env=env,
    steps=100000,
    eval_n_steps=None,
    eval_n_episodes=1,
    eval_interval=3000,
    outdir=""./kaggle_simulations/agent"",
    save_best_so_far_agent=True,
    eval_env=eval_env,
    logger=logger
)"
9652,44665747,15,"import csv

def text_csv_converter(datas):
    file_csv = datas.replace(""txt"", ""csv"")
    with open(datas) as rf:
        with open(file_csv, ""w"") as wf:
            readfile = rf.readlines()
            for read_text in readfile:
                read_text = read_text.split()
                writer = csv.writer(wf, delimiter=',')
                writer.writerow(read_text)

filename = ""./kaggle_simulations/agent/scores.txt""
text_csv_converter(filename)"
9653,44665747,16,!ls -la ./kaggle_simulations/agent
9654,44665747,17,"import pandas as pd
scores = pd.read_csv(""./kaggle_simulations/agent/scores.csv"")
scores.head()"
9655,44665747,18,"# visualize reward each episodes
fig = plt.figure(figsize=(15, 5))
ax1 = fig.add_subplot(121)
ax2 = fig.add_subplot(122)
ax1.set_title(""median reward"")
ax2.set_title(""average loss"")
sns.lineplot(x=""episodes"", y=""median"", data=scores, ax=ax1)
sns.lineplot(x=""episodes"", y=""average_loss"", data=scores,ax=ax2)
plt.show()"
9656,44665747,19,"%%writefile ./kaggle_simulations/agent/main.py
import cv2
import collections
import gym
import numpy as np
import os
import sys
import torch

from gfootball.env import observation_preprocessing
from gfootball.env import wrappers

# PFRL
import pfrl
from pfrl.agents import CategoricalDoubleDQN
from pfrl import experiments
from pfrl import explorers
from pfrl import nn as pnn
from pfrl import utils
from pfrl import replay_buffers
from pfrl.q_functions import DistributionalDuelingDQN


def phi(x):
    # Feature extractor
    return np.asarray(x, dtype=np.float32) / 255

def make_model():
    global device
    # Q_function
    n_atoms = 51
    v_max = 10
    v_min = -10
    obs_n_channels = 4
    n_actions = 19
    q_func = DistributionalDuelingDQN(n_actions, n_atoms, v_min, v_max, obs_n_channels)

    # Noisy nets
    pnn.to_factorized_noisy(q_func, sigma_scale=0.5)

    # Turn off explorer
    explorer = explorers.Greedy()

    # Use the same hyper parameters as https://arxiv.org/abs/1710.02298
    opt = torch.optim.Adam(q_func.parameters(), 6.25e-5, eps=1.5 * 10 ** -4)

    # Prioritized Replay
    # Anneal beta from beta0 to 1 throughout training
    update_interval = 4
    betasteps = 5 * 10 ** 7 / update_interval
    rbuf = replay_buffers.PrioritizedReplayBuffer(
            10 ** 6,
            alpha=0.5,
            beta0=0.4,
            betasteps=betasteps,
            num_steps=3,
            normalize_by_max=""memory"",
        )


    # prepare agent
    model = CategoricalDoubleDQN(
            q_func,
            opt,
            rbuf,
            gpu=-1,  
            gamma=0.99,
            explorer=explorer,
            minibatch_size=32,
            replay_start_size=2 * 10 ** 4,
            target_update_interval=32000,
            update_interval=update_interval,
            batch_accumulator=""mean"",
            phi=phi,
        )
    
    model.load(""./kaggle_simulations/agent/100000_finish"")
    # model.load(""./kaggle_simulations/agent/best"")
    return model.model.to(device)


device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")
model = make_model()

def agent(obs):
    global device
    global model
    
    # Get observations for the first (and only one) player we control.
    obs = obs['players_raw'][0]
    # Agent we trained uses Super Mini Map (SMM) representation.
    # See https://github.com/google-research/seed_rl/blob/master/football/env.py for details.
    obs = observation_preprocessing.generate_smm([obs])[0]
    # preprocess for obs
    obs = cv2.resize(obs, (84,84))           # resize
    obs = np.transpose(obs, [2,0,1])         # transpose to chw
    obs = torch.tensor(obs).float()          # to tensor
    obs = torch.unsqueeze(obs,0).to(device)  # add batch

    action = model(obs)
    action = action.greedy_actions.cpu().numpy()
    return list(action)"
9657,44665747,20,"from kaggle_environments import make
from kaggle_simulations.agent import main
env = make(""football"", configuration={""save_video"": True, ""scenario_name"": ""11_vs_11_kaggle"", ""running_in_notebook"": True})
obs = env.state[0][""observation""]
action = main.agent(obs)
print(action)"
9658,44665747,21,"from kaggle_environments import make
env = make(""football"", configuration={""save_video"": True, ""scenario_name"": ""11_vs_11_kaggle"", ""running_in_notebook"": True}, debug=True)
output = env.run([""./kaggle_simulations/agent/main.py"", ""run_right""])[-1]
print('Left player: action = %s, reward = %s, status = %s, info = %s' % (output[0][""action""], output[0]['reward'], output[0]['status'], output[0]['info']))
print('Right player: action = %s, reward = %s, status = %s, info = %s' % (output[1][""action""], output[1]['reward'], output[1]['status'], output[1]['info']))
env.render(mode=""human"", width=800, height=600)"
9659,44665747,22,"# Prepare a submision package containing trained model and the main execution logic.
!cd ./kaggle_simulations/agent && tar -czvf /kaggle/working/submit.tar.gz main.py best"
9660,46523501,0,"%%capture
# Install:
# Kaggle environments.
!git clone https://github.com/Kaggle/kaggle-environments.git
!cd kaggle-environments && pip install .

# GFootball environment.
!apt-get update -y
!apt-get install -y libsdl2-gfx-dev libsdl2-ttf-dev

# Make sure that the Branch in git clone and in wget call matches !!
!git clone -b v2.8 https://github.com/google-research/football.git
!mkdir -p football/third_party/gfootball_engine/lib

!wget https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_v2.8.so -O football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so
!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install ."
9661,46523501,1,"%%writefile submission.py
from kaggle_environments.envs.football.helpers import *
from random import randint


# Function to calculate distance 
def get_distance(pos1,pos2):
    return (((pos1[0]-pos2[0])**2)+((pos1[1]-pos2[1])**2))**0.5

# Function to cross ball from wing
def cross_ball():
    pass

# Movement directions
directions = [
[Action.TopLeft, Action.Top, Action.TopRight],
[Action.Left, Action.Idle, Action.Right],
[Action.BottomLeft, Action.Bottom, Action.BottomRight]]

dirsign = lambda x: 1 if abs(x) < 0.01 else (0 if x < 0 else 2)

# Set game plan parameters
goalRange = 0.65
wingRange = 0.21

@human_readable_agent
def agent(obs):
    
    # Add direction to action
    def sticky_check(action, direction):
        if direction in obs['sticky_actions']:
            return action
        else:
            return direction
    
    controlled_player_pos = obs['left_team'][obs['active']]
    
    
    # Pass when KickOff or ThrowIn
    if obs['game_mode'] == GameMode.KickOff or obs['game_mode'] == GameMode.ThrowIn:
        return sticky_check(Action.ShortPass, Action.Right) 
    
    # Shoot when freekick in goal range; If on wing then cross; Otherwise just pass
    if obs['game_mode'] == GameMode.FreeKick:
        # Shoot if in range
        if controlled_player_pos[0] > goalRange and controlled_player_pos[1] < wingRange and controlled_player_pos[1] > -(wingRange):
            ydir = randint(0,2)
            return sticky_check(Action.Shot, directions[ydir][2]) 
        # Cross from right
        if controlled_player_pos[0] > goalRange and controlled_player_pos[1] > wingRange:
            return sticky_check(Action.HighPass, Action.TopRight)
        
        # Cross from left
        if controlled_player_pos[0] > goalRange and controlled_player_pos[1] < -(wingRange):
            return sticky_check(Action.HighPass, Action.BottomRight)
    
    # Cross in for corner
    if obs['game_mode'] == GameMode.Corner and obs['ball'][1] < 0:
        return sticky_check(Action.HighPass, Action.Bottom)
    elif obs['game_mode'] == GameMode.Corner and obs['ball'][1] > 0:
        return sticky_check(Action.HighPass, Action.Top)
        
    # High pass when GoalKick 
    if obs['game_mode'] == GameMode.GoalKick:
        ydir = randint(0,2)
        return sticky_check(Action.HighPass, directions[ydir][2])
    
    # Shoot when Penalty
    if obs['game_mode'] == GameMode.Penalty:
        xdir = randint(0,2)
        ydir = randint(0,2)
        return sticky_check(Action.Shot, directions[ydir][xdir])
    
    # Make sure player is running.
    if Action.Sprint not in obs['sticky_actions']:
        return Action.Sprint
    # We always control left team (observations and actions
    # are mirrored appropriately by the environment).
    controlled_player_pos = obs['left_team'][obs['active']]
    
    # Check if we are in possession
    if obs['ball_owned_player'] == obs['active'] and obs['ball_owned_team'] == 0:
        
        # Clear if we are near our goal
        if controlled_player_pos[0] < -(goalRange):
            return sticky_check(Action.HighPass, Action.Right)
        
        # Shoot if we are in the final third and not at an acute angle
        if controlled_player_pos[0] > goalRange and controlled_player_pos[1] < wingRange and controlled_player_pos[1] > -(wingRange):
            ydir = randint(0,2)
            return sticky_check(Action.Shot, directions[ydir][2])
        #if the goalie is coming out on player near goal shoot
        elif obs['right_team'][0][0] < 0.8 or abs(obs['right_team'][0][1]) > 0.05:
            return Action.Shot
        
        # Cross from right
        if controlled_player_pos[0] > goalRange and controlled_player_pos[1] > wingRange:
            return sticky_check(Action.HighPass, Action.TopRight)
        
        # Cross from left
        if controlled_player_pos[0] > goalRange and controlled_player_pos[1] < -(wingRange):
            return sticky_check(Action.HighPass, Action.BottomRight)
        
        # Run towards the goal otherwise.
        return Action.Right
    else:
        #where ball is going we add the direction xy to ball current location
        ball_targetx=obs['ball'][0]+(1.5 * obs['ball_direction'][0])
        ball_targety=obs['ball'][1]+(1.5 * obs['ball_direction'][1])

        # Euclidian distance to ball
        e_dist=get_distance(obs['left_team'][obs['active']],obs['ball'])

        if e_dist >.005:
            # Run where ball will be
            xdir = dirsign(ball_targetx - controlled_player_pos[0])
            ydir = dirsign(ball_targety - controlled_player_pos[1])
            return directions[ydir][xdir]
        else:
            prob = randint(0,100)
            if prob > 70 and controlled_player_pos[0] < obs['right_team'][obs['active']][0]:
                return Action.Slide
            # Run towards the ball.
            xdir = dirsign(obs['ball'][0] - controlled_player_pos[0])
            ydir = dirsign(obs['ball'][1] - controlled_player_pos[1])
            return directions[ydir][xdir]
"
9662,46523501,2,"# Set up the Environment.
from kaggle_environments import make
env = make(""football"", debug=True, configuration={""save_video"": True, ""scenario_name"": ""11_vs_11_kaggle"", ""running_in_notebook"": True})
output = env.run([""/kaggle/working/submission.py"", ""do_nothing""])[-1]
print('Left player: reward = %s, status = %s, info = %s' % (output[0]['reward'], output[0]['status'], output[0]['info']))
print('Right player: reward = %s, status = %s, info = %s' % (output[1]['reward'], output[1]['status'], output[1]['info']))
env.render(mode=""human"", width=800, height=600)"
9663,46523501,3,"# Dont slide from behind
# Find player density on pitch to pass to open space / run into open space
# Dribbling
# Shoot if goalie off the line"
9664,48292226,0,"%%bash
# dependencies
apt-get -y update > /dev/null
apt-get -y install libsdl2-gfx-dev libsdl2-ttf-dev > /dev/null

# cloudpickle, pytorch, gym
pip3 install ""cloudpickle==1.3.0""
pip3 install ""torch==1.5.1""
pip3 install ""gym==0.17.2""

# gfootball
GRF_VER=v2.8
GRF_PATH=football/third_party/gfootball_engine/lib
GRF_URL=https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_${GRF_VER}.so
git clone -b ${GRF_VER} https://github.com/google-research/football.git
mkdir -p ${GRF_PATH}
wget -q ${GRF_URL} -O ${GRF_PATH}/prebuilt_gameplayfootball.so
cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install . && cd ..

# kaggle-environments
git clone https://github.com/Kaggle/kaggle-environments.git
cd kaggle-environments && pip3 install . && cd ..

# stable-baselines3
git clone https://github.com/DLR-RM/stable-baselines3.git
cd stable-baselines3 && pip3 install . && cd ..

# housekeeping
rm -rf football kaggle-environments stable-baselines3"
9665,48292226,1,"!cp ""../input/gfootball-academy/visualizer.py"" ."
9666,48292226,2,"import os
import base64
import pickle
import zlib
import gym
import numpy as np
import pandas as pd
import torch as th
from torch import nn, tensor
from collections import deque
from gym.spaces import Box, Discrete
from kaggle_environments import make
from kaggle_environments.envs.football.helpers import *
from gfootball.env import create_environment, observation_preprocessing
from stable_baselines3 import PPO
from stable_baselines3.ppo import CnnPolicy
from stable_baselines3.common import results_plotter
from stable_baselines3.common.callbacks import BaseCallback
from stable_baselines3.common.env_checker import check_env
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.torch_layers import BaseFeaturesExtractor
from stable_baselines3.common.vec_env.dummy_vec_env import DummyVecEnv
from stable_baselines3.common.vec_env.subproc_vec_env import SubprocVecEnv
from IPython.display import HTML
from visualizer import visualize
from matplotlib import pyplot as plt
%matplotlib inline"
9667,48292226,3,"class FootballGym(gym.Env):
    spec = None
    metadata = None
    
    def __init__(self, config=None):
        super(FootballGym, self).__init__()
        env_name = ""academy_empty_goal_close""
        rewards = ""scoring,checkpoints""
        if config is not None:
            env_name = config.get(""env_name"", env_name)
            rewards = config.get(""rewards"", rewards)
        self.env = create_environment(
            env_name=env_name,
            stacked=False,
            representation=""raw"",
            rewards = rewards,
            write_goal_dumps=False,
            write_full_episode_dumps=False,
            render=False,
            write_video=False,
            dump_frequency=1,
            logdir=""."",
            extra_players=None,
            number_of_left_players_agent_controls=1,
            number_of_right_players_agent_controls=0)  
        self.action_space = Discrete(19)
        self.observation_space = Box(low=0, high=255, shape=(72, 96, 16), dtype=np.uint8)
        self.reward_range = (-1, 1)
        self.obs_stack = deque([], maxlen=4)
        
    def transform_obs(self, raw_obs):
        obs = raw_obs[0]
        obs = observation_preprocessing.generate_smm([obs])
        if not self.obs_stack:
            self.obs_stack.extend([obs] * 4)
        else:
            self.obs_stack.append(obs)
        obs = np.concatenate(list(self.obs_stack), axis=-1)
        obs = np.squeeze(obs)
        return obs

    def reset(self):
        self.obs_stack.clear()
        obs = self.env.reset()
        obs = self.transform_obs(obs)
        return obs
    
    def step(self, action):
        obs, reward, done, info = self.env.step([action])
        obs = self.transform_obs(obs)
        return obs, float(reward), done, info
    
check_env(env=FootballGym(), warn=True)"
9668,48292226,4,"def conv3x3(in_channels, out_channels, stride=1):
    return nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=True)

class ResidualBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1):
        super().__init__()
        self.relu = nn.ReLU()
        self.conv1 = conv3x3(in_channels, out_channels, stride)
        self.conv2 = conv3x3(out_channels, out_channels, stride)
        
    def forward(self, x):
        residual = x
        out = self.relu(x)
        out = self.conv1(out)
        out = self.relu(out)
        out = self.conv2(out)
        out += residual
        return out
    
class FootballCNN(BaseFeaturesExtractor):
    def __init__(self, observation_space, features_dim=256):
        super().__init__(observation_space, features_dim)
        in_channels = observation_space.shape[0]  # channels x height x width
        self.cnn = nn.Sequential(
            conv3x3(in_channels=in_channels, out_channels=32),
            nn.MaxPool2d(kernel_size=3, stride=2, dilation=1, ceil_mode=False),
            ResidualBlock(in_channels=32, out_channels=32),
            ResidualBlock(in_channels=32, out_channels=32),
            nn.ReLU(),
            nn.Flatten(),
        )
        self.linear = nn.Sequential(
          nn.Linear(in_features=52640, out_features=features_dim, bias=True),
          nn.ReLU(),
        )

    def forward(self, obs):
        return self.linear(self.cnn(obs))"
9669,48292226,5,"scenarios = {0: ""academy_empty_goal_close"",
             1: ""academy_empty_goal"",
             2: ""academy_run_to_score"",
             3: ""academy_run_to_score_with_keeper"",
             4: ""academy_pass_and_shoot_with_keeper"",
             5: ""academy_run_pass_and_shoot_with_keeper"",
             6: ""academy_3_vs_1_with_keeper"",
             7: ""academy_corner"",
             8: ""academy_counterattack_easy"",
             9: ""academy_counterattack_hard"",
             10: ""academy_single_goal_versus_lazy"",
             11: ""11_vs_11_kaggle""}
scenario_name = scenarios[7]"
9670,48292226,6,"def make_env(config=None, rank=0):
    def _init():
        env = FootballGym(config)
        log_file = os.path.join(""."", str(rank))
        env = Monitor(env, log_file, allow_early_resets=True)
        return env
    return _init"
9671,48292226,7,"n_envs = 4
config={""env_name"":scenario_name}
train_env = DummyVecEnv([make_env(config, rank=i) for i in range(n_envs)])
# train_env = SubprocVecEnv([make_env(config, rank=i) for i in range(n_envs)])

n_steps = 512
policy_kwargs = dict(features_extractor_class=FootballCNN,
                     features_extractor_kwargs=dict(features_dim=256))
# model = PPO(CnnPolicy, train_env, 
#             policy_kwargs=policy_kwargs, 
#             learning_rate=0.000343, 
#             n_steps=n_steps, 
#             batch_size=8, 
#             n_epochs=2, 
#             gamma=0.993,
#             gae_lambda=0.95,
#             clip_range=0.08, 
#             ent_coef=0.003, 
#             vf_coef=0.5, 
#             max_grad_norm=0.64, 
#             verbose=0)
model = PPO.load(""../input/gfootball-stable-baselines3/ppo_gfootball.zip"", train_env)"
9672,48292226,8,"from tqdm.notebook import tqdm
class ProgressBar(BaseCallback):
    def __init__(self, verbose=0):
        super(ProgressBar, self).__init__(verbose)
        self.pbar = None

    def _on_training_start(self):
        factor = np.ceil(self.locals['total_timesteps'] / self.model.n_steps)
        n = 1
        try:
            n = len(self.training_env.envs)
        except AttributeError:
            try:
                n = len(self.training_env.remotes)
            except AttributeError:
                n = 1
        total = int(self.model.n_steps * factor / n)
        self.pbar = tqdm(total=total)

    def _on_rollout_start(self):
        self.pbar.refresh()

    def _on_step(self):
        self.pbar.update(1)
        return True

    def _on_rollout_end(self):
        self.pbar.refresh()

    def _on_training_end(self):
        self.pbar.close()
        self.pbar = None

progressbar = ProgressBar()"
9673,48292226,9,"total_epochs = 200
total_timesteps = n_steps * n_envs * total_epochs
model.learn(total_timesteps=total_timesteps, callback=progressbar)
model.save(""ppo_gfootball"")"
9674,48292226,10,"plt.style.use(['seaborn-whitegrid'])
results_plotter.plot_results(["".""], total_timesteps, results_plotter.X_TIMESTEPS, ""GFootball Timesteps"")
results_plotter.plot_results(["".""], total_timesteps, results_plotter.X_EPISODES, ""GFootball Episodes"")"
9675,48292226,11,"plt.style.use(['seaborn-whitegrid'])
log_files = [os.path.join(""."", f""{i}.monitor.csv"") for i in range(n_envs)]

nrows = np.ceil(n_envs/2)
fig = plt.figure(figsize=(8, 2 * nrows))
for i, log_file in enumerate(log_files):
    if os.path.isfile(log_file):
        df = pd.read_csv(log_file, skiprows=1)
        plt.subplot(nrows, 2, i+1, label=log_file)
        df['r'].rolling(window=100).mean().plot(title=f""Rewards: Env {i}"")
        plt.tight_layout()
plt.show()"
9676,48292226,12,"model = PPO.load(""ppo_gfootball"")
test_env = FootballGym({""env_name"":scenario_name})
obs = test_env.reset()
done = False
while not done:
    action, state = model.predict(obs, deterministic=True)
    obs, reward, done, info = test_env.step(action)
    print(f""{Action(action).name.ljust(16,' ')}\t{round(reward,2)}\t{info}"")"
9677,48292226,13,"%%writefile submission.py
import base64
import pickle
import zlib
import numpy as np
import torch as th
from torch import nn, tensor
from collections import deque
from gfootball.env import observation_preprocessing

state_dict = _STATE_DICT_

state_dict = pickle.loads(zlib.decompress(base64.b64decode(state_dict)))

def conv3x3(in_channels, out_channels, stride=1):
    return nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=True)

class ResidualBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1):
        super().__init__()
        self.relu = nn.ReLU()
        self.conv1 = conv3x3(in_channels, out_channels, stride)
        self.conv2 = conv3x3(out_channels, out_channels, stride)
        
    def forward(self, x):
        residual = x
        out = self.relu(x)
        out = self.conv1(out)
        out = self.relu(out)
        out = self.conv2(out)
        out += residual
        return out
    
class PyTorchCnnPolicy(nn.Module):
    global state_dict
    def __init__(self):
        super().__init__()
        self.cnn = nn.Sequential(
            conv3x3(in_channels=16, out_channels=32),
            nn.MaxPool2d(kernel_size=3, stride=2, dilation=1, ceil_mode=False),
            ResidualBlock(in_channels=32, out_channels=32),
            ResidualBlock(in_channels=32, out_channels=32),
            nn.ReLU(),
            nn.Flatten(),
        )
        self.linear = nn.Sequential(
          nn.Linear(in_features=52640, out_features=256, bias=True),
          nn.ReLU(),
        )
        self.action_net = nn.Sequential(
          nn.Linear(in_features=256, out_features=19, bias=True),
          nn.ReLU(),
        )
        self.out_activ = nn.Softmax(dim=1)
        self.load_state_dict(state_dict)

    def forward(self, x):
        x = tensor(x).float() / 255.0  # normalize
        x = x.permute(0, 3, 1, 2).contiguous()  # 1 x channels x height x width
        x = self.cnn(x)
        x = self.linear(x)
        x = self.action_net(x)
        x = self.out_activ(x)
        return int(x.argmax())
    
obs_stack = deque([], maxlen=4)
def transform_obs(raw_obs):
    global obs_stack
    obs = raw_obs['players_raw'][0]
    obs = observation_preprocessing.generate_smm([obs])
    if not obs_stack:
        obs_stack.extend([obs] * 4)
    else:
        obs_stack.append(obs)
    obs = np.concatenate(list(obs_stack), axis=-1)
    return obs

policy = PyTorchCnnPolicy()
policy = policy.float().to('cpu').eval()
def agent(raw_obs):
    obs = transform_obs(raw_obs)
    action = policy(obs)
    return [action]"
9678,48292226,14,"model = PPO.load(""ppo_gfootball"")
_state_dict = model.policy.to('cpu').state_dict()
state_dict = {
    ""cnn.0.weight"":_state_dict['features_extractor.cnn.0.weight'], 
    ""cnn.0.bias"":_state_dict['features_extractor.cnn.0.bias'], 
    ""cnn.2.conv1.weight"":_state_dict['features_extractor.cnn.2.conv1.weight'], 
    ""cnn.2.conv1.bias"":_state_dict['features_extractor.cnn.2.conv1.bias'],
    ""cnn.2.conv2.weight"":_state_dict['features_extractor.cnn.2.conv2.weight'], 
    ""cnn.2.conv2.bias"":_state_dict['features_extractor.cnn.2.conv2.bias'], 
    ""cnn.3.conv1.weight"":_state_dict['features_extractor.cnn.3.conv1.weight'], 
    ""cnn.3.conv1.bias"":_state_dict['features_extractor.cnn.3.conv1.bias'], 
    ""cnn.3.conv2.weight"":_state_dict['features_extractor.cnn.3.conv2.weight'], 
    ""cnn.3.conv2.bias"":_state_dict['features_extractor.cnn.3.conv2.bias'], 
    ""linear.0.weight"":_state_dict['features_extractor.linear.0.weight'], 
    ""linear.0.bias"":_state_dict['features_extractor.linear.0.bias'], 
    ""action_net.0.weight"":_state_dict['action_net.weight'],
    ""action_net.0.bias"":_state_dict['action_net.bias'],
}
state_dict = base64.b64encode(zlib.compress(pickle.dumps(state_dict)))
with open('submission.py', 'r') as file:
    src = file.read()
src = src.replace(""_STATE_DICT_"", f""{state_dict}"")
with open('submission.py', 'w') as file:
    file.write(src)"
9679,48292226,15,"kaggle_env = make(""football"", debug = False,
                  configuration={""scenario_name"": scenario_name, 
                                 ""running_in_notebook"": True,
                                 ""save_video"": False})"
9680,48292226,16,"output = kaggle_env.run([""submission.py"", ""do_nothing""])"
9681,48292226,17,"scores = output[-1][0][""observation""][""players_raw""][0][""score""]
print(""Scores  {0} : {1}"".format(*scores))
print(""Rewards {0} : {1}"".format(output[-1][0][""reward""], output[-1][1][""reward""]))"
9682,48292226,18,viz = visualize(output)
9683,48292226,19,HTML(viz.to_html5_video())
9684,47881384,0,"# Install:
# Kaggle environments.
!git clone https://github.com/Kaggle/kaggle-environments.git
!cd kaggle-environments && pip install .

# GFootball environment.
!apt-get update -y
!apt-get install -y libsdl2-gfx-dev libsdl2-ttf-dev

# Make sure that the Branch in git clone and in wget call matches !!
!git clone -b v2.7 https://github.com/google-research/football.git
!mkdir -p football/third_party/gfootball_engine/lib

!wget https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_v2.7.so -O football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so
!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install ."
9685,47881384,1,"%%writefile agent.py

# Importing Important Imports
from kaggle_environments.envs.football.helpers import *
import math

class Vector:

	'''
	Vector Object
	Parameters: Iterable of length 2 or 3
	'''

	def __init__(self, positions = [0, 0, 0]):
		if len(positions) < 3: positions.append(0)
		self.x, self.y, self.z = positions
		self.vel = None

	def dist(self, other):
		''' Euclidean distance '''
		return math.hypot(other.x - self.x, other.y - self.y)
	
	def add(self, vel):
		''' Adds one vector to the other '''
		return Vector([self.x + vel.x, self.y + vel.y, self.z + vel.z])

	def mult(self, x):
		''' Scales the vector by x '''
		return Vector([self.x * x, self.y * x, self.z * x])

@human_readable_agent
def agent(obs):

	''' Main Agent '''

	# Loading Variables

	N = len(obs['left_team'])

	# Teams
	team = list(map(Vector, obs['left_team']))
	opponents = list(map(Vector, obs['right_team']))

	# Indexes of Active Players
	baller = obs['ball_owned_player']
	active = obs['active']

	# Key Players
	player = team[active]
	goalkeeper = opponents[0]

	# Ball Variables
	ballOwned = (obs['ball_owned_team'] == 0 and active == baller)
	ball = Vector(obs['ball'])
	ball.vel = Vector(obs['ball_direction'])

	# Special Helpers
	sticky = obs['sticky_actions']
	mode = obs['game_mode']

	# Enemy Goal
	target = Vector([1, 0])

	# Directions for movement
	directions = [
		[Action.TopLeft, Action.Top, Action.TopRight],
		[Action.Left, Action.Idle, Action.Right],
		[Action.BottomLeft, Action.Bottom, Action.BottomRight]
	]

	def stickyCheck(action, direction):
		''' Checking for direction and actions '''
		if direction not in sticky:
			return direction
		return action
	
	def dirsign(value):
		''' Getting index for directions '''
		if abs(value) < 0.01: return 1
		elif value < 0: return 0
		return 2

	def getDirection(target, position = player):
		''' Getting direction to move from position to target '''
		xdir = dirsign(target.x - position.x)
		ydir = dirsign(target.y - position.y)
		return directions[ydir][xdir]

	# Always Sprint
	if Action.Sprint not in sticky:
		return Action.Sprint

	# Offense Patterns
	if ballOwned:

		# Special Situations
		if mode in [GameMode.Penalty, GameMode.Corner, GameMode.FreeKick]:
			if player.x > 0: return Action.Shot
			return Action.LongPass

		# Goalkeeper Check
		if baller == 0: 
			return Action.LongPass
		
		# Bad Angle Pass
		if abs(player.y) > 0.2 and player.x > 0.7: 
			return Action.HighPass
			
		# Close to Goalkeeper Shot
		if player.dist(goalkeeper) < 0.4:
			return Action.Shot

		# Goalkeeper is Out
		if goalkeeper.dist(target) > 0.2:
			if player.x > 0:
				return Action.Shot

		#####################
		## Your Ideas Here ##
		#####################

		# Run to Goal
		return getDirection(target)

	# Defensive Patterns
	else:

		# Find the Ball's Next Positions
		nextBall = ball.add(ball.vel.mult(3))

		# Running to the next Ball Position
		if ball.dist(player) > 0.005:
			return getDirection(nextBall)
		
		# Sliding
		elif ball.dist(player) <= 0.005:
			return Action.Slide
		
		# Running Directly at the Ball
		return getDirection(ball)"
9686,47881384,2,"%%writefile visualizer.py
from matplotlib import animation, patches, rcParams
from matplotlib import pyplot as plt
from kaggle_environments.envs.football.helpers import *

WIDTH = 110
HEIGHT = 46.2
PADDING = 10


def initFigure(figwidth=12):
    figheight = figwidth * (HEIGHT + 2 * PADDING) / (WIDTH + 2 * PADDING)

    fig = plt.figure(figsize=(figwidth, figheight))
    ax = plt.axes(xlim=(-PADDING, WIDTH + PADDING), ylim=(-PADDING, HEIGHT + PADDING))
    plt.axis(""off"")
    return fig, ax


def drawPitch(ax):
    paint = ""white""

    # Grass around pitch
    rect = patches.Rectangle((-PADDING / 2, -PADDING / 2), WIDTH + PADDING, HEIGHT + PADDING,
                             lw=1, ec=""black"", fc=""#3f995b"", capstyle=""round"")
    ax.add_patch(rect)

    # Pitch boundaries
    rect = plt.Rectangle((0, 0), WIDTH, HEIGHT, ec=paint, fc=""None"", lw=2)
    ax.add_patch(rect)

    # Middle line
    plt.plot([WIDTH / 2, WIDTH / 2], [0, HEIGHT], color=paint, lw=2)

    # Dots
    dots_x = [11, WIDTH / 2, WIDTH - 11]
    for x in dots_x:
        plt.plot(x, HEIGHT / 2, ""o"", color=paint, lw=2)

    # Penalty box
    penalty_box_dim = [16.5, 40.3]
    penalty_box_pos_y = (HEIGHT - penalty_box_dim[1]) / 2

    rect = plt.Rectangle((0, penalty_box_pos_y),
                         penalty_box_dim[0], penalty_box_dim[1], ec=paint, fc=""None"", lw=2)
    ax.add_patch(rect)
    rect = plt.Rectangle((WIDTH, penalty_box_pos_y), -
                         penalty_box_dim[0], penalty_box_dim[1], ec=paint, fc=""None"", lw=2)
    ax.add_patch(rect)

    # Goal box
    goal_box_dim = [5.5, penalty_box_dim[1] - 11 * 2]
    goal_box_pos_y = (penalty_box_pos_y + 11)

    rect = plt.Rectangle((0, goal_box_pos_y),
                         goal_box_dim[0], goal_box_dim[1], ec=paint, fc=""None"", lw=2)
    ax.add_patch(rect)
    rect = plt.Rectangle((WIDTH, goal_box_pos_y),
                         -goal_box_dim[0], goal_box_dim[1], ec=paint, fc=""None"", lw=2)
    ax.add_patch(rect)

    # Goals
    goal_width = 0.044 / 0.42 * HEIGHT
    goal_pos_y = (HEIGHT / 2 - goal_width / 2)
    rect = plt.Rectangle((0, goal_pos_y), -2, goal_width,
                         ec=paint, fc=paint, lw=2, alpha=0.3)
    ax.add_patch(rect)
    rect = plt.Rectangle((WIDTH, goal_pos_y), 2, goal_width,
                         ec=paint, fc=paint, lw=2, alpha=0.3)
    ax.add_patch(rect)

    # Middle circle
    mid_circle = plt.Circle([WIDTH / 2, HEIGHT / 2], 9.15, color=paint, fc=""None"", lw=2)
    ax.add_artist(mid_circle)

    # Penalty box arcs
    left = patches.Arc([11, HEIGHT / 2], 2 * 9.15, 2 * 9.15,
                       color=paint, fc=""None"", lw=2, angle=0, theta1=308, theta2=52)
    ax.add_patch(left)
    right = patches.Arc([WIDTH - 11, HEIGHT / 2], 2 * 9.15, 2 * 9.15,
                        color=paint, fc=""None"", lw=2, angle=180, theta1=308, theta2=52)
    ax.add_patch(right)

    # Arcs on corners
    corners = [[0, 0], [WIDTH, 0], [WIDTH, HEIGHT], [0, HEIGHT]]
    angle = 0
    for x, y in corners:
        c = patches.Arc([x, y], 2, 2,
                        color=paint, fc=""None"", lw=2, angle=angle, theta1=0, theta2=90)
        ax.add_patch(c)
        angle += 90


def scale_x(x):
    return (x + 1) * (WIDTH / 2)


def scale_y(y):
    return (y + 0.42) * (HEIGHT / 0.42 / 2)


def extract_data(raw_obs):
    obs = raw_obs[0][""observation""][""players_raw""][0]
    res = dict()
    res[""left_team""] = [(scale_x(x), scale_y(y)) for x, y in obs[""left_team""]]
    res[""right_team""] = [(scale_x(x), scale_y(y)) for x, y in obs[""right_team""]]

    ball_x, ball_y, ball_z = obs[""ball""]
    res[""ball""] = [scale_x(ball_x), scale_y(ball_y), ball_z]
    res[""score""] = obs[""score""]
    res[""steps_left""] = obs[""steps_left""]
    res[""ball_owned_team""] = obs[""ball_owned_team""]

    left_active = raw_obs[0][""observation""][""players_raw""][0][""active""]
    res[""left_player""] = res[""left_team""][left_active]

    right_active = raw_obs[1][""observation""][""players_raw""][0][""active""]
    res[""right_player""] = res[""right_team""][right_active]

    res[""right_team_roles""] = obs[""right_team_roles""]
    res[""left_team_roles""] = obs[""left_team_roles""]
    res[""left_team_direction""] = obs[""left_team_direction""]
    res[""right_team_direction""] = obs[""right_team_direction""]
    res[""game_mode""] = GameMode(obs[""game_mode""]).name
    return res


def draw_team(obs, team, side):
    x_coords, y_coords = zip(*obs[side])
    team.set_data(x_coords, y_coords)


def draw_ball(obs, ball):
    ball.set_markersize(8 + obs[""ball""][2])  # Scale size of ball based on height
    ball.set_data(obs[""ball""][:2])


def draw_active_players(obs, left_player, right_player):
    x1, y1 = obs[""left_player""]
    left_player.set_data(x1, y1)

    x2, y2 = obs[""right_player""]
    right_player.set_data(x2, y2)

    if obs[""ball_owned_team""] == 0:
        left_player.set_markerfacecolor(""yellow"")
        left_player.set_markersize(20)
        right_player.set_markerfacecolor(""blue"")
        right_player.set_markersize(18)
    elif obs[""ball_owned_team""] == 1:
        left_player.set_markerfacecolor(""firebrick"")
        left_player.set_markersize(18)
        right_player.set_markerfacecolor(""yellow"")
        right_player.set_markersize(20)
    else:
        left_player.set_markerfacecolor(""firebrick"")
        left_player.set_markersize(18)
        right_player.set_markerfacecolor(""blue"")
        right_player.set_markersize(18)


def draw_team_active(obs, team_left_active, team_right_active):
    team_left_active.set_data(WIDTH / 2 - 7, -7)
    team_right_active.set_data(WIDTH / 2 + 7, -7)

    if obs[""ball_owned_team""] == 0:
        team_left_active.set_markerfacecolor(""indianred"")
    else:
        team_left_active.set_markerfacecolor(""mistyrose"")

    if obs[""ball_owned_team""] == 1:
        team_right_active.set_markerfacecolor(""royalblue"")
    else:
        team_right_active.set_markerfacecolor(""lightcyan"")


def draw_players_directions(obs, directions, side):
    index = 0
    if ""right"" in side:
        index = 11
    for i, player_dir in enumerate(obs[f""{side}_direction""]):
        x_dir, y_dir = player_dir
        dist = (x_dir ** 2 + y_dir ** 2)**0.5 + 0.00001  # to prevent division by 0
        x = obs[side][i][0]
        y = obs[side][i][1]
        directions[i + index].set_data([x, x + x_dir / dist], [y, y + y_dir / dist])


def player_actions(step, side):
    if side == 0:
        actions = {0: ""idle"", 1: """", 2: """", 3: """", 4: """", 5: """", 6: """", 7: """", 8: """",
                   9: ""l_pass"", 10: ""h_pass"", 11: ""s_pass"", 12: ""shot"",
                   13: ""sprint"", 14: ""rel_dir"", 15: ""rel_spr"",
                   16: ""slide"", 17: ""dribble"", 18: ""stp_drb""}
    else:
        actions = {0: ""idle"", 1: """", 2: """", 3: """", 4: """", 5: """", 6: """", 7: """", 8: """",
                   9: ""l_pass"", 10: ""h_pass"", 11: ""s_pass"", 12: ""shot"",
                   13: ""sprint"", 14: ""rel_dir"", 15: ""rel_spr"",
                   16: ""slide"", 17: ""dribble"", 18: ""stp_drb""}

    obs = step[side][""observation""][""players_raw""][0]

    if obs[""sticky_actions""][8]:
        spr = ""+spr""
    else:
        spr = ""-spr""

    if obs[""sticky_actions""][9]:
        drb = ""+drb""
    else:
        drb = ""-drb""

    if 1 in obs[""sticky_actions""][0:8]:
        i = obs[""sticky_actions""][0:8].index(1) + 1
        drn = actions[i]
    else:
        drn = ""|""

    if step[side][""action""]:
        act = actions[step[side][""action""][0]]
    else:
        act = ""idle""

    return f""{spr} {drb} {drn} [{act}]"".ljust(24, "" "")


steps = None
drawings = None
directions = None
ball = left_player = right_player = None
team_left = team_right = None
team_left_active = team_right_active = None
team_left_actions = team_right_actions = None
team_left_number = team_right_number = None
team_left_direction = team_right_direction = None
text_frame = game_mode = match_info = None


def init():
    ball.set_data([], [])
    left_player.set_data([], [])
    right_player.set_data([], [])
    team_left.set_data([], [])
    team_right.set_data([], [])
    team_left_active.set_data([], [])
    team_right_active.set_data([], [])
    return drawings


def animate(i):
    obs = extract_data(steps[i])

    # Draw info about ball possesion
    draw_active_players(obs, left_player, right_player)
    draw_team_active(obs, team_left_active, team_right_active)

    # Draw players
    draw_team(obs, team_left, ""left_team"")
    draw_team(obs, team_right, ""right_team"")

    draw_players_directions(obs, directions, ""left_team"")
    draw_players_directions(obs, directions, ""right_team"")

    draw_ball(obs, ball)

    # Draw textual informations
    text_frame.set_text(f""Step {i}/{obs['steps_left'] + i - 1}"")
    game_mode.set_text(f""{obs['game_mode']} Mode"")

    score_a, score_b = obs[""score""]
    match_info.set_text(f""{score_a} : {score_b}"")

    team_left_actions.set_text(player_actions(steps[i], 0))
    team_right_actions.set_text(player_actions(steps[i], 1))

    team_left_number.set_text(str(steps[i][0][""observation""][""players_raw""][0][""active""]))
    team_right_number.set_text(str(steps[i][1][""observation""][""players_raw""][0][""active""]))

    return drawings


def visualize(trace):
    global steps
    global drawings
    global directions
    global ball, left_player, right_player
    global team_left, team_right
    global team_left_active, team_right_active
    global text_frame, game_mode, match_info
    global team_left_actions, team_right_actions
    global team_left_number, team_right_number
    global team_left_direction, team_right_direction

    rcParams['font.family'] = 'monospace'
    rcParams['font.size'] = 12

    steps = trace

    fig, ax = initFigure()
    drawPitch(ax)
    ax.invert_yaxis()

    left_player, = ax.plot([], [], ""o"", ms=18, mfc=""firebrick"", mew=0, alpha=0.5)
    right_player, = ax.plot([], [], ""o"", ms=18, mfc=""blue"", mew=0, alpha=0.5)
    team_left, = ax.plot([], [], ""o"", ms=12, mfc=""firebrick"", mew=1, mec=""white"")
    team_right, = ax.plot([], [], ""o"", ms=12, mfc=""blue"", mew=1, mec=""white"")
    ball, = ax.plot([], [], ""o"", ms=8, mfc=""wheat"", mew=1, mec=""black"")

    team_left_active, = ax.plot([], [], ""o"", ms=16, mfc=""mistyrose"", mec=""None"")
    team_right_active, = ax.plot([], [], ""o"", ms=16, mfc=""lightcyan"", mec=""None"")

    textheight = -6
    text_frame = ax.text(-5, textheight, """", ha=""left"")
    match_info = ax.text(WIDTH / 2, textheight, """", ha=""center"", fontweight=""bold"")
    game_mode = ax.text(WIDTH + 5, textheight, """", ha=""right"")

    team_left_actions = ax.text(WIDTH / 4 + 2, textheight, """", ha=""center"")
    team_right_actions = ax.text(3 * WIDTH / 4 + 2, textheight, """", ha=""center"")

    team_left_number = ax.text(WIDTH / 2 - 7, -6.3, """", ha=""center"", fontsize=10)
    team_right_number = ax.text(WIDTH / 2 + 7, -6.3, """", ha=""center"", fontsize=10)

    # Drawing of directions definitely can be done in a better way
    directions = []
    for _ in range(22):
        direction, = ax.plot([], [], color=""yellow"", lw=1.5)
        directions.append(direction)

    drawings = [team_left_active, team_right_active, left_player, right_player,
                team_left, team_right, ball, text_frame, match_info,
                game_mode, team_left_actions, team_right_actions, team_left_number, team_right_number]

    fig.subplots_adjust(left=0, bottom=0, right=1, top=1, wspace=None, hspace=None)
    anim = animation.FuncAnimation(fig, animate, init_func=init, blit=True,
                                   interval=100, frames=len(steps), repeat=True)
    return anim"
9687,47881384,3,"from kaggle_environments import make
from kaggle_environments import agent

from visualizer import visualize
from IPython.display import HTML

env = make(""football"", configuration={""save_video"": True, ""scenario_name"": ""11_vs_11_kaggle"", ""running_in_notebook"": True}, debug = True)
output = env.run(['agent.py', 'do_nothing'])
scores = output[-1][0][""observation""][""players_raw""][0][""score""]

print(""Scores  {0} : {1}"".format(*scores))

viz = visualize(output)
HTML(viz.to_html5_video())"
9688,46920963,0,"import cv2
import numpy as np
import matplotlib.pyplot as plt"
9689,46920963,1,"# check if episode is rendered in 3D or 2D
# returns:
# 1: 3D
# 0: 2D
# None: could not open episode

def is3d(episode, verbose=False):

    green_channel = 1
    threshold = 100
    
    # open video
    url = f'https://www.kaggleusercontent.com/episodes/{episode}.webm'
    vcap = cv2.VideoCapture(url)
    # get first frame
    ret, frame = vcap.read()
    
    #close video
    vcap.release()

    if ret == False:
        if verbose:
            print(f'cannot read episode: {episode}!')
        return False
    else:
        mean = np.mean(frame[:,:,green_channel])
        if mean > threshold:
            # 3D
            result = 1
        else:
            result = 0
        if verbose:
            print(f'Green Channel mean: {mean}')
            if result == 1:
                print(""3D"")
            else:
                print(""2D"")
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            plt.imshow(frame)
            plt.show()
        return result"
9690,46920963,2,"print(is3d(episode='4682501', verbose=True))"
9691,46920963,3,"print(is3d(episode='4576290', verbose=True))"
9692,46920963,4,"print(is3d(episode='asdsa', verbose=True)) "
9693,46920963,5,
9694,45963277,0,"# Install:
# Kaggle environments.
!git clone https://github.com/Kaggle/kaggle-environments.git
!cd kaggle-environments && pip install .

# GFootball environment.
!apt-get update -y
!apt-get install -y libsdl2-gfx-dev libsdl2-ttf-dev

# Make sure that the Branch in git clone and in wget call matches !!
!git clone -b v2.6 https://github.com/google-research/football.git
!mkdir -p football/third_party/gfootball_engine/lib

!wget https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_v2.6.so -O football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so
!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install ."
9695,45963277,1,"%%writefile submission.py
from kaggle_environments.envs.football.helpers import *
from math import sqrt

directions = [
[Action.TopLeft, Action.Top, Action.TopRight],
[Action.Left, Action.Idle, Action.Right],
[Action.BottomLeft, Action.Bottom, Action.BottomRight]]

dirsign = lambda x: 1 if abs(x) < 0.01 else (0 if x < 0 else 2)

enemyGoal = [1, 0]  


def inside(pos, area):
    return area[0][0] <= pos[0] <= area[0][1] and area[1][0] <= pos[1] <= area[1][1]


def get_distance(pos1,pos2):
    return ((pos1[0]-pos2[0])**2+(pos1[1]-pos2[1])**2)**0.5


def player_direction(obs):
    controlled_player_pos = obs['left_team'][obs['active']]
    controlled_player_dir = obs['left_team_direction'][obs['active']]
    x = controlled_player_pos[0]
    y = controlled_player_pos[1]
    dx = controlled_player_dir[0]
    dy = controlled_player_dir[1]
#      if x <= dx:
#         return 0
#     if x > dx:
#         return 1
    X_dir, Y_dir = dirsign(dx), dirsign(dy) # goes straight
    return directions[Y_dir][X_dir]


def do_toward(obs, action, CUR_DIR, x, y, tx, ty, just_x=False):
    try:
        xdir = dirsign(tx - x)
        ydir = dirsign(ty - y)
        SEL_DIR = directions[ydir][xdir]
        if just_x:
            # for quick action
            # todo: select closer dir by finding which ydir is currently
            Accepted_dirs = [directions[tmp][xdir] for tmp in [0,1,2]]
            if CUR_DIR not in Accepted_dirs:
                return SEL_DIR
        else:
            if SEL_DIR != CUR_DIR:
                return SEL_DIR
    except:
        pass
    
    if SEL_DIR not in obs['sticky_actions']:
        return SEL_DIR
    
    return action


def run_pass(left_team,right_team,x,y):
    # Go and Pass
    teammateL=0
    teammateR=0
    for i in range(len(left_team)):
        #is there a teamate close to left
        if left_team[i][0] >= x:
            if left_team[i][1] < y:
                if abs(left_team[i][1] - x) <.05:
                    teammateL=teammateL+1
        
        #is there a teamate to right
        if left_team[i][0] >= x:
            if left_team[i][1] > y:
                if abs(left_team[i][1] - x) <.05:
                    teammateR=teammateR+1
    #pass only close to goal
    if x >.75:
        if teammateL > 0 or teammateR > 0:
            return Action.ShortPass
    return Action.Right
    

@human_readable_agent
def agent(obs):
    
    CUR_DIR = player_direction(obs)
    left_team,right_team = obs['left_team'],obs['right_team']
    # our selected player
    controlled_player_pos = left_team[obs['active']]
    # player position
    x,y = controlled_player_pos[0],controlled_player_pos[1]
    #vector where ball is going
    ball_targetx=obs['ball'][0]+obs['ball_direction'][0]
    ball_targety=obs['ball'][1]+obs['ball_direction'][1]
    
    
    # special plays. new start
    if obs[""game_mode""] == GameMode.Penalty:
        return Action.Shot
    if obs[""game_mode""] == GameMode.Corner:
        if controlled_player_pos[0] > 0:
            return Action.Shot
    if obs[""game_mode""] == GameMode.FreeKick:
        return Action.Shot
    if obs[""game_mode""] == GameMode.GoalKick:
        return Action.HighPass
    
    if obs[""game_mode""] in [GameMode.KickOff]:
        # start of game / after goal
        pass
    
    # SPRINT
    if  Action.Sprint not in obs['sticky_actions']:
        return Action.Sprint


    # Does the player we control have the ball?
    if obs['ball_owned_player'] == obs['active'] and obs['ball_owned_team'] == 0:
        
        if obs['right_team'][0][0] < 0.8:
            return Action.Shot
        
        if inside(controlled_player_pos, [[0.6, 1], [-0.2, 0.2]]):
            if x < obs['ball'][0]:
                return Action.Shot
        
        if inside(controlled_player_pos, [[0.2, 1], [-0.25, 0.25]]):
            if y>0:
                return do_toward(obs, Action.Shot, CUR_DIR, x, y, 1.02, 0.03)
            else:
                return do_toward(obs, Action.Shot, CUR_DIR, x, y, 1.02, -0.03)
        
        # if close to goal and too wide for shot pass the ball
        if x >.75:
            if abs(y) >.3:
                return Action.HighPass
            elif abs(y) >.20:
                return Action.LongPass
        
        if -0.2 < x < 0.2:
            return do_toward(obs, Action.ShortPass, CUR_DIR, x, y, 1.02, 0.03, True)
        
        # defense
        if x < -0.2:
            return Action.HighPass
            
        # from sides
        if abs(y) > 0.2:
            if x > 0:
                return Action.LongPass
            else:
                return Action.HighPass
        
        # which way should we run or pass
        return run_pass(left_team,right_team,x,y)
    else:
        #euclidian distance to the ball so we head off movement until very close
        e_dist=get_distance(obs['left_team'][obs['active']],obs['ball'])
        
        #if not close to ball move to where it is going
        if e_dist >.005:
            # Run where ball will be
            xdir = dirsign(ball_targetx - x)
            ydir = dirsign(ball_targety - y)
            return directions[ydir][xdir]
        else:
            #if close to ball go to ball
            return Action.Slide
"
9696,45963277,2,"# A sample game: Our agent vs. do-nothing BOT
from kaggle_environments import make
env = make(""football"", configuration={""save_video"": True, ""scenario_name"": ""11_vs_11_kaggle"", ""running_in_notebook"": True})
output = env.run([""/kaggle/working/submission.py"",""do_nothing""])[-1]
print('vs do nothing')
print('(THIS)Left player: reward = %s, status = %s, info = %s' % (output[0]['reward'], output[0]['status'], output[0]['info']))
print('(BOT)Right player: reward = %s, status = %s, info = %s' % (output[1]['reward'], output[1]['status'], output[1]['info']))
env.render(mode=""human"", width=600, height=400)"
9697,45963277,3,"!find . ! -name 'submission.py' -type d -exec rm -r -f {} +
!ls"
9698,43660797,0,"!apt-get update
!apt-get install -y libsdl2-gfx-dev libsdl2-ttf-dev
!pip3 install tensorflow==2.2
!pip3 install tensorflow_probability==0.9.0
!pip3 install kaggle-environments -U

from IPython.display import Image, clear_output

clear_output()"
9699,43660797,1,"!cp -r /kaggle/input/gfootball-baseline/* .

!mkdir -p football/third_party/gfootball_engine/lib
!wget https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_v2.3.so -O football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so
!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install .

!mkdir -p football/third_party/gfootball_engine
!cd seed_rl && git checkout 34fb2874d41241eb4d5a03344619fb4e34dd9be6

!mkdir /kaggle_simulations/agent

clear_output()"
9700,43660797,2,"%%writefile train.sh
# Training launcher script.

# Make SEED RL visible to Python.
export PYTHONPATH=$PYTHONPATH:$(pwd)
#export PYTHONPATH=$PYTHONPATH:

ENVIRONMENT=$1
AGENT=$2
NUM_ACTORS=$3
shift 3

# Start actor tasks which run environment loop.
actor=0
while [ ""$actor"" -lt ${NUM_ACTORS} ]; do
  python3 seed_rl/${ENVIRONMENT}/${AGENT}_main.py --run_mode=actor --logtostderr $@ --num_actors=${NUM_ACTORS} --task=${actor} 2>/dev/null >/dev/null &
  actor=$(( actor + 1 ))
done
# Start learner task which performs training of the agent.
python3 seed_rl/${ENVIRONMENT}/${AGENT}_main.py --run_mode=learner --logtostderr $@ --num_actors=""${NUM_ACTORS}"""
9701,43660797,3,"!bash train.sh football vtrace 8 '--total_environment_frames=1000000 --game=11_vs_11_kaggle --reward_experiment=scoring,checkpoints --logdir=/kaggle_simulations/agent/'

#clear_output()"
9702,43660797,4,!ls -la /kaggle_simulations/agent/saved_model
9703,43660797,5,"%%writefile /kaggle_simulations/agent/main.py

import collections
import gym
import numpy as np
import os
import sys
import tensorflow as tf

from gfootball.env import observation_preprocessing
from gfootball.env import wrappers

EnvOutput = collections.namedtuple(
    'EnvOutput', 'reward done observation abandoned episode_step')

def prepare_agent_input(observation, prev_action, state):
    # SEED RL agent accepts input in a form of EnvOutput. When not training
    # only observation is used for generating action, so we use a dummy values
    # for the rest.
    env_output = EnvOutput(reward=tf.zeros(shape=[], dtype=tf.float32),
        done=tf.zeros(shape=[], dtype=tf.bool),
        observation=observation, abandoned=False,
        episode_step=tf.zeros(shape=[], dtype=tf.int32))
    # add batch dimension
    prev_action, env_output = tf.nest.map_structure(
        lambda t: tf.expand_dims(t, 0), (prev_action, env_output))

    return (prev_action, env_output, state)

# Previously executed action
previous_action = tf.constant(0, dtype=tf.int64)
# Queue of recent observations (SEED agent we trained uses frame stacking).
observations = collections.deque([], maxlen=4)
# Current state of the agent (used by recurrent agents).
state = ()

# Load previously trained Tensorflow model.
policy = tf.compat.v2.saved_model.load('/kaggle_simulations/agent/saved_model')

def agent(obs):
    global step
    global previous_action
    global observations
    global state
    global policy
    # Get observations for the first (and only one) player we control.
    obs = obs['players_raw'][0]
    # Agent we trained uses Super Mini Map (SMM) representation.
    # See https://github.com/google-research/seed_rl/blob/master/football/env.py for details.
    obs = observation_preprocessing.generate_smm([obs])[0]
    if not observations:
        observations.extend([obs] * 4)
    else:
        observations.append(obs)
    
    # SEED packs observations to reduce transfer times.
    # See PackedBitsObservation in https://github.com/google-research/seed_rl/blob/master/football/observation.py
    obs = np.packbits(obs, axis=-1)
    if obs.shape[-1] % 2 == 1:
        obs = np.pad(obs, [(0, 0)] * (obs.ndim - 1) + [(0, 1)], 'constant')
    obs = obs.view(np.uint16)

    # Execute our agent to obtain action to take.
    agent_output, state = policy.get_action(*prepare_agent_input(obs, previous_action, state))
    previous_action = agent_output.action[0]
    return [int(previous_action)]"
9704,43660797,6,"from kaggle_environments import make
env = make(""football"", configuration={""save_video"": True, ""scenario_name"": ""11_vs_11_kaggle"", ""running_in_notebook"": True})
env.run([""/kaggle_simulations/agent/main.py"", ""run_right""])
env.render(mode=""human"", width=800, height=600)"
9705,43660797,7,!cd /kaggle_simulations/agent && tar -czvf /kaggle/working/submit.tar.gz main.py saved_model
9706,48788547,0,"# Install:
# Kaggle environments.
#!git clone https://github.com/Kaggle/kaggle-environments.git
#!cd kaggle-environments && pip install .

# GFootball environment.
!apt-get update -y
!apt-get install -y libsdl2-gfx-dev libsdl2-ttf-dev

# Make sure that the Branch in git clone and in wget call matches !!
!git clone -b v2.6 https://github.com/google-research/football.git
!mkdir -p football/third_party/gfootball_engine/lib

!wget https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_v2.6.so -O football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so
!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install ."
9707,48788547,1,"!rm -r /kaggle_simulations

!mkdir /kaggle_simulations
!mkdir /kaggle_simulations/agent
!mkdir /kaggle_simulations/agent/saved_model

!cp /kaggle/input/defensive-movementh5/defensive_movement.h5 /kaggle_simulations/agent/saved_model/defensive-movement.h5
!cp /kaggle/input/modelsav/model.sav /kaggle_simulations/agent/saved_model/model.sav
!cp /kaggle/input/model1sav/model1.sav /kaggle_simulations/agent/saved_model/model1.sav"
9708,48788547,2,"%%writefile /kaggle_simulations/agent/main.py
from kaggle_environments.envs.football.helpers import *
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
import pickle
import math

import tensorflow as tf

# load the model from disk
filename = '/kaggle_simulations/agent/saved_model/model1.sav'
loaded_model = pickle.load(open(filename, 'rb'))
filename1 = '/kaggle_simulations/agent/saved_model/model.sav'
loaded_model1 = pickle.load(open(filename1, 'rb'))
#result = loaded_model.score(X_test, y_test)
#print(result)

filename = '/kaggle_simulations/agent/saved_model/defensive-movement.h5'
defensive_movement = tf.keras.models.load_model(filename)
defensive_movement.compile(optimizer='adam',
        loss=['mse'],
        metrics=['mae'])

directions = [[Action.TopLeft, Action.Top, Action.TopRight],
[Action.Left, Action.Idle, Action.Right],
[Action.BottomLeft, Action.Bottom, Action.BottomRight]]

#track raw data to troubleshoot...
track_raw_data=[]

perfectRange = [[0.7, 0.95], [-0.12, 0.12]]

def inside(pos, area):
    return area[0][0] <= pos[0] <= area[0][1] and area[1][0] <= pos[1] <= area[1][1]

def get_distance(pos1,pos2):
    return ((pos1[0]-pos2[0])**2+(pos1[1]-pos2[1])**2)**0.5

def get_heading(pos1,pos2):
    raw_head=math.atan2(pos1[0]-pos2[0],pos1[1]-pos2[1])/math.pi*180

    if raw_head<0:
        head=360+raw_head
    else:
        head=raw_head
    return head

def get_action(action_num):
    if action_num==0:
        return Action.Idle
    if action_num==1:
        return Action.Left
    if action_num==2:
        return Action.TopLeft
    if action_num==3:
        return Action.Top
    if action_num==4:
        return Action.TopRight
    if action_num==5:
        return Action.Right
    if action_num==6:
        return Action.BottomRight
    if action_num==7:
        return Action.Bottom
    if action_num==8:
        return Action.BottomLeft
    if action_num==9:
        return Action.LongPass
    if action_num==10:
        return Action.HighPass
    if action_num==11:
        return Action.ShortPass
    if action_num==12:
        return Action.Shot
    if action_num==13:
        return Action.Sprint
    if action_num==14:
        return Action.ReleaseDirection
    if action_num==15:
        return Action.ReleaseSprint
    if action_num==16:
        #return Action.Sliding
        return Action.Idle
    if action_num==17:
        return Action.Dribble
    if action_num==18:
        #return Action.ReleaseDribble
        return Action.Idle
    return Action.Right

@human_readable_agent
def agent(obs):

    
    controlled_player_pos = obs['left_team'][obs['active']]
    x = controlled_player_pos[0]
    y = controlled_player_pos[1]
    pactive=obs['active']
    
    if obs[""game_mode""] == GameMode.Penalty:
        return Action.Shot
    if obs[""game_mode""] == GameMode.Corner:
        if controlled_player_pos[0] > 0:
            return Action.Shot
    if obs[""game_mode""] == GameMode.FreeKick:
        return Action.Shot
    
    # Make sure player is running.
    if  0 < controlled_player_pos[0] < 0.6 and Action.Sprint not in obs['sticky_actions']:
        return Action.Sprint
    elif 0.6 < controlled_player_pos[0] and Action.Sprint in obs['sticky_actions']:
        return Action.ReleaseSprint
    
    #if we have the ball
    if obs['ball_owned_player'] == obs['active'] and obs['ball_owned_team'] == 0:
        dat=[]
        
        to_append=[]
        to_append1= []
        #return Action.Right
        #get controller player pos
        controlled_player_pos = obs['left_team'][obs['active']]

        if inside(controlled_player_pos, perfectRange) and controlled_player_pos[0] < obs['ball'][0]:
            return Action.Shot
        
        goalx=0.0
        goaly=0.0

        sidelinex=0.0
        sideliney=0.42

        to_append.append(x)
        to_append.append(y)
        
        goal_dist=get_distance((x,y),(goalx,goaly))
        sideline_dist=get_distance((x,y),(sidelinex,sideliney))
        to_append.append(goal_dist)
        to_append.append(sideline_dist)
        to_append1.append(goal_dist)
        to_append1.append(sideline_dist)
        
        for i in range(len(obs['left_team'])):
            dist=get_distance((x,y),(obs['left_team'][i][0],obs['left_team'][i][1]))
            head=get_heading((x,y),(obs['left_team'][i][0],obs['left_team'][i][1]))
            to_append.append(dist)
            to_append.append(head)
            to_append1.append(dist)
            to_append1.append(head)
        
        for i in range(len(obs['right_team'])):
            dist=get_distance((x,y),(obs['right_team'][i][0],obs['right_team'][i][1]))
            head=get_heading((x,y),(obs['right_team'][i][0],obs['right_team'][i][1]))
            to_append.append(dist)
            to_append.append(head)
            to_append1.append(dist)
            to_append1.append(head)
        
        
        if (len(obs['sticky_actions']) != 10):
            dat1 = []
            dat1.append(to_append1)
            predicted1=loaded_model1.predict(dat1)
            do=get_action(predicted1)
            if do == None:
                return Action.Right
            else:
                return do


        for i in range(10):
            to_append.append(obs['sticky_actions'][i])
        
        dat.append(to_append)
        dat1 = []
        dat1.append(to_append1)
        
        predicted=loaded_model.predict(dat)
        predicted1=loaded_model1.predict(dat1)

        if (predicted >= 9 and predicted <= 12):
            predicted1 = predicted
        
        do=get_action(predicted1)
        
        if do == None:
            return Action.Right
        else:
            return do
    
    # if we don't have ball run to ball
    else:

        to_append = []
        dat = []

        controlled_player_pos = obs['left_team'][obs['active']]
        x = controlled_player_pos[0]
        y = controlled_player_pos[1]
        # controlled_player_dir = obs['left_team_direction'][obs['active']]

        to_append.append(x)
        to_append.append(y)
        # to_append.append(controlled_player_dir[0])
        # to_append.append(controlled_player_dir[1])

        ballpos = obs['ball']

        to_append.append(ballpos[0])
        to_append.append(ballpos[1])
        # to_append.append(ballpos[2])

        balldir = obs['ball_direction']

        to_append.append(balldir[0])
        to_append.append(balldir[1])
        # to_append.append(balldir[2])


        if (obs['ball_owned_team'] == 1):
            to_append.append(1)
        else:
            to_append.append(0)


        if Action.Sprint in obs['sticky_actions']:
            to_append.append(1)
        else:
            to_append.append(0)

        if Action.Dribble in obs['sticky_actions']:
            return Action.ReleaseDribble
        
        
        dirsign = lambda x: 1 if abs(x) < 0.01 else (0 if x < 0 else 2)
        #where ball is going
        ball_targetx=obs['ball'][0]+(obs['ball_direction'][0]*5)
        ball_targety=obs['ball'][1]+(obs['ball_direction'][1]*5)
        e_dist=get_distance(obs['left_team'][obs['active']],obs['ball'])
        if e_dist > 1:
            if e_dist >.01:
                # Run where ball will be
                xdir = dirsign(ball_targetx - controlled_player_pos[0])
                ydir = dirsign(ball_targety - controlled_player_pos[1])
                return directions[ydir][xdir]
            else:
                # Run towards the ball.
                xdir = dirsign(obs['ball'][0] - controlled_player_pos[0])
                ydir = dirsign(obs['ball'][1] - controlled_player_pos[1])
                return directions[ydir][xdir]

        dat.append(to_append)
        predicted = defensive_movement.predict(dat)

        ydir = int(round(predicted[0][0]))
        xdir = int(round(predicted[0][1]))

        return directions[ydir][xdir]"
9709,48788547,3,"# Set up the Environment.
from kaggle_environments import make
env = make(""football"", debug=False, configuration={""save_video"": True, ""scenario_name"": ""11_vs_11_kaggle"", ""running_in_notebook"": True})
output = env.run([""/kaggle_simulations/agent/main.py"", ""do_nothing""])[-1]
print('Left player: reward = %s, status = %s, info = %s' % (output[0]['reward'], output[0]['status'], output[0]['info']))
print('Right player: reward = %s, status = %s, info = %s' % (output[1]['reward'], output[1]['status'], output[1]['info']))
env.render(mode=""human"", width=800, height=600)"
9710,48788547,4,!cd /kaggle_simulations/agent && tar -czvf /kaggle/working/submit.tar.gz main.py saved_model
9711,47213571,0,"### This is a fork of https://www.kaggle.com/david1013/tunable-baseline-bot


##  Any upvotes below to his original notebook."
9712,47213571,1,"%%writefile submission.py

## TUNABLE BASELINE BOT

# Tune Here:
SPRINT_RANGE = 0.6

SHOT_RANGE_X = 0.7  
SHOT_RANGE_Y = 0.2

GOALIE_OUT = 0.2
LONG_SHOT_X = 0.4
LONG_SHOT_Y = 0.2
"
9713,47213571,2,
9714,47213571,3,"# Install:
# Kaggle environments.
!git clone --quiet https://github.com/Kaggle/kaggle-environments.git
!cd kaggle-environments && pip install -q .

# GFootball environment.
!apt-get -qq update -y
!apt-get -qq install -y libsdl2-gfx-dev libsdl2-ttf-dev

# Make sure that the Branch in git clone and in wget call matches !!
!git clone --quiet -b v2.3 https://github.com/google-research/football.git
!mkdir -p football/third_party/gfootball_engine/lib

!wget -q https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_v2.3.so -O football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so
!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install -q .
"
9715,47213571,4,
9716,47213571,5,"%%writefile -a submission.py

from kaggle_environments.envs.football.helpers import *
from math import sqrt

directions = [
[Action.TopLeft, Action.Top, Action.TopRight],
[Action.Left, Action.Idle, Action.Right],
[Action.BottomLeft, Action.Bottom, Action.BottomRight]]

dirsign = lambda x: 1 if abs(x) < 0.01 else (0 if x < 0 else 2)

enemyGoal = [1, 0]
GOALKEEPER = 0
"
9717,47213571,6,"%%writefile -a submission.py

shot_range = [[SHOT_RANGE_X, 1], [-SHOT_RANGE_Y, SHOT_RANGE_Y]]
"
9718,47213571,7,"%%writefile -a submission.py

def inside(pos, area):
    return area[0][0] <= pos[0] <= area[0][1] and area[1][0] <= pos[1] <= area[1][1]"
9719,47213571,8,"%%writefile -a submission.py

@human_readable_agent
def agent(obs):
    controlled_player_pos = obs['left_team'][obs['active']]
    
    if obs[""game_mode""] == GameMode.Penalty:
        return Action.Shot
    if obs[""game_mode""] == GameMode.Corner:
        if controlled_player_pos[0] > 0:
            return Action.Shot
    if obs[""game_mode""] == GameMode.FreeKick:
        return Action.Shot
    
    # Make sure player is running down the field.
    if  0 < controlled_player_pos[0] < SPRINT_RANGE and Action.Sprint not in obs['sticky_actions']:
        return Action.Sprint
    elif SPRINT_RANGE < controlled_player_pos[0] and Action.Sprint in obs['sticky_actions']:
        return Action.ReleaseSprint

    # If our player controls the ball:
    if obs['ball_owned_player'] == obs['active'] and obs['ball_owned_team'] == 0:
        
        if inside(controlled_player_pos, shot_range) and controlled_player_pos[0] < obs['ball'][0]:
            return Action.Shot
        
        elif ( abs(obs['right_team'][GOALKEEPER][0] - 1) > GOALIE_OUT   
                and controlled_player_pos[0] > LONG_SHOT_X and abs(controlled_player_pos[1]) < LONG_SHOT_Y ):
            return Action.Shot
        
        else:
            xdir = dirsign(enemyGoal[0] - controlled_player_pos[0])
            ydir = dirsign(enemyGoal[1] - controlled_player_pos[1])
            return directions[ydir][xdir]
        
    # if we we do not have the ball:
    else:
        # Run towards the ball.
        xdir = dirsign(obs['ball'][0] - controlled_player_pos[0])
        ydir = dirsign(obs['ball'][1] - controlled_player_pos[1])
        return directions[ydir][xdir]"
9720,47213571,9,#Test the agent
9721,47213571,10,"import os
import sys 
sys.path.append(os.path.abspath(""/kaggle_simulations/agent/""))
import submission"
9722,47213571,11,"from kaggle_environments import make
env = make(""football"", 
           debug=True,
           configuration={""save_video"": True, 
                          ""scenario_name"": ""11_vs_11_kaggle"", 
                          ""running_in_notebook"": True,
                          #""actTimeout"": 30,
                         })  
output = env.run([submission.agent, ""/kaggle_simulations/agent/submission.py""])[-1]
print('Left player: reward = %s, status = %s, info = %s' % (output[0]['reward'], output[0]['status'], output[0]['info']))
print('Right player: reward = %s, status = %s, info = %s' % (output[1]['reward'], output[1]['status'], output[1]['info']))
env.render(mode=""human"", width=800, height=600)"
9723,47213571,12,
9724,46889474,0,"# Install:
# Kaggle environments.
!git clone https://github.com/Kaggle/kaggle-environments.git
!cd kaggle-environments && pip install .

# GFootball environment.
!apt-get update -y
!apt-get install -y libsdl2-gfx-dev libsdl2-ttf-dev

# Make sure that the Branch in git clone and in wget call matches !!
!git clone -b v2.7 https://github.com/google-research/football.git
!mkdir -p football/third_party/gfootball_engine/lib

!wget https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_v2.7.so -O football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so
!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install . 

# Install Gym
!pip install gym

from IPython.display import clear_output
clear_output()"
9725,46889474,1,"import gfootball
import gym
import numpy as np
import tensorflow as tf
import random
from collections import deque"
9726,46889474,2,"class ReplayBuffer:
    def __init__(self, size=1000000):
        self.memory = deque(maxlen=size)
        
    def remember(self, s_t, a_t, r_t, s_t_next, d_t):
        self.memory.append((s_t, a_t, r_t, s_t_next, d_t))
        
    def sample(self,batch_size):
        batch_size = min(batch_size, len(self.memory))
        return random.sample(self.memory,batch_size)"
9727,46889474,3,"class SplitLayer(tf.keras.layers.Layer):
    def __init__(self):
        super().__init__()

    def call(self, inputs):
        split0,split1,split2,split3 = tf.split(inputs,4,3)
        return [split0,split1,split2,split3]"
9728,46889474,4,"class ConvNN:

    def __init__(self,state_shape,num_actions):
        self.state_shape = state_shape
        self.num_actions = num_actions

    def _model_architecture(self):
    
        frames_input = tf.keras.layers.Input(shape=self.state_shape,name='input',batch_size=1)
        frames_split = SplitLayer()(frames_input)
        conv_branches = []
        for f,frame in enumerate(frames_split):
            conv_branches.append(self._build_conv_branch(frame,f))

        concat = tf.keras.layers.concatenate(conv_branches)
        
        fc0 = tf.keras.layers.Dense(units=8192, name='fc0', 
                                     activation='relu')(concat)
        fc1 = tf.keras.layers.Dense(units=2048, name='fc1', 
                                     activation='relu')(fc0)
        fc2 = tf.keras.layers.Dense(units=512, name='fc2', 
                                     activation='relu')(fc1)
        fc3 = tf.keras.layers.Dense(units=256, name='fc3', 
                                     activation='relu')(fc2)
        fc4 = tf.keras.layers.Dense(units=64, name='fc4', 
                                     activation='relu')(fc3)
        
        action_output = tf.keras.layers.Dense(units=self.num_actions, name='output',
                                               activation='relu')(fc4)
        return frames_input,action_output
        
    @staticmethod
    def _build_conv_branch(frame,number):
        conv1 = tf.keras.layers.Conv2D(16, kernel_size=(8, 8), strides=(4, 4),
                                        name='conv1_frame'+str(number), padding='same',
                                        activation='relu')(frame)
        mp1 = tf.keras.layers.MaxPooling2D(pool_size=2, name=""mp1_frame""+str(number))(conv1)
        conv2 = tf.keras.layers.Conv2D(24, kernel_size=(4, 4), strides=(2, 2),
                                        name='conv2_frame'+str(number),padding='same',
                                        activation='relu')(mp1)
        mp2 = tf.keras.layers.MaxPooling2D(pool_size=2, name=""mp2_frame""+str(number))(conv2)
        conv3 = tf.keras.layers.Conv2D(32, kernel_size=(3, 3), strides=(1, 1),
                                        name='conv3_frame'+str(number),padding='same',
                                        activation='relu')(mp2)
        mp3 = tf.keras.layers.MaxPooling2D(pool_size=2, name=""mp3_frame""+str(number))(conv3)
        conv4 = tf.keras.layers.Conv2D(64, kernel_size=(2, 2), strides=(1, 1),
                                        name='conv4_frame'+str(number), padding='same',
                                        activation='relu')(mp3)
        mp4 = tf.keras.layers.MaxPooling2D(pool_size=1, name=""mp4_frame""+str(number))(conv4)

        flatten = tf.keras.layers.Flatten(name='flatten'+str(number))(mp4)

        return flatten
        
    def build(self):
        frames_input,action_output = self._model_architecture()
        model = tf.keras.Model(inputs=[frames_input], outputs=[action_output])
        return model"
9729,46889474,5,"class Agent:
    def __init__(self, state_shape, num_actions,alpha, gamma, epsilon_i=1.0, epsilon_f=0.01, n_epsilon=0.1):
        self.epsilon_i = epsilon_i
        self.epsilon_f = epsilon_f
        self.n_epsilon = n_epsilon
        self.epsilon = epsilon_i
        self.gamma = gamma
        self.state_shape = state_shape
        self.num_actions = num_actions
        self.optimizer = tf.keras.optimizers.Adam(alpha) 

        self.Q = ConvNN(state_shape,num_actions).build()
        self.Q_ = ConvNN(state_shape,num_actions).build()
        
    def synchronize(self):
        self.Q_.set_weights(self.Q.get_weights())

    def act(self, s_t):
        if np.random.random() < self.epsilon:
            return np.random.randint(self.num_actions,size=3)
        return np.argmax(self.Q(s_t), axis=1)
    
    def decay_epsilon(self, n):
        self.epsilon = max(
            self.epsilon_f, 
            self.epsilon_i - (n/self.n_epsilon)*(self.epsilon_i - self.epsilon_f))

    def update(self, s_t, a_t, r_t, s_t_next, d_t):
        with tf.GradientTape() as tape:
            Q_next = tf.stop_gradient(tf.reduce_max(self.Q_(s_t_next), axis=1))
            Q_pred = tf.reduce_sum(self.Q(s_t)*tf.one_hot(a_t, self.num_actions, dtype=tf.float32), axis=1)
            loss = tf.reduce_mean(0.5*(r_t + (1-d_t)*self.gamma*Q_next - Q_pred)**2)
        grads = tape.gradient(loss, self.Q.trainable_variables)
        self.optimizer.apply_gradients(zip(grads, self.Q.trainable_variables))"
9730,46889474,6,"class VectorizedEnvWrapper(gym.Wrapper):
    def __init__(self, make_env, num_envs=1):
        super().__init__(make_env())
        self.num_envs = num_envs
        self.envs = [make_env() for env_index in range(num_envs)]
    
    def reset(self):
        return np.asarray([env.reset() for env in self.envs])
    
    def reset_at(self, env_index):
        return self.envs[env_index].reset()
    
    def step(self, actions):
        next_states, rewards, dones, infos = [], [], [], []
        for env, action in zip(self.envs, actions):
            next_state, reward, done, info = env.step(action)
            next_states.append(next_state)
            rewards.append(reward)
            dones.append(done)
            infos.append(info)
        return np.asarray(next_states), np.asarray(rewards), \
            np.asarray(dones), np.asarray(infos)"
9731,46889474,7,"class NormalizationWrapper(gym.ObservationWrapper):
    def __init__(self, env):
        super().__init__(env)
    
    def observation(self, obs):
        return obs/255"
9732,46889474,8,"train_agent = Agent((72,96,4),19,alpha=0.1,gamma=0.95)
train_agent.Q.save_weights(""QWeights"")
train_agent.Q_.save_weights(""Q_Weights"")
smm_env = VectorizedEnvWrapper(lambda: NormalizationWrapper(gym.make(""GFootball-11_vs_11_kaggle-SMM-v0"")),1)
train_buffer = ReplayBuffer()"
9733,46889474,9,"def train(env=smm_env,T=3001,batch_size=3,sync_every=10,agent=train_agent,buffer=train_buffer):
    
    agent.Q.load_weights(""QWeights"")
    agent.Q_.load_weights(""Q_Weights"")
    rewards = []
    episode_rewards = 0
    s_t = env.reset()
    
    for t in range(T):
        if t%sync_every == 0:
            agent.synchronize()
        a_t = agent.act(s_t)
        s_t_next, r_t, d_t, info = env.step(a_t)
        buffer.remember(s_t, a_t, r_t, s_t_next, d_t)
        for batch in buffer.sample(batch_size):
            agent.update(*batch)
        agent.decay_epsilon(t/T)
        episode_rewards += r_t
           
    for i in range(env.num_envs):
        rewards.append(episode_rewards[i])
        episode_rewards[i] = 0
        s_t[i] = env.reset_at(i)
        
    agent.epsilon_i = 1.0
    agent.epsilon_f = 0.01
    agent.n_epsilon = 0.1
    agent.epsilon = agent.epsilon_i
    
    return rewards"
9734,46889474,10,"for i in range(2):
    print(train())
    train_agent.Q.save_weights(""QWeights"")
    train_agent.Q_.save_weights(""Q_Weights"")"
9735,46889474,11,"%%writefile submission.py

from gfootball.env import observation_preprocessing
import numpy as np
import tensorflow as tf

class SplitLayer(tf.keras.layers.Layer):
    def __init__(self):
        super().__init__()

    def call(self, inputs):
        split0,split1,split2,split3 = tf.split(inputs,4,3)
        return [split0,split1,split2,split3]
    
class ConvNN:

    def __init__(self,state_shape,num_actions):
        self.state_shape = state_shape
        self.num_actions = num_actions

    def _model_architecture(self):
    
        frames_input = tf.keras.layers.Input(shape=self.state_shape,name='input',batch_size=1)
        frames_split = SplitLayer()(frames_input)
        conv_branches = []
        for f,frame in enumerate(frames_split):
            conv_branches.append(self._build_conv_branch(frame,f))

        concat = tf.keras.layers.concatenate(conv_branches)
        
        fc0 = tf.keras.layers.Dense(units=8192, name='fc0', 
                                     activation='relu')(concat)
        fc1 = tf.keras.layers.Dense(units=2048, name='fc1', 
                                     activation='relu')(fc0)
        fc2 = tf.keras.layers.Dense(units=512, name='fc2', 
                                     activation='relu')(fc1)
        fc3 = tf.keras.layers.Dense(units=256, name='fc3', 
                                     activation='relu')(fc2)
        fc4 = tf.keras.layers.Dense(units=64, name='fc4', 
                                     activation='relu')(fc3)
        
        action_output = tf.keras.layers.Dense(units=self.num_actions, name='output',
                                               activation='relu')(fc4)
        return frames_input,action_output
        
    @staticmethod
    def _build_conv_branch(frame,number):
        conv1 = tf.keras.layers.Conv2D(16, kernel_size=(8, 8), strides=(4, 4),
                                        name='conv1_frame'+str(number), padding='same',
                                        activation='relu')(frame)
        mp1 = tf.keras.layers.MaxPooling2D(pool_size=2, name=""mp1_frame""+str(number))(conv1)
        conv2 = tf.keras.layers.Conv2D(24, kernel_size=(4, 4), strides=(2, 2),
                                        name='conv2_frame'+str(number),padding='same',
                                        activation='relu')(mp1)
        mp2 = tf.keras.layers.MaxPooling2D(pool_size=2, name=""mp2_frame""+str(number))(conv2)
        conv3 = tf.keras.layers.Conv2D(32, kernel_size=(3, 3), strides=(1, 1),
                                        name='conv3_frame'+str(number),padding='same',
                                        activation='relu')(mp2)
        mp3 = tf.keras.layers.MaxPooling2D(pool_size=2, name=""mp3_frame""+str(number))(conv3)
        conv4 = tf.keras.layers.Conv2D(64, kernel_size=(2, 2), strides=(1, 1),
                                        name='conv4_frame'+str(number), padding='same',
                                        activation='relu')(mp3)
        mp4 = tf.keras.layers.MaxPooling2D(pool_size=1, name=""mp4_frame""+str(number))(conv4)

        flatten = tf.keras.layers.Flatten(name='flatten'+str(number))(mp4)

        return flatten
        
    def build(self):
        frames_input,action_output = self._model_architecture()
        model = tf.keras.Model(inputs=[frames_input], outputs=[action_output])
        return model

class Agent:
    def __init__(self, state_shape, num_actions):
        
        self.state_shape = state_shape
        self.num_actions = num_actions

        self.Q = ConvNN(state_shape,num_actions).build()
        self.Q_ = ConvNN(state_shape,num_actions).build()
        
    def act(self, s_t):
        return np.argmax(self.Q(s_t), axis=1)
    
DQN_Agent = Agent((72,96,4),19)

DQN_Agent.Q.load_weights(""QWeights"")
DQN_Agent.Q_.load_weights(""Q_Weights"")

def agent(obs):
    obs = obs['players_raw'][0]
    obs = observation_preprocessing.generate_smm([obs])
    obs = obs/255 #Normalization
    action = DQN_Agent.act(obs)
    return [action[0]]"
9736,46889474,12,"# Set up the Environment.
from kaggle_environments import make
env = make(""football"", configuration={""save_video"": True, ""scenario_name"": ""11_vs_11_kaggle"", ""running_in_notebook"": True})
output = env.run([""/kaggle/working/submission.py"", ""do_nothing""])[-1]
print('Left player: reward = %s, status = %s, info = %s' % (output[0]['reward'], output[0]['status'], output[0]['info']))
print('Right player: reward = %s, status = %s, info = %s' % (output[1]['reward'], output[1]['status'], output[1]['info']))
env.render(mode=""human"", width=800, height=600)"
9737,47152034,0,"# Install:
# Kaggle environments.
!git clone -q https://github.com/Kaggle/kaggle-environments.git
!cd kaggle-environments && pip install .

# GFootball environment.
!apt-get update -y -qq > /dev/null
!apt-get install -y -qq libsdl2-gfx-dev libsdl2-ttf-dev

# Make sure that the Branch in git clone and in wget call matches !!
!git clone -q -b v2.8 https://github.com/google-research/football.git
!mkdir -p football/third_party/gfootball_engine/lib

!wget -q https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_v2.8.so -O football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so
!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip install ."
9738,47152034,1,!pip install rl-replicas
9739,47152034,2,"import os

import gfootball
import gym
import torch
import torch.nn as nn

from rl_replicas.algorithms import TRPO
from rl_replicas.common.policy import Policy
from rl_replicas.common.value_function import ValueFunction
from rl_replicas.common.optimizers import ConjugateGradientOptimizer
from rl_replicas.common.torch_net import mlp

algorithm_name = 'trpo'
environment_name = 'GFootball-11_vs_11_kaggle-simple115v2-v0'
epochs = 5
steps_per_epoch = 4000
policy_network_architecture = [64, 64]
value_function_network_architecture = [64, 64]
value_function_learning_rate = 1e-3
output_dir = './trpo'

env: gym.Env = gym.make(environment_name)

policy_network = mlp(
    sizes = [env.observation_space.shape[0]]+policy_network_architecture+[env.action_space.n]
)

policy: Policy = Policy(
    network = policy_network,
    optimizer = ConjugateGradientOptimizer(params=policy_network.parameters())
)

value_function_network = mlp(
    sizes = [env.observation_space.shape[0]]+value_function_network_architecture+[1]
)
value_function: ValueFunction = ValueFunction(
    network = value_function_network,
    optimizer = torch.optim.Adam(value_function_network.parameters(), lr=value_function_learning_rate)
)

model: TRPO = TRPO(policy, value_function, env, seed=0)

print('an experiment to: {}'.format(output_dir))

print('algorithm:           {}'.format(algorithm_name))
print('epochs:              {}'.format(epochs))
print('steps_per_epoch:     {}'.format(steps_per_epoch))
print('environment:         {}'.format(environment_name))

print('value_function_learning_rate: {}'.format(value_function_learning_rate))
print('policy network:')
print(policy.network)"
9740,47152034,3,"model.learn(
    epochs=epochs,
    steps_per_epoch=steps_per_epoch,
    output_dir=output_dir,
    model_saving=True
)"
9741,47152034,4,"%%writefile ./agent.py
import time

import torch
import gfootball
import gym
from gfootball.env.wrappers import Simple115StateWrapper

from rl_replicas.common.policy import Policy
from rl_replicas.common.torch_net import mlp

start_setup_time: float = time.time()

num_observation = 115
num_action = 19
policy_network_architecture = [64, 64]
model_location = './trpo/model.pt'
model = torch.load(model_location)

policy_network = mlp(
    sizes = [num_observation]+policy_network_architecture+[num_action]
)

policy_network.load_state_dict(model['policy_state_dict'])

policy: Policy = Policy(
    network = policy_network,
    optimizer = None
)

current_step: int = 0

print('Set up Time: {:<8.3g}'.format(time.time() - start_setup_time))

def agent(observation):
    global policy
    global current_step

    start_time: float = time.time()
    current_step += 1

    raw_observation = observation['players_raw']
    simple_115_observation = Simple115StateWrapper.convert_observation(raw_observation, fixed_positions=False)
    observation_tensor: torch.Tensor = torch.from_numpy(simple_115_observation).float()

    action = policy.predict(observation_tensor)
    
    if (current_step%100) == 0:
        print('Current Step: {}'.format(current_step))

    one_step_time = time.time() - start_time
    if one_step_time >= 0.2:
        print('One Step Time exceeded 0.2 seconds: {:<8.3g}'.format(one_step_time))

    return [action.item()]
"
9742,47152034,5,"from kaggle_environments import make

env = make(""football"", 
           configuration={
             ""save_video"": True, 
             ""scenario_name"": ""11_vs_11_kaggle"",
             ""running_in_notebook"": True,
           })

output = env.run([""./agent.py"", ""do_nothing""])[-1]

print('Left player: reward = {}, status = {}, info = {}'.format(output[0]['reward'], output[0]['status'], output[0]['info']))
print('Right player: reward = {}, status = {}, info = {}'.format(output[1]['reward'], output[1]['status'], output[1]['info']))

env.render(mode=""human"", width=800, height=600)"
9743,47152034,6,
9744,47496157,0,"%%capture
# Update kaggle-environments to the newest version.
!pip3 install kaggle-environments -U

# GFootball environment.
!apt-get update -y
!apt-get install -y libsdl2-gfx-dev libsdl2-ttf-dev

# Make sure that the Branch in git clone and in wget call matches !!
!git clone -b v2.7 https://github.com/google-research/football.git
!mkdir -p football/third_party/gfootball_engine/lib

!wget https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_v2.7.so -O football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so
!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install ."
9745,47496157,1,"%%capture
!pip install pfrl==0.1.0"
9746,47496157,2,"import os
import cv2
import sys
import glob 
import random
import imageio
import pathlib
import collections
from collections import deque
import numpy as np
import argparse
import matplotlib.pyplot as plt
import seaborn as sns
sns.set()
%matplotlib inline

from gym import spaces
from tqdm import tqdm
from logging import getLogger, StreamHandler, FileHandler, DEBUG, INFO
from typing import Union, Callable, List, Tuple, Iterable, Any, Dict
from dataclasses import dataclass
from IPython.display import Image, display
sns.set()


# PyTorch
import pfrl
from pfrl.agents import CategoricalDoubleDQN
from pfrl import experiments
from pfrl import explorers
from pfrl import nn as pnn
from pfrl import utils
from pfrl import replay_buffers
from pfrl.wrappers import atari_wrappers
from pfrl.q_functions import DistributionalDuelingDQN

import torch
from torch import nn

# Env
import gym
import gfootball
import gfootball.env as football_env
from gfootball.env import observation_preprocessing"
9747,47496157,3,"# Check we can use GPU
print(torch.cuda.is_available())

# set gpu id
if torch.cuda.is_available(): 
    # NOTE: it is not number of gpu but id which start from 0
    gpu = 0
else:
    # cpu=>-1
    gpu = -1"
9748,47496157,4,"# set logger
def logger_config():
    logger = getLogger(__name__)
    handler = StreamHandler()
    handler.setLevel(""DEBUG"")
    logger.setLevel(""DEBUG"")
    logger.addHandler(handler)
    logger.propagate = False

    filepath = './result.log'
    file_handler = FileHandler(filepath)
    logger.addHandler(file_handler)
    return logger

logger = logger_config()"
9749,47496157,5,"# fixed random seed
# but this is NOT enough to fix the result of rewards.Please tell me the reason.
def seed_everything(seed=1234):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True
    utils.set_random_seed(seed)  # for PFRL
    
# Set a random seed used in PFRL.
seed = 5046
seed_everything(seed)

# Set different random seeds for train and test envs.
train_seed = seed
test_seed = 2 ** 31 - 1 - seed"
9750,47496157,6,"# wrapper for env(resize and transpose channel order)
class TransEnv(gym.ObservationWrapper):
    def __init__(self, env, channel_order=""hwc""):

        gym.ObservationWrapper.__init__(self, env)
        self.height = 84
        self.width = 84
        self.ch = env.observation_space.shape[2]
        shape = {
            ""hwc"": (self.height, self.width, self.ch),
            ""chw"": (self.ch, self.height, self.width),
        }
        self.observation_space = spaces.Box(
            low=0, high=255, shape=shape[channel_order], dtype=np.uint8
        )
        

    def observation(self, frame):
        frame = cv2.resize(frame, (self.width, self.height), interpolation=cv2.INTER_AREA)
        return frame.reshape(self.observation_space.low.shape)"
9751,47496157,7,"def make_env(test):
    # Use different random seeds for train and test envs
    env_seed = test_seed if test else train_seed
    
    # env = gym.make('GFootball-11_vs_11_kaggle-SMM-v0')
    env = football_env.create_environment(
      env_name='11_vs_11_easy_stochastic',  # easy mode
      stacked=False,
      representation='extracted',  # SMM
      rewards='scoring,checkpoints',
      write_goal_dumps=False,
      write_full_episode_dumps=False,
      render=False,
      write_video=False,
      dump_frequency=1,
      logdir='./',
      extra_players=None,
      number_of_left_players_agent_controls=1,
      number_of_right_players_agent_controls=0
    )
    env = TransEnv(env, channel_order=""chw"")

    env.seed(int(env_seed))
    if test:
        # Randomize actions like epsilon-greedy in evaluation as well
        env = pfrl.wrappers.RandomizeAction(env, random_fraction=0.0)
    return env

env = make_env(test=False)
eval_env = make_env(test=True)"
9752,47496157,8,"print('observation space:', env.observation_space.low.shape)
print('action space:', env.action_space)"
9753,47496157,9,"env.reset()
action = env.action_space.sample()
obs, r, done, info = env.step(action)
print('next observation:', obs.shape)
print('reward:', r)
print('done:', done)
print('info:', info)"
9754,47496157,10,"obs_n_channels = env.observation_space.low.shape[0]
n_actions = env.action_space.n
print(""obs_n_channels: "", obs_n_channels)
print(""n_actions: "", n_actions)

# params based the original paper
n_atoms = 51
v_max = 10
v_min = -10
q_func = DistributionalDuelingDQN(n_actions, n_atoms, v_min, v_max, obs_n_channels)
print(q_func)"
9755,47496157,11,"# Noisy nets
pnn.to_factorized_noisy(q_func, sigma_scale=0.5)

# Turn off explorer
explorer = explorers.Greedy()

# Use the same hyper parameters as https://arxiv.org/abs/1710.02298
opt = torch.optim.Adam(q_func.parameters(), 6.25e-5, eps=1.5 * 10 ** -4)

# Prioritized Replay
# Anneal beta from beta0 to 1 throughout training
update_interval = 4
betasteps = 5 * 10 ** 7 / update_interval
rbuf = replay_buffers.PrioritizedReplayBuffer(
        10 ** 5 * 2,  # Default value is 10 ** 6 but it is too large in this notebook. I chose 10 ** 5.
        alpha=0.5,
        beta0=0.4,
        betasteps=betasteps,
        num_steps=3,
        normalize_by_max=""memory"",
    )


def phi(x):
    # Feature extractor
    return np.asarray(x, dtype=np.float32) / 255"
9756,47496157,12,"agent = CategoricalDoubleDQN(
        q_func,
        opt,
        rbuf,
        gpu=gpu,  
        gamma=0.99,
        explorer=explorer,
        minibatch_size=32,
        replay_start_size=2 * 10 ** 4,
        target_update_interval=32000,
        update_interval=update_interval,
        batch_accumulator=""mean"",
        phi=phi,
    )"
9757,47496157,13,"# if you have a pretrained model, agent can load pretrained weight. 
use_pretrained = False
pretrained_path = None
if use_pretrained:
    agent.load(pretrained_path)"
9758,47496157,14,"#num_steps = 1000000
num_steps = 800000"
9759,47496157,15,"%%time
experiments.train_agent_with_evaluation(
    agent=agent,
    env=env,
    steps=num_steps,
    eval_n_steps=None,
    eval_n_episodes=1,
    eval_interval=3000,
    outdir=""./"",
    checkpoint_freq=100000,
    save_best_so_far_agent=True,
    eval_env=eval_env,
    logger=logger
)"
9760,47496157,16,"import csv

def text_csv_converter(datas):
    file_csv = datas.replace(""txt"", ""csv"")
    with open(datas) as rf:
        with open(file_csv, ""w"") as wf:
            readfile = rf.readlines()
            for read_text in readfile:
                read_text = read_text.split()
                writer = csv.writer(wf, delimiter=',')
                writer.writerow(read_text)

filename = ""scores.txt""
text_csv_converter(filename)"
9761,47496157,17,"import pandas as pd
scores = pd.read_csv(""scores.csv"")
scores.tail()"
9762,47496157,18,"# visualize reward each episodes
fig = plt.figure(figsize=(15, 5))
ax1 = fig.add_subplot(121)
ax2 = fig.add_subplot(122)
ax1.set_title(""median reward"")
ax2.set_title(""average loss"")
sns.lineplot(x=""episodes"", y=""median"", data=scores, ax=ax1)
sns.lineplot(x=""episodes"", y=""average_loss"", data=scores,ax=ax2)
plt.show()"
9763,47496157,19,"# clone pfrl repo and move to the directory with main.py 
!git clone https://github.com/pfnet/pfrl.git
!mkdir sub
!mv ./pfrl/pfrl sub
!rm -r ./pfrl"
9764,47496157,20,"import base64

with open(f'./{num_steps}_finish/model.pt', 'rb') as f:
    encoded_string = base64.b64encode(f.read())

with open('./sub/model_weights.py', 'w') as f:
    f.write(f'model_string={encoded_string}')"
9765,47496157,21,"%cd sub
!ls -la "
9766,47496157,22,"%%writefile main.py
import os
import sys
import cv2
import collections
import numpy as np
from gfootball.env import observation_preprocessing

# PFRL
import torch
import pfrl

import base64
from model_weights import model_string

def make_model():
    # Q_function
    model = pfrl.q_functions.DistributionalDuelingDQN(n_actions=19, n_atoms=51, v_min=-10, v_max=10, n_input_channels=4)
    
    # Noisy nets
    pfrl.nn.to_factorized_noisy(model, sigma_scale=0.5)
    
    # load weights
    with open(""model.dat"", ""wb"") as f:
        f.write(base64.b64decode(model_string))
    weights = torch.load(""model.dat"", map_location=torch.device('cpu'))
    model.load_state_dict(weights)
    return model


model = make_model()

def agent(obs):
    global model
    
    # Get observations for the first (and only one) player we control.
    obs = obs['players_raw'][0]
    # Agent we trained uses Super Mini Map (SMM) representation.
    # See https://github.com/google-research/seed_rl/blob/master/football/env.py for details.
    obs = observation_preprocessing.generate_smm([obs])[0]
    # preprocess for obs
    obs = cv2.resize(obs, (84,84))    # resize
    obs = np.transpose(obs, [2,0,1])  # transpose to chw
    obs = torch.tensor(obs).float()   # to tensor
    obs = torch.unsqueeze(obs,0)      # add batch

    actions = model(obs)
    action = int(actions.greedy_actions.numpy()[0])  # modified
    return [action]"
9767,47496157,23,!pip install stickytape
9768,47496157,24,"!stickytape main.py --add-python-path pfrl --add-python-path . > /kaggle/working/submission.py
!rm -r pfrl
%cd /kaggle/working
!ls -la"
9769,47496157,25,"from kaggle_environments import make
from submission import agent
env = make(""football"", configuration={""save_video"": True, ""scenario_name"": ""11_vs_11_kaggle"", ""running_in_notebook"": True})
obs = env.state[0][""observation""]
action = agent(obs)
print(action)"
9770,47496157,26,"from kaggle_environments import make
env = make(""football"", configuration={""save_video"": True, ""scenario_name"": ""11_vs_11_kaggle"", ""running_in_notebook"": True}, debug=True)
agent = ""submission.py""
output = env.run([agent, agent])[-1]
print('Left player: action = %s, reward = %s, status = %s, info = %s' % (output[0][""action""], output[0]['reward'], output[0]['status'], output[0]['info']))
print('Right player: action = %s, reward = %s, status = %s, info = %s' % (output[1][""action""], output[1]['reward'], output[1]['status'], output[1]['info']))
env.render(mode=""human"", width=800, height=600)"
9771,47496157,27,"# to clean output folder
!rm -r /kaggle/working/football
!rm -r /kaggle/working/kaggle-environments"
9772,46268552,0,"def get_closest_opponent(obs):
    posX = obs['left_team'][obs['active']][0]
    posY = obs['left_team'][obs['active']][1]
    shortest_distance = None
    opponent = None
    
    for i in range(1, len(obs[""right_team""])):
        distance_to_opponent = calc_distance(posX, posY, obs[""right_team""][i][0], obs[""right_team""][i][1])
             if shortest_distance == None or distance_to_opponent < shortest_distance:
                shortest_distance = distance_to_opponent
                opponent = obs[""right_team""][i]
    
    return shortest_distance
    

def calc_distance(x1, y1, x2, y2):
    return math.sqrt((x1 - x2) ** 2 + (y1 - y2) ** 2) 
"
9773,46268552,1,
9774,43675529,0,"# Install:
# GFootball environment (https://github.com/google-research/football/),
# SEED RL for training an agent (https://github.com/google-research/seed_rl/),
# Tensorflow 2.2, which is needed by SEED RL.

!apt-get update
!apt-get install -y libsdl2-gfx-dev libsdl2-ttf-dev
!pip3 install tensorflow==2.2
!pip3 install tensorflow_probability==0.9.0

# Update kaggle-environments to the newest version.
!pip3 install kaggle-environments -U

# Make sure that the Branch in git clone and in wget call matches !!
!git clone -b v2.3 https://github.com/google-research/football.git
!mkdir -p football/third_party/gfootball_engine/lib

!wget https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_v2.3.so -O football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so
!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install .

!git clone https://github.com/google-research/seed_rl.git
!cd seed_rl && git checkout 34fb2874d41241eb4d5a03344619fb4e34dd9be6
!mkdir /kaggle_simulations/agent"
9775,43675529,1,"%%writefile train.sh
# Training launcher script.

# Make SEED RL visible to Python.
export PYTHONPATH=$PYTHONPATH:$(pwd)
ENVIRONMENT=$1
AGENT=$2
NUM_ACTORS=$3
shift 3

# Start actor tasks which run environment loop.
actor=0
while [ ""$actor"" -lt ${NUM_ACTORS} ]; do
  python3 seed_rl/${ENVIRONMENT}/${AGENT}_main.py --run_mode=actor --logtostderr $@ --num_actors=${NUM_ACTORS} --task=${actor} 2>/dev/null >/dev/null &
  actor=$(( actor + 1 ))
done
# Start learner task which performs training of the agent.
python3 seed_rl/${ENVIRONMENT}/${AGENT}_main.py --run_mode=learner --logtostderr $@ --num_actors=""${NUM_ACTORS}""
"
9776,43675529,2,"!bash train.sh football vtrace 4 '--total_environment_frames=10000 --game=11_vs_11_kaggle --reward_experiment=scoring,checkpoints --logdir=/kaggle_simulations/agent/'"
9777,43675529,3,!ls -la /kaggle_simulations/agent/saved_model
9778,43675529,4,"%%writefile /kaggle_simulations/agent/main.py

import collections
import gym
import numpy as np
import os
import sys
import tensorflow as tf

from gfootball.env import observation_preprocessing
from gfootball.env import wrappers

EnvOutput = collections.namedtuple(
    'EnvOutput', 'reward done observation abandoned episode_step')

def prepare_agent_input(observation, prev_action, state):
    # SEED RL agent accepts input in a form of EnvOutput. When not training
    # only observation is used for generating action, so we use a dummy values
    # for the rest.
    env_output = EnvOutput(reward=tf.zeros(shape=[], dtype=tf.float32),
        done=tf.zeros(shape=[], dtype=tf.bool),
        observation=observation, abandoned=False,
        episode_step=tf.zeros(shape=[], dtype=tf.int32))
    # add batch dimension
    prev_action, env_output = tf.nest.map_structure(
        lambda t: tf.expand_dims(t, 0), (prev_action, env_output))

    return (prev_action, env_output, state)

# Previously executed action
previous_action = tf.constant(0, dtype=tf.int64)
# Queue of recent observations (SEED agent we trained uses frame stacking).
observations = collections.deque([], maxlen=4)
# Current state of the agent (used by recurrent agents).
state = ()

# Load previously trained Tensorflow model.
policy = tf.compat.v2.saved_model.load('/kaggle_simulations/agent/saved_model')

def agent(obs):
    global step
    global previous_action
    global observations
    global state
    global policy
    # Get observations for the first (and only one) player we control.
    obs = obs['players_raw'][0]
    # Agent we trained uses Super Mini Map (SMM) representation.
    # See https://github.com/google-research/seed_rl/blob/master/football/env.py for details.
    obs = observation_preprocessing.generate_smm([obs])[0]
    if not observations:
        observations.extend([obs] * 4)
    else:
        observations.append(obs)
    
    # SEED packs observations to reduce transfer times.
    # See PackedBitsObservation in https://github.com/google-research/seed_rl/blob/master/football/observation.py
    obs = np.packbits(obs, axis=-1)
    if obs.shape[-1] % 2 == 1:
        obs = np.pad(obs, [(0, 0)] * (obs.ndim - 1) + [(0, 1)], 'constant')
    obs = obs.view(np.uint16)

    # Execute our agent to obtain action to take.
    agent_output, state = policy.get_action(*prepare_agent_input(obs, previous_action, state))
    previous_action = agent_output.action[0]
    return [int(previous_action)]"
9779,43675529,5,"from kaggle_environments import make
env = make(""football"", configuration={""save_video"": True, ""scenario_name"": ""11_vs_11_kaggle"", ""running_in_notebook"": True})
env.run([""/kaggle_simulations/agent/main.py"", ""run_right""])
env.render(mode=""human"", width=800, height=600)"
9780,43675529,6,"# Prepare a submision package containing trained model and the main execution logic.
!cd /kaggle_simulations/agent && tar -czvf /kaggle/working/submit.tar.gz main.py saved_model"
9781,44335473,0,"# Install:
# Kaggle environments.
!git clone https://github.com/Kaggle/kaggle-environments.git
!cd kaggle-environments && pip install .

# GFootball environment.
!apt-get update -y
!apt-get install -y libsdl2-gfx-dev libsdl2-ttf-dev

# Make sure that the Branch in git clone and in wget call matches !!
!git clone -b v2.6 https://github.com/google-research/football.git
!mkdir -p football/third_party/gfootball_engine/lib

!wget https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_v2.6.so -O football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so
!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install ."
9782,44335473,1,"%%writefile submission.py
from kaggle_environments.envs.football.helpers import *

# @human_readable_agent wrapper modifies raw observations 
# provided by the environment:
# https://github.com/google-research/football/blob/master/gfootball/doc/observation.md#raw-observations
# into a form easier to work with by humans.
# Following modifications are applied:
# - Action, PlayerRole and GameMode enums are introduced.
# - 'sticky_actions' are turned into a set of active actions (Action enum)
#    see usage example below.
# - 'game_mode' is turned into GameMode enum.
# - 'designated' field is removed, as it always equals to 'active'
#    when a single player is controlled on the team.
# - 'left_team_roles'/'right_team_roles' are turned into PlayerRole enums.
# - Action enum is to be returned by the agent function.
@human_readable_agent
def agent(obs):
    # Make sure player is running.
    if Action.Sprint not in obs['sticky_actions']:
        return Action.Sprint
    # We always control left team (observations and actions
    # are mirrored appropriately by the environment).
    controlled_player_pos = obs['left_team'][obs['active']]
    # Does the player we control have the ball?
    if obs['ball_owned_player'] == obs['active'] and obs['ball_owned_team'] == 0:
        # Shot if we are 'close' to the goal (based on 'x' coordinate).
        if controlled_player_pos[0] > 0.5:
            return Action.Shot
        # Run towards the goal otherwise.
        return Action.Right
    else:
        # Run towards the ball.
        if obs['ball'][0] > controlled_player_pos[0] + 0.05:
            return Action.Right
        if obs['ball'][0] < controlled_player_pos[0] - 0.05:
            return Action.Left
        if obs['ball'][1] > controlled_player_pos[1] + 0.05:
            return Action.Bottom
        if obs['ball'][1] < controlled_player_pos[1] - 0.05:
            return Action.Top
        # Try to take over the ball if close to the ball.
        return Action.Slide"
9783,44335473,2,"# Set up the Environment.
from kaggle_environments import make
env = make(""football"", configuration={""save_video"": True, ""scenario_name"": ""11_vs_11_kaggle"", ""running_in_notebook"": True})
output = env.run([""/kaggle/working/submission.py"", ""do_nothing""])[-1]
print('Left player: reward = %s, status = %s, info = %s' % (output[0]['reward'], output[0]['status'], output[0]['info']))
print('Right player: reward = %s, status = %s, info = %s' % (output[1]['reward'], output[1]['status'], output[1]['info']))
env.render(mode=""human"", width=800, height=600)"
9784,44925730,0,"import matplotlib.pyplot as plt
import pprint
import glob 
import imageio
import pathlib
import numpy as np
from typing import Tuple
from tqdm import tqdm
import os
import sys
from IPython.display import Image"
9785,44925730,1,"# GFootball environment.
!pip install kaggle_environments
!apt-get update -y
!apt-get install -y libsdl2-gfx-dev libsdl2-ttf-dev
!git clone -b v2.3 https://github.com/google-research/football.git
!mkdir -p football/third_party/gfootball_engine/lib
!wget https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_v2.3.so -O football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so
!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install .

# Some helper code
!git clone https://github.com/garethjns/kaggle-football.git
sys.path.append(""/kaggle/working/kaggle-football/"")"
9786,44925730,2,"import gym
import gfootball  # Required as envs registered on import

simple_env = gym.make(""GFootball-11_vs_11_kaggle-simple115v2-v0"")
pixels_env = gym.make(""GFootball-11_vs_11_kaggle-Pixels-v0"")
smm_env = gym.make(""GFootball-11_vs_11_kaggle-SMM-v0"")

print(f""simple115v2:\n {simple_env.__str__()}\n"")
print(f""Pixels:\n {pixels_env.__str__()}\n"")
print(f""SMM:\n {smm_env.__str__()}\n"")"
9787,44925730,3,"from gfootball.env.football_env import FootballEnv

env_name = ""GFootballBase-v0""
gym.envs.register(id=env_name,
                  entry_point=""gfootball.env.football_env:FootballEnv"",
                  max_episode_steps=10000)"
9788,44925730,4,"from gfootball.env.config import Config

base_env = gym.make(env_name, config=Config())"
9789,44925730,5,"obs = base_env.reset()

pprint.pprint(obs[0])"
9790,44925730,6,"obs = simple_env.reset()

print(obs.shape)

pprint.pprint(obs)"
9791,44925730,7,"from kaggle_football.viz import generate_gif, plot_smm_obs

smm_env = gym.make(""GFootball-11_vs_11_kaggle-SMM-v0"")
print(smm_env.reset().shape)

generate_gif(smm_env, n_steps=200)
Image(filename='smm_env_replay.gif', format='png')"
9792,44925730,8,"from gfootball.env import create_environment

# (These are the args set by the kaggle_environments package)
COMMON_KWARGS = {""stacked"": False, ""representation"": 'raw', ""write_goal_dumps"": False,
                 ""write_full_episode_dumps"": False, ""write_video"": False, ""render"": False,
                 ""number_of_left_players_agent_controls"": 1, ""number_of_right_players_agent_controls"": 0}

create_environment(env_name='11_vs_11_kaggle')"
9793,44925730,9,"chk_reward_env = create_environment(env_name='11_vs_11_kaggle', rewards='scoring,checkpoints')

_ = chk_reward_env.reset()
for s in range(100):
    _, r, _, _ = chk_reward_env.step(5)
    if r > 0:
        print(f""Step {s} checkpoint reward recieved: {r}"")"
9794,44925730,10,run_to_score_env = create_environment(env_name='academy_run_to_score')
9795,44925730,11,"%%writefile random_agent.py
  
from typing import Any
from typing import List

import numpy as np


class RandomAgent:
    def get_action(self, obs: Any) -> int:
        return np.random.randint(19)


AGENT = RandomAgent()


def agent(obs) -> List[int]:
    return [AGENT.get_action(obs)]"
9796,44925730,12,"from kaggle_environments import make  
env = make(""football"", configuration={""save_video"": True,
                                      ""scenario_name"": ""11_vs_11_kaggle""})

# Define players
left_player = ""random_agent.py""  # A custom agent, eg. random_agent.py or example_agent.py
right_player = ""run_right""  # eg. A built in 'AI' agent

# Run the whole sim
# Output returned is a list of length n_steps. Each step is a list containing the output for each player as a dict.
# steps
output = env.run([left_player, right_player])

for s, (left, right) in enumerate(output):
    
    # Just print the last few steps of the output
    if s > 2990:
        print(f""\nStep {s}"")

        print(f""Left player ({left_player}): \n""
              f""actions taken: {left['action']}, ""
              f""reward: {left['reward']}, ""
              f""status: {left['status']}, ""
              f""info: {left['info']}"")

        print(f""Right player ({right_player}): \n""
              f""actions taken: {right['action']}, ""
              f""reward: {right['reward']}, ""
              f""status: {right['status']}, ""
              f""info: {right['info']}\n"")

print(f""Final score: {sum([r['reward'] for r in output[0]])} : {sum([r['reward'] for r in output[1]])}"")

env.render(mode=""human"", width=800, height=600)"
9797,44925730,13,"print(output[-1][0].keys())
print(f""Left player: {output[-1][0]['status']}: {output[-1][0]['info']}"")
print(f""Right player: {output[-1][0]['status']}: {output[-1][1]['info']}"")"
9798,44925730,14,"%%writefile broken_agent.py
  
from typing import Any
from typing import List

class DeliberateException(Exception):
    pass


class BrokenAgent:
    def get_action(self, obs: Any) -> int:
        raise DeliberateException(f""I am broken."")


AGENT = BrokenAgent()


def agent(obs) -> List[int]:
    return [AGENT.get_action(obs)]"
9799,44925730,15,"env = make(""football"", configuration={""save_video"": True,
                                      ""scenario_name"": ""11_vs_11_kaggle""})

output = env.run([""random_agent.py"", ""broken_agent.py""])

print(len(output))
print(f""Left player: {output[-1][0]['status']}: {output[-1][0]['info']}"")
print(f""Right player: {output[-1][0]['status']}: {output[-1][1]['info']}"")"
9800,44925730,16,"env = make(""football"", debug=True,
           configuration={""save_video"": True,
                          ""scenario_name"": ""11_vs_11_kaggle""})

try:
    output = env.run([""random_agent.py"", ""broken_agent.py""])
except DeliberateException as e:
    print(e)"
9801,44925730,17,"from random_agent import agent  


env = make(""football"", configuration={""save_video"": True, ""scenario_name"": ""11_vs_11_kaggle""})
env.reset()

# This is the observation that is passed to agent function
obs_kag_env = env.state[0]['observation']

for _ in range(3000):
    action = agent(obs_kag_env)

    # Environment step is list of agent actions, ie [[agent_1], [agent_2]], 
    # here there is 1 action per agent.
    other_agent_action = [0]
    full_obs = env.step([action, other_agent_action])
    obs_kag_env = full_obs[0]['observation']"
9802,44925730,18,!pip install reinforcement_learning_keras
9803,44925730,19,"import gym
from reinforcement_learning_keras.agents.components.history.training_history import TrainingHistory
from reinforcement_learning_keras.agents.q_learning.exploration.epsilon_greedy import EpsilonGreedy
from reinforcement_learning_keras.agents.q_learning.linear_q_agent import LinearQAgent
from sklearn.exceptions import DataConversionWarning

import warnings


agent = LinearQAgent(name=""linear_q"",
                     env_spec=""GFootball-11_vs_11_kaggle-simple115v2-v0"",
                     eps=EpsilonGreedy(eps_initial=0.9, decay=0.001, eps_min=0.01, 
                                       decay_schedule='linear'),
                     training_history=TrainingHistory(agent_name='linear_q', 
                                                      plotting_on=True, plot_every=25, 
                                                      rolling_average=1))

with warnings.catch_warnings():
    warnings.simplefilter('ignore', DataConversionWarning)
    agent.train(verbose=True, render=False,
                n_episodes=25, max_episode_steps=2000)"
9804,46716012,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only ""../input/"" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
9805,46716012,1,"!apt-get update -y
!apt-get install -y libsdl2-gfx-dev libsdl2-ttf-dev

# Make sure that the Branch in git clone and in wget call matches !!
!git clone -b v2.8 https://github.com/google-research/football.git
!mkdir -p football/third_party/gfootball_engine/lib

!wget https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_v2.8.so -O football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so
!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install ."
9806,46716012,2,"%%writefile submission.py
from kaggle_environments.envs.football.helpers import *
from random import randint


# Function to calculate distance 
def get_distance(pos1,pos2):
    return (((pos1[0]-pos2[0])**2)+((pos1[1]-pos2[1])**2))**0.5

# Function to cross ball from wing
def cross_ball():
    pass

# Movement directions
directions = [
[Action.TopLeft, Action.Top, Action.TopRight],
[Action.Left, Action.Idle, Action.Right],
[Action.BottomLeft, Action.Bottom, Action.BottomRight]]

dirsign = lambda x: 1 if abs(x) < 0.01 else (0 if x < 0 else 2)

# Set game plan parameters
goalRange = 0.65
wingRange = 0.21

@human_readable_agent
def agent(obs):
    
    # Add direction to action
    def sticky_check(action, direction):
        if direction in obs['sticky_actions']:
            return action
        else:
            return direction
    
    controlled_player_pos = obs['left_team'][obs['active']]
    
    
    # Pass when KickOff or ThrowIn
    if obs['game_mode'] == GameMode.KickOff or obs['game_mode'] == GameMode.ThrowIn:
        return sticky_check(Action.ShortPass, Action.Right) 
    
    # Shoot when freekick in goal range; If on wing then cross; Otherwise just pass
    if obs['game_mode'] == GameMode.FreeKick:
        # Shoot if in range
        if controlled_player_pos[0] > goalRange and controlled_player_pos[1] < wingRange and controlled_player_pos[1] > -(wingRange):
            ydir = randint(0,2)
            return sticky_check(Action.Shot, directions[ydir][2]) 
        # Cross from right
        if controlled_player_pos[0] > goalRange and controlled_player_pos[1] > wingRange:
            return sticky_check(Action.HighPass, Action.TopRight)
        
        # Cross from left
        if controlled_player_pos[0] > goalRange and controlled_player_pos[1] < -(wingRange):
            return sticky_check(Action.HighPass, Action.BottomRight)
    
    # Cross in for corner
    if obs['game_mode'] == GameMode.Corner and obs['ball'][1] < 0:
        return sticky_check(Action.HighPass, Action.Bottom)
    elif obs['game_mode'] == GameMode.Corner and obs['ball'][1] > 0:
        return sticky_check(Action.HighPass, Action.Top)
        
    # High pass when GoalKick 
    if obs['game_mode'] == GameMode.GoalKick:
        ydir = randint(0,2)
        return sticky_check(Action.HighPass, directions[ydir][2])
    
    # Shoot when Penalty
    if obs['game_mode'] == GameMode.Penalty:
        xdir = randint(0,2)
        ydir = randint(0,2)
        return sticky_check(Action.Shot, directions[ydir][xdir])
    
    # Make sure player is running.
    if Action.Sprint not in obs['sticky_actions']:
        return Action.Sprint
    # We always control left team (observations and actions
    # are mirrored appropriately by the environment).
    controlled_player_pos = obs['left_team'][obs['active']]
    
    # Check if we are in possession
    if obs['ball_owned_player'] == obs['active'] and obs['ball_owned_team'] == 0:
        
        # Clear if we are near our goal
        if controlled_player_pos[0] < -(goalRange):
            return sticky_check(Action.HighPass, Action.Right)
        
        # Shoot if we are in the final third and not at an acute angle
        if controlled_player_pos[0] > goalRange and controlled_player_pos[1] < wingRange and controlled_player_pos[1] > -(wingRange):
            ydir = randint(0,2)
            return sticky_check(Action.Shot, directions[ydir][2])
        #if the goalie is coming out on player near goal shoot
        elif obs['right_team'][0][0] < 0.8 or abs(obs['right_team'][0][1]) > 0.05:
            return Action.Shot
        
        # Cross from right
        if controlled_player_pos[0] > goalRange and controlled_player_pos[1] > wingRange:
            return sticky_check(Action.HighPass, Action.TopRight)
        
        # Cross from left
        if controlled_player_pos[0] > goalRange and controlled_player_pos[1] < -(wingRange):
            return sticky_check(Action.HighPass, Action.BottomRight)
        
        # Run towards the goal otherwise.
        return Action.Right
    else:
        #where ball is going we add the direction xy to ball current location
        ball_targetx=obs['ball'][0]+(1.5 * obs['ball_direction'][0])
        ball_targety=obs['ball'][1]+(1.5 * obs['ball_direction'][1])

        # Euclidian distance to ball
        e_dist=get_distance(obs['left_team'][obs['active']],obs['ball'])

        if e_dist >.005:
            # Run where ball will be
            xdir = dirsign(ball_targetx - controlled_player_pos[0])
            ydir = dirsign(ball_targety - controlled_player_pos[1])
            return directions[ydir][xdir]
        else:
            prob = randint(0,100)
            if prob > 70 and controlled_player_pos[0] < obs['right_team'][obs['active']][0]:
                return Action.Slide
            # Run towards the ball.
            xdir = dirsign(obs['ball'][0] - controlled_player_pos[0])
            ydir = dirsign(obs['ball'][1] - controlled_player_pos[1])
            return directions[ydir][xdir]"
9807,46716012,3,"# Set up the Environment.
from kaggle_environments import make
env = make(""football"", debug=True, configuration={""save_video"": True, ""scenario_name"": ""11_vs_11_kaggle"", ""running_in_notebook"": True})
output = env.run([""/kaggle/working/submission.py"", ""do_nothing""])[-1]
print('Left player: reward = %s, status = %s, info = %s' % (output[0]['reward'], output[0]['status'], output[0]['info']))
print('Right player: reward = %s, status = %s, info = %s' % (output[1]['reward'], output[1]['status'], output[1]['info']))
env.render(mode=""human"", width=800, height=600)"
9808,47227031,0,"import tensorflow as tf

class Net(tf.keras.Model):
  """"""A simple linear model.""""""

  def __init__(self):
    super(Net, self).__init__()
    self.l1 = tf.keras.layers.Dense(5)

  def call(self, x):
    return self.l1(x)"
9809,47227031,1,"def toy_dataset():
  inputs = tf.range(10.)[:, None]
  labels = inputs * 5. + tf.range(5.)[None, :]
  return tf.data.Dataset.from_tensor_slices(
    dict(x=inputs, y=labels)).repeat().batch(2)

def train_step(net, example, optimizer):
  """"""Trains `net` on `example` using `optimizer`.""""""
  with tf.GradientTape() as tape:
    output = net(example['x'])
    loss = tf.reduce_mean(tf.abs(output - example['y']))
  variables = net.trainable_variables
  gradients = tape.gradient(loss, variables)
  optimizer.apply_gradients(zip(gradients, variables))
  return loss"
9810,47227031,2,"net = Net()

opt = tf.keras.optimizers.Adam(0.1)
dataset = toy_dataset()
iterator = iter(dataset)

for i in range(50):
  example = next(iterator)
  loss = train_step(net, example, opt)
  if i % 10 == 0:
    print(""loss {:1.2f}"".format(loss.numpy()))

net.save_weights('model_weights.ckp')"
9811,47227031,3,"%%writefile model.py
import tensorflow as tf

class Net(tf.keras.Model):
  """"""A simple linear model.""""""

  def __init__(self):
    super(Net, self).__init__()
    self.l1 = tf.keras.layers.Dense(5)

  def call(self, x):
    return self.l1(x)"
9812,47227031,4,"%%writefile main.py
import sys
import os

ext_folder = '/kaggle_simulations/agent/'
sys.path.append(ext_folder) 

import tensorflow as tf
from model import Net

model_name = 'model_weights.ckp'

net = Net()
net.load_weights(os.path.join(ext_folder, model_name))

def agent(obs):
    _ = net.predict(tf.range(10.)[:, None])
    return [1]"
9813,47227031,5,!tar -czvf submission.tar.gz main.py model_weights.* model.py
9814,47227031,6,
9815,43778259,0,"# Install:
# Kaggle environments.
!git clone https://github.com/Kaggle/kaggle-environments.git
!cd kaggle-environments && pip install .

# GFootball environment.
!apt-get update -y
!apt-get install -y libsdl2-gfx-dev libsdl2-ttf-dev

# Make sure that the Branch in git clone and in wget call matches !!
!git clone -b v2.3 https://github.com/google-research/football.git
!mkdir -p football/third_party/gfootball_engine/lib

!wget https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_v2.3.so -O football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so
!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install ."
9816,43778259,1,"%%writefile submission.py
from kaggle_environments.envs.football.helpers import *

@human_readable_agent
def agent(obs):
    import numpy as np
    import pandas as pd
    from pandas import Series, DataFrame
    
    ###### 0. CONSTANTS ######
    
    ENEMY_TARGET = [ 1, 0]
    OWN_TARGET   = [-1, 0]

    STEP_HARD_DIST = 0.1
    STEP_EASY_DIST = 0.5 * STEP_HARD_DIST
    SAFE_DIST = 0.1

    ###### 1. SMART CONTROL: FUNCTIONS ######

    def get_action_steps(step_dist):
        import numpy as np

        return {
            Action.Idle:          [                                0,                                 0],
            Action.Left:          [step_dist * np.cos(        np.pi), step_dist * np.sin(        np.pi)],
            Action.TopLeft:       [step_dist * np.cos( 0.75 * np.pi), step_dist * np.sin( 0.75 * np.pi)],
            Action.Top:           [step_dist * np.cos( 0.5  * np.pi), step_dist * np.sin( 0.5  * np.pi)],
            Action.TopRight:      [step_dist * np.cos( 0.25 * np.pi), step_dist * np.sin( 0.25 * np.pi)],
            Action.Right:         [step_dist * np.cos(            0), step_dist * np.sin(            0)],
            Action.BottomRight:   [step_dist * np.cos(-0.25 * np.pi), step_dist * np.sin(-0.25 * np.pi)],
            Action.Bottom:        [step_dist * np.cos(-0.5  * np.pi), step_dist * np.sin(-0.5  * np.pi)],
            Action.BottomLeft:    [step_dist * np.cos(-0.75 * np.pi), step_dist * np.sin(-0.75 * np.pi)]
        }


    def get_point_point_dist(point0, point):
        import numpy as np

        x0, y0 = point0[0], point0[1]
        x, y = point[0], point[1]
        return np.sqrt((x0 - x) ** 2 + (y0 - y) ** 2)


    def correct_point(point0):
        x, y = point0[0], point0[1]
        return (-1 <= x <= 1) and (-1 <= y <= 1)


    def get_move_action_info(point0, enemy_points, step_dist=STEP_EASY_DIST):
        import numpy as np

        target_dists = {}
        for action, step in get_action_steps(step_dist).items():
            step_point = [point0[0] + step[0], point0[1] + step[1]]
            if correct_point(step_point):
                ## 1. Enemy min distance
                enemy_distances = [
                    get_point_point_dist(step_point, enemy_point)
                    for enemy_point in enemy_points
                ]
                enemy_dist = np.array(enemy_distances).min()

                ## 2. Target distance
                target_dist = get_point_point_dist(step_point, ENEMY_TARGET)

                target_dists[action] = {
                    ""target_dist"" : round(target_dist, 3),
                    ""enemy_dist"" : round(enemy_dist, 3)
                }

        return target_dists


    def get_best_move_action(get_move_action_info, safe_dist=SAFE_DIST):
        from pandas import Series

        safe_actions = {}
        for action, info in get_move_action_info.items():
            target_dist, enemy_dist = info[""target_dist""], info[""enemy_dist""]
            if enemy_dist >= SAFE_DIST:
                safe_actions[action] = target_dist

        if len(safe_actions) == 0:
            return Action.Right ### fix in the future

        target_action = Series(safe_actions).idxmin()

        return target_action


    def make_decision(point0, move_action_info):
        x0, y0 = point0[0], point0[1]

        ## Shot decision
        if x0 >= 0.5:
            return Action.Shot

        ## Move decision
        best_move_action = get_best_move_action(move_action_info)
        return best_move_action
    
    
    ###### 2. GAME START ######
    
    own_points = obs['left_team']
    enemy_points = obs['right_team']
    point0 = obs['left_team'][obs['active']]
    
    if Action.Sprint not in obs['sticky_actions']:
        return Action.Sprint

    if obs['ball_owned_player'] == obs['active'] and obs['ball_owned_team'] == 0:
        ###### OFFENSE: SMART CONTROL ######
        move_action_info = get_move_action_info(point0, enemy_points)
        return make_decision(point0, move_action_info)
    else:
        ###### DEFENCE: OLD STRATEGY ######
        if obs['ball'][0] > point0[0] + 0.05:
            return Action.Right
        if obs['ball'][0] < point0[0] - 0.05:
            return Action.Left
        if obs['ball'][1] > point0[1] + 0.05:
            return Action.Bottom
        if obs['ball'][1] < point0[1] - 0.05:
            return Action.Top
        return Action.Slide"
9817,43778259,2,"# Set up the Environment.
from kaggle_environments import make

env = make(
    ""football"",
    configuration={
        ""save_video"": True,
        ""scenario_name"": ""11_vs_11_kaggle"",
        ""running_in_notebook"": True
    }
)

output = env.run([""/kaggle/working/submission.py"", ""/kaggle/working/submission.py""])[-1]

print('Left player: reward = %s, status = %s, info = %s' % (output[0]['reward'], output[0]['status'], output[0]['info']))
print('Right player: reward = %s, status = %s, info = %s' % (output[1]['reward'], output[1]['status'], output[1]['info']))
env.render(mode=""human"", width=800, height=600)"
9818,46400835,0,"# Install:
# GFootball environment (https://github.com/google-research/football/),
# SEED RL for training an agent (https://github.com/google-research/seed_rl/),
# Tensorflow 2.2, which is needed by SEED RL.

!apt-get update
!apt-get install -y libsdl2-gfx-dev libsdl2-ttf-dev
!pip3 install tensorflow==2.3.1
!pip3 install keras==2.4.3
!pip3 install tensorflow_probability==0.9.0

# Update kaggle-environments to the newest version.
!pip3 install kaggle-environments -U

# Make sure that the Branch in git clone and in wget call matches !!
!git clone -b v2.6 https://github.com/google-research/football.git
!mkdir -p football/third_party/gfootball_engine/lib

!wget https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_v2.6.so -O football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so
!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install .

!git clone https://github.com/google-research/seed_rl.git
!cd seed_rl && git checkout 34fb2874d41241eb4d5a03344619fb4e34dd9be6
!mkdir /kaggle_simulations/agent"
9819,46400835,1,
9820,46400835,2,"!mkdir /kaggle_simulations
!mkdir /kaggle_simulations/agent
!cp -r /kaggle/input//* /kaggle_simulations/agent/"
9821,46400835,3,"!ls /kaggle_simulations/agent/trainlocal/train_320.hdf5
"
9822,46400835,4,"%%writefile train.sh
# Training launcher script.

# Make SEED RL visible to Python.
export PYTHONPATH=$PYTHONPATH:$(pwd)
ENVIRONMENT=$1
AGENT=$2
NUM_ACTORS=$3
shift 3

# Start actor tasks which run environment loop.
actor=0
while [ ""$actor"" -lt ${NUM_ACTORS} ]; do
  python3 seed_rl/${ENVIRONMENT}/${AGENT}_main.py --run_mode=actor --logtostderr $@ --num_actors=${NUM_ACTORS} --task=${actor} 2>/dev/null >/dev/null &
  actor=$(( actor + 1 ))
done
# Start learner task which performs training of the agent.
python3 seed_rl/${ENVIRONMENT}/${AGENT}_main.py --run_mode=learner --logtostderr $@ --num_actors=""${NUM_ACTORS}""
"
9823,46400835,5,"!bash train.sh football vtrace 4 '--total_environment_frames=10 --game=11_vs_11_kaggle --reward_experiment=scoring,checkpoints --logdir=/kaggle_simulations/agent/'"
9824,46400835,6,#!ls -la /kaggle_simulations/agent/trained
9825,46400835,7,#!cp -r /kaggle/input//* /kaggle_simulations/agent/
9826,46400835,8,"
import numpy as np
import tensorflow as tf
from keras.callbacks import TensorBoard
from keras.layers import Input, Dense, Conv2D, MaxPooling2D, Flatten
from keras.models import Model
from keras.optimizers import Adam
from keras import backend as K
from keras.applications.mobilenet_v2 import MobileNetV2
from keras.models import load_model
import tensorflow as tf
model_actor = load_model(""../input/trainlocal/train_320.hdf5"", compile=False)"
9827,46400835,9,"%%writefile /kaggle_simulations/agent/trainlocal/main.py
import collections
import gym
import numpy as np
import os
import sys
import numpy as np
import tensorflow as tf
from keras.callbacks import TensorBoard
from keras.layers import Input, Dense, Conv2D, MaxPooling2D, Flatten
from keras.models import Model
from keras.optimizers import Adam
from keras import backend as K
from keras.applications.mobilenet_v2 import MobileNetV2
from keras.models import load_model
import tensorflow as tf

from gfootball.env import observation_preprocessing
from gfootball.env import wrappers

EnvOutput = collections.namedtuple(
    'EnvOutput', 'reward done observation abandoned episode_step')


sys.path.append(""/kaggle_simulations/agent"")
# Load previously trained Tensorflow model.
model_actor = load_model(""/kaggle_simulations/agent/trainlocal/train_320.hdf5"", compile=False)

dummy_n = np.zeros((1, 1, 19))
dummy_1 = np.zeros((1, 1, 1))


def agent(obs):
    global step_nr
    global previous_action
    global observations
    global state
    global policy
    # Get observations for the first (and only one) player we control.
    obs = obs['players_raw'][0]
    # Agent we trained uses Super Mini Map (SMM) representation.
    # See https://github.com/google-research/seed_rl/blob/master/football/env.py for details.
    obs = observation_preprocessing.generate_smm([obs])[0]

    state_input=obs
    state_input = K.expand_dims(state, 0)
    action_dist = model_actor.predict([state_input, dummy_n, dummy_1, dummy_1, dummy_1], steps=1)
    action = np.random.choice(n_actions, p=action_dist[0, :])
    return [action]
"
9828,46400835,10,
9829,46400835,11,"from kaggle_environments import make
env = make(""football"", configuration={""save_video"": True, ""scenario_name"": ""11_vs_11_kaggle"", ""running_in_notebook"": True})
env.run([""/kaggle_simulations/agent/trainlocal/main.py"", ""run_right""])
env.render(mode=""human"", width=800, height=600)"
9830,46400835,12,!mkdir /kaggle_simulations/agent/trainlocal/saved_model
9831,46400835,13,mv /kaggle_simulations/agent/trainlocal/train_320.hdf5 /kaggle_simulations/agent/trainlocal/saved_model/
9832,46400835,14,"# Prepare a submision package containing trained model and the main execution logic.
!cd /kaggle_simulations/agent/trainlocal && tar -czvf /kaggle/working/submit.tar.gz main.py saved_model"
9833,46400835,15,
9834,46400835,16,
9835,45043550,0,"# Install:
# Kaggle environments.
!git clone --quiet https://github.com/Kaggle/kaggle-environments.git
!cd kaggle-environments && pip install -q .

# GFootball environment.
!apt-get -qq update -y
!apt-get -qq install -y libsdl2-gfx-dev libsdl2-ttf-dev

# Make sure that the Branch in git clone and in wget call matches !!
!git clone --quiet -b v2.3 https://github.com/google-research/football.git
!mkdir -p football/third_party/gfootball_engine/lib

!wget -q https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_v2.3.so -O football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so
!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install -q .
"
9836,45043550,1,"%%writefile submission.py

from math import sqrt
import os

from kaggle_environments.envs.football.helpers import *

SPRINT_RANGE = float(os.environ[""SPRINT_RANGE""])
SHOT_RANGE_X = float(os.environ[""SHOT_RANGE_X""])
SHOT_RANGE_Y = float(os.environ[""SHOT_RANGE_Y""])
GOALIE_OUT = float(os.environ[""GOALIE_OUT""])
LONG_SHOT_X = float(os.environ[""LONG_SHOT_X""])
LONG_SHOT_Y = float(os.environ[""LONG_SHOT_Y""])

directions = [
    [Action.TopLeft, Action.Top, Action.TopRight],
    [Action.Left, Action.Idle, Action.Right],
    [Action.BottomLeft, Action.Bottom, Action.BottomRight]]

dirsign = lambda x: 1 if abs(x) < 0.01 else (0 if x < 0 else 2)

enemyGoal = [1, 0]
GOALKEEPER = 0

shot_range = [[SHOT_RANGE_X, 1], 
              [-SHOT_RANGE_Y, SHOT_RANGE_Y]]

def inside(pos, area):
    return area[0][0] <= pos[0] <= area[0][1] and area[1][0] <= pos[1] <= area[1][1]

@human_readable_agent
def agent(obs):
    controlled_player_pos = obs['left_team'][obs['active']]
    
    if obs[""game_mode""] == GameMode.Penalty:
        return Action.Shot
    if obs[""game_mode""] == GameMode.Corner:
        if controlled_player_pos[0] > 0:
            return Action.Shot
    if obs[""game_mode""] == GameMode.FreeKick:
        return Action.Shot
    
    # Make sure player is running down the field.
    if  0 < controlled_player_pos[0] < SPRINT_RANGE and Action.Sprint not in obs['sticky_actions']:
        return Action.Sprint
    elif SPRINT_RANGE < controlled_player_pos[0] and Action.Sprint in obs['sticky_actions']:
        return Action.ReleaseSprint

    # If our player controls the ball:
    if obs['ball_owned_player'] == obs['active'] and obs['ball_owned_team'] == 0:
        
        if inside(controlled_player_pos, shot_range) and controlled_player_pos[0] < obs['ball'][0]:
            return Action.Shot
        
        elif ( abs(obs['right_team'][GOALKEEPER][0] - 1) > GOALIE_OUT   
                and controlled_player_pos[0] > LONG_SHOT_X and abs(controlled_player_pos[1]) < LONG_SHOT_Y ):
            return Action.Shot
        
        else:
            xdir = dirsign(enemyGoal[0] - controlled_player_pos[0])
            ydir = dirsign(enemyGoal[1] - controlled_player_pos[1])
            return directions[ydir][xdir]
        
    # if we we do not have the ball:
    else:
        # Run towards the ball.
        xdir = dirsign(obs['ball'][0] - controlled_player_pos[0])
        ydir = dirsign(obs['ball'][1] - controlled_player_pos[1])
        return directions[ydir][xdir]"
9837,45043550,2,"import os

from kaggle_environments import make
import numpy as np
import optuna

# Optuna searches parameters that maximized the returned value from this objective function.
def objective(trial):
    # You can get Optuna's parameter suggestion with the `suggest_float` method.
    os.environ[""SPRINT_RANGE""] = str(trial.suggest_float(""SPRINT_RANGE"", 0.0, 1.0))
    os.environ[""SHOT_RANGE_X""] = str(trial.suggest_float(""SHOT_RANGE_X"", 0.0, 1.0))  
    os.environ[""SHOT_RANGE_Y""] = str(trial.suggest_float(""SHOT_RANGE_Y"", 0.0, 1.0))
    os.environ[""GOALIE_OUT""] = str(trial.suggest_float(""GOALIE_OUT"", 0.0, 1.0))
    os.environ[""LONG_SHOT_X""] = str(trial.suggest_float(""LONG_SHOT_X"", 0.0, 1.0))
    os.environ[""LONG_SHOT_Y""] = str(trial.suggest_float(""LONG_SHOT_Y"", 0.0, 1.0))

    # To reduce the noise in reward, let's run the game 5 times for each trial.
    rewards = []
    for _ in range(5):
        env = make(""football"", configuration={""scenario_name"": ""11_vs_11_kaggle""})
        result = env.run([""submission.py"", ""do_nothing""])
        rewards.append(result[-1][0][""reward""])

    return np.mean(rewards)

# You can run the optimization just passing the objective function and the number of trials to Optuna.
# Here, Optuna repeats to run the objective function for 35 times.
study = optuna.create_study(direction=""maximize"")
study.optimize(objective, n_trials=35, show_progress_bar=True)"
9838,45043550,3,study.best_value
9839,45043550,4,study.best_params
9840,45043550,5,optuna.visualization.plot_optimization_history(study)
9841,45043550,6,study.trials_dataframe().head()
9842,45043550,7,"# The importance plot visualizes which parameters has been dominant in the optimization.
optuna.visualization.plot_param_importances(study)"
9843,45043550,8,"# The slice plot shows the objective values along each parameter.
# Here, let's focus on the most dominant parameter.
optuna.visualization.plot_slice(study, params=[""SHOT_RANGE_X""])"
9844,45043550,9,"# The parallel coordinate plot shows the relationship among multiple parameters and the objective function.
optuna.visualization.plot_parallel_coordinate(study)"
9845,45043550,10,"def objective(trial):
    os.environ[""SPRINT_RANGE""] = str(trial.suggest_float(""SPRINT_RANGE"", 0.25, 0.9))
    os.environ[""SHOT_RANGE_X""] = str(trial.suggest_float(""SHOT_RANGE_X"", 0.5, 1.0))  
    os.environ[""SHOT_RANGE_Y""] = str(trial.suggest_float(""SHOT_RANGE_Y"", 0.0, 1.0))
    os.environ[""GOALIE_OUT""] = str(trial.suggest_float(""GOALIE_OUT"", 0.0, 0.4))
    os.environ[""LONG_SHOT_X""] = str(trial.suggest_float(""LONG_SHOT_X"", 0.25, 0.75))
    os.environ[""LONG_SHOT_Y""] = str(trial.suggest_float(""LONG_SHOT_Y"", 0.5, 1.0))
    
    rewards = []
    for _ in range(5):
        env = make(""football"", configuration={""scenario_name"": ""11_vs_11_kaggle""})
        result = env.run([""submission.py"", ""do_nothing""])
        rewards.append(result[-1][0][""reward""])

    return np.mean(rewards)

# You can reuse the study object to run additional 15 trials.
study.optimize(objective, n_trials=15, show_progress_bar=True)"
9846,45043550,11,optuna.visualization.plot_optimization_history(study)
9847,45043550,12,study.best_value
9848,45043550,13,study.best_params
9849,45043550,14,"import pickle
with open(""study.pkl"", ""wb"") as fw: 
    pickle.dump(study, fw)

# You can load as:
# study = pickle.load(open(""study.pkl"", ""rb""))"
9850,45043550,15,
9851,43842728,0,"# Install:
# Kaggle environments.
!git clone -q https://github.com/Kaggle/kaggle-environments.git
!cd kaggle-environments && pip install -q .
# GFootball environment.
!apt-get update -qy 
!apt-get install -qy libsdl2-gfx-dev libsdl2-ttf-dev
# Make sure that the Branch in git clone and in wget call matches !!
!git clone -b v2.3 https://github.com/google-research/football.git
!mkdir -p football/third_party/gfootball_engine/lib
!wget -q --show-progress https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_v2.3.so -O football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so
!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install -q ."
9852,43842728,1,"%%writefile submission.py
import numpy as np
from kaggle_environments.envs.football.helpers import *

@human_readable_agent
def agent(obs):
    
    # Global param
    goal_threshold = 0.5
    gravity = 0.098
    pick_height = 0.5
    step_length = 0.015 # As we always sprint
    body_radius = 0.012
    slide_threshold = step_length + body_radius
    
    # Ignore drag to estimate the landing point
    def ball_landing(ball, ball_direction):
        start_height = ball[2]
        end_height = pick_height
        start_speed = ball_direction[2]
        time = np.sqrt(start_speed**2/gravity**2 - 2/gravity*(end_height-start_height)) + start_speed/gravity
        return [ball[0]+ball_direction[0]*time, ball[1]+ball_direction[1]*time]
    
    # Check whether pressing on direction buttons and take action if so
    # Else press on direction first
    def sticky_check(action, direction):
        if direction in obs['sticky_actions']:
            return action
        else:
            return direction
    
    # Find right team positions
    def_team_pos = obs['right_team']
    # Fix goalkeeper index here as PlayerRole has issues
    # Default PlayerRole [0, 7, 9, 2, 1, 1, 3, 5, 5, 5, 6]
    def_keeper_pos = obs['right_team'][0]
    
    # We always control left team (observations and actions
    # are mirrored appropriately by the environment).
    controlled_player_pos = obs['left_team'][obs['active']]
    # Get team size
    N = len(obs['left_team'])
    
    # Does the player we control have the ball?
    if obs['ball_owned_player'] == obs['active'] and obs['ball_owned_team'] == 0:
        # Kickoff strategy: short pass to teammate
        if obs['game_mode'] == GameMode.KickOff:
            return sticky_check(Action.ShortPass, Action.Top) if controlled_player_pos[1] > 0 else sticky_check(Action.ShortPass, Action.Bottom)
        # Goalkick strategy: high pass to front
        if obs['game_mode'] == GameMode.GoalKick:
            return sticky_check(Action.LongPass, Action.Right)
        # Freekick strategy: make shot when close to goal, high pass when in back field, and short pass in mid field
        if obs['game_mode'] == GameMode.FreeKick:
            if controlled_player_pos[0] > goal_threshold:
                if abs(controlled_player_pos[1]) < 0.1:
                    return sticky_check(Action.Shot, Action.Right)
                if abs(controlled_player_pos[1]) < 0.3:
                    return sticky_check(Action.Shot, Action.TopRight) if controlled_player_pos[1]>0 else sticky_check(Action.Shot, Action.BottomRight)
                return sticky_check(Action.HighPass, Action.Top) if controlled_player_pos[1]>0 else sticky_check(Action.HighPass, Action.Bottom)
            
            if controlled_player_pos[0] < -goal_threshold:
                if abs(controlled_player_pos[1]) < 0.3:
                    return sticky_check(Action.HighPass, Action.Right)
                return sticky_check(Action.HighPass, Action.Top) if controlled_player_pos[1]>0 else sticky_check(Action.HighPass, Action.Bottom)
            
            if abs(controlled_player_pos[1]) < 0.3:
                return sticky_check(Action.ShortPass, Action.Right)
            return sticky_check(Action.ShortPass, Action.Top) if controlled_player_pos[1]>0 else sticky_check(Action.ShortPass, Action.Bottom)
        # Corner strategy: high pass to goal area
        if obs['game_mode'] == GameMode.Corner:
            return sticky_check(Action.HighPass, Action.Top) if controlled_player_pos[1]>0 else sticky_check(Action.HighPass, Action.Bottom)
        # Throwin strategy: short pass into field
        if obs['game_mode'] == GameMode.ThrowIn:
            return sticky_check(Action.ShortPass, Action.Top) if controlled_player_pos[1]>0 else sticky_check(Action.ShortPass, Action.Bottom)
        # Penalty strategy: make a shot
        if obs['game_mode'] == GameMode.Penalty:
            right_actions = [Action.TopRight, Action.BottomRight, Action.Right]
            for action in right_actions:
                if action in obs['sticky_actions']:
                    return Action.Shot
            return np.random.choice(right_actions)
            
        # Defending strategy
        if controlled_player_pos[0] < -goal_threshold:
            if abs(controlled_player_pos[1]) < 0.3:
                return sticky_check(Action.HighPass, Action.Right)
            return sticky_check(Action.HighPass, Action.Top) if controlled_player_pos[1]>0 else sticky_check(Action.HighPass, Action.Bottom)
            
        # Make sure player is running.
        if Action.Sprint not in obs['sticky_actions']:
            return Action.Sprint
        
        # Shot if we are 'close' to the goal (based on 'x' coordinate).
        if controlled_player_pos[0] > goal_threshold:
            if abs(controlled_player_pos[1]) < 0.1:
                return sticky_check(Action.Shot, Action.Right)
            if abs(controlled_player_pos[1]) < 0.3:
                return sticky_check(Action.Shot, Action.TopRight) if controlled_player_pos[1]>0 else sticky_check(Action.Shot, Action.BottomRight)
            elif controlled_player_pos[0] < 0.85:
                return Action.Right
            else:
                return sticky_check(Action.HighPass, Action.Top) if controlled_player_pos[1]>0 else sticky_check(Action.HighPass, Action.Bottom)
        
        # Run towards the goal otherwise.
        return Action.Right
    else:
        # when the ball is generally on the ground not flying
        if obs['ball'][2] <= pick_height:
            # Run towards the ball's left position.
            if obs['ball'][0] > controlled_player_pos[0] + slide_threshold:
                if obs['ball'][1] > controlled_player_pos[1] + slide_threshold:
                    return Action.BottomRight
                elif obs['ball'][1] < controlled_player_pos[1] - slide_threshold:
                    return Action.TopRight
                else:
                    return Action.Right
            elif obs['ball'][0] < controlled_player_pos[0] + slide_threshold:
                if obs['ball'][1] > controlled_player_pos[1] + slide_threshold:
                    return Action.BottomLeft
                elif obs['ball'][1] < controlled_player_pos[1] - slide_threshold:
                    return Action.TopLeft
                else:
                    return Action.Left
            # When close to the ball, try to take over.
            else:
                return Action.Slide
        # when the ball is flying
        else:
            landing_point = ball_landing(obs['ball'], obs['ball_direction'])
            # Run towards the landing point's left position.
            if landing_point[0] - body_radius > controlled_player_pos[0] + slide_threshold:
                if landing_point[1] > controlled_player_pos[1] + slide_threshold:
                    return Action.BottomRight
                elif landing_point[1] < controlled_player_pos[1] - slide_threshold:
                    return Action.TopRight
                else:
                    return Action.Right
            elif landing_point[0] - body_radius < controlled_player_pos[0] + slide_threshold:
                if landing_point[1] > controlled_player_pos[1] + slide_threshold:
                    return Action.BottomLeft
                elif landing_point[1] < controlled_player_pos[1] - slide_threshold:
                    return Action.TopLeft
                else:
                    return Action.Left
            # Try to take over the ball if close to the ball.
            elif controlled_player_pos[0] > goal_threshold:
                # Keep making shot when around landing point
                return sticky_check(Action.Shot, Action.Right) if ['ball'][2] <= pick_height else Action.Idle
            else:
                return sticky_check(Action.Slide, Action.Right) if ['ball'][2] <= pick_height else Action.Idle"
9853,43842728,2,"# Set up the Environment.
from kaggle_environments import make
env = make(""football"", configuration={""save_video"": True, ""scenario_name"": ""11_vs_11_kaggle"", ""running_in_notebook"": True})
output = env.run([""/kaggle/working/submission.py"", ""do_nothing""])[-1]
print('Left player: reward = %s, status = %s, info = %s' % (output[0]['reward'], output[0]['status'], output[0]['info']))
print('Right player: reward = %s, status = %s, info = %s' % (output[1]['reward'], output[1]['status'], output[1]['info']))
env.render(mode=""human"", width=800, height=600)"
9854,43842728,3,"# Validation
from datetime import datetime
from kaggle_environments import make
start = datetime.now()
env = make(""football"", configuration={""save_video"": True, ""scenario_name"": ""11_vs_11_kaggle"", ""running_in_notebook"": True})
output = env.run([""/kaggle/working/submission.py"", ""/kaggle/working/submission.py""])[-1]
print('Left player: reward = %s, status = %s, info = %s' % (output[0]['reward'], output[0]['status'], output[0]['info']))
print('Right player: reward = %s, status = %s, info = %s' % (output[1]['reward'], output[1]['status'], output[1]['info']))
print(datetime.now()-start)
env.render(mode=""human"", width=800, height=600)"
9855,43842728,4,"import pandas as pd
log = pd.DataFrame(env.steps)"
9856,43842728,5,log[0].head()
9857,43842728,6,"log.iloc[0,0]"
9858,43842728,7,"ball_log = pd.DataFrame()
ball_log['ball'] = log[0].apply(lambda x: x['observation']['players_raw'][0]['ball'])
ball_log['ball_direction'] = log[0].apply(lambda x: x['observation']['players_raw'][0]['ball_direction'])
ball_log.head(20)"
9859,43842728,8,"print('Ball position at step 9 is ', ball_log.iloc[9,0])
print('Ball position at step 10 is ', ball_log.iloc[10,0])
print('Ball speed at step 9 is ', ball_log.iloc[9,1])
print('Ball speed at step 10 is ', ball_log.iloc[10,1])
print('Ball position change between step 9 and 10 is ',[b - a for a, b in zip(ball_log.iloc[9,0], ball_log.iloc[10,0])])
print('Ball speed change between step 9 and 10 is ',[b - a for a, b in zip(ball_log.iloc[9,1], ball_log.iloc[10,1])])"
9860,43842728,9,"print('Ball position at step 9 is ', ball_log.iloc[8,0])
print('Ball position at step 10 is ', ball_log.iloc[9,0])
print('Ball speed at step 9 is ', ball_log.iloc[8,1])
print('Ball speed at step 10 is ', ball_log.iloc[9,1])
print('Ball position change between step 9 and 10 is ',[b - a for a, b in zip(ball_log.iloc[8,0], ball_log.iloc[9,0])])
print('Ball speed change between step 9 and 10 is ',[b - a for a, b in zip(ball_log.iloc[8,1], ball_log.iloc[9,1])])"
9861,43842728,10,"right1 = pd.DataFrame()
right1['position'] = log[0].apply(lambda x: x['observation']['players_raw'][0]['right_team'][1])
right1['speed'] = log[0].apply(lambda x: x['observation']['players_raw'][0]['right_team_direction'][1])"
9862,43842728,11,"print('Right team player 1 position at step 35 is ',right1['position'][35])
print('Right team player 1 position at step 36 is ',right1['position'][36])
print('Right team player 1 speed at step 35 is ',right1['speed'][35])
print('Right team player 1 speed at step 36 is ',right1['speed'][36])
print('Right team player 1 position change at step 35 is ',[b - a for a, b in zip(right1.iloc[35,0], right1.iloc[36,0])])
print('Right team player 1 speed change at step 35 is ',[b - a for a, b in zip(right1.iloc[35,1], right1.iloc[36,1])])"
9863,43842728,12,"step = 70
player = pd.DataFrame()
player['position'] = log[0].apply(lambda x: x['observation']['players_raw'][0]['left_team'][8])
player['speed'] = log[0].apply(lambda x: x['observation']['players_raw'][0]['left_team_direction'][8])
ball = pd.DataFrame()
ball['position'] = log[0].apply(lambda x: x['observation']['players_raw'][0]['ball'])
ball['speed'] = log[0].apply(lambda x: x['observation']['players_raw'][0]['ball_direction'])
print('Player position at step ',step,' is ',player['position'][step])
print('Ball position at step ',step,' is ',ball['position'][step])
print('Player position at step ',step+1,' is ',player['position'][step+1])
print('Ball position at step ',step+1,' is ',ball['position'][step+1])
print('Player position at step ',step+2,' is ',player['position'][step+2])
print('Ball position at step ',step+2,' is ',ball['position'][step+2])
print('Player position at step ',step+3,' is ',player['position'][step+3])
print('Ball position at step ',step+3,' is ',ball['position'][step+3])"
9864,43842728,13,"step = 150
player = pd.DataFrame()
player['position'] = log[0].apply(lambda x: x['observation']['players_raw'][0]['left_team'][9])
player['speed'] = log[0].apply(lambda x: x['observation']['players_raw'][0]['left_team_direction'][9])
ball = pd.DataFrame()
ball['position'] = log[0].apply(lambda x: x['observation']['players_raw'][0]['ball'])
ball['speed'] = log[0].apply(lambda x: x['observation']['players_raw'][0]['ball_direction'])
for i in range(5):
    print('Player position at step ',step+i,' is ',player['position'][step+i])
    print('Ball position at step ',step+i,' is ',ball['position'][step+i])
    print('Player speed at step ',step+i,' is ',player['speed'][step+i])
    print('Ball speed at step ',step+i,' is ',ball['speed'][step+i])"
9865,43842728,14,
9866,43684243,0,"# Install:
# Kaggle environments.
!git clone https://github.com/Kaggle/kaggle-environments.git
!cd kaggle-environments && pip install .

# GFootball environment.
!apt-get update -y
!apt-get install -y libsdl2-gfx-dev libsdl2-ttf-dev

# Make sure that the Branch in git clone and in wget call matches !!
!git clone -b v2.3 https://github.com/google-research/football.git
!mkdir -p football/third_party/gfootball_engine/lib

!wget https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_v2.3.so -O football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so
!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install ."
9867,43684243,1,"%%writefile submission.py
from kaggle_environments.envs.football.helpers import *

# @human_readable_agent wrapper modifies raw observations 
# provided by the environment:
# https://github.com/google-research/football/blob/master/gfootball/doc/observation.md#raw-observations
# into a form easier to work with by humans.
# Following modifications are applied:
# - Action, PlayerRole and GameMode enums are introduced.
# - 'sticky_actions' are turned into a set of active actions (Action enum)
#    see usage example below.
# - 'game_mode' is turned into GameMode enum.
# - 'designated' field is removed, as it always equals to 'active'
#    when a single player is controlled on the team.
# - 'left_team_roles'/'right_team_roles' are turned into PlayerRole enums.
# - Action enum is to be returned by the agent function.
@human_readable_agent
def agent(obs):
    # Make sure player is running.
    if Action.Sprint not in obs['sticky_actions']:
        return Action.Sprint
    # We always control left team (observations and actions
    # are mirrored appropriately by the environment).
    controlled_player_pos = obs['left_team'][obs['active']]
    # Does the player we control have the ball?
    if obs['ball_owned_player'] == obs['active'] and obs['ball_owned_team'] == 0:
        # Shot if we are 'close' to the goal (based on 'x' coordinate).
        if controlled_player_pos[0] > 0.5:
            return Action.Shot
        # pass if not above halfline
        if controlled_player_pos[0] < -0.1:
            return Action.ShortPass
        # dribble just above halfline
        if controlled_player_pos[0] > 0 and controlled_player_pos[0] < 0.3:
            return Action.Dribble
        # Run towards the goal otherwise.
        return Action.Right
    else:
        # Run towards the ball.
        if obs['ball'][0] > controlled_player_pos[0] + 0.05:
            return Action.Right
        if obs['ball'][0] < controlled_player_pos[0] - 0.05:
            return Action.Left
        if obs['ball'][1] > controlled_player_pos[1] + 0.05:
            return Action.Bottom
        if obs['ball'][1] < controlled_player_pos[1] - 0.05:
            return Action.Top
        # Try to take over the ball if close to the ball.
        return Action.Slide"
9868,43684243,2,"# Set up the Environment.
from kaggle_environments import make
env = make(""football"", configuration={""save_video"": True, ""scenario_name"": ""11_vs_11_kaggle"", ""running_in_notebook"": True})
output = env.run([""/kaggle/working/submission.py"", ""../input/only-shoot-and-sprint/submission_onlyshootandsprint.py""])[-1]
print('Left player: reward = %s, status = %s, info = %s' % (output[0]['reward'], output[0]['status'], output[0]['info']))
print('Right player: reward = %s, status = %s, info = %s' % (output[1]['reward'], output[1]['status'], output[1]['info']))
env.render(mode=""human"", width=800, height=600)"
9869,43684243,3,
9870,44351245,0,"# Kaggle environments.
!git clone https://github.com/Kaggle/kaggle-environments.git
!cd kaggle-environments && pip install .

# GFootball environment.
!apt-get update -y
!apt-get install -y libsdl2-gfx-dev libsdl2-ttf-dev

# Make sure that the Branch in git clone and in wget call matches !!
!git clone -b v2.6 https://github.com/google-research/football.git
!mkdir -p football/third_party/gfootball_engine/lib

!wget https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_v2.6.so -O football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so
!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install ."
9871,44351245,1,"from kaggle_environments.envs.football.helpers import *

@human_readable_agent
def agent(obs):
    # Make sure player is running.
    if Action.Sprint not in obs['sticky_actions']:
        return Action.Sprint
    # We always control left team (observations and actions
    # are mirrored appropriately by the environment).
    controlled_player_pos = obs['left_team'][obs['active']]
    # Does the player we control have the ball?
    if obs['ball_owned_player'] == obs['active'] and obs['ball_owned_team'] == 0:
        # Shot if we are 'close' to the goal (based on 'x' coordinate).
        if controlled_player_pos[0] > 0.5:
            return Action.Shot
        # Run towards the goal otherwise.
        return Action.Right
    else:
        # Run towards the ball.
        if obs['ball'][0] > controlled_player_pos[0] + 0.05:
            return Action.Right
        if obs['ball'][0] < controlled_player_pos[0] - 0.05:
            return Action.Left
        if obs['ball'][1] > controlled_player_pos[1] + 0.05:
            return Action.Bottom
        if obs['ball'][1] < controlled_player_pos[1] - 0.05:
            return Action.Top
        # Try to take over the ball if close to the ball.
        return Action.Slide"
9872,44351245,2,"# Set up the Environment.
from kaggle_environments import make
import pprint

# 1000 steps are only generated - to speed up process of generation
env = make(""football"", configuration={""save_video"": True, ""scenario_name"": ""11_vs_11_kaggle"", ""running_in_notebook"": True, ""episodeSteps"": 1000}, debug=True)
output = env.run([agent, agent])
print('Left player: reward = %s, status = %s, info = %s' % (output[-1][0]['reward'], output[-1][0]['status'], output[-1][0]['info']))
print('Right player: reward = %s, status = %s, info = %s' % (output[-1][1]['reward'], output[-1][1]['status'], output[-1][1]['info']))
#env.render(mode=""human"", width=800, height=600) - used to output video"
9873,44351245,3,"import matplotlib.patches as patches
from  matplotlib.patches import Arc
from matplotlib import pyplot as plt
from matplotlib import animation
import matplotlib.patches as mpatches

# Change size of figure
plt.rcParams['figure.figsize'] = [20, 16]
def drawPitch(width, height, color=""w""):

  fig = plt.figure()
  ax = plt.axes(xlim=(-10, width + 10), ylim=(-15, height + 5))
  plt.axis('off')

  # Grass around pitch
  rect = patches.Rectangle((-5,-5), width + 10, height + 10, linewidth=1, edgecolor='gray',facecolor='#3f995b', capstyle='round')
  ax.add_patch(rect)

  # Pitch boundaries
  rect = plt.Rectangle((0, 0), width, height, ec=color, fc=""None"", lw=2)
  ax.add_patch(rect)

  # Middle line
  plt.plot([width/2, width/2], [0, height], color=color, linewidth=2)
  
  # Dots
  dots_x = [11, width/2, width-11]
  for x in dots_x:
    plt.plot(x, height/2, 'o', color=color, linewidth=2)

  # Penalty box  
  penalty_box_dim = [16.5, 40.3]
  penalty_box_pos_y = (height - penalty_box_dim[1]) / 2

  rect = plt.Rectangle((0, penalty_box_pos_y), penalty_box_dim[0], penalty_box_dim[1], ec=color, fc=""None"", lw=2)
  ax.add_patch(rect)
  rect = plt.Rectangle((width, penalty_box_pos_y), -penalty_box_dim[0], penalty_box_dim[1], ec=color, fc=""None"", lw=2)
  ax.add_patch(rect)

  #Goal box
  goal_box_dim = [5.5, penalty_box_dim[1] - 11 * 2]
  goal_box_pos_y = (penalty_box_pos_y + 11)

  rect = plt.Rectangle((0, goal_box_pos_y), goal_box_dim[0], goal_box_dim[1], ec=color, fc=""None"", lw=2)
  ax.add_patch(rect)
  rect = plt.Rectangle((width, goal_box_pos_y), -goal_box_dim[0], goal_box_dim[1], ec=color, fc=""None"", lw=2)
  ax.add_patch(rect)

  #Goals
  rect = plt.Rectangle((0, penalty_box_pos_y + 16.5), -3, 7.5, ec=color, fc=color, lw=2, alpha=0.3)
  ax.add_patch(rect)
  rect = plt.Rectangle((width, penalty_box_pos_y + 16.5), 3, 7.5, ec=color, fc=color, lw=2, alpha=0.3)
  ax.add_patch(rect)
    
  # Middle circle
  mid_circle = plt.Circle([width/2, height/2], 9.15, color=color, fc=""None"", lw=2)
  ax.add_artist(mid_circle)


  # Penalty box arcs
  left  = patches.Arc([11, height/2], 2*9.15, 2*9.15, color=color, fc=""None"", lw=2, angle=0, theta1=308, theta2=52)
  ax.add_patch(left)
  right = patches.Arc([width - 11, height/2], 2*9.15, 2*9.15, color=color, fc=""None"", lw=2, angle=180, theta1=308, theta2=52)
  ax.add_patch(right)

  # Arcs on corners
  corners = [[0, 0], [width, 0], [width, height], [0, height]]
  angle = 0
  for x,y in corners:
    c = patches.Arc([x, y], 2, 2, color=color, fc=""None"", lw=2, angle=angle,theta1=0, theta2=90)
    ax.add_patch(c)
    angle += 90
  return fig, ax"
9874,44351245,4,"WIDTH = 105
HEIGHT = 68

drawPitch(WIDTH, HEIGHT)"
9875,44351245,5,"X_RESIZE = WIDTH
Y_RESIZE = HEIGHT / 0.42

class GameMode(Enum):
    Normal = 0
    KickOff = 1
    GoalKick = 2
    FreeKick = 3
    Corner = 4
    ThrowIn = 5
    Penalty = 6

def scale_x(x):
  return (x + 1) * (X_RESIZE/2)

def scale_y(y):
  return (y + 0.42) * (Y_RESIZE/2)


def extract_data(frame):
  res = {}
  obs = frame[0]['observation']['players_raw'][0]
  res[""left_team""] = [(scale_x(x), scale_y(y)) for x, y in obs[""left_team""]]
  res[""right_team""] = [(scale_x(x), scale_y(y)) for x, y in obs[""right_team""]]

  ball_x, ball_y, ball_z = obs[""ball""]
  res[""ball""] = [scale_x(ball_x),  scale_y(ball_y), ball_z]
  res[""score""] = obs[""score""]
  res[""steps_left""] = obs[""steps_left""]
  res[""ball_owned_team""] = obs[""ball_owned_team""]
  res[""ball_owned_player""] = obs[""ball_owned_player""]
  res[""right_team_roles""] = obs[""right_team_roles""]
  res[""left_team_roles""] = obs[""left_team_roles""]
  res[""left_team_direction""] = obs[""left_team_direction""]
  res[""right_team_direction""] = obs[""right_team_direction""]
  res[""game_mode""] = GameMode(obs[""game_mode""]).name
  return res"
9876,44351245,6,"import math


def draw_team(obs, team, side):
  X = []
  Y = []
  for x, y in obs[side]:
    X.append(x)
    Y.append(y)
  team.set_data(X, Y)

def draw_ball(obs, ball):
  ball.set_markersize(10 + 5 * obs[""ball""][2]) # Scale size of ball based on height
  ball.set_data(obs[""ball""][:2])

def draw_ball_owner(obs, ball_owner, team_active):
  if obs[""ball_owned_team""] == 0:
    x, y = obs[""left_team""][obs[""ball_owned_player""]]
    ball_owner.set_data(x, y)
    team_active.set_data(WIDTH / 4 + 7, -7)
    team_active.set_markerfacecolor(""red"")
  elif obs[""ball_owned_team""] == 1:
    x, y = obs[""right_team""][obs[""ball_owned_player""]]
    ball_owner.set_data(x, y)
    team_active.set_data(WIDTH / 4 + 50, -7)
    team_active.set_markerfacecolor(""blue"")
  else:
    ball_owner.set_data([], [])
    team_active.set_data([], [])
    
def draw_players_directions(obs, directions, side):
  index = 0
  if ""right"" in side:
    index = 11
  for i, player_dir in enumerate(obs[f""{side}_direction""]):
    x_dir, y_dir = player_dir
    dist = math.sqrt(x_dir ** 2 + y_dir ** 2) + 0.00001 # to prevent division by 0
    x = obs[side][i][0]
    y = obs[side][i][1] 
    directions[i + index].set_data([x, x + x_dir / dist ], [y, y + y_dir / dist])"
9877,44351245,7,"import numpy as np
from IPython.display import HTML

fig, ax = drawPitch(WIDTH, HEIGHT)
ax.invert_yaxis()

ball_owner, = ax.plot([], [], 'o', markersize=30,  markerfacecolor=""yellow"", alpha=0.5)
team_active, = ax.plot([], [], 'o', markersize=30,  markerfacecolor=""blue"", markeredgecolor=""None"")

team_left, = ax.plot([], [], 'o', markersize=20, markerfacecolor=""r"", markeredgewidth=2, markeredgecolor=""white"")
team_right, = ax.plot([], [], 'o', markersize=20,  markerfacecolor=""b"", markeredgewidth=2, markeredgecolor=""white"")

ball, = ax.plot([], [], 'o', markersize=10,  markerfacecolor=""black"", markeredgewidth=2, markeredgecolor=""white"")
text_frame = ax.text(-5, -5, '', fontsize=25)
match_info = ax.text(105 / 4 + 10, -5, '', fontsize=25)
game_mode = ax.text(105 - 25, -5, '', fontsize=25)
goal_notification = ax.text(105 / 4 + 10, 0, '', fontsize=25)

# Drawing of directions definitely can be done in a better way
directions = []
for i in range(22):
  direction, = ax.plot([], [], color='yellow', lw=3)
  directions.append(direction)

  
drawings = [team_active, ball_owner, team_left, team_right, ball, text_frame, match_info, game_mode, goal_notification]

def init():
    team_left.set_data([], [])
    team_right.set_data([], [])
    ball_owner.set_data([], [])
    team_active.set_data([], [])
    ball.set_data([], [])
    return drawings 

def animate(i):
  global prev_score_a, prev_score_b
  obs = extract_data(output[i])

  # Draw info about ball possesion
  draw_ball_owner(obs, ball_owner, team_active)

  # Draw players
  draw_team(obs, team_left, ""left_team"")
  draw_team(obs, team_right, ""right_team"")

  draw_players_directions(obs, directions, ""left_team"")
  draw_players_directions(obs, directions, ""right_team"")
    
  draw_ball(obs, ball)

  # Draw textual informations
  text_frame.set_text(f""Frame: {i}/{obs['steps_left'] + i - 1}"")
  game_mode.set_text(f""Game mode: {obs['game_mode']}"")
  
  score_a, score_b = obs[""score""]
  match_info.set_text(f""Left team {score_a} : {score_b} Right Team"")

  return drawings  

# May take a while
anim = animation.FuncAnimation(fig, animate, init_func=init,
                               frames=1000, interval=100, blit=True)

HTML(anim.to_html5_video())"
9878,52748519,0,"import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import os"
9879,52748519,1,"# Kaggle environments.
!git clone https://github.com/Kaggle/kaggle-environments.git
!cd kaggle-environments && pip install .

# GFootball environment.
!apt-get update -y
!apt-get install -y libsdl2-gfx-dev libsdl2-ttf-dev

# Make sure that the Branch in git clone and in wget call matches !!
!git clone -b v2.6 https://github.com/google-research/football.git
!mkdir -p football/third_party/gfootball_engine/lib

!wget https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_v2.6.so -O football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so
!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install .

# keras reinforcement learning
!pip install reinforcement_learning_keras"
9880,52748519,2,"import keras
import math
from keras.models import Sequential
from keras.layers import Dense, Dropout

class RLNAgent:
    def __init__(self):
        self.memory = []
        self.rewards = []
        self.gamma = 0.3
        self.epsilon = 0.7
        self.epsilon_decay = 0.85
        self.epsilon_min = 0.05
        self.learning_rate = 0.0002
        self._build_model()
    
    # setting mechanic learning model
    def _build_model(self):
        model = Sequential()
        model.add(Dense(157, input_dim = 103, activation = 'linear'))
        #model.add(Dropout(0.1))
        model.add(Dense(101, activation = 'tanh'))
        model.add(Dense(83, activation = 'tanh'))
        model.add(Dense(29, activation = 'tanh'))
        model.add(Dense(18, activation = 'tanh'))
        model.compile(loss='mean_squared_error', optimizer = keras.optimizers.Adam(lr=self.learning_rate))
        self.model = model
    
    # transfor json type to (1,103) array
    def state_as_sample103(self, obs):
        sample103 = np.concatenate((
            np.array(obs['left_team'][obs['active']]).flatten(),
            np.array(obs['left_team_direction'][obs['active']]).flatten(),
            np.array(obs['ball']).flatten(),
            np.array(obs['ball_direction']).flatten(),
            np.array(obs['left_team'][obs['active']]).flatten() - np.array(obs['ball']).flatten()[0:1],
            np.array(obs['left_team']).flatten(),
            np.array(obs['left_team_direction']).flatten(),
            np.array(obs['right_team']).flatten(),
            np.array(obs['right_team_direction']).flatten(),
            (lambda x: [1,0,0] if x == -1 else ([0,1,0] if x == 0 else [0,0,1]))(obs['ball_owned_team'])
        ))
        return sample103.reshape((1,103))
    
    # remember the play state
    def remember(self, state, predict, action, reward, next_state):
        self.memory.append((state, predict, action, reward, next_state))
        self.rewards.append(reward)

    # get the model predict for the player's best action
    def act_predict(self, state):
        return self.model.predict(state)
    
    # using reward to train the model
    def replay(self, batch_size, maxreward):
        batches = np.arange(1, len(self.memory), 1)
        memory_max_length = len(self.memory)
        
        for i in batches:
            state, predict, action, reward, next_state = self.memory[i]

            # add noise
            target_f = (predict * (1-self.epsilon)) + (np.random.rand(1,18) * self.epsilon)
            
            # the training target value is the addition rewards with follow-up (1.5s) effect            
            followup_rewards = 0
            for j in range(i, min(i + 30, memory_max_length), 1):
                followup_rewards = followup_rewards + (self.rewards[j] * math.exp( (i - j) * 2 ))
            
            #if followup_rewards < 1 and followup_rewards > 0 and (self.rewards[i] - self.rewards[i-1]) < 0:
            #    continue
            #print(""{}, followup_rewards:{}"".format(i,followup_rewards))
            
            target = target_f[0][action] + (followup_rewards * 0.01) + (self.rewards[i] - self.rewards[i-1]) * 0.5
            
            # limit the max target
            if target > 0.8:
                target = 0.8

            target_f[0][action] = target
            
            # training
            self.model.fit(state, target_f, epochs = 1, verbose = 0)
        
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
                        
    def save(self):
        self.model.save('rl_model')
        
    def load(self):
        self.model = keras.models.load_model('rl_model')"
9881,52748519,3,"from kaggle_environments import make
import copy
import random

env = make(""football"",
           configuration={""save_video"": False, 
                          ""scenario_name"": ""11_vs_11_kaggle"", 
                          ""running_in_notebook"": True})"
9882,52748519,4,"def closed_contestant_num(obs, controlled_player_pos):
    closed_num = 0
    for i in range(1, len(obs[""right_team""])):
        if abs(controlled_player_pos[0] - obs[""right_team""][i][0]) < 0.08 and abs(controlled_player_pos[1] - obs[""right_team""][i][1]) < 0.04 :
            closed_num = closed_num + 1
    return closed_num

def rule_based_agent(obs):
    controlled_player_pos = obs['left_team'][obs['active']]
    if obs['ball_owned_player'] == obs['active'] and obs['ball_owned_team'] == 0:
        closed_people = closed_contestant_num(obs, controlled_player_pos)
        if controlled_player_pos[0] > 0.5 and closed_people < 4:
            return 12
        if controlled_player_pos[0] > 0.5 and controlled_player_pos[1] > 0.3 and closed_people > 5:
            return 3
        if controlled_player_pos[0] > 0.5 and controlled_player_pos[1] < -0.3 and closed_people > 5:
            return 7
        return 5
    else:        
        if obs['ball'][0] > controlled_player_pos[0] + 0.05:
            return 5
        if obs['ball'][0] < controlled_player_pos[0] - 0.05:
            return 1
        if obs['ball'][1] > controlled_player_pos[1] + 0.05:
            return 7
        if obs['ball'][1] < controlled_player_pos[1] - 0.05:
            return 3
        return 16"
9883,52748519,5,"agent = RLNAgent()

episodes = 150  # playing the game up to {episodes} times
steps = 2000  # each game with {steps} steps

for e in range(episodes):
    
    env.reset()
    agent.memory = []
    agent.rewards = []
    
    trainer = env.train([None, ""run_right""])
    trainer.reset()
    
    obs = env.state[0]['observation']['players_raw'][0]
    maxreward = -10
    
    for time_t in range(steps):    # simulating the game step by step
        
        action = 0
        
        state = agent.state_as_sample103(obs)
        predict = agent.act_predict(state)
        action = np.argmax(predict)
        
        # learning from rule based agent coach
        if e < 10:
            action = rule_based_agent(obs)
            predict = np.reshape([(lambda x, y: 1 if x==y else 0)(x, action) for x in range(18)], (1,18))
        
        next_obs, reward, done, info = trainer.step([action])
        
        reward = - 10 if reward == None else reward
            
        next_obs = next_obs['players_raw'][0]
        next_state = agent.state_as_sample103(next_obs)
        
        # if we steal the ball, the reward will gain 0.05 point
        if obs['ball_owned_team'] == 1 and next_obs['ball_owned_team'] == 0:
            reward += 0.05

        # if we get the ball, the reward will gain with the right moving distince
        if obs['ball_owned_team'] == 0:
            reward += ( next_obs['ball'][0] - obs['ball'][0] ) * 0.1 - abs( next_obs['ball'][1] ) * 0.01

        # if active player far from the ball, the reward will lose by the moving distance
        distance = abs(obs['left_team'][obs['active']][0] - obs['ball'][0]) + abs(obs['left_team'][obs['active']][1] - obs['ball'][1])
        next_distance = abs(next_obs['left_team'][next_obs['active']][0] - next_obs['ball'][0]) + abs(next_obs['left_team'][next_obs['active']][1] - next_obs['ball'][1])
        
        if abs(next_obs['left_team'][next_obs['active']][0] - obs['left_team'][obs['active']][0]) < 0.01 and abs(next_obs['left_team'][next_obs['active']][1] - obs['left_team'][obs['active']][1]) < 0.01 :
            reward -= 0.5
        
        if next_obs['ball_owned_team'] != 0:
            reward += - distance - (next_distance - distance) * 2

        # if the next action is shooting the ball, the reward will gain 0.2 point
        if obs['ball_owned_team'] == 0 and obs['left_team'][obs['active']][0] > 0.5 and action == 12:
            reward += 0.2
            
        if obs['left_team'][obs['active']][0] < 0 and action == 12:
            reward -= 0.05
            
        # if we go out the square, the reward will lose 2 point
        if obs['left_team'][obs['active']][0] > 0.98 or obs['left_team'][obs['active']][0] < - 0.98 or obs['left_team'][obs['active']][1] < - 0.39 or obs['left_team'][obs['active']][1] > 0.39:
            reward -= 2
        
        agent.remember(state, predict, action, reward, next_state)
        
        obs = copy.deepcopy(next_obs)
        
        if maxreward < reward:
            maxreward = reward

    print(""episode: {}/{}, score: {}, action: {}, probability: {}"".format(e + 1, episodes, maxreward, action, np.max(predict,axis = 1)))
    agent.replay(steps, maxreward)"
9884,52748519,6,agent.save()
9885,52748519,7,"%%writefile main.py
# for making a video

import numpy as np # linear algebra
from kaggle_environments.envs.football.helpers import *
import keras
from keras.models import Sequential
from keras.layers import Dense


class QNAgent:
    def __init__(self):
        self.load()
        
    def act(self, state):
        act_values = self.model.predict(state)
        return np.argmax(act_values[0])
        
    def load(self):
        self.model = keras.models.load_model('/kaggle/working/rl_model')

    def state_as_sample103(self, obs):
        sample103 = np.concatenate((
            np.array(obs['left_team'][obs['active']]).flatten(),
            np.array(obs['left_team_direction'][obs['active']]).flatten(),
            np.array(obs['ball']).flatten(),
            np.array(obs['ball_direction']).flatten(),
            np.array(obs['left_team'][obs['active']]).flatten() - np.array(obs['ball']).flatten()[0:1],
            np.array(obs['left_team']).flatten(),
            np.array(obs['left_team_direction']).flatten(),
            np.array(obs['right_team']).flatten(),
            np.array(obs['right_team_direction']).flatten(),
            (lambda x: [1,0,0] if x == -1 else ([0,1,0] if x == 0 else [0,0,1]))(obs['ball_owned_team'])
        ))
        return sample103.reshape((1,103))


qagent = QNAgent()

ActionDic = [Action.Idle,
             Action.Left,
             Action.TopLeft,
             Action.Top,
             Action.TopRight,
             Action.Right,
             Action.BottomRight,
             Action.Bottom,
             Action.BottomLeft,
             Action.LongPass,
             Action.HighPass,
             Action.ShortPass,
             Action.Shot,
             Action.Sprint,
             Action.ReleaseDirection,
             Action.ReleaseSprint,
             Action.Slide,
             Action.Dribble,
             Action.ReleaseDribble
            ]

@human_readable_agent
def agent(obs):
    state = qagent.state_as_sample103(obs)
    action = qagent.act(state)    
    return ActionDic[action]"
9886,52748519,8,"from kaggle_environments import make

env = make(""football"", configuration={""save_video"": True, ""scenario_name"": ""11_vs_11_kaggle"", ""running_in_notebook"": True})
output = env.run([""/kaggle/working/main.py"", ""run_right""])[-1]

print('Left player: reward = %s, status = %s, info = %s' % (output[0]['reward'], output[0]['status'], output[0]['info']))
print('Right player: reward = %s, status = %s, info = %s' % (output[1]['reward'], output[1]['status'], output[1]['info']))

env.render(mode=""human"", width=800, height=600)"
9887,52748519,9,"%%writefile main.py
# for submition

import numpy as np # linear algebra
from kaggle_environments.envs.football.helpers import *
import keras
from keras.models import Sequential
from keras.layers import Dense


class QNAgent:
    def __init__(self):
        self.load()
        
    def act(self, state):
        act_values = self.model.predict(state)
        return np.argmax(act_values[0])
        
    def load(self):
        self.model = keras.models.load_model('/kaggle_simulations/agent/rl_model')

    def state_as_sample103(self, obs):
        sample103 = np.concatenate((
            np.array(obs['left_team'][obs['active']]).flatten(),
            np.array(obs['left_team_direction'][obs['active']]).flatten(),
            np.array(obs['ball']).flatten(),
            np.array(obs['ball_direction']).flatten(),
            np.array(obs['left_team'][obs['active']]).flatten() - np.array(obs['ball']).flatten()[0:1],
            np.array(obs['left_team']).flatten(),
            np.array(obs['left_team_direction']).flatten(),
            np.array(obs['right_team']).flatten(),
            np.array(obs['right_team_direction']).flatten(),
            (lambda x: [1,0,0] if x == -1 else ([0,1,0] if x == 0 else [0,0,1]))(obs['ball_owned_team'])
        ))
        return sample103.reshape((1,103))


qagent = QNAgent()

ActionDic = [Action.Idle,
             Action.Left,
             Action.TopLeft,
             Action.Top,
             Action.TopRight,
             Action.Right,
             Action.BottomRight,
             Action.Bottom,
             Action.BottomLeft,
             Action.LongPass,
             Action.HighPass,
             Action.ShortPass,
             Action.Shot,
             Action.Sprint,
             Action.ReleaseDirection,
             Action.ReleaseSprint,
             Action.Slide,
             Action.Dribble,
             Action.ReleaseDribble
            ]

@human_readable_agent
def agent(obs):
    state = qagent.state_as_sample103(obs)
    action = qagent.act(state)    
    return ActionDic[action]"
9888,52748519,10,!tar -czvf submission.tar.gz main.py rl_model
9889,52748519,11,
9890,44809691,0,"# Install:
# Kaggle environments.
!git clone https://github.com/Kaggle/kaggle-environments.git
!cd kaggle-environments && pip install .

# GFootball environment.
!apt-get update -y
!apt-get install -y libsdl2-gfx-dev libsdl2-ttf-dev

# Make sure that the Branch in git clone and in wget call matches !!
!git clone -b v2.6 https://github.com/google-research/football.git
!mkdir -p football/third_party/gfootball_engine/lib

!wget https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_v2.6.so -O football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so
!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install ."
9891,44809691,1,"%%writefile AlbertEinsteinAcademic.py
from kaggle_environments.envs.football.helpers import *
import numpy

# @human_readable_agent wrapper modifies raw observations 
# provided by the environment:
# https://github.com/google-research/football/blob/master/gfootball/doc/observation.md#raw-observations
# into a form easier to work with by humans.
# Following modifications are applied:
# - Action, PlayerRole and GameMode enums are introduced.
# - 'sticky_actions' are turned into a set of active actions (Action enum)
#    see usage example below.
# - 'game_mode' is turned into GameMode enum.
# - 'designated' field is removed, as it always equals to 'active'
#    when a single player is controlled on the team.
# - 'left_team_roles'/'right_team_roles' are turned into PlayerRole enums.
# - Action enum is to be returned by the agent function.        

@human_readable_agent
def agent(obs):
        
    # Make sure player is running.
    if Action.Sprint not in obs['sticky_actions']:
        return Action.Sprint
    # We always control left team (observations and actions
    # are mirrored appropriately by the environment).
    controlled_player_pos = obs['left_team'][obs['active']]
    # Does the player we control have the ball?
    
    
    if obs['ball_owned_team'] == 0 and obs['ball_owned_player'] == obs['active']:
        
        # Shot if we are 'close' to the goal (based on 'x-y' coordinate).
        if controlled_player_pos[0] >= 0.7 and controlled_player_pos[1] > 0.3:
            return numpy.random.choice([Action.Shot, Action.TopRight, Action.Right])
        
        if controlled_player_pos[0] >= 0.7 and controlled_player_pos[1] < -0.3:
            return  numpy.random.choice([Action.Shot, Action.BottomRight, Action.Right])
        
        if controlled_player_pos[0] > 0.5 and (controlled_player_pos[1] >= -0.5 and controlled_player_pos[1] <= 0.5):
            return numpy.random.choice([Action.Shot, Action.TopRight, Action.BottomRight]) 
        
        if controlled_player_pos[0] >= 0.7:
            if controlled_player_pos[1] >= -0.3 and controlled_player_pos[1] <= 0.3:
                return Action.Shot 
            
        
        if controlled_player_pos[0] < 0.0:
            return numpy.random.choice([Action.ShortPass, Action.LongPass, Action.HighPass])
        
        #go forward and remove ball in dangerous place.
        if controlled_player_pos[0] < -0.5:
            return Action.Right
        
        if controlled_player_pos[0] >= 0.0:
            return Action.Right

        
        # Run towards the goal otherwise.
        return Action.Right 
        
    else:
        
        right_player_pos = obs['right_team'][obs['active']]
        
        # Run towards the ball.
        if obs['ball'][0] > controlled_player_pos[0] + 0.05:
            return numpy.random.choice([Action.Right, Action.BottomRight, Action.TopRight])

        if obs['ball'][0] < controlled_player_pos[0] - 0.05:
            return numpy.random.choice([Action.Left, Action.TopLeft, Action.BottomLeft])

        if obs['ball'][1] > controlled_player_pos[1] + 0.05:
            return numpy.random.choice([Action.Bottom, Action.BottomLeft, Action.BottomRight])

        if obs['ball'][1] < controlled_player_pos[1] - 0.05:
            return numpy.random.choice([Action.Top, Action.TopLeft, Action.TopRight])
        
        #run toward the opposite player
        if right_player_pos[0] > controlled_player_pos[0] + 0.05:
            return numpy.random.choice([Action.Right, Action.BottomRight, Action.TopRight])
        
        if right_player_pos[0] < controlled_player_pos[0] - 0.05:
            return numpy.random.choice([Action.Left, Action.BottomLeft, Action.TopLeft])
        
        if right_player_pos[1] > controlled_player_pos[1] + 0.05:
            return numpy.random.choice([Action.Bottom, Action.BottomRight, Action.BottomLeft])
        
        if right_player_pos[1] < controlled_player_pos[1] - 0.05:
            return numpy.random.choice([Action.Top, Action.TopLeft, Action.TopRight])

        # Try to take over the ball if close to the ball.
        return Action.Slide"
9892,44809691,2,"%%writefile NeilsBohrAcademic.py
from kaggle_environments.envs.football.helpers import *
import numpy

# @human_readable_agent wrapper modifies raw observations 
# provided by the environment:
# https://github.com/google-research/football/blob/master/gfootball/doc/observation.md#raw-observations
# into a form easier to work with by humans.
# Following modifications are applied:
# - Action, PlayerRole and GameMode enums are introduced.
# - 'sticky_actions' are turned into a set of active actions (Action enum)
#    see usage example below.
# - 'game_mode' is turned into GameMode enum.
# - 'designated' field is removed, as it always equals to 'active'
#    when a single player is controlled on the team.
# - 'left_team_roles'/'right_team_roles' are turned into PlayerRole enums.
# - Action enum is to be returned by the agent function.        

@human_readable_agent
def agent(obs):
        
    # Make sure player is running.
    if Action.Sprint not in obs['sticky_actions']:
        return Action.Sprint
    # We always control left team (observations and actions
    # are mirrored appropriately by the environment).
    controlled_player_pos = obs['left_team'][obs['active'] - 5]
    
    # Does the player we control have the ball?
    right_player_pos = obs['right_team'][obs['active']]
    goalKeeperR = obs['right_team'][0] #opponent goal keeper
    
################################### Team Strategy ###############################################################   

    if obs['ball_owned_team'] == 0 and obs['ball_owned_player'] == obs['active']:
        
        # single player have a ball.
        player_pos = obs['left_team'][obs['active']]
        u_vector = numpy.zeros((2, 1)) # 
        w_vector = u_vector.copy()
        
        u_vector[0] = goalKeeperR[0] - player_pos[0]
        u_vector[1] = goalKeeperR[1] - player_pos[1]
        distanceG_P = numpy.linalg.norm(u_vector, 2) # distance between active players and goal keeper oppenent
        
        w_vector[0] = player_pos[0] - right_player_pos[0]
        w_vector[1] = player_pos[1] - right_player_pos[0]
        distanceP_P = numpy.linalg.norm(w_vector, 2) # distance between active player and opponent
        
        #left goal keeper kicks a ball in dangerous place
        if obs['active'] == 0: 
            x = numpy.random.choice([Action.Shot, Action.LongPass])
            print('GoalKeeper: ', x, ' Active players: ', obs['active'])
            return x
        
        
        #*************************** controlling only one players *******************************#
        
        # player's prepares  to score.
        if player_pos[0] > 0.5 and (player_pos[1] >= -0.5 and player_pos[1] <= 0.5):
            x = numpy.random.choice([Action.TopRight, Action.BottomRight, Action.Right, 
                                     Action.Shot, Action.Top, Action.Bottom]) 
            return x
        
        #player echap or dribble
        if distanceP_P < 0.025:
            return numpy.random.choice([Action.ReleaseSprint, Action.BottomRight, Action.TopRight, 
                                        Action.ReleaseDribble])
        
        # Shot if we are 'close' to the goal (based on 'x-y' coordinate).
        if player_pos[0] >= 0.7:
            
            #player tends to adjust a ball to go to score
            if player_pos[1] > 0.5:
                x = numpy.random.choice([Action.TopRight, Action.Top, Action.Shot])
                return x
            
            if player_pos[1] < -0.5:
                x = numpy.random.choice([Action.BottomRight, Action.Bottom, Action.Shot])
                return  x
            
            if distanceG_P < 0.05:
                return numpy.random.choice([Action.BottomRight, Action.TopRight, Action.Shot])
            
            if player_pos[1] >= -0.5 and player_pos[1] <= 0.5:
                x = Action.Shot
                return x
        #******************************************************************************************#
            
        # make some passing ball forward
        if (controlled_player_pos[0] - player_pos[0]) > 0: 
            x = numpy.random.choice([Action.ShortPass, Action.LongPass, Action.HighPass])
            return x
        
        # Run towards the goal otherwise.
        return Action.Right 
        
    else:
        
        # Run towards the ball.
        if (obs['ball'][0] > controlled_player_pos[0] + 0.05) and (right_player_pos[0] > controlled_player_pos[0] + 0.05):
            return numpy.random.choice([Action.Right, Action.BottomRight, Action.TopRight])

        if (obs['ball'][0] < controlled_player_pos[0] - 0.05) and (right_player_pos[0] < controlled_player_pos[0] - 0.05):
            return numpy.random.choice([Action.Left, Action.TopLeft, Action.BottomLeft])

        if (obs['ball'][1] > controlled_player_pos[1] + 0.05) and (right_player_pos[1] > controlled_player_pos[1] + 0.05):
            return numpy.random.choice([Action.Bottom, Action.BottomLeft, Action.BottomRight])

        if (obs['ball'][1] < controlled_player_pos[1] - 0.05) and (right_player_pos[1] < controlled_player_pos[1] - 0.05):
            return numpy.random.choice([Action.Top, Action.TopLeft, Action.TopRight])
        
        # Run towards the ball.
        if obs['ball'][0] > controlled_player_pos[0] + 0.05:
            return numpy.random.choice([Action.Right, Action.BottomRight, Action.TopRight])

        if obs['ball'][0] < controlled_player_pos[0] - 0.05:
            return numpy.random.choice([Action.Left, Action.TopLeft, Action.BottomLeft])

        if obs['ball'][1] > controlled_player_pos[1] + 0.05:
            return numpy.random.choice([Action.Bottom, Action.BottomLeft, Action.BottomRight])

        if obs['ball'][1] < controlled_player_pos[1] - 0.05:
            return numpy.random.choice([Action.Top, Action.TopLeft, Action.TopRight])

        # Try to take over the ball if close to the ball.
        return Action.Slide"
9893,44809691,3,print('Welcome to this beautiful Kaggle stadium. We are going to watch a big derby between Einstein Academic Physics football and Bohr Academic Physics football. Goodluck!')
9894,44809691,4,"from kaggle_environments import make
env = make(""football"", configuration={""save_video"": True, ""scenario_name"": ""11_vs_11_kaggle"", ""running_in_notebook"": True})
output = env.run([""/kaggle/working/AlbertEinsteinAcademic.py"", ""/kaggle/working/NeilsBohrAcademic.py""])[-1]
print('Left player: reward = %s, status = %s, info = %s' % (output[0]['reward'], output[0]['status'], output[0]['info']))
print('Right player: reward = %s, status = %s, info = %s' % (output[1]['reward'], output[1]['status'], output[1]['info']))
env.render(mode=""human"", width=300, height=400)"
9895,44809691,5,
9896,48325623,0,"!git clone https://github.com/Kaggle/kaggle-environments.git
!cd kaggle-environments && pip install .
!apt-get update -y
!apt-get install -y libsdl2-gfx-dev libsdl2-ttf-dev
!git clone -b v2.8 https://github.com/google-research/football.git
!mkdir -p football/third_party/gfootball_engine/lib
!wget https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_v2.8.so -O football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so
!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install ."
9897,48325623,1,"%%writefile submission.py
from kaggle_environments.envs.football.helpers import *
import random
import math

@human_readable_agent
def agent(obs):
    if Action.Dribble in obs['sticky_actions']:
        return Action.ReleaseDribble
    #Action.ReleaseDirection
    #Action.ReleaseSprint
    
    if GameMode.KickOff == obs['game_mode']:
        return Action.ShortPass
    if GameMode.Penalty == obs['game_mode']:
        return Action.Shot
    if GameMode.GoalKick == obs['game_mode']:
        return Action.LongPass
    if GameMode.Corner == obs['game_mode']:
        return Action.LongPass
    if GameMode.FreeKick == obs['game_mode']:
        return Action.Shot
    if GameMode.ThrowIn == obs['game_mode']:
        return Action.ShortPass
    if GameMode.Normal == obs['game_mode']:
        if Action.Sprint not in obs['sticky_actions']:
            return Action.Sprint
        controlled_player_pos = obs['left_team'][obs['active']]
        if obs['ball_owned_player'] == obs['active'] and obs['ball_owned_team'] == 0:
            player_x, player_y = controlled_player_pos[0], controlled_player_pos[1]
            if controlled_player_pos[0] > 0.5:
                goalkeeper_x = obs[""right_team""][0][0] + obs[""right_team_direction""][0][0] * 13
                goalkeeper_y = obs[""right_team""][0][1] + obs[""right_team_direction""][0][1] * 13
                x1, x2, y1, y2 = player_x, player_y, goalkeeper_x, goalkeeper_y
                if math.sqrt((x1 - x2) ** 2 + (y1 * 2.38 - y2 * 2.38) ** 2) < 0.3:
                    return Action.Shot
                else:
                    dribbled = False
                    for i in range(1, len(obs[""right_team""])):
                        # if opponent is ahead of player
                        if obs[""right_team""][i][0] > (player_x - 0.02): 
                            #distance_to_opponent
                            x1, x2, y1, y2 = player_x, player_y, obs[""right_team""][i][0], obs[""right_team""][i][1]
                            if math.sqrt((x1 - x2) ** 2 + (y1 * 2.38 - y2 * 2.38) ** 2) < 0.03:
                                dribbled = True
                                return Action.Dribble
                                break
                    if dribbled == False:
                        return random.choice([Action.Right, Action.Right, Action.Shot])
            return random.choice([Action.BottomRight, Action.TopRight, Action.Right, Action.HighPass, Action.LongPass, Action.HighPass, Action.LongPass, Action.ShortPass])
        else:
            if obs['ball'][0] > controlled_player_pos[0] + 0.05:
                return Action.Right
            if obs['ball'][0] < controlled_player_pos[0] - 0.05:
                return Action.Left
            if obs['ball'][1] > controlled_player_pos[1] + 0.05:
                return Action.Bottom
            if obs['ball'][1] < controlled_player_pos[1] - 0.05:
                return Action.Top
            return Action.Slide"
9898,48325623,2,"from kaggle_environments import make
env = make(""football"", configuration={""save_video"": True, ""scenario_name"": ""11_vs_11_kaggle"", ""running_in_notebook"": True})
output = env.run([""/kaggle/working/submission.py"", ""run_right""])[-1]
print('Left player: reward = %s, status = %s, info = %s' % (output[0]['reward'], output[0]['status'], output[0]['info']))
print('Right player: reward = %s, status = %s, info = %s' % (output[1]['reward'], output[1]['status'], output[1]['info']))
env.render(mode=""human"", width=800, height=600)"
9899,43983810,0,"# Install:
# Kaggle environments.
!git clone https://github.com/Kaggle/kaggle-environments.git
!cd kaggle-environments && pip install .

# GFootball environment.
!apt-get update -y
!apt-get install -y libsdl2-gfx-dev libsdl2-ttf-dev

# Make sure that the Branch in git clone and in wget call matches !!
!git clone -b v2.3 https://github.com/google-research/football.git
!mkdir -p football/third_party/gfootball_engine/lib

!wget https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_v2.3.so -O football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so
!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install ."
9900,43983810,1,"%%writefile submission.py
from kaggle_environments.envs.football.helpers import *

# @human_readable_agent wrapper modifies raw observations 
# provided by the environment:
# https://github.com/google-research/football/blob/master/gfootball/doc/observation.md#raw-observations
# into a form easier to work with by humans.
# Following modifications are applied:
# - Action, PlayerRole and GameMode enums are introduced.
# - 'sticky_actions' are turned into a set of active actions (Action enum)
#    see usage example below.
# - 'game_mode' is turned into GameMode enum.
# - 'designated' field is removed, as it always equals to 'active'
#    when a single player is controlled on the team.
# - 'left_team_roles'/'right_team_roles' are turned into PlayerRole enums.
# - Action enum is to be returned by the agent function.
@human_readable_agent
def agent(obs):
    # Make sure player is running.
    if Action.Sprint not in obs['sticky_actions']:
        return Action.Sprint
    # We always control left team (observations and actions
    # are mirrored appropriately by the environment).
    controlled_player_pos = obs['left_team'][obs['active']]
    # Does the player we control have the ball?
    if obs['ball_owned_player'] == obs['active'] and obs['ball_owned_team'] == 0:
        # Shot if we are 'close' to the goal (based on 'x' coordinate).
        if controlled_player_pos[0] > 0.5:
            return Action.Shot
        # Run towards the goal otherwise.
        return Action.Right
    else:
        # Run towards the ball.
        if obs['ball'][0] > controlled_player_pos[0] + 0.05:
            return Action.Right
        if obs['ball'][0] < controlled_player_pos[0] - 0.05:
            return Action.Left
        if obs['ball'][1] > controlled_player_pos[1] + 0.05:
            return Action.Bottom
        if obs['ball'][1] < controlled_player_pos[1] - 0.05:
            return Action.Top
        # Try to take over the ball if close to the ball.
        return Action.Slide
    
"
9901,43983810,2,"# Set up the Environment.
# This may take a few minutes.
from kaggle_environments import make
env = make(""football"", configuration={""save_video"": True, ""scenario_name"": ""11_vs_11_kaggle"", ""running_in_notebook"": True})
output = env.run([""/kaggle/working/submission.py"", ""do_nothing""])[-1]
print('Left player: reward = %s, status = %s, info = %s' % (output[0]['reward'], output[0]['status'], output[0]['info']))
print('Right player: reward = %s, status = %s, info = %s' % (output[1]['reward'], output[1]['status'], output[1]['info']))
env.render(mode=""human"", width=800, height=600)"
9902,45436490,0,"# Install:
# Kaggle environments.
!git clone https://github.com/Kaggle/kaggle-environments.git
!cd kaggle-environments && pip install .

# GFootball environment.
!apt-get update -y
!apt-get install -y libsdl2-gfx-dev libsdl2-ttf-dev

# Make sure that the Branch in git clone and in wget call matches !!
!git clone -b v2.7 https://github.com/google-research/football.git
!mkdir -p football/third_party/gfootball_engine/lib

!wget https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_v2.7.so -O football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so
!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install ."
9903,45436490,1,"%%writefile submission.py
from kaggle_environments.envs.football.helpers import *

# @human_readable_agent wrapper modifies raw observations 
# provided by the environment:
# https://github.com/google-research/football/blob/master/gfootball/doc/observation.md#raw-observations
# into a form easier to work with by humans.
# Following modifications are applied:
# - Action, PlayerRole and GameMode enums are introduced.
# - 'sticky_actions' are turned into a set of active actions (Action enum)
#    see usage example below.
# - 'game_mode' is turned into GameMode enum.
# - 'designated' field is removed, as it always equals to 'active'
#    when a single player is controlled on the team.
# - 'left_team_roles'/'right_team_roles' are turned into PlayerRole enums.
# - Action enum is to be returned by the agent function.
@human_readable_agent
def agent(obs):
    # Make sure player is running.
    if Action.Sprint not in obs['sticky_actions']:
        return Action.Sprint
    # We always control left team (observations and actions
    # are mirrored appropriately by the environment).
    controlled_player_pos = obs['left_team'][obs['active']]
    # Does the player we control have the ball?
    if obs['ball_owned_player'] == obs['active'] and obs['ball_owned_team'] == 0:
        # Shot if we are 'close' to the goal (based on 'x' coordinate).
        if controlled_player_pos[0] > 0.7:
            return Action.Shot
        # Run towards the goal otherwise.
        return Action.Right
    else:
        # Run towards the ball.
        if obs['ball'][0] > controlled_player_pos[0] + 0.01:
            return Action.Right
        if obs['ball'][0] < controlled_player_pos[0] - 0.01:
            return Action.Left
        if obs['ball'][1] > controlled_player_pos[1] + 0.01:
            return Action.Bottom
        if obs['ball'][1] < controlled_player_pos[1] - 0.01:
            return Action.Top
        # Try to take over the ball if close to the ball.
        return Action.Slide"
9904,45436490,2,"# Set up the Environment.
from kaggle_environments import make
env = make(""football"", configuration={""save_video"": True, ""scenario_name"": ""11_vs_11_kaggle"", ""running_in_notebook"": True})
output = env.run([""/kaggle/working/submission.py"", ""do_nothing""])[-1]
print('Left player: reward = %s, status = %s, info = %s' % (output[0]['reward'], output[0]['status'], output[0]['info']))
print('Right player: reward = %s, status = %s, info = %s' % (output[1]['reward'], output[1]['status'], output[1]['info']))
env.render(mode=""human"", width=800, height=600)"
9905,44361370,0,"# Install:
# Kaggle environments.
!git clone https://github.com/Kaggle/kaggle-environments.git
!cd kaggle-environments && pip install .

# GFootball environment.
!apt-get update -y
!apt-get install -y libsdl2-gfx-dev libsdl2-ttf-dev

# Make sure that the Branch in git clone and in wget call matches !!
!git clone -b v2.6 https://github.com/google-research/football.git
!mkdir -p football/third_party/gfootball_engine/lib

!wget https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_v2.6.so -O football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so
!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install ."
9906,44361370,1,"%%writefile submission.py
from kaggle_environments.envs.football.helpers import *

# @human_readable_agent wrapper modifies raw observations 
# provided by the environment:
# https://github.com/google-research/football/blob/master/gfootball/doc/observation.md#raw-observations
# into a form easier to work with by humans.
# Following modifications are applied:
# - Action, PlayerRole and GameMode enums are introduced.
# - 'sticky_actions' are turned into a set of active actions (Action enum)
#    see usage example below.
# - 'game_mode' is turned into GameMode enum.
# - 'designated' field is removed, as it always equals to 'active'
#    when a single player is controlled on the team.
# - 'left_team_roles'/'right_team_roles' are turned into PlayerRole enums.
# - Action enum is to be returned by the agent function.
@human_readable_agent
def agent(obs):
    # Make sure player is running.
    if Action.Sprint not in obs['sticky_actions']:
        return Action.Sprint
    # We always control left team (observations and actions
    # are mirrored appropriately by the environment).
    controlled_player_pos = obs['left_team'][obs['active']]
    # Does the player we control have the ball?
    if obs['ball_owned_player'] == obs['active'] and obs['ball_owned_team'] == 0:
        # Shot if we are 'close' to the goal (based on 'x' coordinate).
        if controlled_player_pos[0] > 0.5:
            return Action.Shot
        # Run towards the goal otherwise.
        return Action.Right
    else:
        # Run towards the ball.
        if obs['ball'][0] > controlled_player_pos[0] + 0.05:
            return Action.Right
        if obs['ball'][0] < controlled_player_pos[0] - 0.05:
            return Action.Left
        if obs['ball'][1] > controlled_player_pos[1] + 0.05:
            return Action.Bottom
        if obs['ball'][1] < controlled_player_pos[1] - 0.05:
            return Action.Top
        # Try to take over the ball if close to the ball.
        return Action.Slide"
9907,44361370,2,"# Set up the Environment.
from kaggle_environments import make

env = make(""football"", configuration={""save_video"": True, ""scenario_name"": ""11_vs_11_kaggle"", ""running_in_notebook"": True})

output = env.run([""/kaggle/working/submission.py"", ""do_nothing""])[-1]

print('Left player: reward = %s, status = %s, info = %s' % (output[0]['reward'], output[0]['status'], output[0]['info']))
print('Right player: reward = %s, status = %s, info = %s' % (output[1]['reward'], output[1]['status'], output[1]['info']))

env.render(mode=""human"", width=800, height=600)"
9908,44361370,3,
9909,45835722,0,"# Install:
# Kaggle environments.
!git clone https://github.com/Kaggle/kaggle-environments.git
!cd kaggle-environments && pip install .

# GFootball environment.
!apt-get update -y
!apt-get install -y libsdl2-gfx-dev libsdl2-ttf-dev

# Make sure that the Branch in git clone and in wget call matches !!
!git clone -b v2.8 https://github.com/google-research/football.git
!mkdir -p football/third_party/gfootball_engine/lib

!wget https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_v2.8.so -O football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so
!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install ."
9910,45835722,1,"%%writefile submission.py
from kaggle_environments.envs.football.helpers import *

# @human_readable_agent wrapper modifies raw observations 
# provided by the environment:
# https://github.com/google-research/football/blob/master/gfootball/doc/observation.md#raw-observations
# into a form easier to work with by humans.
# Following modifications are applied:
# - Action, PlayerRole and GameMode enums are introduced.
# - 'sticky_actions' are turned into a set of active actions (Action enum)
#    see usage example below.
# - 'game_mode' is turned into GameMode enum.
# - 'designated' field is removed, as it always equals to 'active'
#    when a single player is controlled on the team.
# - 'left_team_roles'/'right_team_roles' are turned into PlayerRole enums.
# - Action enum is to be returned by the agent function.
@human_readable_agent
def agent(obs):
    # Make sure player is running.
    if Action.Sprint not in obs['sticky_actions']:
        return Action.Sprint
    # We always control left team (observations and actions
    # are mirrored appropriately by the environment).
    controlled_player_pos = obs['left_team'][obs['active']]
    # Does the player we control have the ball?
    if obs['ball_owned_player'] == obs['active'] and obs['ball_owned_team'] == 0:
        # Shot if we are 'close' to the goal (based on 'x' coordinate).
        if controlled_player_pos[0] > 0.5:
            return Action.Shot
        # Run towards the goal otherwise.
        return Action.Right
    else:
        # Run towards the ball.
        if obs['ball'][0] > controlled_player_pos[0] + 0.05:
            return Action.Right
        if obs['ball'][0] < controlled_player_pos[0] - 0.05:
            return Action.Left
        if obs['ball'][1] > controlled_player_pos[1] + 0.05:
            return Action.Bottom
        if obs['ball'][1] < controlled_player_pos[1] - 0.05:
            return Action.Top
        # Try to take over the ball if close to the ball.
        return Action.Slide"
9911,45835722,2,"# Set up the Environment.
from kaggle_environments import make
env = make(""football"", configuration={""save_video"": True, ""scenario_name"": ""11_vs_11_kaggle"", ""running_in_notebook"": True})
output = env.run([""/kaggle/working/submission.py"", ""do_nothing""])[-1]
print('Left player: reward = %s, status = %s, info = %s' % (output[0]['reward'], output[0]['status'], output[0]['info']))
print('Right player: reward = %s, status = %s, info = %s' % (output[1]['reward'], output[1]['status'], output[1]['info']))
env.render(mode=""human"", width=800, height=600)"
9912,44888618,0,"#importing the tensorflow
!pip install tensorflow==1.15.3 --user"
9913,44888618,1,"#importing the numpy and tensorflow
import numpy as np
import tensorflow as tf"
9914,44888618,2,"train_ds='../input/109-1-ntut-dl-app-hw1/emnist-byclass-train.npz'
test_ds='../input/109-1-ntut-dl-app-hw1/emnist-byclass-test.npz'
"
9915,44888618,3,"df=np.load(train_ds) #loading to the train set

ti=df['training_images'] #ti->train images, calling the label column"
9916,44888618,4,"df=np.load(train_ds) #loading to the train set
tl=df['training_labels'] #tl->train labels, calling the  label column
# detect and init the TPU
tpu = tf.distribute.cluster_resolver.TPUClusterResolver()
tf.config.experimental_connect_to_cluster(tpu)
tf.tpu.experimental.initialize_tpu_system(tpu)

# instantiate a distribution strategy
tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)

# instantiating the model in the strategy scope creates the model on the TPU
with tpu_strategy.scope():
    # define your model normally
    model= tf.keras.models.Sequential()
    model.add(tf.keras.layers.Conv2D(32, kernel_size=(5, 5),activation='relu'))
    model.add(tf.keras.layers.Dropout(0.1))
    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))

    model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu'))
    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))
    model.add(tf.keras.layers.Dropout(0.1))
    model.add(tf.keras.layers.Flatten())
    
    model.add(tf.keras.layers.Dense(128,activation=tf.nn.relu))
    model.add(tf.keras.layers.Dropout(0.15))
    model.add(tf.keras.layers.Dense(62, activation=tf.nn.softmax))
"
9917,44888618,5,model.summary()
9918,44888618,6,"model.compile(optimizer='adam',loss='sparse_categorical_crossentropy', metrics=['accuracy'])
"
9919,44888618,7,"model.fit(ti, tl, epochs=9,batch_size=128)    
"
9920,44888618,8,ti = np.load(test_ds)['testing_images']
9921,44888618,9,r=model.predict_classes(ti)
9922,44888618,10,"        
# Print results in CSV format and upload to Kaggle
with open('Emnist_pred_results.csv', 'w') as f:
    f.write('Id,Category\n')
    for i in range(len(r)):
        f.write(str(i) + ',' + str(r[i]) + '\n')"
9923,44888618,11,"# Download your results!
from IPython.display import FileLink
FileLink('Emnist_pred_results.csv')"
9924,44888618,12,
9925,44439640,0,"#Importing the required libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.optimize as opt

plt.style.use('ggplot')
plt.rcParams['figure.figsize'] = (12,12)"
9926,44439640,1,"# data preprocessing
from sklearn.preprocessing import StandardScaler
# data splitting
from sklearn.model_selection import train_test_split
# data modeling
from sklearn.metrics import confusion_matrix,accuracy_score,roc_curve,classification_report
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from xgboost import XGBClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
#ensembling
from mlxtend.classifier import StackingCVClassifier"
9927,44439640,2,"#loading the training and testing dataset
df = pd.read_csv('final.csv')
x_test = pd.read_csv('Test.csv')
test_ID = x_test['ID']
#This shows the first 5 rows of the dataset
df.head()"
9928,44439640,3,"# Initializing the training data
y = df[""target""]
X = df.drop('target',axis=1)
X_train = X
y_train = y"
9929,44439640,4,"# Grouping the data in terms of the positive - 1 and negative cases - 0 with their mean values
df.groupby('target').mean()"
9930,44439640,5,"# Describing the dataset 
df.describe()"
9931,44439640,6,"# Getting the required information about the dataset
df.info()"
9932,44439640,7,"# Calculating the mean for each column
df.mean()"
9933,44439640,8,"# Checking if there are any null values in the dataset
df.isnull().sum()"
9934,44439640,9,"# Using the groupby function to calculate the mean
df.groupby('target').mean()"
9935,44439640,10,"# Plotting a histogram representation of all the features
df.hist(figsize = (12, 8))
plt.show()"
9936,44439640,11,"
negative_cases = len(df[y == 0])
positive_cases = len(df[y == 1])
per_negative_cases = (negative_cases/len(y))*100 # percentage of negative cases
per_positive_cases = (positive_cases/len(y))*100 # percentage of positive cases

# We know that cardiovascular or Heart diseases affect more women than men and are responsible for more than 40% of all deaths in American women.
# Assuming 0 - Female and 1 - Male

female = len(df[df['sex']== 0])
male = len(df[df['sex'] == 1])
per_female = (female/len(y))*100
per_male = (male/len(y))*100

#This figure consists of two subplots to present the data graphical
fig, ax =plt.subplots(1,2)
sns.countplot(df['target'], ax=ax[0])# counts the number of positive and negative cases in the dataset
sns.countplot(df['sex'], ax=ax[1])# counts the number of male and female patients in the dataset
fig.show()
"
9937,44439640,12,"# Analysing the 'Chest Pain Type' feature
df[""cp""].unique()"
9938,44439640,13,"# Plotting a barplot for the 'Chest Pain' feature
sns.barplot(df[""cp""],y)"
9939,44439640,14,"# Analysing the FBS feature
df['fbs'].unique()"
9940,44439640,15,"# Plotting a barplot for FBS feature
sns.barplot(df[""fbs""],y)"
9941,44439640,16,"# Analysing the 'ca' feature
df[""ca""].unique()"
9942,44439640,17,"# Using a count plot to count
sns.countplot(df[""ca""])"
9943,44439640,18,"# Plotting a graph for Age Vs Maximum Heart rate achieved
plt.scatter(x=df.age[df['target']==1], y=df.thalach[(df['target']==1)], c=""red"")
plt.scatter(x=df.age[df['target']==0], y=df.thalach[(df['target']==0)], c = ""green"")
plt.legend([""+ve"", ""-ve""])
plt.xlabel(""Age"")
plt.ylabel(""maximum heart rate achieved"")
plt.show()"
9944,44439640,19,"# Finding the correlation between the features
sns.heatmap(df.corr(),annot=True,fmt='.2f')
plt.show()"
9945,44439640,20,"from xgboost import XGBClassifier


scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(x_test)

m4 = 'Extreme Gradient Boost'
xgb = XGBClassifier(learning_rate=0.01, n_estimators=25, max_depth=15,gamma=0.6, subsample=0.52,colsample_bytree=0.6,seed=27, 
                    reg_lambda=2, booster='dart', colsample_bylevel=0.6, colsample_bynode=0.5)
xgb.fit(X_train, y_train)
xgb_predicted = xgb.predict(X_test)
"
9946,44439640,21,"df_2 = pd.DataFrame({'ID':test_ID,'target':xgb_predicted})
df_2.to_csv('submission.csv',index=False)"
9947,44439640,22,
9975,45365843,0,"import numpy as np
import pandas as pd 
import xgboost as xgb
from xgboost.sklearn import XGBClassifier
from sklearn.metrics import roc_auc_score, confusion_matrix
from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV
from xgboost import plot_importance
import seaborn as sns
import matplotlib.pyplot as plt
import operator as op"
9976,45365843,1,"train = pd.read_csv(""../input/stat441datachallenge1/online_jewellery_shop_train.csv"")
test = pd.read_csv(""../input/stat441datachallenge1/online_jewellery_shop_test_final.csv"")"
9977,45365843,2,train.describe()
9978,45365843,3,"# seperate labels and explanatory variables
y_train = train[""Revenue""]
X_train = train.drop([""Revenue""], axis=1)
X_test = test
n = len(y_train)"
9979,45365843,4,X_train.columns
9980,45365843,5,"# inspect variables by using a heat map (a correlation matrix using spearman as it a non-parametric test)
data = pd.concat([X_train,y_train], axis = 1)
corrmat = data.corr(method=""spearman"")
top_corr_features = corrmat.index
plt.figure(figsize=(25,25))
# plt heat map
g = sns.heatmap(data[top_corr_features].corr(), annot=True, cmap=""RdYlGn"")"
9981,45365843,6,"# modify string variables into numerical variables
def encode(X):
    # make VisitorType and Month into continuous variables
    visitor_type_dict = {""New_Visitor"": 1, ""Returning_Visitor"": 0, ""Other"": 2}
    X[""VisitorType""] = X[""VisitorType""].map(visitor_type_dict)
    month_dict = {""Jan"":1, ""Feb"":2, ""Mar"":3, ""Apr"":4, ""May"": 5, ""June"": 6, ""Jul"": 7, ""Aug"": 8, ""Sep"": 9, ""Oct"": 10, ""Nov"": 11, ""Dec"": 12}
    X[""Month""] = X[""Month""].map(month_dict)
    # setting up interaction variables
    interac_vars = [""Administrative"", ""Administrative_Duration"", ""Informational"", ""Informational_Duration"", ""ProductRelated"", ""ProductRelated_Duration""]
    for i in range(0,6):
        for j in range(1,6):
            var1 = interac_vars[i]
            var2 = interac_vars[j]
            if(var1 < var2):
                X[var1+""*""+var2] = X[var1]* X[var2]
    return X

X_train = encode(X_train)"
9982,45365843,7,"# inspect variables by using a heat map a correlation matrix using spearman as it a non-parametric test after adding in interation terms
data = pd.concat([X_train,y_train], axis = 1)
corrmat = data.corr(method=""spearman"")
top_corr_features = corrmat.index
plt.figure(figsize=(25,25))
# plt heat map
g = sns.heatmap(data[top_corr_features].corr(), annot=True, cmap=""RdYlGn"")"
9983,45365843,8,"# top 15 potential statistically insignificant features using the spearman test
(abs(corrmat.iloc[-1]).sort_values()).head(10)"
9984,45365843,9,print(X_train.isnull().sum().max())
9985,45365843,10,"# check ratio of 0s and 1s
print(sum(y_train == 0)/n) "
9986,45365843,11,"def grid_search(X, y, default_param, param_grid):
    grid_search = GridSearchCV (estimator=XGBClassifier(**default_param), param_grid=param_grid, scoring='roc_auc', n_jobs=-1, return_train_score=True, verbose=3, refit='roc_auc')
    grid_search.fit(X, y)
    print(grid_search.best_estimator_)
    print(grid_search.best_score_)"
9987,45365843,12,"# scale_pos_weight is set because this is a highly imbalanced dataset
# use gamma, alpha, lambda to perform regulization
# use min_child_weight, max_depth to make my model more conservative
params = {
    'objective': 'binary:logistic',
    'learning_rate': 0.1, 
    'eval_metric': 'auc',
    'scale_pos_weight': (sum(y_train == 1))/ (sum(y_train == 0)),
    'tree_method': 'exact',
    # grid1
    'min_child_weight': 1,
    # grid11
    'max_depth': 4,
    # grid2
    'gamma': 1,
    # grid3
    'subsample': 1,
    # grid4
    'colsample_bytree': 0.6,
    # grid5
    'max_delta_step': 2,
    # grid6
    'reg_alpha': 1e-5,
    'reg_lambda': 1,
}
param_grid1 = {
    'max_depth':range(0,10),
}
param_grid11 = {
    'min_child_weight':range(0,10)
}
param_grid2 = {
    'gamma':[i/2 for i in range(0,10)]
}
param_grid3 = {
    'subsample':[i/10 for i in range(6,11)],
}

param_grid4 = {
    'colsample_bytree':[i/10 for i in range(6,11)],
}
param_grid5 = {
    'max_delta_step':[0, 0.5, 1, 1.5, 2, 2.5, 3],
}
param_grid6 = {
    'reg_alpha':[1e-5, 1e-4, 1e-3, 1e-2, 0, 0.1, 1],
    'reg_lambda':[1e-5, 1e-4, 1e-3, 1e-2, 0, 0.1, 1]
}
#grid_search(X_train, y_train, params, param_grid6)"
9988,45365843,13,"model = XGBClassifier(**params)
model.fit(X_train, y_train)
sorted((model.get_booster().get_score(importance_type = 'gain')).items(), key = op.itemgetter(1), reverse=True)[::-1][0:20]"
9989,45365843,14,"def preprocess(X):
    X = encode(X)
    return X"
9990,45365843,15,"# # Prepare 20% of test data 
# sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)
# X_untouched_test = 0
# y_untouched_test = 0
# for train_index, test_index in sss.split(X_train, y_train):
#     X_train, X_untouched_test = X_train.iloc[train_index], X_train.iloc[test_index]
#     y_train ,y_untouched_test = y_train.iloc[train_index], y_train.iloc[test_index]"
9991,45365843,16,"# Cross Validation
s = StratifiedShuffleSplit(n_splits=4, test_size=0.2, random_state=0)
best_X_train = []
best_X_test = []
best_y_train = []
best_y_test = []
best_precision_score = 0
for train_index, test_index in s.split(X_train, y_train):
    print(""TRAIN:"", train_index, ""TEST:"", test_index)
    X_new_train, X_new_test = X_train.iloc[train_index], X_train.iloc[test_index]
    y_new_train, y_new_test = y_train.iloc[train_index], y_train.iloc[test_index]

    model.fit(X_new_train, y_new_train)
  
    y_new_test_pred = model.predict_proba(X_new_test)[:,1]
    
    auc = roc_auc_score(y_new_test, y_new_test_pred)
    if auc > best_precision_score:
      print(""New best with a AUC score of {0} "".format(auc))
      best_precision_score = auc
      best_X_train = X_new_train
      best_X_test = X_new_test
      best_y_train = y_new_train
      best_y_test = y_new_test
      
    else:
      print(""Current AUC score is {0}"".format(auc))"
9992,45365843,17,"X_train = best_X_train
y_train = best_y_train
X_test = best_X_test
y_test = best_y_test"
9993,45365843,18,"def print_report(true_y, pred_y_prob):
    cm = confusion_matrix(true_y, pred_y_prob>0.5)
    auc = roc_auc_score(true_y, pred_y_prob)
    print(""AUC Score: %f"" % roc_auc_score(true_y, pred_y_prob))
    tn, fp, fn, tp = cm.ravel()
    print(""True Negative: {0}   False Positive: {1}\nFalse Negative: {2}   True Positive: {3}"".format(tn, fp, fn, tp))"
9994,45365843,19,"# Model check on train data
print_report(y_train, model.predict_proba(X_train)[:,1])"
9995,45365843,20,"# Model check on test data
print_report(y_test, model.predict_proba(X_test)[:,1])"
9996,45365843,21,"# # Model auc check on untouched test data
# print_report(y_untouched_test, model.predict_proba(X_untouched_test)[:,1])"
9997,45365843,22,X_test = preprocess(test)
9998,45365843,23,print(X_test.isnull().sum().max())
9999,45365843,24,X_test
10000,45365843,25,"y_pred_test = model.predict_proba(X_test)[:,1]"
10001,45365843,26,"submission = pd.DataFrame({""ID"": test[""ID""], ""Revenue"": y_pred_test})"
10002,45365843,27,submission
10003,45365843,28,"sum(submission[""Revenue""] <= 0.5) / 3700"
10004,45365843,29,"submission.to_csv(""submission.csv"", index=False)"
10005,46662684,0,########################### LOADING DATA ############################################
10006,46662684,1,#!unzip tau-ethiopic-digit-recognition.zip
10007,46662684,2,"# LOAD TRAINING DATA
import numpy as np
from PIL import Image
from sklearn.datasets import load_files
path=""train/train/""
data=load_files(path)
X=np.zeros((data.filenames.shape[0],28,28))
for i,file in enumerate(data.filenames):
    X[i,:]=np.array(Image.open(file))
y=data.target
# LOADING TEST DATA
import os
path=""test/test/""
X_test=np.zeros((len(os.listdir(path)),28,28))
ID=[]
for i,file in enumerate(os.listdir(path)):
    X_test[i,:]=np.array(Image.open(path+file))
    ID.append(int(file.split(""."")[0]))"
10008,46662684,3,###########################SPLITTING DATA ##################################
10009,46662684,4,"from sklearn.model_selection import train_test_split
X_train,X_val,y_train,y_val=train_test_split(X, y,test_size=0.2)"
10010,46662684,5,########################### CNN MODEL ##################################
10011,46662684,6,"from keras.models import Model, Sequential
from keras.constraints import maxnorm
from keras.layers import *

def make_model():
    model = Sequential()

    model.add(Conv2D(56, (3, 3),input_shape=(28, 28,1), activation='relu', padding='same'))
    model.add(GaussianNoise(0.2))
    model.add(BatchNormalization(axis=-1))
    model.add(Conv2D(56, (3, 3), activation='relu', padding='same'))
    model.add(BatchNormalization(axis=-1))
    model.add(MaxPooling2D(pool_size=(2, 2), strides=2))
    model.add(SpatialDropout2D(0.1))

    model.add(Conv2D(112, (3, 3), activation='relu', padding='same'))
    model.add(BatchNormalization(axis=-1))
    model.add(Conv2D(112, (3, 3), activation='relu', padding='same'))
    model.add(BatchNormalization(axis=-1))
    model.add(MaxPooling2D(pool_size=(2, 2), strides=2))
    model.add(SpatialDropout2D(0.1))


    model.add(Conv2D(224, (3, 3), activation='relu', padding='same'))
    model.add(BatchNormalization(axis=-1))
    model.add(Conv2D(224, (3, 3), activation='relu', padding='same'))
    model.add(BatchNormalization(axis=-1))
    model.add(MaxPooling2D(pool_size=(2, 2), strides=2))
    model.add(SpatialDropout2D(0.1))
    
    model.add(Flatten())
    model.add(Dense(784, activation='relu', kernel_constraint=maxnorm(3)))
    model.add(BatchNormalization())
    model.add(Dropout(0.2))
    model.add(Dense(98, activation='relu', kernel_constraint=maxnorm(3)))
    model.add(BatchNormalization())
    model.add(Dropout(0.2))
    model.add(Dense(10, activation='softmax'))
    return model"
10012,46662684,7,"# Creating the CNN model and printing summary
model=make_model()
model.summary()"
10013,46662684,8,"# Creating optimizer and compiling model
from keras.optimizers import Adam
epochs=100
lrate=0.001
decay=lrate/100
optimizer=Adam(lr=lrate,decay=decay, beta_1=0.9, beta_2=0.999, epsilon=1e-08)
model.compile(loss='sparse_categorical_crossentropy',optimizer=optimizer, metrics=['accuracy']) "
10014,46662684,9,"# Creating an image augmentation object and fitting it
from tensorflow.keras.preprocessing.image import ImageDataGenerator
aug = ImageDataGenerator(
    #featurewise_center=True,
    #featurewise_std_normalization=True,
    rotation_range=45,
    width_shift_range=0.2,
    height_shift_range=0.2,
    horizontal_flip=True,
    zoom_range=0.3)
aug.fit(X.reshape(X.shape[0],28,28,1))"
10015,46662684,10,"import tensorflow as tf
checkpoint_filepath = 'checkpoint'
model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_filepath,
    save_weights_only=True,
    monitor='val_accuracy',
    mode='max',
    save_best_only=True)"
10016,46662684,11,"from keras.utils import np_utils
history=model.fit_generator(aug.flow(X_train.reshape(-1,28,28,1),
                                     y_train, batch_size=128), # np_utils.to_categorical(y_train)
                            steps_per_epoch = X_train.shape[0] / 128
                  ,epochs=epochs, validation_data=(X_val.reshape(-1,28,28,1),
                                                  y_val ), verbose=1, # np_utils.to_categorical(y_val)
                           callbacks=[model_checkpoint_callback]) "
10017,46662684,12,"import time
model.load_weights(checkpoint_filepath)
model.save(""model-100-csc-{}"".format(time.time()))"
10018,46662684,13,"y_test_pred=model.predict(X_test.reshape(-1,28,28,1))
y_test_pred"
10019,46662684,14,"y=np.argmax(y_test_pred, axis=1)
y"
10020,46662684,15,ids=np.array(ID)
10021,46662684,16,"data.target_names, data.target"
10022,46662684,17,"target_dict={}
for i,t in enumerate(data.target_names):
    target_dict[i]=t"
10023,46662684,18,"y_real=[]
for Y in y:
    y_real.append(target_dict[Y])"
10024,46662684,19,y_real
10025,46662684,20,"import pandas as pd
res=pd.DataFrame(data=ids, columns=[""Id""])
res[""Category""]=y_real
res"
10026,46662684,21,"res.to_csv(""submission.csv"", index=False)"
10027,46662684,22,"model.evaluate(X_val.reshape(-1,28,28,1),y_val)"