code_blocks_index,kernel_id,code_block_id,code_block,code_smell_prediction,smell_label
0,28356656,0,"import numpy as np 
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
from catboost import Pool, cv, CatBoostClassifier, CatBoostRegressor

from sklearn.metrics import mean_squared_error, classification_report
from sklearn.preprocessing import LabelEncoder

from sklearn.model_selection import train_test_split

# import xgboost
# import lightgbm as lgb
# from lightgbm import LGBMClassifier

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))
        
import gc
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn import metrics
pd.set_option('max_rows', 300)
import re

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

pd.set_option('display.max_columns', 300)
np.random.seed(566)
pd.set_option('display.max_rows', 200)
pd.set_option('display.width', 1000)
pd.set_option('display.float_format', '{:20,.2f}'.format)
pd.set_option('display.max_colwidth', -1)",1,Code Smell
1,28356656,1,"TARGET_COL = ""hospital_death""",0,No Code Smell
2,28356656,2,"df = pd.read_csv(""/kaggle/input/widsdatathon2020/training_v2.csv"")
print(df.shape)
display(df.nunique())
df.head()",1,Code Smell
3,28356656,3,df.isna().sum(),0,No Code Smell
4,28356656,4,df.describe(),0,No Code Smell
5,28356656,5,"test = pd.read_csv(""/kaggle/input/widsdatathon2020/unlabeled.csv"")
print(test.shape)
display(test.nunique())
test.head()",1,Code Smell
6,28356656,6,test.isna().sum(),0,No Code Smell
7,28356656,7,"print([c for c in df.columns if 7<df[c].nunique()<800])
## 
# categorical_cols = ['hospital_id','apache_3j_bodysystem', 'apache_2_bodysystem',
# ""hospital_admit_source"",""icu_id"",""ethnicity""]",0,No Code Smell
8,28356656,8,"## print non numeric columns : We may need to
## define them as categorical / encode as numeric with label encoder, depending on ml model used
print([c for c in df.columns if (1<df[c].nunique()) & (df[c].dtype != np.number)& (df[c].dtype != int) ])",0,No Code Smell
9,28356656,9,"categorical_cols =  ['hospital_id',
 'ethnicity', 'gender', 'hospital_admit_source', 'icu_admit_source', 'icu_stay_type', 'icu_type', 'apache_3j_bodysystem', 'apache_2_bodysystem']

#['apache_3j_bodysystem', 'apache_2_bodysystem',
# ""hospital_admit_source"",""icu_id"",""ethnicity""]",0,No Code Smell
10,28356656,10,"display(df[categorical_cols].dtypes)
display(df[categorical_cols].tail(3))
display(df[categorical_cols].isna().sum())",0,No Code Smell
11,28356656,11,"df[categorical_cols] = df[categorical_cols].fillna("""")

# same transformation for test data
test[categorical_cols] = test[categorical_cols].fillna("""")

df[categorical_cols].isna().sum()",0,No Code Smell
12,28356656,12,"## useful ""hidden"" function - df._get_numeric_data()  - returns only numeric columns from a pandas dataframe. Useful for scikit learn models! 

X_train = df.drop([TARGET_COL],axis=1)
y_train = df[TARGET_COL]",0,No Code Smell
13,28356656,13,"## catBoost Pool object
train_pool = Pool(data=X_train,label = y_train,cat_features=categorical_cols,
#                   baseline= X_train[""""], ## 
#                   group_id = X_train['hospital_id']
                 )

### OPT/TODO:  do train test split for early stopping then add that as an eval pool object : ",0,No Code Smell
14,28356656,14,"model_basic = CatBoostClassifier(verbose=False,iterations=50)#,learning_rate=0.1, task_type=""GPU"",)
model_basic.fit(train_pool, plot=True,silent=True)
print(model_basic.get_best_score())",0,No Code Smell
15,28356656,15,"### hyperparameter tuning example grid for catboost : 
grid = {'learning_rate': [0.04, 0.1],
        'depth': [7, 11],
#         'l2_leaf_reg': [1, 3,9],
#        ""iterations"": [500],
       ""custom_metric"":['Logloss', 'AUC']}

model = CatBoostClassifier()

## can also do randomized search - more efficient typically, especially for large search space - `randomized_search`
grid_search_result = model.grid_search(grid, 
                                       train_pool,
                                       plot=True,
                                       refit = True, #  refit best model on all data
                                      partition_random_seed=42)

print(model.get_best_score())",0,No Code Smell
16,28356656,16,"print(""best model params: \n"",grid_search_result[""params""])",0,No Code Smell
17,28356656,17,"feature_importances = model.get_feature_importance(train_pool)
feature_names = X_train.columns
for score, name in sorted(zip(feature_importances, feature_names), reverse=True):
    if score > 0.05:
        print('{0}: {1:.2f}'.format(name, score))",0,No Code Smell
18,28356656,18,"import shap
shap.initjs()

explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(train_pool)

# visualize the training set predictions
# SHAP plots for all the data is very slow, so we'll only do it for a sample. Taking the head instead of a random sample is dangerous! 
shap.force_plot(explainer.expected_value,shap_values[0,:400], X_train.iloc[0,:400])",0,No Code Smell
19,28356656,19,"# summarize the effects of all the features
shap.summary_plot(shap_values, X_train)",0,No Code Smell
20,28356656,20,"test[TARGET_COL] = model.predict(test.drop([TARGET_COL],axis=1),prediction_type='Probability')[:,1]",0,No Code Smell
21,28356656,21,"test[[""encounter_id"",""hospital_death""]].to_csv(""submission.csv"",index=False)",0,No Code Smell
22,29140873,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the ""../input/"" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# Any results you write to the current directory are saved as output.
import lightgbm as lgb
from sklearn.metrics import roc_auc_score
from sklearn.metrics import auc
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from datetime import datetime, date, time, timedelta

def label_var(data,variables_cat):
    lb=[]
    for m in variables_cat:
        l=LabelEncoder()
        lb.append(l.fit(list(data[m].dropna())))
    
    return lb

def label_enc(data,l,categorical_features):
    i=0
    for m in categorical_features:
        data.loc[data[m].notnull(),m]=l[i].transform(data.loc[data[m].notnull(),m])
        i=i+1

df_tr = pd.read_csv(""/kaggle/input/widsdatathon2020/training_v2.csv"")
df_ts = pd.read_csv(""/kaggle/input/widsdatathon2020/unlabeled.csv"")

train_columns = [x for x in df_tr.columns if x not in ['encounter_id','patient_id','hospital_death','readmission_status']]

categorical_features = []
for m in train_columns:
    if(df_tr[m].dtypes=='object'):
        categorical_features.append(m)
        
df_tr_ts = pd.concat([df_tr[categorical_features],df_ts[categorical_features]])

l = label_var(df_tr_ts, categorical_features)
label_enc(df_tr,l,categorical_features)
label_enc(df_ts,l,categorical_features)

for df in [df_tr, df_ts]:
    for m in categorical_features:
        df[m] = df[m].astype(float)
        
categorical_index = [train_columns.index(x) for x in categorical_features]

target = df_tr['hospital_death']

param = {'task': 'train',
         'boosting': 'gbdt',
         'objective':'binary',
         'metric': 'auc',
         'num_leaves': 15,
         'min_data_in_leaf': 90,
         'learning_rate': 0.01,
         'max_depth': 5,
         'feature_fraction': 0.1,
         'bagging_freq': 1,
         'bagging_fraction': 0.75,
         'use_missing': True,
         'nthread': 4
        }

folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=256)
oof = np.zeros(len(df_tr))
r=[]
predictions = np.zeros(len(df_ts))
for fold_, (trn_idx, val_idx) in enumerate(folds.split(df_tr,target.values)):
    strLog = ""fold {}"".format(fold_)
    print(strLog)
    
    trn_data = lgb.Dataset(df_tr.iloc[trn_idx][train_columns], label=target.iloc[trn_idx])
    val_data = lgb.Dataset(df_tr.iloc[val_idx][train_columns], label=target.iloc[val_idx],reference=trn_data)

    num_round = 7000
    clf = lgb.train(param,trn_data,num_round,valid_sets=val_data,early_stopping_rounds=100,verbose_eval=200,categorical_feature=categorical_index)
    oof[val_idx] = clf.predict(df_tr.iloc[val_idx][train_columns], num_iteration=clf.best_iteration)
   
    a=roc_auc_score(target.loc[val_idx],clf.predict(df_tr.loc[val_idx,train_columns].values, num_iteration=clf.best_iteration))
    r.append(a)
    
    #predictions
    predictions += clf.predict(df_ts[train_columns], num_iteration=clf.best_iteration) / folds.n_splits
    
strAUC = roc_auc_score(target, oof)
print(strAUC)
print (""mean: ""+str(np.mean(np.array(r))))
print (""std: ""+str(np.std(np.array(r))))

df_sub = pd.DataFrame({'encounter_id': df_ts['encounter_id']})
df_sub['hospital_death'] = predictions

df_sub.to_csv(""sub1.csv"",index=False)",1,Code Smell
23,28710497,0,"import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt",1,Code Smell
24,28710497,1,"train = pd.read_csv(""../input/widsdatathon2020/training_v2.csv"")
test = pd.read_csv(""../input/widsdatathon2020/unlabeled.csv"")",1,Code Smell
25,28710497,2,"train.shape, test.shape",0,No Code Smell
26,28710497,3,train.head(),0,No Code Smell
27,28710497,4,test.head(),0,No Code Smell
28,28710497,5,train.describe(),0,No Code Smell
29,28710497,6,test.describe(),0,No Code Smell
30,28710497,7,train.isnull().sum()/len(train)*100,0,No Code Smell
31,28710497,8,"f,ax=plt.subplots(1,2,figsize=(18,8))
train['hospital_death'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',ax=ax[0],shadow=True)
ax[0].set_title('hospital_death')
ax[0].set_ylabel('')
sns.countplot('hospital_death',data=train,ax=ax[1])
ax[1].set_title('hospital_death')
plt.show()",0,No Code Smell
32,28710497,9,"train.drop(""readmission_status"",inplace=True,axis=1)
test.drop(""readmission_status"",inplace=True,axis=1)",0,No Code Smell
33,28710497,10,"test.drop(""hospital_death"",inplace=True,axis=1)
y_train = train[['encounter_id', 'patient_id', 'hospital_id',""hospital_death""]].copy()
train.drop(""hospital_death"",inplace=True,axis=1)",0,No Code Smell
34,28710497,11,"non_categorical = train.loc[:,train.dtypes!=""object""].columns",0,No Code Smell
35,28710497,12,"categorical = [c for c in train[non_categorical].columns if (train[c].nunique()<10)]
non_categorical = [c for c in train[non_categorical].columns if (train[c].nunique()>=10)]",0,No Code Smell
36,28710497,13,"print(train[non_categorical].isnull().sum()/len(train))
a = train[non_categorical].isnull().sum()/len(train)>0.40 ## editable
missing_m40 = train[non_categorical].loc[:,a].columns

a = train[non_categorical].isnull().sum()/len(train)<=0.40 ## editable
missing_l40 = train[non_categorical].loc[:,a].columns
del a",0,No Code Smell
37,28710497,14,"a = train.isnull().sum()/len(train)!= 0 ## editable
missing = train.loc[:,a].columns
del a
for i in missing:
    train[str(i)+""_Na""]=pd.get_dummies(train[i].isnull(),prefix=i).iloc[:,0]
    test[str(i)+""_Na""]=pd.get_dummies(test[i].isnull(),prefix=i).iloc[:,0]

for i in missing_l40:
    for j in train.hospital_id.unique():
        train[i][train.hospital_id==j]=train[i][train.hospital_id==j].fillna(train[i][train.hospital_id==j].median())
    for k in test.hospital_id.unique():
        test[i][test.hospital_id==k]=test[i][test.hospital_id==k].fillna(test[i][test.hospital_id==k].median())",0,No Code Smell
38,28710497,15,"train[""apache_4a_hospital_death_prob""]=train[""apache_4a_hospital_death_prob""].replace({-1:np.nan})
test[""apache_4a_hospital_death_prob""]=test[""apache_4a_hospital_death_prob""].replace({-1:np.nan})

train[""apache_4a_icu_death_prob""]=train[""apache_4a_icu_death_prob""].replace({-1:np.nan})
test[""apache_4a_icu_death_prob""]=test[""apache_4a_icu_death_prob""].replace({-1:np.nan})",0,No Code Smell
39,28710497,16,"a = train[non_categorical].isnull().sum()/len(train)<=0.40 ## editable
missing_l40 = train[non_categorical].loc[:,a].columns
for i in missing_l40:
    train[i] = train[i].fillna(train[i].median())
    
a = test[non_categorical].isnull().sum()/len(test)<=0.40 ## editable
missing_l40 = test[non_categorical].loc[:,a].columns
for i in missing_l40:
    test[i] = test[i].fillna(train[i].median())
del a, missing_l40, missing_m40",0,No Code Smell
40,28710497,17,"categorical=np.concatenate([train.loc[:,train.dtypes==""object""].columns.tolist(),categorical])",0,No Code Smell
41,28710497,18,train[categorical].isnull().sum()/len(train),0,No Code Smell
42,28710497,19,test[categorical].isnull().sum()/len(test),0,No Code Smell
43,28710497,20,train[categorical].nunique(),0,No Code Smell
44,28710497,21,"## imputador gender
train[""gender""][train.height>167]=train[""gender""][train.height>167].fillna(""M"")
train[""gender""][train.height<=167]=train[""gender""][train.height<=167].fillna(""F"")",0,No Code Smell
45,28710497,22,"for i in categorical:
    train[i] = train[i].fillna(train[i].value_counts().index[0])
    test[i] = test[i].fillna(train[i].value_counts().index[0])",0,No Code Smell
46,28710497,23,"categorical = train.loc[:,train.dtypes==""object""].columns.tolist()",0,No Code Smell
47,28710497,24,train[categorical].nunique(),0,No Code Smell
48,28710497,25,"train[""hospital_admit_source""]=train[""hospital_admit_source""].replace({'Other ICU':""ICU"",'ICU to SDU':""SDU"",
                                       'Step-Down Unit (SDU)':""SDU"",
                                      'Acute Care/Floor':""Floor"",
                                      'Other Hospital':""Other""})
test[""hospital_admit_source""]=test[""hospital_admit_source""].replace({'Other ICU':""ICU"",'ICU to SDU':""SDU"",
                                       'Step-Down Unit (SDU)':""SDU"",
                                      'Acute Care/Floor':""Floor"",
                                      'Other Hospital':""Other""})
train[""apache_2_bodysystem""] = train[""apache_2_bodysystem""].replace({'Undefined Diagnoses':""UD"",
                                                                    'Undefined diagnoses':""UD""})
test[""apache_2_bodysystem""] = test[""apache_2_bodysystem""].replace({'Undefined Diagnoses':""UD"",
                                                                    'Undefined diagnoses':""UD""})",0,No Code Smell
49,28710497,26,"train = train.join(pd.get_dummies(train[categorical]).drop(""gender_F"",axis=1))
test = test.join(pd.get_dummies(test[categorical]).drop(""gender_F"",axis=1))
train.drop(categorical,axis=1,inplace=True)
test.drop(categorical,axis=1,inplace=True)",0,No Code Smell
50,28710497,27,"non = ['encounter_id', 'patient_id', 'hospital_id','icu_id']",0,No Code Smell
51,28710497,28,"correlated_features = set()
train1 = train.drop(non,axis=1) 
correlation_matrix = train1.corr()
del train1",1,Code Smell
52,28710497,29,"for i in range(len(correlation_matrix.columns)):
     for j in range(i):
            if abs(correlation_matrix.iloc[i, j]) ==  1:
                colname = correlation_matrix.columns[i]
                correlated_features.add(colname)
correlated_features=list(correlated_features)",0,No Code Smell
53,28710497,30,"train.drop(correlated_features,axis=1,inplace=True)
test.drop(correlated_features,axis=1,inplace=True)",0,No Code Smell
54,28710497,31,"train.shape, test.shape",0,No Code Smell
55,28710497,32,"train.drop(""hospital_admit_source_Observation"",axis=1,inplace=True)",0,No Code Smell
56,28710497,33,"train = train.set_index(""encounter_id"")
test = test.set_index(""encounter_id"")
y_train = y_train.set_index(""encounter_id"")",0,No Code Smell
57,28710497,34,test = test.fillna(0),0,No Code Smell
58,28710497,35,"from lightgbm import LGBMClassifier
from sklearn.model_selection import StratifiedKFold, GroupKFold
drop_cols = ['patient_id', 'hospital_id','icu_id']
#drop_cols = np.concatenate([drop_cols,perdidos])
gf = GroupKFold(n_splits=4)
groups = np.array(train.hospital_id)
test_probs = []
train_probs = []

for i,(a,b) in enumerate(gf.split(train,y_train.loc[train.index, ""hospital_death""],groups)) :
    Xt = train.iloc[a,:]
    yt = y_train.loc[Xt.index, ""hospital_death""]
    Xt = Xt.drop(drop_cols, axis=1)
    Xt = Xt.fillna(0)
    
    Xv = train.iloc[b,:]
    yv = y_train.loc[Xv.index, ""hospital_death""]
    Xv = Xv.drop(drop_cols, axis=1)
    Xv = Xv.fillna(0)
    print(""*+*+*+*+*entrenando fold: {} "".format(i+1))
    
    learner = LGBMClassifier(n_estimators=10,learning_rate=0.03,num_iterations=3400,lambda_l2=7 ,lambda_l1 =7
                                 ,num_leaves =7,max_depth=5,min_data_in_leaf =500,early_stopping_rounds=200,feature_fraction= 0.8
                            ,bagging_fraction=0.85,bagging_freq=10)
    
    learner.fit(Xt, yt  , eval_metric=""auc"",eval_set= [(Xt, yt),(Xv, yv)], verbose=50)
    
    
    train_probs.append(pd.Series(learner.predict_proba(Xv)[:, -1],
                                index=Xv.index, name=""probs""+ str(i)))
    test_probs.append(pd.Series(learner.predict_proba(test.drop(drop_cols, axis=1))[:, -1],
                                index=test.index, name=""fold_"" + str(i)  ))
      
test_probs = pd.concat(test_probs, axis=1).mean(axis=1)
train_probs = pd.concat(train_probs, axis=1)",0,No Code Smell
59,28710497,36,"test_probs = pd.DataFrame(test_probs.rename(""hospital_death""))
test_probs.to_csv(""lightgbm_baseline.csv"", header=True)",0,No Code Smell
60,28985739,0,"# In this notebook I have used fancyimpute technique to impute missing values in the entire WiDS2020 dataset. 
# I have used LGBM to train the model. I got accuracy of 0.88. Parameter tuning might help in bettring this score.
",0,No Code Smell
61,28985739,1,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import seaborn as sns

# Input data files are available in the ""../input/"" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# Any results you write to the current directory are saved as output.",1,Code Smell
62,28985739,2,"from fancyimpute import KNN
from sklearn.preprocessing import OrdinalEncoder
from sklearn.model_selection import train_test_split
import lightgbm as lgb",1,Code Smell
63,28985739,3,"#read the data and drop noisy columns
train=pd.read_csv(""/kaggle/input/widsdatathon2020/training_v2.csv"")
test=pd.read_csv(""/kaggle/input/widsdatathon2020/unlabeled.csv"")
solution = pd.read_csv(""/kaggle/input/solutiontemplate/solution_template.csv"")
trainv1=train.drop(['encounter_id','patient_id','icu_id', 'hospital_id', 'readmission_status','ethnicity'],axis=1)
testv1=test.drop(['encounter_id','patient_id','icu_id', 'hospital_id', 'readmission_status','hospital_death','ethnicity'],axis=1)
print(""number of rows and columns in training set is \n"",trainv1.shape)
print(""number of rows and columns in test set is \n"",testv1.shape)",1,Code Smell
64,28985739,4,"#Exploring the data
trainv1.info()
trainv1.describe()
trainv1.isna().sum()
testv1.isna().sum()
trainv1['hospital_death'].value_counts()*100/len(trainv1['hospital_death'])
sns.countplot(trainv1['hospital_death'])",0,No Code Smell
65,28985739,5,"#Seperate categorical and numerical variables
cattrain=trainv1.select_dtypes('object')
numtrain=trainv1.select_dtypes('number')
cattest=testv1.select_dtypes('object')
numtest=testv1.select_dtypes('number')",0,No Code Smell
66,28985739,6,"#encoding categorical test variables
#instantiate both packages to use
encoder = OrdinalEncoder()
imputer = KNN()
# create a list of categorical columns to iterate over
cat_cols = cattest.columns

def encode(data):
    '''function to encode non-null data and replace it in the original data'''
    #retains only non-null values
    nonulls = np.array(data.dropna())
    #reshapes the data for encoding
    impute_reshape = nonulls.reshape(-1,1)
    #encode data
    impute_ordinal = encoder.fit_transform(impute_reshape)
    #Assign back encoded values to non-null values
    data.loc[data.notnull()] = np.squeeze(impute_ordinal)
    return data

#create a for loop to iterate through each column in the data
for columns in cat_cols:
    encode(cattest[columns])",0,No Code Smell
67,28985739,7,"#encoding categorical train variables
#instantiate both packages to use
encoder = OrdinalEncoder()
imputer = KNN()
# create a list of categorical columns to iterate over
cat_cols = cattrain.columns

def encode(data):
    '''function to encode non-null data and replace it in the original data'''
    #retains only non-null values
    nonulls = np.array(data.dropna())
    #reshapes the data for encoding
    impute_reshape = nonulls.reshape(-1,1)
    #encode data
    impute_ordinal = encoder.fit_transform(impute_reshape)
    #Assign back encoded values to non-null values
    data.loc[data.notnull()] = np.squeeze(impute_ordinal)
    return data

#create a for loop to iterate through each column in the data
for columns in cat_cols:
    encode(cattrain[columns])",0,No Code Smell
68,28985739,8,"#splitting train values into sections for faster imputing
numtrain1=numtrain[0:20000]
numtrain2=numtrain[20000:40000]
numtrain3=numtrain[40000:60000]
numtrain4=numtrain[60000:80000]
numtrain5=numtrain[80000:]

cattrain1=cattrain[0:20000]
cattrain2=cattrain[20000:40000]
cattrain3=cattrain[40000:60000]
cattrain4=cattrain[60000:80000]
cattrain5=cattrain[80000:]",0,No Code Smell
69,28985739,9,"#splitting test values into sections for faster imputing
cattest1=cattest[0:20000]
cattest2=cattest[20000:]

numtest1=numtest[0:20000]
numtest2=numtest[20000:]",0,No Code Smell
70,28985739,10,"# impute catgorical test data and convert                                                                                                                                                   
encode_testdata1 = pd.DataFrame(np.round(imputer.fit_transform(cattest1)),columns = cattest.columns)
encode_testdata2 = pd.DataFrame(np.round(imputer.fit_transform(cattest2)),columns = cattest.columns)
",0,No Code Smell
71,28985739,11,"# impute catgorical train data and convert                                                                                                                                                   
encode_data1 = pd.DataFrame(np.round(imputer.fit_transform(cattrain1)),columns = cattrain.columns)
encode_data2 = pd.DataFrame(np.round(imputer.fit_transform(cattrain2)),columns = cattrain.columns)
encode_data3 = pd.DataFrame(np.round(imputer.fit_transform(cattrain3)),columns = cattrain.columns)
encode_data4 = pd.DataFrame(np.round(imputer.fit_transform(cattrain4)),columns = cattrain.columns)
encode_data5 = pd.DataFrame(np.round(imputer.fit_transform(cattrain5)),columns = cattrain.columns)",0,No Code Smell
72,28985739,12,"cattrainfill=pd.concat([encode_data1,encode_data2,encode_data3,encode_data4,encode_data5])
cattestfill=pd.concat([encode_testdata1,encode_testdata2])",0,No Code Smell
73,28985739,13,"#impute numerical test data
encode_testdatanum = pd.DataFrame(np.round(imputer.fit_transform(numtest1)),columns = numtest.columns)
encode_testdatanum2 = pd.DataFrame(np.round(imputer.fit_transform(numtest2)),columns = numtest.columns)
",0,No Code Smell
74,28985739,14,"#impute numerical train data
encode_datanum1 = pd.DataFrame(np.round(imputer.fit_transform(numtrain1)),columns = numtrain.columns)
encode_datanum2 = pd.DataFrame(np.round(imputer.fit_transform(numtrain2)),columns = numtrain.columns)
encode_datanum3 = pd.DataFrame(np.round(imputer.fit_transform(numtrain3)),columns = numtrain.columns)
encode_datanum4 = pd.DataFrame(np.round(imputer.fit_transform(numtrain4)),columns = numtrain.columns)
encode_datanum5 = pd.DataFrame(np.round(imputer.fit_transform(numtrain5)),columns = numtrain.columns)",0,No Code Smell
75,28985739,15,"numtrainfill=pd.concat([encode_datanum1,encode_datanum2,encode_datanum3,encode_datanum4,encode_datanum5])
numtestfill=pd.concat([encode_testdatanum,encode_testdatanum2])",0,No Code Smell
76,28985739,16,"trainv6=pd.concat([numtrainfill,cattrainfill],axis=1,join='inner')
testv6=pd.concat([numtestfill,cattestfill],axis=1,join='inner')",0,No Code Smell
77,28985739,17,"y=trainv6['hospital_death']
trainv7=trainv6.drop(['hospital_death'], axis=1)",0,No Code Smell
78,28985739,18,"# Split into training and validation set
x_train, x_val, y_train, y_val = train_test_split(trainv7, y, test_size = 0.25, random_state = 1)",0,No Code Smell
79,28985739,19,"#Model building
d_train = lgb.Dataset(x_train, label=y_train)
params = {}
params['learning_rate'] = 0.003
params['boosting_type'] = 'gbdt'
params['objective'] = 'binary'
params['metric'] = 'binary_logloss'
params['sub_feature'] = 0.5
params['num_leaves'] = 100
params['min_data'] = 50
params['max_depth'] = 10
clf = lgb.train(params, d_train, 100)",0,No Code Smell
80,28985739,20,"#Prediction
y_pred=clf.predict(x_val)
y_pred1=np.round(y_pred)",0,No Code Smell
81,28985739,21,"#Measure accuracy
#Confusion matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_val, y_pred1)
print (cm)
#Accuracy
from sklearn.metrics import accuracy_score
accuracy=accuracy_score(y_pred1,y_val)
print (accuracy)",0,No Code Smell
82,28985739,22,"#Prediction on Test variables
pred_on_test=clf.predict(testv6)",0,No Code Smell
83,28985739,23,"solution.hospital_death = pred_on_test
solution.to_csv(""submissionlgbm.csv"", index=0)",0,No Code Smell
84,34088724,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the ""../input/"" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# Any results you write to the current directory are saved as output.
import lightgbm as lgb
from sklearn.metrics import roc_auc_score
from sklearn.metrics import auc
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from datetime import datetime, date, time, timedelta

def label_var(data,variables_cat):
    lb=[]
    for m in variables_cat:
        l=LabelEncoder()
        lb.append(l.fit(list(data[m].dropna())))
    
    return lb

def label_enc(data,l,categorical_features):
    i=0
    for m in categorical_features:
        data.loc[data[m].notnull(),m]=l[i].transform(data.loc[data[m].notnull(),m])
        i=i+1
        

df_tr = pd.read_csv(""/kaggle/input/widsdatathon2020/training_v2.csv"")
df_ts = pd.read_csv(""/kaggle/input/widsdatathon2020/unlabeled.csv"")",1,Code Smell
85,34088724,1,"feature_importance_dfs = {}

for key, value in feature_importance_dfs.items():
    print('Feature Importance for type ', encoders['type'].classes_[key], ' :')
    print(
    value.groupby(['Feature'])[['importance']].mean().sort_values(
        ""importance"", ascending=False).head(20)
         )",0,No Code Smell
86,34088724,2,feature_importance_dfs = {},0,No Code Smell
87,34088724,3,"train_columns = [x for x in df_tr.columns if x not in ['encounter_id','patient_id','hospital_death','readmission_status']]
categorical_features = []
for m in train_columns:

    if(df_tr[m].dtypes=='object'):
        categorical_features.append(m)
        ",0,No Code Smell
88,34088724,4,"df_tr_ts = pd.concat([df_tr[categorical_features],df_ts[categorical_features]])

l = label_var(df_tr_ts, categorical_features)
label_enc(df_tr,l,categorical_features)
label_enc(df_ts,l,categorical_features)

for df in [df_tr, df_ts]:
    for m in categorical_features:
        df[m] = df[m].astype(float)",0,No Code Smell
89,34088724,5,"categorical_index = [train_columns.index(x) for x in categorical_features]

target = df_tr['hospital_death']

param = {'task': 'train',
         'boosting': 'gbdt',
         'objective':'binary',
         'metric': 'auc',
         'num_leaves': 15,
         'min_data_in_leaf': 90,
         'learning_rate': 0.01,
         'max_depth': 5,
         'feature_fraction': 0.1,
         'bagging_freq': 1,
         'bagging_fraction': 0.75,
         'use_missing': True,
         'nthread': 4
        }",0,No Code Smell
90,34088724,6,"folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=256)
oof = np.zeros(len(df_tr))
r=[]
predictions = np.zeros(len(df_ts))
feature_importance_df = pd.DataFrame()
features = train_columns#[col for col in df_tr.columns if col != 'fc' and col != 'id' and col not in ['hospital_death']]
evals_result = None
for fold_, (trn_idx, val_idx) in enumerate(folds.split(df_tr,target.values)):
    strLog = ""fold {}"".format(fold_)
    print(strLog)
    evals_result = {}
    trn_data = lgb.Dataset(df_tr.iloc[trn_idx][train_columns], label=target.iloc[trn_idx])
    val_data = lgb.Dataset(df_tr.iloc[val_idx][train_columns], label=target.iloc[val_idx],reference=trn_data)

    num_round = 7000
    clf = lgb.train(param,trn_data,num_round,valid_sets=val_data,early_stopping_rounds=100,verbose_eval=200,categorical_feature=categorical_index,evals_result=evals_result)
    oof[val_idx] = clf.predict(df_tr.iloc[val_idx][train_columns], num_iteration=clf.best_iteration)
    
    fold_importance_df = pd.DataFrame()
    fold_importance_df[""Feature""] = features
    fold_importance_df[""importance""] = clf.feature_importance()
    fold_importance_df[""fold""] = fold_ + 1
    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)
        
    a=roc_auc_score(target.loc[val_idx],clf.predict(df_tr.loc[val_idx,train_columns].values, num_iteration=clf.best_iteration))
    r.append(a)
    
    
    #predictions
    predictions += clf.predict(df_ts[train_columns], num_iteration=clf.best_iteration) / folds.n_splits
    
strAUC = roc_auc_score(target, oof)
print(strAUC)
print (""mean: ""+str(np.mean(np.array(r))))
print (""std: ""+str(np.std(np.array(r))))

df_sub = pd.DataFrame({'encounter_id': df_ts['encounter_id']})
df_sub['hospital_death'] = predictions

df_sub.to_csv(""sub1.csv"",index=False)




",0,No Code Smell
91,34088724,7,len(evals_result['valid_0']['auc']),0,No Code Smell
92,34088724,8,"df = pd.DataFrame({ 'iteration': [i for i in range(len(evals_result['valid_0']['auc']))],'auc': evals_result['valid_0']['auc']})

fig = px.line(df, x=""iteration"", y=""auc"", title='LGBM Validation AUC')
fig.show()",0,No Code Smell
93,34088724,9,"feature_importance_df.head(5).sort_values(by=['importance'], ascending=False)",0,No Code Smell
94,34088724,10,"feature_importance_df.groupby(['Feature'])[['importance']].mean().sort_values(
        ""importance"", ascending=False).head(10)",0,No Code Smell
95,34088724,11,"import plotly.express as px
df = feature_importance_df.groupby(['Feature'])[['importance']].mean().sort_values(
        ""importance"", ascending=True).tail(30).reset_index()
fig = px.bar(df, x=""importance"", y=""Feature"", orientation='h')
fig.show()",0,No Code Smell
96,34088724,12,clf.,0,No Code Smell
97,34088724,13,"# for key, value in feature_importance_dfs.items():
#     print('Feature Importance for type ', encoders['type'].classes_[key], ' :')
#     print(
#     value.groupby(['Feature'])[['importance']].mean().sort_values(
#         ""importance"", ascending=False).head(20)
#          )
",0,No Code Smell
98,34088724,14,"grouped = df_tr.groupby(['apache_3j_diagnosis'])[
'd1_heartrate_min',
'apache_4a_hospital_death_prob',
'age',
'd1_temp_max',
'd1_platelets_min','bmi','d1_spo2_min',
'd1_heartrate_max',
'd1_creatinine_max',
'd1_wbc_min',
'urineoutput_apache',
'h1_heartrate_max',
'heart_rate_apache',
'd1_sodium_max',
'glucose_apache',
'd1_glucose_min',
'd1_lactate_min',
'weight',
'd1_bun_min',
'd1_bun_max',
'd1_arterial_ph_max',
'd1_glucose_max',
'd1_wbc_max',
'wbc_apache',
'd1_hemaglobin_max',
'd1_sysbp_noninvasive_max',
'd1_resprate_min',
'd1_creatinine_min',
'd1_calcium_min',
'd1_resprate_max',
'h1_heartrate_min',
'd1_pao2fio2ratio_min',
'd1_pao2fio2ratio_max',
'd1_hco3_max',
'd1_platelets_min',
'd1_lactate_max',
'hematocrit_apache',
'h1_temp_max',
'd1_temp_min',
'd1_arterial_pco2_max',
'bun_apache',
'd1_hematocrit_min',
'creatinine_apache','pre_icu_los_days'].mean()
",0,No Code Smell
99,34088724,15,"df_tr = pd.merge(df_tr, grouped, how='left', on=['apache_3j_diagnosis'])

df_ts = pd.merge(df_ts, grouped, how='left', on=['apache_3j_diagnosis'])
grouped",0,No Code Smell
100,34088724,16,"l = label_var(df_tr_ts, categorical_features)
label_enc(df_tr,l,categorical_features)
label_enc(df_ts,l,categorical_features)

for df in [df_tr, df_ts]:
    for m in categorical_features:
        df[m] = df[m].astype(float)
        
categorical_index = [train_columns.index(x) for x in categorical_features]

target = df_tr['hospital_death']

param = {'task': 'train',
         'boosting': 'gbdt',
         'objective':'binary',
         'metric': 'auc',
         'num_leaves': 15,
         'min_data_in_leaf': 90,
         'learning_rate': 0.01,
         'max_depth': 5,
         'feature_fraction': 0.1,
         'bagging_freq': 1,
         'bagging_fraction': 0.75,
         'use_missing': True,
         'nthread': 4
        }

folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=256)
oof = np.zeros(len(df_tr))
r=[]
predictions = np.zeros(len(df_ts))
feature_importance_df = pd.DataFrame()
features = train_columns#[col for col in df_tr.columns if col != 'fc' and col != 'id' and col not in ['hospital_death']]

for fold_, (trn_idx, val_idx) in enumerate(folds.split(df_tr,target.values)):
    strLog = ""fold {}"".format(fold_)
    print(strLog)
    
    trn_data = lgb.Dataset(df_tr.iloc[trn_idx][train_columns], label=target.iloc[trn_idx])
    val_data = lgb.Dataset(df_tr.iloc[val_idx][train_columns], label=target.iloc[val_idx],reference=trn_data)

    num_round = 7000
    clf = lgb.train(param,trn_data,num_round,valid_sets=val_data,early_stopping_rounds=100,verbose_eval=200,categorical_feature=categorical_index)
    oof[val_idx] = clf.predict(df_tr.iloc[val_idx][train_columns], num_iteration=clf.best_iteration)
    
    fold_importance_df = pd.DataFrame()
    fold_importance_df[""Feature""] = features
    fold_importance_df[""importance""] = clf.feature_importance()
    fold_importance_df[""fold""] = fold_ + 1
    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)
        
    a=roc_auc_score(target.loc[val_idx],clf.predict(df_tr.loc[val_idx,train_columns].values, num_iteration=clf.best_iteration))
    r.append(a)
    
    
    #predictions
    predictions += clf.predict(df_ts[train_columns], num_iteration=clf.best_iteration) / folds.n_splits
    
strAUC = roc_auc_score(target, oof)
print(strAUC)
print (""mean: ""+str(np.mean(np.array(r))))
print (""std: ""+str(np.std(np.array(r))))

df_sub = pd.DataFrame({'encounter_id': df_ts['encounter_id']})
df_sub['hospital_death'] = predictions

df_sub.to_csv(""sub1.csv"",index=False)




",0,No Code Smell
101,34088724,17,"np.mean(r), np.std(r)",1,Code Smell
102,34088724,18,SHAP Feature Importance,0,No Code Smell
103,34088724,19,"def plot_importances(importances_):
    mean_gain = importances_[['gain', 'feature']].groupby('feature').mean()
    importances_['mean_gain'] = importances_['feature'].map(mean_gain['gain'])
    plt.figure(figsize=(18, 44))
    data_imp = importances_.sort_values('mean_gain', ascending=False)
    sns.barplot(x='gain', y='feature', data=data_imp[:300])
    plt.tight_layout()
    plt.savefig('importances.png')
    plt.show()",0,No Code Smell
104,34088724,20,,0,No Code Smell
105,29070286,0,"import numpy as np
import pandas as pd

import random
random.seed(28)
np.random.seed(28)

import matplotlib.pyplot as plt
from matplotlib_venn import venn2

from sklearn.metrics import (confusion_matrix, precision_recall_curve, auc,
                             roc_curve, recall_score, classification_report, f1_score,
                             precision_recall_fscore_support)
import os
import copy

pd.options.display.precision = 15

from collections import defaultdict
import lightgbm as lgb
import xgboost as xgb
import time
from collections import Counter
import datetime
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, GroupKFold, GridSearchCV, train_test_split, TimeSeriesSplit, RepeatedStratifiedKFold
from sklearn import metrics
from sklearn.metrics import mean_absolute_error
import gc
import seaborn as sns
import warnings
warnings.filterwarnings(""ignore"")
from bayes_opt import BayesianOptimization
#import eli5
import shap
from IPython.display import HTML
import json

import matplotlib.pyplot as plt
%matplotlib inline
import os

pd.set_option('max_rows', 500)
import re

pd.set_option('display.max_columns', 1000)
pd.set_option('display.max_rows', 500)
pd.set_option('display.width', 1000)
pd.set_option('display.float_format', '{:20,.2f}'.format)
pd.set_option('display.max_colwidth', -1)

np.random.seed(2206)",0,No Code Smell
106,29070286,1,"train = pd.read_csv(""../input/widsdatathon2020/training_v2.csv"")
samplesubmission = pd.read_csv(""../input/widsdatathon2020/samplesubmission.csv"")
test = pd.read_csv(""../input/widsdatathon2020/unlabeled.csv"")
dictionary = pd.read_csv(""../input/widsdatathon2020/WiDS Datathon 2020 Dictionary.csv"")
solution_template = pd.read_csv(""../input/widsdatathon2020/solution_template.csv"")

print('train ' , train.shape)
print('test ' , test.shape)
print('samplesubmission ' , samplesubmission.shape)
print('solution_template ' , solution_template.shape)
print('dictionary ' , dictionary.shape)",1,Code Smell
107,29070286,2,"dico = pd.DataFrame(dictionary.T.head(6))
dico.columns=list(dico.loc[dico.index == 'Variable Name'].unstack())
dico = dico.loc[dico.index != 'Variable Name']
dico.columns
train_stat = pd.DataFrame(train.describe())
train_stat2 = pd.concat([dico,train_stat],axis=0)
train_stat2.head(20)",0,No Code Smell
108,29070286,3,train_stat2.T.head(200),0,No Code Smell
109,29070286,4,"# Missing Values
train.isna().sum()",0,No Code Smell
110,29070286,5,"# function to evaluate the score of our model
def eval_auc(pred,real):
    false_positive_rate, recall, thresholds = roc_curve(real, pred)
    roc_auc = auc(false_positive_rate, recall)
    return roc_auc    ",0,No Code Smell
111,29070286,6,"# a wrapper class  that we can have the same ouput whatever the model we choose
class Base_Model(object):
    
    def __init__(self, train_df, test_df, features, categoricals=[], n_splits=5, verbose=True,ps={}):
        self.train_df = train_df
        self.test_df = test_df
        self.features = features
        self.n_splits = n_splits
        self.categoricals = categoricals
        self.target = 'hospital_death'
        self.cv = self.get_cv()
        self.verbose = verbose
#         self.params = self.get_params()
        self.params = self.set_params(ps)
        self.y_pred, self.score, self.model , self.oof_pred = self.fit()
        
    def train_model(self, train_set, val_set):
        raise NotImplementedError
        
    def get_cv(self):
        cv = StratifiedKFold(n_splits=self.n_splits, shuffle=True, random_state=42)
        return cv.split(self.train_df, self.train_df[self.target])
    
    def get_params(self):
        raise NotImplementedError
        
    def convert_dataset(self, x_train, y_train, x_val, y_val):
        raise NotImplementedError
        
    def convert_x(self, x):
        return x
        
    def fit(self):
        oof_pred = np.zeros((len(self.train_df), ))
        y_pred = np.zeros((len(self.test_df), ))
        for fold, (train_idx, val_idx) in enumerate(self.cv):
            x_train, x_val = self.train_df[self.features].iloc[train_idx], self.train_df[self.features].iloc[val_idx]
            y_train, y_val = self.train_df[self.target][train_idx], self.train_df[self.target][val_idx]
            train_set, val_set = self.convert_dataset(x_train, y_train, x_val, y_val)
            model = self.train_model(train_set, val_set)
            conv_x_val = self.convert_x(x_val)
            oof_pred[val_idx] = model.predict(conv_x_val).reshape(oof_pred[val_idx].shape)
            x_test = self.convert_x(self.test_df[self.features])
            y_pred += model.predict(x_test).reshape(y_pred.shape) / self.n_splits

            print('Partial score of fold {} is: {}'.format(fold,eval_auc(oof_pred[val_idx],y_val) ))
        #print(oof_pred, self.train_df[self.target].values)
        loss_score = eval_auc(oof_pred,self.train_df[self.target].values) 
        if self.verbose:
            print('Our oof AUC score is: ', loss_score)
        return y_pred, loss_score, model , oof_pred",0,No Code Smell
112,29070286,7,"#we choose to try a LightGbM using the Base_Model class
class Lgb_Model(Base_Model):
    
    def train_model(self, train_set, val_set):
        verbosity = 100 if self.verbose else 0
        return lgb.train(self.params, train_set, valid_sets=[train_set, val_set], verbose_eval=verbosity)
        
    def convert_dataset(self, x_train, y_train, x_val, y_val):
        train_set = lgb.Dataset(x_train, y_train, categorical_feature=self.categoricals)
        val_set   = lgb.Dataset(x_val,    y_val,  categorical_feature=self.categoricals)
        return train_set, val_set
        
    def get_params(self):
        params = {'n_estimators':5000,
                  'boosting_type': 'gbdt',
                  'objective': 'binary',
                  'metric': 'auc',
                  'subsample': 0.75,
                  'subsample_freq': 1,
                  'learning_rate': 0.1,
                  'feature_fraction': 0.9,
                  'max_depth': 15,
                  'lambda_l1': 1,  
                  'lambda_l2': 1,
                  'early_stopping_rounds': 100,
                  #'is_unbalance' : True ,
                  'scale_pos_weight' : 3,
                  'device': 'gpu',
                  'gpu_platform_id': 0,
                  'gpu_device_id': 0,
                  'num_leaves': 31
                    }
        return params
    def set_params(self,ps={}):
        params = self.get_params()
        if 'subsample_freq' in ps:
            params['subsample_freq']=int(ps['subsample_freq'])
            params['learning_rate']=ps['learning_rate']
            params['feature_fraction']=ps['feature_fraction']
            params['lambda_l1']=ps['lambda_l1']
            params['lambda_l2']=ps['lambda_l2']
            params['scale_pos_weight']=ps['scale_pos_weight']
            params['max_depth']=int(ps['max_depth'])
            params['subsample']=ps['subsample']
            params['num_leaves']=int(ps['num_leaves'])
            params['min_split_gain']=ps['min_split_gain']
#             params['min_child_weight']=ps['min_child_weight']
        
        return params  ",0,No Code Smell
113,29070286,8,"def plot_importances(importances_, plot_name):
    mean_gain = importances_[['gain', 'feature']].groupby('feature').mean()
    importances_['mean_gain'] = importances_['feature'].map(mean_gain['gain'])
    plt.figure(figsize=(18, 44))
    data_imp = importances_.sort_values('mean_gain', ascending=False)
    sns.barplot(x='gain', y='feature', data=data_imp[:300])
    plt.tight_layout()
    plt.savefig('{}.png'.format(plot_name))
    plt.show()",0,No Code Smell
114,29070286,9,"# Replace values

print('Replacing: {}'.format('hospital_admit_source'))

replace_hospital_admit_source =  {'Other ICU': 'ICU',
                                  'ICU to SDU':'SDU', 
                                  'Step-Down Unit (SDU)': 'SDU', 
                                  'Other Hospital':'Other',
                                  'Observation': 'Recovery Room',
                                  'Acute Care/Floor': 'Acute Care'}
train['hospital_admit_source'].replace(replace_hospital_admit_source, inplace=True)
test['hospital_admit_source'].replace(replace_hospital_admit_source, inplace=True)

#combined_dataset['icu_type'] = combined_dataset['icu_type'].replace({'CCU-CTICU': 'Grpd_CICU', 'CTICU':'Grpd_CICU', 'Cardiac ICU':'Grpd_CICU'})

print('Replacing: {}'.format('apache_2_bodysystem'))

replace_apache_2_bodysystem =  {'Undefined diagnoses': 'Undefined Diagnoses'}
train['apache_2_bodysystem'].replace(replace_apache_2_bodysystem, inplace=True)
test['apache_2_bodysystem'].replace(replace_apache_2_bodysystem, inplace=True)",0,No Code Smell
115,29070286,10,"#we are going to drop these columns because we dont want our ML model to be bias toward these consideration
#(we also remove the target and the ids.)
to_drop = ['gender','ethnicity' ,'encounter_id', 'patient_id',  'hospital_death']

# this is a list of features that look like to be categorical
categoricals_features = ['hospital_id','ethnicity','gender','hospital_admit_source','icu_admit_source',
                         'icu_stay_type','icu_type','apache_3j_bodysystem','apache_2_bodysystem']
categoricals_features = [col for col in categoricals_features if col not in to_drop]

# this is the list of all input feature we would like our model to use 
features = [col for col in train.columns if col not in to_drop ]
print('numerber of features ' , len(features))
print('shape of train / test ', train.shape , test.shape)",0,No Code Smell
116,29070286,11,"# categorical feature need to be transform to numeric for mathematical purpose.
# different technics of categorical encoding exists here we will rely on our model API to deal with categorical
# still we need to encode each categorical value to an id , for this purpose we use LabelEncoder

print('Transform all String features to category.\n')
for usecol in categoricals_features:
    train[usecol] = train[usecol].astype('str')
    test[usecol] = test[usecol].astype('str')
    
    #Fit LabelEncoder
    le = LabelEncoder().fit(
            np.unique(train[usecol].unique().tolist()+
                      test[usecol].unique().tolist()))

    #At the end 0 will be used for dropped values
    train[usecol] = le.transform(train[usecol])+1
    test[usecol]  = le.transform(test[usecol])+1
    
    train[usecol] = train[usecol].replace(np.nan, 0).astype('int').astype('category')
    test[usecol]  = test[usecol].replace(np.nan, 0).astype('int').astype('category')",0,No Code Smell
117,29070286,12,"def adversarial_validation(train, test, features):
    tr_data   = train.copy()
    tst_data = test.copy()
    tr_data['target']  = 0 
    tst_data['target'] = 1
    av_data = pd.concat([tr_data, tst_data], axis = 0)
    av_data.reset_index(drop = True)        
    params = {
            'learning_rate': 0.1, 
            'seed': 50,
            'objective':'binary',
            'boosting_type':'gbdt',
            'metric': 'auc',
        }    
    # define a KFold strategy
    kf = StratifiedKFold(n_splits = 5, shuffle=True, random_state=42)
    target = 'target'
    oof_pred = np.zeros(len(av_data))
    important_features = pd.DataFrame()
    fold_auc = []    
    
    for fold, (tr_ind, val_ind) in enumerate(kf.split(av_data, av_data[target])) :
        print('Fold {}'.format(fold + 1))
        x_train, x_val = av_data[features].iloc[tr_ind], av_data[features].iloc[val_ind]
        y_train, y_val = av_data[target].iloc[tr_ind], av_data[target].iloc[val_ind]
        train_set = lgb.Dataset(x_train, y_train)
        val_set   = lgb.Dataset(x_val, y_val)
        
        model = lgb.train(params, train_set, num_boost_round = 1000, early_stopping_rounds = 20, valid_sets = [train_set, val_set], verbose_eval = 100)
        
        fold_importance = pd.DataFrame()
        fold_importance['feature'] = features
        fold_importance['gain'] = model.feature_importance()
        important_features = pd.concat([important_features, fold_importance], axis = 0)
        
        oof_pred[val_ind] = model.predict(x_val)
        fold_auc.append(metrics.roc_auc_score(y_train, model.predict(x_train)))
        
    print('Our mean train roc auc score is :', np.mean(fold_auc))
    print('Our oof roc auc score is :', metrics.roc_auc_score(av_data[target], oof_pred))
    return important_features",0,No Code Smell
118,29070286,13,"# run the adversatial model with all the feature we used :
    
adversarial_features = adversarial_validation(train, test, features)",0,No Code Smell
119,29070286,14,"# AUC is almost perfect so we can expect that some feature are perfectly different between train / test

adversarial_features = adversarial_features[['gain', 'feature']].groupby('feature').mean().reset_index()
adversarial_features= adversarial_features.sort_values('gain', ascending=False)

plot_importances(adversarial_features, 'importances-lgb-v6')",0,No Code Smell
120,29070286,15,"# So icu_id columns seems to be the feature that dominate the feature importance for the adversarial 
# validation model, so it is likely to be totally different between train and test, 
# lets check the distribution of the top features :

def plot_differente_between_train_test(adversarial_features):
    import warnings
    warnings.filterwarnings(""ignore"")
    warnings.simplefilter(action='ignore', category=UserWarning)
    i=0
    for index, row in adversarial_features.sort_values(by=['gain'],ascending=False).iterrows():  
        column=row['feature']
        if i< 10:
                print(column,i,""gain :"",row['gain'])
                df1      = train.copy()
                df2      = test.copy()

                fig = plt.figure(figsize=(20,4))
                sns.distplot(df1[column].dropna(),  color='yellow', label='train', kde=True); 
                sns.distplot(df2[column].dropna(),  color='violet', label='test', kde=True); 
                fig=plt.legend(loc='best')
                plt.xlabel(column, fontsize=12);
                plt.show()
                i=i+1

plot_differente_between_train_test(adversarial_features)",0,No Code Smell
121,29070286,16,"# it is .... Let's remove icu_id and see the results ..
adversarial_features2 = adversarial_validation(train, test, [ f for f in features if f not in ['icu_id'] ])",0,No Code Smell
122,29070286,17,"# Let`s check again the difference between train / test

adversarial_features2 = adversarial_features2[['gain', 'feature']].groupby('feature').mean().reset_index()
adversarial_features2= adversarial_features2.sort_values('gain', ascending=False)

plot_importances(adversarial_features2, 'importances-lgb-v6')",0,No Code Smell
123,29070286,18,plot_differente_between_train_test(adversarial_features2),0,No Code Smell
124,29070286,19,"# hospital_id seems to be also from a different distribution. 
# We can check it directly, obviously only few hospital are common to both dataset ..

common_id  = list([id for id in train['hospital_id'].unique() if id in test['hospital_id'].unique() ])
id_only_in_train  = [id for id in train['hospital_id'].unique() if id not in test['hospital_id'].unique() ]
id_only_in_test   = [id for id in test['hospital_id'].unique()  if id not in train['hospital_id'].unique() ]
count_common_train = train.loc[train['hospital_id'].isin(common_id)].shape[0]
count_common_test  = test.loc[test['hospital_id'].isin(common_id)].shape[0]

count_train = train.loc[train['hospital_id'].isin(id_only_in_train)].shape[0]
count_test  = test.loc[test['hospital_id'].isin(id_only_in_test)].shape[0]

 
fig = plt.figure(figsize=(20,6))
venn2(subsets = (count_train,  count_test, count_common_train+count_common_test), set_labels = ('Hospital only in train', 'Hospital only in test'),set_colors=('purple', 'yellow'), alpha = 0.7);
plt.show()",0,No Code Smell
125,29070286,20,"# Let's do an ultimate try without 'icu_id','hospitaadversarial_features3 = adversarial_validation(train, test, [ f for f in features if f not in ['icu_id','hospital_id'] ])l_id'
adversarial_features3 = adversarial_validation(train, test, [ f for f in features if f not in ['icu_id','hospital_id'] ])",0,No Code Smell
126,29070286,21,"# I leave it to you to see what you can do with other features..
adversarial_features3 = adversarial_features3[['gain', 'feature']].groupby('feature').mean().reset_index()
adversarial_features3= adversarial_features3.sort_values('gain', ascending=False)

plot_importances(adversarial_features3, 'importances-lgb-v6')",0,No Code Smell
127,29070286,22,plot_differente_between_train_test(adversarial_features3),0,No Code Smell
128,29070286,23,"# Lets remove hospital_id and icu_id

print('Difference between train and teste> -- hospital_id: ')
print(len(list(set(train['hospital_id']) - set(test['hospital_id']))))

print('\nDifference between train and teste> -- icu_id: ')
print(len(list(set(train['icu_id']) - set(test['icu_id']))))


# Drop features with zero importance
print('\nLength train features: {}'.format(len(features)))
for feat_to_remove in ['icu_id', 'hospital_id']:
    if feat_to_remove in categoricals_features:
        print('Removing from categoricals_features....{}'.format(feat_to_remove))
        categoricals_features.remove(feat_to_remove)
    if feat_to_remove in features:
        print('Removing from features....{}'.format(feat_to_remove))
        features.remove(feat_to_remove)
    
print('\nNew length train features: {}'.format(len(features)))",0,No Code Smell
129,29070286,24,"# percentage of death , hopefully it s a bit unbalanced
train['hospital_death'].sum()/train['hospital_death'].count()",0,No Code Smell
130,29070286,25,"# You want Bayesian Optimization?

boll_BayesianOptimization = False
# boll_BayesianOptimization = True",0,No Code Smell
131,29070286,26,"%time

def LGB_Beyes(subsample_freq,
                    learning_rate,
                    feature_fraction,
                    max_depth,
                    lambda_l1,
                    lambda_l2,
                    scale_pos_weight,
                    subsample,
                    num_leaves,
                    min_split_gain):
#                     min_child_weight):
    params={}
    params['subsample_freq']=subsample_freq
    params['learning_rate']=learning_rate
    params['feature_fraction']=feature_fraction
    params['lambda_l1']=lambda_l1
    params['lambda_l2']=lambda_l2
    params['max_depth']=max_depth
    params['scale_pos_weight']=scale_pos_weight
    params['subsample']=subsample
    params['num_leaves']=num_leaves
    params['min_split_gain']=min_split_gain
   # params['min_child_weight']=min_child_weight
    
    
    lgb_model= Lgb_Model(train, test, features, categoricals=categoricals_features,ps=params)
    print('auc: ',lgb_model.score)
    return lgb_model.score

bounds_LGB = {
    'max_depth': (5, 17),
    'subsample': (0.5, 1),
    'num_leaves': (10, 45),
    'feature_fraction': (0.1, 1),
    'min_split_gain': (0.0, 0.1),
#     'min_child_weight': (1e-3, 50),
    'subsample_freq': (1, 10),
    'learning_rate': (0.005, 0.02),
    'lambda_l1': (0, 5),
    'lambda_l2': (0, 5),
    'scale_pos_weight': (1, 10)
}

# ACTIVATE it if you want to search for better parameter
if boll_BayesianOptimization: 
    LGB_BO = BayesianOptimization(LGB_Beyes, bounds_LGB, random_state=1029)
    import warnings
    init_points = 16
    n_iter = 16
    with warnings.catch_warnings():
        warnings.filterwarnings('ignore')    
        LGB_BO.maximize(init_points=init_points, n_iter=n_iter, acq='ucb', xi=0.0, alpha=1e-6)",0,No Code Smell
132,29070286,27,"if boll_BayesianOptimization and LGB_BO:
    print(LGB_BO.max['params'])",0,No Code Smell
133,29070286,28,"# params = {'feature_fraction': 0.9,
#  'lambda_l1': 1,
#  'lambda_l2': 1,
#  'learning_rate': 0.1,
#  'max_depth': 13,
#  'subsample_freq': 1,
#  'scale_pos_weight':1}

# Best Hyperparams from Bayesian Optimization in notebook lgb-v2
# params = {'feature_fraction': 0.524207414205945,
#  'lambda_l1': 4.171808735757517,
#  'lambda_l2': 4.6435328298317256,
#  'learning_rate': 0.007897539397989824,
#  'max_depth': 16.62053004755999,
#  'scale_pos_weight': 1.2199266532301127,
#  'subsample_freq': 1.0276518730971627}


# # Best Hyperparams from Bayesian Optimization in notebook lgb-v3
# params = {'feature_fraction': 0.524207414205945,
#  'lambda_l1': 4.171808735757517,
#  'lambda_l2': 4.6435328298317256,
#  'learning_rate': 0.007897539397989824,
#  'max_depth': 16.62053004755999,
#  'scale_pos_weight': 1.2199266532301127,
#  'subsample_freq': 1.0276518730971627}

# # Best Hyperparams from Bayesian Optimization in notebook lgb-v4
# params = {'feature_fraction': 0.5348508368206359,
#  'lambda_l1': 0.0009370993396629057,
#  'lambda_l2': 4.743745312344983,
#  'learning_rate': 0.012891827059322746,
#  'max_depth': 15.784155449197529,
#  'scale_pos_weight': 1.0325760631926175,
#  'subsample_freq': 1.0744384574974872}


# Best Hyperparams from Bayesian Optimization in notebook lgb-v5
# params = {'feature_fraction': 0.3245039721724266,
#  'lambda_l1': 1.416727346446085,
#  'lambda_l2': 2.779776916582821,
#  'learning_rate': 0.006854369969433722,
#  'max_depth': 16.673905691676964,
#  'min_split_gain': 0.05643417986130283,
#  'num_leaves': 44.8672896759208,
#  'scale_pos_weight': 1.1577974342088542,
#  'subsample': 0.630352165410007,
#  'subsample_freq': 1.2158674819047501}

# Best Hyperparams from Bayesian Optimization in notebook lgb-v6 -- BEST MODEL
params = {
   ""feature_fraction"":0.1743912077888097,
   ""lambda_l1"":2.838660318794291,
   ""lambda_l2"":0.292397357257721,
   ""learning_rate"":0.012602188092427687,
   ""max_depth"":16.575351761228106,
   ""min_split_gain"":0.04631934372471113,
   ""num_leaves"":44.81666226482246,
   ""scale_pos_weight"":1.0897617979884857,
   ""subsample"":0.8260779721854892,
   ""subsample_freq"":1.2473380372944387
}",0,No Code Smell
134,29070286,29,"%time

if boll_BayesianOptimization: # ACTIVATE it if you want to search/use for better parameter
    lgb_model = Lgb_Model(train,test, features, categoricals=categoricals_features, ps= LGB_BO.max['params'])
else :
    lgb_model = Lgb_Model(train,test, features, categoricals=categoricals_features, ps=params)",0,No Code Smell
135,29070286,30,"imp_df = pd.DataFrame()
imp_df['feature'] = features
imp_df['gain']  = lgb_model.model.feature_importance(importance_type='gain')
imp_df['split'] = lgb_model.model.feature_importance(importance_type='split')",0,No Code Smell
136,29070286,31,"plot_importances(imp_df, 'importances-lgb-v6-lgb_model')",0,No Code Smell
137,29070286,32,"import shap
explainer   =  shap.TreeExplainer(lgb_model.model)
shap_values = explainer.shap_values(train[features].iloc[:1000,:])
shap.summary_plot(shap_values, train[features].iloc[:1000,:])",0,No Code Smell
138,29070286,33,"print('AUC Version 1: ', lgb_model.score)
#print('AUC:Version 2: ', lgb_model_v2.score)",1,Code Smell
139,29070286,34,"test[""hospital_death""] = lgb_model.y_pred
#test[[""encounter_id"",""hospital_death""]].to_csv(""submission6-lgb-v6.csv"",index=False)

test[[""encounter_id"",""hospital_death""]].head()",0,No Code Smell
140,27795188,0,"import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))
#Linear Algebra
import numpy as np
#Data preprocessing
import pandas as pd

#setting display options
pd.set_option('display.max_rows', 5000)
pd.set_option('display.max_columns', 500)
pd.set_option('max_colwidth', 500)
np.set_printoptions(linewidth =400)

from matplotlib import pyplot as plt
%matplotlib inline
#Advance-style plotting
import seaborn as sns
color =sns.color_palette()
sns.set_style('darkgrid')

#Ignore annoying warning from sklearn and seaborn
import warnings
def ignore_warn(*args, **kwargs):
    pass
warnings.warn = ignore_warn

#other libraiaries
import os
import copy
from collections import defaultdict
from collections import Counter
from sklearn import metrics
import matplotlib.pyplot as plt
%matplotlib inline
import os
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import metrics
import re
import plotly.graph_objs as go
from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot
init_notebook_mode(connected=True)",0,No Code Smell
141,27795188,1,description = pd.read_csv('/kaggle/input/widsdatathon2020/WiDS Datathon 2020 Dictionary.csv'); description,1,Code Smell
142,27795188,2,"#read the data
train = pd.read_csv('/kaggle/input/widsdatathon2020/training_v2.csv')
test  = pd.read_csv('/kaggle/input/widsdatathon2020/unlabeled.csv')",1,Code Smell
143,27795188,3,train.columns,0,No Code Smell
144,27795188,4,"print(train.shape , test.shape)",0,No Code Smell
145,27795188,5,"#column 1 Unique identifier associated with a patient unit stay
print (train['encounter_id'].nunique() , test['encounter_id'].nunique())",0,No Code Smell
146,27795188,6,"#column 2 Unique identifier associated with a hospital
print (train['hospital_id'].nunique() , test['hospital_id'].nunique())",0,No Code Smell
147,27795188,7,"#column 3 Unique identifier associated with a patient
print (train['patient_id'].nunique() , test['patient_id'].nunique())",0,No Code Smell
148,27795188,8,"#column 4
Yes = len(train[train.hospital_death ==1])
No = len(train[train.hospital_death ==0])
Total = len(train)
print ('There are imbalanace datset with a %i/%i ratio'%((No/Total*100), (Yes/Total*100)+1))",0,No Code Smell
149,27795188,9,"sns.catplot(x ='hospital_death', kind ='count',palette='pastel', data = train);",0,No Code Smell
150,27795188,10,"#columnn 5
train['age'].describe()",1,Code Smell
151,27795188,11,#Hint: ensure all units of each columns are having relationship with respect to each other..,0,No Code Smell
152,28727928,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import StratifiedKFold, KFold
import matplotlib.pyplot as plt 
from sklearn.metrics import roc_auc_score
pd.set_option('display.max_columns', 500)
pd.set_option('display.max_rows', 500)

import lightgbm

# Input data files are available in the ""../input/"" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))
        


# Any results you write to the current directory are saved as output.",1,Code Smell
153,28727928,1,"drop_cols = ['encounter_id','patient_id','icu_id', 'hospital_id', 'readmission_status']
drop_cols_test = ['encounter_id','patient_id','icu_id','hospital_death','hospital_id', 'readmission_status']

train = pd.read_csv(""/kaggle/input/widsdatathon2020/training_v2.csv"").drop(drop_cols, axis=1)
# sample_submission = pd.read_csv(""/kaggle/input/widsdatathon2020/samplesubmission.csv"")
test = pd.read_csv(""/kaggle/input/widsdatathon2020/unlabeled.csv"").drop(drop_cols_test, axis=1)
data_dictionary = pd.read_csv(""/kaggle/input/widsdatathon2020/WiDS Datathon 2020 Dictionary.csv"")
solution_template = pd.read_csv(""/kaggle/input/widsdatathon2020/solution_template.csv"")

target = 'hospital_death'",1,Code Smell
154,28727928,2,"print(f'Rows in train data : {train.shape[0]} and columns in train data: {train.shape[1]}')
print(f'Rows in test data  : {test.shape[0]} and columns in train data: {test.shape[1]}')",0,No Code Smell
155,28727928,3,"np.round(train[target].value_counts()*100/len(train[target]),2)",0,No Code Smell
156,28727928,4,"ax = sns.countplot(train[target])
for p in ax.patches:
    ax.annotate('{}'.format(p.get_height()), (p.get_x()+0.15, p.get_height()+1))
plt.show()",0,No Code Smell
157,28727928,5,train.info(),1,Code Smell
158,28727928,6,train.head(),0,No Code Smell
159,28727928,7,"np.round(train.isna().mean()*100,2)",0,No Code Smell
160,28727928,8,"np.round(test.isna().mean()*100,2)",0,No Code Smell
161,28727928,9,train.dtypes,0,No Code Smell
162,28727928,10,train.select_dtypes(include='O').columns.values.tolist(),0,No Code Smell
163,28727928,11,test.select_dtypes(include='O').columns.values.tolist(),0,No Code Smell
164,28727928,12,"def explore_variable(col_name):
    """"""
    Helper function for categorical variable
    """"""
    print(f""Unique values in train: {train[col_name].unique()}"")
    print(f""Unique values in test:  {test[col_name].unique()}"")
    print(f""Number of unique values in train : {train[col_name].nunique()}"") 
    print(f""Number of unique values in test: {test[col_name].nunique()}"")

def count_plot(col_name, fig_size=(10,10)):
    """"""
    Helper function for count plot. 
    Here in count plot I have ordered by train[col].value_counts so it is easy compare distribution between train and test
    """"""
    fig = plt.figure(figsize=fig_size)
    fig.add_subplot(2,1,1)            
    ax1 = sns.countplot(x=col_name, data=train, order = train[col_name].value_counts().index)
    for p in ax1.patches:
        ax1.annotate('{}'.format(p.get_height()), (p.get_x()+0.15, p.get_height()+1))
    ax1.set_title(""Train distribution"", fontsize='large')
    ax1.set_ylabel(col_name)
    fig.add_subplot(2,1,2)            
    ax2 = sns.countplot(x=col_name, data=test, order = train[col_name].value_counts().index)
    ax2.set_title(""Test distribution"", fontsize='large')
    ax2.set_ylabel(col_name)
    for p in ax2.patches:
        ax2.annotate('{}'.format(p.get_height()), (p.get_x()+0.15, p.get_height()+1))
    
    plt.show()                        ",0,No Code Smell
165,28727928,13,explore_variable('ethnicity'),0,No Code Smell
166,28727928,14,"count_plot(col_name ='ethnicity', fig_size=(20,10))",0,No Code Smell
167,28727928,15,explore_variable('age'),0,No Code Smell
168,28727928,16,"count_plot(col_name ='age', fig_size=(40,15))",0,No Code Smell
169,28727928,17,explore_variable('hospital_admit_source'),0,No Code Smell
170,28727928,18,"count_plot(col_name ='hospital_admit_source', fig_size=(30,12))",0,No Code Smell
171,28727928,19,explore_variable('icu_admit_source'),0,No Code Smell
172,28727928,20,"count_plot(col_name ='icu_admit_source', fig_size=(30,12))",0,No Code Smell
173,28727928,21,explore_variable('icu_stay_type'),0,No Code Smell
174,28727928,22,"count_plot(col_name ='icu_stay_type', fig_size=(30,10))",0,No Code Smell
175,28727928,23,explore_variable('icu_type'),0,No Code Smell
176,28727928,24,"count_plot(col_name ='icu_type', fig_size=(20,10))",0,No Code Smell
177,28727928,25,explore_variable('apache_3j_bodysystem'),0,No Code Smell
178,28727928,26,"count_plot(col_name ='apache_3j_bodysystem', fig_size=(25,12))",0,No Code Smell
179,28727928,27,explore_variable('apache_2_bodysystem'),0,No Code Smell
180,28727928,28,"count_plot(col_name ='apache_2_bodysystem', fig_size=(25,12))",0,No Code Smell
181,28727928,29,"cat_cols =  train.select_dtypes(include='O').columns.values.tolist()
for col in cat_cols: 
    if col in train.columns: 
        le = LabelEncoder() 
        le.fit(list(train[col].astype(str).values) + list(test[col].astype(str).values)) 
        train[col] = le.transform(list(train[col].astype(str).values)) 
        test[col] = le.transform(list(test[col].astype(str).values)) ",0,No Code Smell
182,28727928,30,"y=train[target]
train=train.drop(target, axis=1)",0,No Code Smell
183,28727928,31,"# Parameters
params = {""objective"": ""binary"", 
          ""boosting"": ""gbdt"",
          ""metric"": ""auc"",
          ""n_jobs"":-1,
          ""verbose"":-1}

num_folds = 10
roc_auc = list()
feature_importances = pd.DataFrame()
feature_importances['feature'] = train.columns
pred_on_test = np.zeros(test.shape[0])


kf = StratifiedKFold(n_splits=num_folds,shuffle=True, random_state=2020)
for index, (train_index, valid_index) in enumerate(kf.split(X=train,y=y)):
    print(f""FOLD {index+1}"")

    X_train_fold, y_train_fold = train.iloc[train_index], y.iloc[train_index]
    X_valid_fold, y_valid_fold = train.iloc[valid_index], y.iloc[valid_index]

    dtrain = lightgbm.Dataset(X_train_fold, label=y_train_fold)
    dvalid = lightgbm.Dataset(X_valid_fold, label=y_valid_fold)

    lgb = lightgbm.train(params=params, train_set=dtrain, num_boost_round=2000, 
                         valid_sets=[dtrain, dvalid], verbose_eval=250, early_stopping_rounds=500)

    feature_importances[f'fold_{index + 1}'] = lgb.feature_importance()

    y_valid_pred = (lgb.predict(X_valid_fold,num_iteration=lgb.best_iteration))
    pred_on_test += (lgb.predict(test,num_iteration=lgb.best_iteration)) / num_folds

    # winsorization
    y_valid_pred = np.clip(a=y_valid_pred, a_min=0, a_max=1)
    pred_on_test = np.clip(a=pred_on_test, a_min=0, a_max=1)

    print(f""FOLD {index+1}: ROC_AUC  => {np.round(roc_auc_score(y_true=y_valid_fold, y_score=y_valid_pred),5)}"")
    roc_auc.append(roc_auc_score(y_true=y_valid_fold, y_score=y_valid_pred)/num_folds)
    
print(f""Mean roc_auc for {num_folds} folds: {np.round(sum(roc_auc),5)}"")",0,No Code Smell
184,28727928,32,"def plot_feature_importance(df, k_fold_object):
    df['average_feature_imp'] = df[['fold_{}'.format(fold + 1) for fold in range(k_fold_object.n_splits)]].mean(axis=1)
    plt.figure(figsize=(10, 40))
    sns.barplot(data=df.sort_values(by='average_feature_imp', ascending=False), x='average_feature_imp', y='feature');
    plt.title('Feature importance over {} folds average'.format(k_fold_object.n_splits))
    plt.show()",0,No Code Smell
185,28727928,33,"plot_feature_importance(df=feature_importances, k_fold_object=kf)",0,No Code Smell
186,28727928,34,"solution_template.hospital_death = pred_on_test
solution_template.to_csv(""Version_1.csv"", index=0)",0,No Code Smell
187,27606681,0,from fastai.tabular import * ,1,Code Smell
188,27606681,1,"train = pd.read_csv(""../input/widsdatathon2020/training_v2.csv"")
test = pd.read_csv(""../input/widsdatathon2020/unlabeled.csv"")",1,Code Smell
189,27606681,2,"procs = [FillMissing, Categorify, Normalize]
",0,No Code Smell
190,27606681,3,"valid_idx = range(int(len(train)*0.9), len(train))",0,No Code Smell
191,27606681,4,dep_var = 'hospital_death',0,No Code Smell
192,27606681,5,"def to_cat(c): 
    train[c] = train[c].apply(str)
    test[c]  = test[c].apply(str)
    
[to_cat(c) for c in ['apache_3j_diagnosis', 'apache_2_diagnosis']]",0,No Code Smell
193,27606681,6,"cat_names = ['ethnicity', 'gender', 'hospital_admit_source', 'icu_admit_source',  
             'icu_stay_type', 'icu_type', 'apache_3j_bodysystem', 'apache_2_bodysystem', 
             'elective_surgery', 'apache_post_operative', 'arf_apache', 'gcs_eyes_apache', 
             'gcs_motor_apache', 'gcs_unable_apache', 'gcs_verbal_apache', 'intubated_apache', 
             'ventilated_apache', 'aids', 'cirrhosis', 'diabetes_mellitus', 'hepatic_failure', 
             'immunosuppression', 'leukemia', 'lymphoma', 'solid_tumor_with_metastasis', 
             'icu_id', 'apache_3j_diagnosis' , 'apache_2_diagnosis']",0,No Code Smell
194,27606681,7,"cont_names = list(set(train)-set(cat_names)-
                  {dep_var, 'hospital_id', 'patient_id', 'encounter_id'})
cont_names",0,No Code Smell
195,27606681,8,"train[""apache_4a_hospital_death_prob""]=train[""apache_4a_hospital_death_prob""].replace({-1:np.nan})
test[""apache_4a_hospital_death_prob""]=test[""apache_4a_hospital_death_prob""].replace({-1:np.nan})

train[""apache_4a_icu_death_prob""]=train[""apache_4a_icu_death_prob""].replace({-1:np.nan})
test[""apache_4a_icu_death_prob""]=test[""apache_4a_icu_death_prob""].replace({-1:np.nan})",0,No Code Smell
196,27606681,9,"data = TabularDataBunch.from_df('.', train, dep_var, 
                                valid_idx=valid_idx, procs=procs, 
                                cat_names=cat_names, cont_names=cont_names)",0,No Code Smell
197,27606681,10,"learn = tabular_learner(data, layers=[100,100], ps=0.5, emb_drop=0.5, metrics=[accuracy, AUROC()])",0,No Code Smell
198,27606681,11,"learn.fit_one_cycle(3, 1e-2)",0,No Code Smell
199,27606681,12,"data.add_test(TabularList.from_df(test,path='.' ,cat_names=cat_names, cont_names=cont_names))",0,No Code Smell
200,27606681,13,"probs = learn.get_preds(DatasetType.Test)[0][:,1]
probs",0,No Code Smell
201,27606681,14,"sub = pd.read_csv('../input/widsdatathon2020/solution_template.csv')
sub['hospital_death'] = probs
sub.head()",1,Code Smell
202,27606681,15,"sub.to_csv(""sub.csv"", header=True, index=False)",0,No Code Smell
203,27606681,16,! head sub.csv,0,No Code Smell
204,27654742,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the ""../input/"" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# Any results you write to the current directory are saved as output.

import warnings
warnings.filterwarnings(""ignore"")

import seaborn as sns
import math
import matplotlib as p
import matplotlib.pyplot as plt
%matplotlib inline
import scipy.stats as sps
import re",1,Code Smell
205,27654742,1,"train = pd.read_csv('../input/widsdatathon2020/training_v2.csv')
test = pd.read_csv('../input/widsdatathon2020/unlabeled.csv')
st = pd.read_csv('../input/widsdatathon2020/solution_template.csv')
ss = pd.read_csv('../input/widsdatathon2020/samplesubmission.csv')
dictionary = pd.read_csv('../input/widsdatathon2020/WiDS Datathon 2020 Dictionary.csv')

pd.set_option('display.max_columns', 500)
print('solution template shape', st.shape)
display(st.head())
print('dictionary shape', dictionary.shape)
display(dictionary.T.head())
print('train shape', train.shape)
display(train.head())
print('test shape', test.shape)
display(test.head())",1,Code Smell
206,27654742,2,"# Dropping patient_id for now
train = train.copy().drop('patient_id', axis = 1)
test = test.copy().drop('patient_id', axis = 1)",0,No Code Smell
207,27654742,3,"from sklearn.model_selection import train_test_split

Train, Validation = train_test_split(train, test_size = 0.3)",0,No Code Smell
208,27654742,4,"X_train = Train.copy().drop('hospital_death', axis = 1)
y_train = Train[['encounter_id','hospital_death']]
X_val = Validation.copy().drop('hospital_death', axis = 1)
y_val = Validation[['encounter_id','hospital_death']]",0,No Code Smell
209,27654742,5,"X_test = test.copy().drop('hospital_death', axis = 1)
y_test = test[['encounter_id','hospital_death']]",0,No Code Smell
210,27654742,6,"sns.catplot('hospital_death', data= train, kind='count', alpha=0.7, height=6, aspect=1)

# Get current axis on current figure
ax = plt.gca()

# Max value to be set
y_max = train['hospital_death'].value_counts().max() 

# Iterate through the list of axes' patches
for p in ax.patches:
    ax.text(p.get_x() + p.get_width()/5., p.get_height(),'%d' % int(p.get_height()),
            fontsize=13, color='blue', ha='center', va='bottom')
plt.title('Frequency plot of Hospital Deaths', fontsize = 20, color = 'black')
plt.show()",0,No Code Smell
211,27654742,7,"plt.figure(figsize=(30,15))
ethnicity_vs_death = sns.catplot(x='ethnicity', col='hospital_death', kind='count', data=train, 
                                 order = train['ethnicity'].value_counts().index, height = 7, aspect = 1);
ethnicity_vs_death.set_xticklabels(rotation=90);",0,No Code Smell
212,27654742,8,"plt.figure(figsize=(30,15))
has_vs_death = sns.catplot(x='hospital_admit_source', col='hospital_death', kind='count', data=train, 
                           order = train['hospital_admit_source'].value_counts().index, height = 7, aspect = 1.5);
has_vs_death.set_xticklabels(rotation=90);",0,No Code Smell
213,27654742,9,"plt.figure(figsize=(30,15))
ias_vs_death = sns.catplot(x='icu_admit_source', col='hospital_death', kind='count', data=train, 
                           order = train['icu_admit_source'].value_counts().index, height = 7, aspect = 1.5);
ias_vs_death.set_xticklabels(rotation=90);",0,No Code Smell
214,27654742,10,"# Freq plot of Hospital ID for hospital_death = 0
train_hID_0 = train[train['hospital_death'] == 0]
plt.figure(figsize=(30,15))
hID_vs_death = sns.catplot(y='hospital_id',  orient = ""v"", kind='count', data=train_hID_0, order = train_hID_0['hospital_id'].value_counts().index, 
                           height = 30, aspect = 1)",0,No Code Smell
215,27654742,11,"# Freq plot of Hospital ID for hospital_death = 1
train_hID_1 = train[train['hospital_death'] != 0]
plt.figure(figsize=(30,20))
hID_vs_death = sns.catplot(y='hospital_id',  orient = ""v"", kind='count', data=train_hID_1, order = train_hID_1['hospital_id'].value_counts().index, 
                           height = 30, aspect = 1);",0,No Code Smell
216,27654742,12,"# Freq plot of Hospital ID for hospital_death = 0 & 1
plt.figure(figsize=(30,40))
hID_vs_death = sns.catplot(x = 'hospital_id', col='hospital_death', kind='count', data=train, order = train['hospital_id'].value_counts().index, 
                           height = 5, aspect = 2.8);
hID_vs_death.set_xticklabels(rotation=90);",0,No Code Smell
217,27654742,13,"plt.figure(figsize = (15,5))
sns.kdeplot(train_hID_1['age'], shade=True, color=""r"")
sns.kdeplot(train_hID_0['age'], shade=True, color=""b"")",0,No Code Smell
218,27654742,14,"sns.jointplot(x=""age"", y=""bmi"", data=train, kind = ""kde"")
sns.jointplot(x=""age"", y=""height"", data=train, kind = ""kde"")
sns.jointplot(x=""age"", y=""weight"", data=train, kind = ""kde"")",0,No Code Smell
219,27654742,15,"dataset = pd.concat(objs=[X_train, X_val], axis=0)",0,No Code Smell
220,27654742,16,col_1 = dataset.columns,0,No Code Smell
221,27654742,17,"for i in col_1:
    if X_train[i].nunique() == 1:
        print('in Train', i)
    if X_val[i].nunique() == 1:
        print('in Val', i)
    if X_test[i].nunique() == 1:
        print('in Test', i)
    ",0,No Code Smell
222,27654742,18,"# Dropping 'readmission_status'
X_train = X_train.drop(['readmission_status'], axis=1)
X_val = X_val.drop(['readmission_status'], axis=1)
X_test = X_test.drop(['readmission_status'], axis=1)",0,No Code Smell
223,27654742,19,"print('For Train')
d1 = X_train.nunique()
print(sorted(d1))
print(""=============================="")
print('For Validation')
d2 = X_val.nunique()
print(sorted(d2))

# Considering columns with <= 15 unique values for conversion",0,No Code Smell
224,27654742,20,"d = pd.concat(objs=[X_train, X_val], axis=0)",0,No Code Smell
225,27654742,21,col = d.columns ,0,No Code Smell
226,27654742,22,"# For Train data
l1 = []
for i in col:
    if X_train[i].nunique() <= 15:
        l1.append(i)
        
l1",0,No Code Smell
227,27654742,23,"# For Val data
l2 = []
for i in col:
    if X_val[i].nunique() <= 15:
        l2.append(i)
        
l2",0,No Code Smell
228,27654742,24,"# For Test data
l3 = []
for i in col:
    if X_test[i].nunique() <= 15:
        l3.append(i)
        
l3",0,No Code Smell
229,27654742,25,"# Checking for columns in X_train and X_validation
set(l1) & set(l2)",0,No Code Smell
230,27654742,26,"# Checking for columns in X_train and X_test
set(l1) & set(l3)",0,No Code Smell
231,27654742,27,"print('Train', len(l1))
print('Validation', len(l2))
print('Common', len(set(l1) & set(l2)))",0,No Code Smell
232,27654742,28,"print('Train', len(l1))
print('Test', len(l3))
print('Common', len(set(l1) & set(l3)))",0,No Code Smell
233,27654742,29,X_train[l1].dtypes,0,No Code Smell
234,27654742,30,"X_val[l2].dtypes
# Not a necessary step since we already confirmed the common columns. Included just for reference. ",0,No Code Smell
235,27654742,31,"X_train[l1] = pd.Categorical(X_train[l1])
X_val[l2] = pd.Categorical(X_val[l2])
X_test[l3] = pd.Categorical(X_test[l3])
print('Train dtypes:')
print(X_train[l1].dtypes)
print('======================================')
print('Validation dtypes:')
print(X_val[l2].dtypes)
print('======================================')
print('Test dtypes:')
print(X_test[l3].dtypes)",0,No Code Smell
236,27654742,32,"# On train data
pd.set_option('display.max_rows', 500)
NA_col = pd.DataFrame(X_train.isna().sum(), columns = ['NA_Count'])
NA_col['% of NA'] = (NA_col.NA_Count/len(X_train))*100
NA_col.sort_values(by = ['% of NA'], ascending = False, na_position = 'first')",0,No Code Smell
237,27654742,33,"# On val data
pd.set_option('display.max_rows', 500)
NA_col = pd.DataFrame(X_val.isna().sum(), columns = ['NA_Count'])
NA_col['% of NA'] = (NA_col.NA_Count/len(X_val))*100
NA_col.sort_values(by = ['% of NA'], ascending = False, na_position = 'first')",0,No Code Smell
238,27654742,34,"# On test data
pd.set_option('display.max_rows', 500)
NA_col = pd.DataFrame(X_test.isna().sum(), columns = ['NA_Count'])
NA_col['% of NA'] = (NA_col.NA_Count/len(X_test))*100
NA_col.sort_values(by = ['% of NA'], ascending = False, na_position = 'first')",0,No Code Smell
239,27654742,35,"cols = X_train.columns
num_cols = X_train._get_numeric_data().columns
cat_cols = list(set(cols) - set(num_cols))
cat_cols",0,No Code Smell
240,27654742,36,"# Courtesy: https://www.kaggle.com/jayjay75/wids2020-lgb-starter-script
from sklearn.preprocessing import LabelEncoder,OneHotEncoder
for usecol in cat_cols:
    X_train[usecol] = X_train[usecol].astype('str')
    X_val[usecol] = X_val[usecol].astype('str')
    X_test[usecol] = X_test[usecol].astype('str')
    
    #Fit LabelEncoder
    le = LabelEncoder().fit(
            np.unique(X_train[usecol].unique().tolist()+
                      X_val[usecol].unique().tolist()+
                     X_test[usecol].unique().tolist()))

    #At the end 0 will be used for dropped values
    X_train[usecol] = le.transform(X_train[usecol])+1
    X_val[usecol]  = le.transform(X_val[usecol])+1
    X_test[usecol]  = le.transform(X_test[usecol])+1
    
    X_train[usecol] = X_train[usecol].replace(np.nan, 0).astype('int').astype('category')
    X_val[usecol]  = X_val[usecol].replace(np.nan, 0).astype('int').astype('category')
    X_test[usecol]  = X_test[usecol].replace(np.nan, 0).astype('int').astype('category')",0,No Code Smell
241,27654742,37,"X_train.set_index('encounter_id', inplace = True)
y_train.set_index('encounter_id', inplace = True)
X_val.set_index('encounter_id', inplace = True)
y_val.set_index('encounter_id', inplace = True)
X_test.set_index('encounter_id', inplace = True)
y_test.set_index('encounter_id', inplace = True)",0,No Code Smell
242,27654742,38,# y_test.hospital_death = y_test.hospital_death.fillna(0),0,No Code Smell
243,27654742,39,"# y_train['hospital_death'] = pd.Categorical(y_train['hospital_death'])
# y_train.dtypes",0,No Code Smell
244,27654742,40,"# y_test['hospital_death'] = pd.Categorical(y_test['hospital_death'])
# y_test.dtypes",0,No Code Smell
245,27654742,41,"# from sklearn.preprocessing import LabelEncoder,OneHotEncoder
# from pandas import Series
# l=LabelEncoder() 
# l.fit(y_train['hospital_death']) 
# l.classes_ 
# y_train['hospital_death']=Series(l.transform(y_train['hospital_death']))  #label encoding our target variable 
# y_train['hospital_death'].value_counts() ",0,No Code Smell
246,27654742,42,"# l.fit(y_test['hospital_death']) 
# l.classes_ 
#y_test['hospital_death'].fillna(0.0, inplace = True)
# y_test['hospital_death']=Series(l.transform(y_test['hospital_death']))  #label encoding our target variable 
# y_test['hospital_death'].value_counts() ",0,No Code Smell
247,27654742,43,"import lightgbm as lgbm

lgbm_train = lgbm.Dataset(X_train, y_train, categorical_feature=cat_cols)
# lgbm_test = lgbm.Dataset(X_test, y_test, categorical_feature=cat_cols)
lgbm_val = lgbm.Dataset(X_val, y_val, reference = lgbm_train)",0,No Code Smell
248,27654742,44,"params = {'feature_fraction': 0.9,
          'lambda_l1': 1,
          'lambda_l2': 1,
          'learning_rate': 0.01,
          'max_depth': 10,
          'metric': 'auc',
          'num_leaves': 500,
          'min_data_in_leaf': 100,
          'subsample_freq': 1,
          'scale_pos_weight':1,
          'metric': 'auc',
          'is_unbalance': 'true',
          'boosting': 'gbdt',
          'bagging_fraction': 0.5,
          'bagging_freq': 10,}",0,No Code Smell
249,27654742,45,"evals_result = {}  # to record eval results for plotting
model_lgbm = lgbm.train(params,
                lgbm_train,
                num_boost_round=100,
                valid_sets=[lgbm_train, lgbm_val],
                feature_name=['f' + str(i + 1) for i in range(X_train.shape[-1])],
                categorical_feature= [182],
                evals_result=evals_result,
                verbose_eval=10)",0,No Code Smell
250,27654742,46,"ax = lgbm.plot_metric(evals_result, metric='auc', figsize=(15, 8))
plt.show()",0,No Code Smell
251,27654742,47,"test[""hospital_death""] = model_lgbm.predict(X_test, predition_type = 'Probability')
test[[""encounter_id"",""hospital_death""]].to_csv(""submission_lgbm1.csv"",index=False)",0,No Code Smell
252,27654742,48,"test[[""encounter_id"",""hospital_death""]].head()",0,No Code Smell
253,29099590,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the ""../input/"" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# Any results you write to the current directory are saved as output.

import lightgbm as lgb
from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold
from sklearn.metrics import roc_auc_score

from tqdm import tqdm_notebook
import gc",1,Code Smell
254,29099590,1,"train = pd.read_csv('/kaggle/input/widsdatathon2020/training_v2.csv')
test = pd.read_csv('/kaggle/input/widsdatathon2020/unlabeled.csv')
sample_submission = pd.read_csv('/kaggle/input/widsdatathon2020/samplesubmission.csv')
solution_template = pd.read_csv('/kaggle/input/widsdatathon2020/solution_template.csv')
train.shape, test.shape",1,Code Smell
255,29099590,2,train.sample(5),0,No Code Smell
256,29099590,3,test.head(5),0,No Code Smell
257,29099590,4,"def make_submit(y_pred, filename='submission.csv'):
    solution_template['hospital_death'] = y_pred
    solution_template.to_csv(f'{filename}', index=False)
    print('solution file created. Commit notebook and submit file...')
    solution_template['hospital_death'].hist()
    ",0,No Code Smell
258,29099590,5,"# LightGBM GBDT with KFold or Stratified KFold

def kfold_lightgbm(train, test, target_col, params, cols_to_drop=None, cat_features=None, num_folds=5, stratified = False, 
                   debug= False):
    
    print(""Starting LightGBM. Train shape: {}, test shape: {}"".format(train.shape, test.shape))


    
    # Cross validation model
    if stratified:
        folds = StratifiedKFold(n_splits= num_folds, shuffle=True, random_state=1)
    else:
        folds = KFold(n_splits= num_folds, shuffle=True, random_state=1)

    # Create arrays and dataframes to store results
    oof_preds = np.zeros(train.shape[0])
    sub_preds = np.zeros(test.shape[0])
    feature_importance_df = pd.DataFrame()
    if cols_to_drop == None:
        feats = [f for f in train.columns if f not in [target_col]]
    else:
        feats = [f for f in train.columns if f not in cols_to_drop+[target_col]]

    # k-fold
    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train[feats], train[target_col])):
        train_x, train_y = train[feats].iloc[train_idx], train[target_col].iloc[train_idx]
        valid_x, valid_y = train[feats].iloc[valid_idx], train[target_col].iloc[valid_idx]

        # set data structure
        lgb_train = lgb.Dataset(train_x,
                                label=train_y,
                                categorical_feature=cat_features,
                                free_raw_data=False)
        lgb_test = lgb.Dataset(valid_x,
                               label=valid_y,
                               categorical_feature=cat_features,
                               free_raw_data=False)

        # params after optimization
        reg = lgb.train(
                        params,
                        lgb_train,
                        valid_sets=[lgb_train, lgb_test],
                        valid_names=['train', 'test'],
#                         num_boost_round=10000,
#                         early_stopping_rounds= 200,
                        verbose_eval=False
                        )

        roc_auc = []
        oof_preds[valid_idx] = reg.predict(valid_x, num_iteration=reg.best_iteration)
        sub_preds += reg.predict(test[feats], num_iteration=reg.best_iteration) / folds.n_splits

        fold_importance_df = pd.DataFrame()
        fold_importance_df[""feature""] = feats
        fold_importance_df[""importance""] = np.log1p(reg.feature_importance(importance_type='gain', 
                                                                           iteration=reg.best_iteration))
        fold_importance_df[""fold""] = n_fold + 1
        
        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)
        print('Fold %2d ROC-AUC : %.6f' % (n_fold + 1, roc_auc_score(valid_y, oof_preds[valid_idx])))
        roc_auc.append(roc_auc_score(valid_y, oof_preds[valid_idx]))
        del reg, train_x, train_y, valid_x, valid_y
        gc.collect()
        
    print('Mean ROC-AUC : %.6f' % (np.mean(roc_auc)))
    return sub_preds",0,No Code Smell
259,29099590,6,cat_features = [x for x in train.columns if train[x].dtype == 'object' ],0,No Code Smell
260,29099590,7,"from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()

# Для основных датасетов
for col in tqdm_notebook(cat_features):
    train[col] = train[col].astype('str')
    train[col] = le.fit_transform(train[col])
    
for col in tqdm_notebook(cat_features):
    test[col] = test[col].astype('str')
    test[col] = le.fit_transform(test[col])",0,No Code Smell
261,29099590,8,"params ={
    'objective': 'binary',
    'metric': 'roc_auc',
    'categorical_features': cat_features
                }
params_best = {
    'bagging_fraction': 0.15760757965010433, 
    'feature_fraction': 0.11161740354830015, 
    'learning_rate': 0.03, 
    'max_depth': 50, 
    'min_child_weight': 0.008857217513412136, 
    'min_data_in_leaf': 20, 
    'n_estimators': 815, 
    'num_leaves': 96, 
    'reg_alpha': 1.5292311993088907, 
    'reg_lambda': 1.903834634991243}",0,No Code Smell
262,29099590,9,"submit_best_params = kfold_lightgbm(train, test.drop('hospital_death', axis=1), cat_features=cat_features, 
                                 target_col='hospital_death', params=params_best)",0,No Code Smell
263,29099590,10,"make_submit(submit_best_params, 'submission.csv')",0,No Code Smell
264,29099590,11,"baseline_submit = kfold_lightgbm(train, test.drop('hospital_death', axis=1), cat_features=cat_features, 
                                 target_col='hospital_death', params=params)",0,No Code Smell
265,29099590,12,"def bayes_auc_lgb(
    n_estimators,
    learning_rate,
    num_leaves, 
    bagging_fraction,
    feature_fraction,
    min_child_weight, 
    min_data_in_leaf,
    max_depth,
    reg_alpha,
    reg_lambda):
    
    """"""
    До запуска надо переопределить ИСХОДНЫЙ ДАТАФРЕЙМ и КАТЕГОРИАЛЬНЫЕ ПРИЗНАКИ
    """"""
    
    # На вход LightGBM следующие парамерты должны подаваться в виде целых чисел. 
    n_estimators = int(n_estimators)
    num_leaves = int(num_leaves)
    min_data_in_leaf = int(min_data_in_leaf)
    max_depth = int(max_depth)
    
    assert type(n_estimators) == int
    assert type(num_leaves) == int
    assert type(min_data_in_leaf) == int
    assert type(max_depth) == int
    
    params = {
              'n_estimators': n_estimators,
              'num_leaves': num_leaves, 
              'min_data_in_leaf': min_data_in_leaf,
              'min_child_weight': min_child_weight,
              'bagging_fraction' : bagging_fraction,
              'feature_fraction' : feature_fraction,
              'learning_rate' : learning_rate,
              'max_depth': max_depth,
              'reg_alpha': reg_alpha,
              'reg_lambda': reg_lambda,
              'objective': 'binary',
              'save_binary': True,
              'seed': 1337,
              'feature_fraction_seed': 1337,
              'bagging_seed': 1337,
              'drop_seed': 1337,
              'data_random_seed': 1337,
              'boosting_type': 'gbdt',
              'verbose': 1,
              'is_unbalance': False,
              'boost_from_average': True,
              'metric':'f1'}

    
    # кросс-валидация
    folds = StratifiedKFold(n_splits= 5, shuffle=True, random_state=1)


    # Массивы для сохранения результатов
    oof_preds = np.zeros(df.shape[0])

    feats = [f for f in df.columns if f not in ['hospital_death']]

    # k-fold
    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(df[feats], df['hospital_death'])):
        train_x, train_y = df[feats].iloc[train_idx], df['hospital_death'].iloc[train_idx]
        valid_x, valid_y = df[feats].iloc[valid_idx], df['hospital_death'].iloc[valid_idx]

        # датасеты для обучения
        lgb_train = lgbm.Dataset(train_x,
                                label=train_y,
                                categorical_feature=cat_f,
                                free_raw_data=False)
        lgb_test = lgbm.Dataset(valid_x,
                               label=valid_y,
                               categorical_feature=cat_f,
                               free_raw_data=False)

        # Обучение
        clf = lgbm.train(
                        params,
                        lgb_train,
                        valid_sets=[lgb_train, lgb_test],
                        verbose_eval=False
                        )

        auc = []
        oof_preds[valid_idx] = clf.predict(valid_x, num_iteration=clf.best_iteration)



        auc.append(roc_auc_score(valid_y, oof_preds[valid_idx]))

    return np.mean(auc)",0,No Code Smell
266,29099590,13,"# Границы параметров для поиска   
bounds_lgb = {
    'n_estimators': (10, 1000),
    'num_leaves': (10, 500), 
    'min_data_in_leaf': (20, 200),
    'bagging_fraction' : (0.1, 0.9),
    'feature_fraction' : (0.1, 0.9),
    'learning_rate': (0.01, 0.3),
    'min_child_weight': (0.00001, 0.01),   
    'reg_alpha': (1, 2), 
    'reg_lambda': (1, 2),
    'max_depth':(-1,50),
}",0,No Code Smell
267,29099590,14,"def bayes_lgb(score_func, bound_lgb, init_points: int = 10, n_iter:int = 100):
    """"""
    Поиск оптимальных гиперпараметров для алгоритма с использованием байесовской оптимизации. 
    :param score_func: функция для оптимизации. Определена отдельно.
    :param bounds_lgb: границы диапазона для поисков параметров (словарь). Задается отдельно
    :param n_iter: количество итераций
    :return: Оптимальные параметры в виде словаря
    """"""
    # инициализация оптимизатора
    lgb_bo = BayesianOptimization(score_func, bounds_lgb, verbose=0)
    
    # поиск
    lgb_bo.maximize(init_points=init_points, n_iter=n_iter, xi=0.0, alpha=1e-6)
    
    print(""Максимальное значение метрики: "",lgb_bo.max['target'])
    print(""Оптимальные параметры: "", lgb_bo.max['params'])
    
    return lgb_bo.max['params']",0,No Code Smell
268,29099590,15,from bayes_opt import BayesianOptimization,1,Code Smell
269,29099590,16,cat_features.remove('icu_stay_type'),0,No Code Smell
270,29099590,17,"df = train.copy().drop(['encounter_id', 'patient_id', 'hospital_id', 'icu_stay_type', 'icu_id'], axis=1)
cat_f=cat_features

bo_best_params = bayes_lgb(bayes_auc_lgb, bounds_lgb)",0,No Code Smell
271,28312981,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the ""../input/"" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# Any results you write to the current directory are saved as output.",1,Code Smell
272,28312981,1,"# Data
train = pd.read_csv(""/kaggle/input/widsdatathon2020/training_v2.csv"")
test = pd.read_csv(""/kaggle/input/widsdatathon2020/unlabeled.csv"")
# Dictionary
dictionary = pd.read_csv(""/kaggle/input/widsdatathon2020/WiDS Datathon 2020 Dictionary.csv"")
# Samples
samplesubmission = pd.read_csv(""/kaggle/input/widsdatathon2020/samplesubmission.csv"")
solution_template = pd.read_csv(""/kaggle/input/widsdatathon2020/solution_template.csv"")",1,Code Smell
273,28312981,2,dictionary,0,No Code Smell
274,28312981,3,dictionary.Category.unique(),0,No Code Smell
275,28312981,4,#dictionary[dictionary['Data Type'] == 'numeric'],0,No Code Smell
276,28312981,5,train,0,No Code Smell
277,28312981,6,test,0,No Code Smell
278,28312981,7,train.hospital_death.describe(),0,No Code Smell
279,28312981,8,"print('unlabeled data: {}\ntraining data:  {}'.format(test.shape, train.shape))",0,No Code Smell
280,28312981,9,"from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score",1,Code Smell
281,28312981,10,"train.fillna(method='ffill', inplace=True)
test.fillna(method='ffill', inplace=True)",0,No Code Smell
282,28312981,11,"# Train
X_train = train[['age','weight']].values
y_train = train['hospital_death'].values
# Test
X_test = test[['age','weight']].values",0,No Code Smell
283,28312981,12,"model = LogisticRegression()
model.fit(X_train, y_train)",0,No Code Smell
284,28312981,13,y_predict = model.predict_proba(X_test),0,No Code Smell
285,28312981,14,"test['hospital_death'] = y_predict[:,0]",0,No Code Smell
286,28312981,15,test,0,No Code Smell
287,28312981,16,"test[[""encounter_id"",""hospital_death""]].to_csv(""submission.csv"",index=False)",0,No Code Smell
288,28312981,17,"test[[""encounter_id"",""hospital_death""]]",0,No Code Smell
289,82247998,0,"#Import Packages

import numpy as np
import pandas as pd
import os
import matplotlib.pyplot as plt

from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity='all' # shows outputs of all commands executed in 1 cell",1,Code Smell
290,82247998,1,"# Input data files are available in the ""../input/"" directory.

# List all files under the input directory

input_path = '/kaggle/input/widsdatathon2020'

for dirpath, dirname, filenames in os.walk(input_path):
    for name in filenames:
        print (os.path.join(dirpath , name))
        
# Any results you write to the current directory are saved as output.",1,Code Smell
291,82247998,2,"# read file
fname = 'training_v2.csv'
train_df = pd.read_csv(os.path.join(input_path , fname))

fname = 'unlabeled.csv'
test_df = pd.read_csv(os.path.join(input_path , fname))

fname = 'solution_template.csv'
solution_df = pd.read_csv(os.path.join(input_path , fname))

",1,Code Smell
292,82247998,3,"print('solution_df')
solution_df.head() 
solution_df.info()
solution_df.shape",1,Code Smell
293,82247998,4,solution_df['encounter_id'].describe(),0,No Code Smell
294,82247998,5,"print('test_df')
test_df.head()
test_df.info()
test_df.shape
test_df['encounter_id'].describe()",0,No Code Smell
295,82247998,6,"print('train_df')
train_df.head() 
train_df.info()
train_df.shape",0,No Code Smell
296,82247998,7,"train_df['hospital_death'].dtype
test_df['hospital_death'].dtype
",0,No Code Smell
297,82247998,8,"def display_columns_properties(df):
    for i, col in enumerate(df.columns.tolist()):
         print('\n ({} {})  Missing: {}  UniqValsSz: {}'.format(i,col, df[col].isnull().sum() ,df[col].unique().size))
    print('\n')",0,No Code Smell
298,82247998,9,display_columns_properties(train_df),0,No Code Smell
299,82247998,10,display_columns_properties(test_df),0,No Code Smell
300,82247998,11,"cat_train_df = train_df.select_dtypes(include='object')
cat_train_df.head()
cat_train_df.info()",1,Code Smell
301,82247998,12,"cat_test_df = test_df.select_dtypes(include='object')
cat_test_df.head()
cat_test_df.info()",1,Code Smell
302,82247998,13,"def display_columns_uniqvals(df):
    for i, col in enumerate(df.columns.tolist()):
         print('\n ({} {}) Uniq: {}'.format(i,col, df[col].unique() ))
    print('\n')",0,No Code Smell
303,82247998,14,display_columns_uniqvals(cat_test_df),0,No Code Smell
304,82247998,15,"from sklearn.model_selection import train_test_split

# copy the data
train = train_df.copy()

# Select target
y = train['hospital_death']


# To keep things simple, we'll use only numerical predictors
predictors = train.drop(['hospital_death'], axis=1)
X = predictors.select_dtypes(exclude=['object'])



# Divide data into training and validation subsets
X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,
                                                      random_state=0)

X_train.shape
X_valid.shape
",0,No Code Smell
305,82247998,16,"from sklearn.impute import SimpleImputer

# Imputation
my_imputer = SimpleImputer()
imputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))
imputed_X_valid = pd.DataFrame(my_imputer.transform(X_valid))

# Imputation removed column names; put them back
imputed_X_train.columns = X_train.columns
imputed_X_valid.columns = X_valid.columns
",0,No Code Smell
306,82247998,17,"
display_columns_properties(imputed_X_train)",0,No Code Smell
307,82247998,18,"
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_absolute_error


# Define model. Specify a number for random_state to ensure same results each run.
dt_model = DecisionTreeRegressor(random_state=1)

# Fit model using Traing data
dt_model.fit(imputed_X_train, y_train)

# get predicted prices on validation data
predicted_values = dt_model.predict(imputed_X_valid)

# Find difference
score = mean_absolute_error(y_valid, predicted_values)
print('MAE:', score)",0,No Code Smell
308,82247998,19,"
test = test_df.copy()

#Separate target
y_test = test['hospital_death']

# To keep things simple, we'll use only numerical predictors
predictors_test = test.drop(['hospital_death'], axis=1)
X_test = predictors_test.select_dtypes(exclude=['object'])



X_test.shape
X_test.head()",0,No Code Smell
309,82247998,20,"# Imputation
my_imputer = SimpleImputer()
imputed_X_test = pd.DataFrame(my_imputer.fit_transform(X_test))


# Imputation removed column names; put them back
imputed_X_test.columns = X_test.columns",0,No Code Smell
310,82247998,21,imputed_X_test.head(),0,No Code Smell
311,82247998,22,"
# get predictions on test data
preds = dt_model.predict(imputed_X_test)

# Save predictions in format used for competition scoring
output = pd.DataFrame({'encounter_id': imputed_X_test.encounter_id,
                       'hospital_death': preds},dtype=np.int32)
 
output.to_csv('submission.csv', index=False)
",0,No Code Smell
312,82247998,23,output.columns.dtype,0,No Code Smell
313,82247998,24,"### Conclusion
# Used Decision tree model, simple imputation and only numerical columns.
# Random forest Training is taking too long and not getting complete.",0,No Code Smell
314,27625781,0,"#codes from Rodrigo Lima  @rodrigolima82
from IPython.display import Image
Image(url = 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcS0WNpRevX2b8A237rMQ2VQaXkQSv20nnmGW2lOFJFlwjI43aGO2w&s',width=400,height=400)",0,No Code Smell
315,27625781,1,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go

# Input data files are available in the ""../input/"" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# Any results you write to the current directory are saved as output.",1,Code Smell
316,27625781,2,"wids = pd.read_csv(""../input/widsdatathon2020/training_v2.csv"")
test = pd.read_csv(""../input/widsdatathon2020/unlabeled.csv"")
sub = pd.read_csv('../input/widsdatathon2020/samplesubmission.csv')",1,Code Smell
317,27625781,3,"#codes from Rodrigo Lima  @rodrigolima82
from IPython.display import Image
Image(url = 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSQ44CM9UB1kKjsFpFGU-DTRVmlvGugTz3wPRllnbucMXplydui6w&s',width=400,height=400)",0,No Code Smell
318,27625781,4,wids.head(),0,No Code Smell
319,27625781,5,wids.dtypes,0,No Code Smell
320,27625781,6,"#codes from Rodrigo Lima  @rodrigolima82
from IPython.display import Image
Image(url = 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTJE4rPsgdkEJgNcZCknrweDTpASeH3LggoTzdqsZd8iUDTaRz_&s',width=400,height=400)",0,No Code Smell
321,27625781,7,wids.describe(),0,No Code Smell
322,27625781,8,"print(""The number of nulls in each column are \n"", wids.isna().sum())",0,No Code Smell
323,27625781,9,"sns.countplot(wids[""hospital_death""])
plt.xticks(rotation=45)
plt.yticks(rotation=45)
plt.show()",0,No Code Smell
324,27625781,10,"#codes from Rodrigo Lima  @rodrigolima82
from IPython.display import Image
Image(url = 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcT1XBLX-CmEdjlURIK4ovG5Aflo8QKS_hnycP5kDwlw50C5QRWx&s',width=400,height=400)",0,No Code Smell
325,27625781,11,"sns.distplot(wids[""hospital_death""])",0,No Code Smell
326,27625781,12,"sns.scatterplot(x='age',y='hospital_death',data=wids)",0,No Code Smell
327,27625781,13,"print (""Skew is:"", wids.hospital_death.skew())
plt.hist(wids.hospital_death, color='pink')
plt.show()",1,Code Smell
328,27625781,14,"#codes from Rodrigo Lima  @rodrigolima82
from IPython.display import Image
Image(url = 'https://i.pinimg.com/236x/ac/fb/8b/acfb8b6026740e5c50063307c468f12f.jpg',width=400,height=400)",0,No Code Smell
329,27625781,15,"# Necessary Functions: 
def pie_plot(labels, values, colors, title):
    fig = {
      ""data"": [
        {
          ""values"": values,
          ""labels"": labels,
          ""domain"": {""x"": [0, .48]},
          ""name"": ""Job Type"",
          ""sort"": False,
          ""marker"": {'colors': colors},
          ""textinfo"":""percent+label+value"",
          ""textfont"": {'color': '#FFFFFF', 'size': 10},
          ""hole"": .6,
          ""type"": ""pie""
        } ],
        ""layout"": {
            ""title"":title,
            ""annotations"": [
                {
                    ""font"": {
                        ""size"": 25,

                    },
                    ""showarrow"": False,
                    ""text"": """"

                }
            ]
        }
    }
    return fig",0,No Code Smell
330,27625781,16,"sns.boxplot(x=""hospital_death"", y=""patient_id"", data=wids)
",0,No Code Smell
331,27625781,17,"#codes from Rodrigo Lima  @rodrigolima82
from IPython.display import Image
Image(url = 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcS8TMbfKeH67PQNL0ghQrPrwG75wjca1M7g38Bi9Jbj1T3dsh0F&s',width=400,height=400)",0,No Code Smell
332,27625781,18,"#codes from PSVishnu @psvishnu
hospital = [
    'patient_id','hospital_id','hospital_death','encounter_id']",0,No Code Smell
333,27625781,19,"#codes from Rodrigo Lima  @rodrigolima82
from IPython.display import Image
Image(url = 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTxDDQDIfj58v2qLXz1NnNM9fqYg1xdM3cGdJ5d3ktKlMvRpALqqw&s',width=400,height=400)",0,No Code Smell
334,27625781,20,"sns.pairplot(data=wids,diag_kind='kde',vars=hospital,hue='hospital_death')
plt.show()",0,No Code Smell
335,27625781,21,"import plotly.offline as py
value_counts = wids['hospital_id'].value_counts()
labels = value_counts.index.tolist()
py.iplot(pie_plot(labels, value_counts,['#1B9E77', '#7570B3'], ""Hospital Id""))",0,No Code Smell
336,27625781,22,"#codes from Rodrigo Lima  @rodrigolima82
from IPython.display import Image
Image(url = 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSVbT0PMefnpoA7dQYRZKmGvxVO3kXqb6MVCgTcy-guauA-xBX2Cw&s',width=400,height=400)",0,No Code Smell
337,27625781,23,"from collections import Counter
import json
from IPython.display import HTML
import altair as alt
from  altair.vega import v5",1,Code Smell
338,27625781,24,"#codes from Rodrigo Lima  @rodrigolima82
from IPython.display import Image
Image(url = 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcR-TVKsdRr0lRfeZHuaMMrzA4g7qN1pOsU-hnd3MoedAtQKTC3T&s',width=400,height=400)",0,No Code Smell
339,27625781,25,"
##-----------------------------------------------------------
# This whole section 
vega_url = 'https://cdn.jsdelivr.net/npm/vega@' + v5.SCHEMA_VERSION
vega_lib_url = 'https://cdn.jsdelivr.net/npm/vega-lib'
vega_lite_url = 'https://cdn.jsdelivr.net/npm/vega-lite@' + alt.SCHEMA_VERSION
vega_embed_url = 'https://cdn.jsdelivr.net/npm/vega-embed@3'
noext = ""?noext""

paths = {
    'vega': vega_url + noext,
    'vega-lib': vega_lib_url + noext,
    'vega-lite': vega_lite_url + noext,
    'vega-embed': vega_embed_url + noext
}

workaround = """"""
requirejs.config({{
    baseUrl: 'https://cdn.jsdelivr.net/npm/',
    paths: {}
}});
""""""

#------------------------------------------------ Defs for future rendering
def add_autoincrement(render_func):
    # Keep track of unique <div/> IDs
    cache = {}
    def wrapped(chart, id=""vega-chart"", autoincrement=True):
        if autoincrement:
            if id in cache:
                counter = 1 + cache[id]
                cache[id] = counter
            else:
                cache[id] = 0
            actual_id = id if cache[id] == 0 else id + '-' + str(cache[id])
        else:
            if id not in cache:
                cache[id] = 0
            actual_id = id
        return render_func(chart, id=actual_id)
    # Cache will stay outside and 
    return wrapped

@add_autoincrement
def render(chart, id=""vega-chart""):
    chart_str = """"""
    <div id=""{id}""></div><script>
    require([""vega-embed""], function(vg_embed) {{
        const spec = {chart};     
        vg_embed(""#{id}"", spec, {{defaultStyle: true}}).catch(console.warn);
        console.log(""anything?"");
    }});
    console.log(""really...anything?"");
    </script>
    """"""
    return HTML(
        chart_str.format(
            id=id,
            chart=json.dumps(chart) if isinstance(chart, dict) else chart.to_json(indent=None)
        )
    )



HTML("""".join((
    ""<script>"",
    workaround.format(json.dumps(paths)),
    ""</script>"")))",0,No Code Smell
340,27625781,26,"def word_cloud(df, pixwidth=6000, pixheight=350, column=""index"", counts=""count""):
    data= [dict(name=""dataset"", values=df.to_dict(orient=""records""))]
    wordcloud = {
        ""$schema"": ""https://vega.github.io/schema/vega/v5.json"",
        ""width"": pixwidth,
        ""height"": pixheight,
        ""padding"": 0,
        ""title"": ""Hospital - Women in Data Science 2020"",
        ""data"": data
    }
    scale = dict(
        name=""color"",
        type=""ordinal"",
        range=[""cadetblue"", ""royalblue"", ""steelblue"", ""navy"", ""teal""]
    )
    mark = {
        ""type"":""text"",
        ""from"":dict(data=""dataset""),
        ""encode"":dict(
            enter=dict(
                text=dict(field=column),
                align=dict(value=""center""),  
                baseline=dict(value=""alphabetic""),
                fill=dict(scale=""color"", field=column),
                tooltip=dict(signal=""datum.count + ' occurrances'"")
            )
        ),
            ""transform"": [{
            ""type"": ""wordcloud"",
            ""text"": dict(field=column),
            ""size"": [pixwidth, pixheight],
            ""font"": ""Helvetica Neue, Arial"",
            ""fontSize"": dict(field=""datum.{}"".format(counts)),
            ""fontSizeRange"": [10, 60],
            ""padding"": 2
        }]
    }
    wordcloud[""scales""] = [scale]
    wordcloud[""marks""] = [mark]
    
    return wordcloud

from collections import defaultdict

def wordcloud_create(wids):
    ult = {}
    corpus = wids.icu_type.values.tolist()
    final = defaultdict(int) #Declaring an empty dictionary for count (Saves ram usage)
    for words in corpus:
        for word in words.split():
             final[word]+=1
    temp = Counter(final)
    for k, v in  temp.most_common(200):
        ult[k] = v
    corpus = pd.Series(ult) #Creating a dataframe from the final default dict
    return render(word_cloud(corpus.to_frame(name=""count"").reset_index(), pixheight=600, pixwidth=900))",0,No Code Smell
341,27625781,27,wordcloud_create(wids),0,No Code Smell
342,27625781,28,"#codes from Rodrigo Lima  @rodrigolima82
from IPython.display import Image
Image(url = 'https://www.kdnuggets.com/wp-content/uploads/career-progression.jpg',width=400,height=400)",0,No Code Smell
343,27625781,29,"#word cloud
from wordcloud import WordCloud, ImageColorGenerator
text = "" "".join(str(each) for each in wids.apache_2_bodysystem)
# Create and generate a word cloud image:
wordcloud = WordCloud(max_words=200, background_color=""black"").generate(text)
plt.figure(figsize=(10,6))
plt.figure(figsize=(15,10))
# Display the generated image:
plt.imshow(wordcloud, interpolation='Bilinear')
plt.axis(""off"")
plt.show()",0,No Code Smell
344,27625781,30,"#codes from Rodrigo Lima  @rodrigolima82
from IPython.display import Image
Image(url = 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQhs0JIJjvon1mlAC_QGIIIZDTwGUPY5ZByFTdFaSE9f3l2RC3L2g&s',width=400,height=400)",0,No Code Smell
345,29148454,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.api as sm

import warnings
warnings.simplefilter(action = 'ignore')

# Standardization
from sklearn import preprocessing
from sklearn.preprocessing import StandardScaler

# Train-Test split
from sklearn.model_selection import train_test_split 

# Importing classification report and confusion matrix from sklearn metrics
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# Importing the PCA module
from sklearn.decomposition import PCA

# Importing random forest classifier from sklearn library
from sklearn.ensemble import RandomForestClassifier

# Importing Ridge, Lasso and GridSearch
from sklearn.linear_model import Ridge
from sklearn.linear_model import Lasso
from sklearn.model_selection import GridSearchCV

# Importing XGBoost libraries
from sklearn.ensemble import AdaBoostClassifier

import gc # for deleting unused variables

# Importing the below library and configuring to display all columns in a dataframe
from IPython.display import display
pd.options.display.max_columns = None

# Input data files are available in the ""../input/"" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# Any results you write to the current directory are saved as output.
",1,Code Smell
346,29148454,1,"training_df = pd.read_csv('/kaggle/input/widsdatathon2020/training_v2.csv')
training_df.head()",1,Code Smell
347,29148454,2,"test_df = pd.read_csv('/kaggle/input/widsdatathon2020/unlabeled.csv')
df_unlabel = test_df.copy()
test_df.head()",1,Code Smell
348,29148454,3,"print(training_df.shape)
print(test_df.shape)",0,No Code Smell
349,29148454,4,train_len = len(training_df),0,No Code Smell
350,29148454,5,"training_df = pd.concat(objs = [training_df,test_df], axis = 0)
training_df.shape",0,No Code Smell
351,29148454,6,training_df.info(verbose = True),1,Code Smell
352,29148454,7,training_df.isnull().sum(),0,No Code Smell
353,29148454,8,"# percentage of missing values in columns greater than 50%
null_cols = training_df.columns[round(training_df.isnull().sum()/len(training_df.index)*100,2) > 50].tolist()
null_cols",0,No Code Smell
354,29148454,9,"# deleting cols having missing %age greater than 50%
print(training_df.shape)
training_df = training_df.drop(null_cols,axis = 1)
print(training_df.shape)",0,No Code Smell
355,29148454,10,"# numeric columns
df_numeric = training_df.select_dtypes(include=['int16', 'int32', 'int64', 'float16', 'float32', 'float64'])
df_numeric.head() ",0,No Code Smell
356,29148454,11,"# missing values in columns
num_null_cols = df_numeric.columns[df_numeric.isnull().any()].tolist()
print(num_null_cols)
print(len(num_null_cols))",0,No Code Smell
357,29148454,12,"# dividing the null columns in 5 part 
num_null_cols_1 = num_null_cols[:19]
num_null_cols_2 = num_null_cols[19:38]
num_null_cols_3 = num_null_cols[38:57]
num_null_cols_4 = num_null_cols[57:76]
num_null_cols_5 = num_null_cols[76:]",0,No Code Smell
358,29148454,13,"# Visualizing first part of num_null_cols which is num_null_cols_1 using box_plot
plt.figure(figsize=(15,15))
sns.boxplot(x=""value"", y=""variable"", data=pd.melt(df_numeric[num_null_cols_1]))
plt.show()",0,No Code Smell
359,29148454,14,"# Visualizing first part of num_null_cols which is num_null_cols_2 using box_plot
plt.figure(figsize=(15,15))
sns.boxplot(x=""value"", y=""variable"", data=pd.melt(df_numeric[num_null_cols_2]))
plt.show()",0,No Code Smell
360,29148454,15,"# Visualizing first part of num_null_cols which is num_null_cols_3 using box_plot
plt.figure(figsize=(15,15))
sns.boxplot(x=""value"", y=""variable"", data=pd.melt(df_numeric[num_null_cols_3]))
plt.show()",0,No Code Smell
361,29148454,16,"# Visualizing first part of num_null_cols which is num_null_cols_4 using box_plot
plt.figure(figsize=(15,15))
sns.boxplot(x=""value"", y=""variable"", data=pd.melt(df_numeric[num_null_cols_4]))
plt.show()",0,No Code Smell
362,29148454,17,"# Visualizing first part of num_null_cols which is num_null_cols_5 using box_plot
plt.figure(figsize=(15,15))
sns.boxplot(x=""value"", y=""variable"", data=pd.melt(df_numeric[num_null_cols_5]))
plt.show()",0,No Code Smell
363,29148454,18,"# imputing missing values
df_numeric = df_numeric.fillna(df_numeric.median())
print(df_numeric.isnull().sum())",0,No Code Smell
364,29148454,19,"# non numeric columns
df_non_numeric = training_df.select_dtypes(exclude=['int16', 'int32', 'int64', 'float16', 'float32', 'float64'])
print(df_non_numeric.head())",0,No Code Smell
365,29148454,20,"# percentage of missing values
round(df_non_numeric.isnull().sum()/len(training_df.index)*100,2)",0,No Code Smell
366,29148454,21,"# Visualizing them using countplot
plt.figure(figsize=(10,15))
plt.subplot(4,1,1)
sns.countplot(y = 'ethnicity',data= df_non_numeric)
plt.subplot(4,1,2)
sns.countplot(y = 'gender',data= df_non_numeric)
plt.subplot(4,1,3)
sns.countplot(y = 'hospital_admit_source',data= df_non_numeric)
plt.subplot(4,1,4)
sns.countplot(y = 'icu_admit_source',data= df_non_numeric)
plt.show()",0,No Code Smell
367,29148454,22,"# Visualizing them using countplot
plt.figure(figsize=(10,15))
plt.subplot(4,1,1)
sns.countplot(y = 'icu_stay_type',data= df_non_numeric)
plt.subplot(4,1,2)
sns.countplot(y = 'icu_type',data= df_non_numeric)
plt.subplot(4,1,3)
sns.countplot(y = 'apache_3j_bodysystem',data= df_non_numeric)
plt.subplot(4,1,4)
sns.countplot(y = 'apache_2_bodysystem',data= df_non_numeric)
plt.show()",0,No Code Smell
368,29148454,23,"for column in df_non_numeric.columns:
    df_non_numeric[column].fillna(df_non_numeric[column].mode()[0], inplace = True)",0,No Code Smell
369,29148454,24,df_non_numeric.isnull().sum(),0,No Code Smell
370,29148454,25,"# merging df_numeric and df_non_numeric on their index
df_train = pd.concat([df_numeric, df_non_numeric], axis=1)
df_train.head()",0,No Code Smell
371,29148454,26,df_train.shape,0,No Code Smell
372,29148454,27,"# checking outliers
df_train.describe(percentiles=[.25,.5,.75,.90,.95,.99])",1,Code Smell
373,29148454,28,"df_train['hospital_death'].value_counts().plot('bar')
plt.show()",0,No Code Smell
374,29148454,29,df_train['hospital_death'].sum()/len(df_train['hospital_death'].index)*100,0,No Code Smell
375,29148454,30,"# get correlation of 'hospital_death' with other variables
plt.figure(figsize=(30,9))
df_train.corr()['hospital_death'].sort_values(ascending = False).plot('bar')
plt.show()",0,No Code Smell
376,29148454,31,df_train.head(),0,No Code Smell
377,29148454,32,df_train.shape,0,No Code Smell
378,29148454,33,"df_train = df_train.drop(['encounter_id','patient_id'], axis = 1)
df_train.columns",0,No Code Smell
379,29148454,34,"import copy
train = copy.copy(df_train[:train_len])
test = copy.copy(df_train[train_len:])
print('train             ', train.shape)
print('test              ', test.shape)",0,No Code Smell
380,29148454,35,"X = train.drop('hospital_death',1)
X.head()",0,No Code Smell
381,29148454,36,X.shape,0,No Code Smell
382,29148454,37,"y = train['hospital_death']
y.head()",0,No Code Smell
383,29148454,38,"test = test.drop('hospital_death', axis = 1)
test.shape",0,No Code Smell
384,29148454,39,X.info(verbose=True),1,Code Smell
385,29148454,40,X.shape,0,No Code Smell
386,29148454,41,test.shape,0,No Code Smell
387,29148454,42,X_len = len(X),0,No Code Smell
388,29148454,43,"combined_df = pd.concat(objs = [X,test], axis = 0)
combined_df.shape",0,No Code Smell
389,29148454,44,"combined_df[['elective_surgery','readmission_status','apache_post_operative','arf_apache','gcs_unable_apache','intubated_apache','ventilated_apache','aids','cirrhosis','diabetes_mellitus','hepatic_failure','immunosuppression','leukemia','lymphoma','solid_tumor_with_metastasis']] = combined_df[['elective_surgery','readmission_status','apache_post_operative','arf_apache','gcs_unable_apache','intubated_apache','ventilated_apache','aids','cirrhosis','diabetes_mellitus','hepatic_failure','immunosuppression','leukemia','lymphoma','solid_tumor_with_metastasis']].astype(object)",0,No Code Smell
390,29148454,45,"combined_df_categorical = combined_df[['elective_surgery','readmission_status','apache_post_operative','arf_apache','gcs_unable_apache',
                   'intubated_apache','ventilated_apache','aids','cirrhosis','diabetes_mellitus','hepatic_failure',
                   'immunosuppression','leukemia','lymphoma','solid_tumor_with_metastasis','ethnicity','gender','hospital_admit_source',
                   'icu_stay_type','icu_type','apache_3j_bodysystem','apache_2_bodysystem','icu_admit_source']]

# convert into dummies
combined_dummies = pd.get_dummies(combined_df_categorical, drop_first=True)

# drop cateorical variables from X dataframe
combined_df = combined_df.drop(combined_df_categorical, axis = 1)

# concat dummy variables with X dataframe
combined_df = pd.concat([combined_df, combined_dummies], axis = 1)
print(combined_df.shape)",0,No Code Smell
391,29148454,46,"import copy
X = copy.copy(combined_df[:X_len])
test = copy.copy(combined_df[X_len:])
print('X             ', X.shape)
print('test          ', test.shape)",0,No Code Smell
392,29148454,47,X.corr(),1,Code Smell
393,29148454,48,"# create correlation matrix
corr_matrix = X.corr().abs()

# select upper triangle of correlation matrix
upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape),k=1).astype(np.bool))

# find index of feature columns with correlation greater than 0.80
to_drop = [column for column in upper.columns if any((upper[column]>0.80))]
to_drop",1,Code Smell
394,29148454,49,"X = X.drop(to_drop,1)
print(X.shape)",0,No Code Smell
395,29148454,50,"test = test.drop(to_drop ,1)
print(test.shape)",0,No Code Smell
396,29148454,51,"# plotting heat map to see correlation 
plt.figure(figsize=(15,10))
sns.heatmap(data = X.corr())
plt.show()",0,No Code Smell
397,29148454,52,X.info(verbose = True),1,Code Smell
398,29148454,53,"# standardization of X
scaler = preprocessing.StandardScaler().fit(X)
X = scaler.transform(X)
X = pd.DataFrame(X)

# test
test = scaler.transform(test)
test = pd.DataFrame(test)",0,No Code Smell
399,29148454,54,test.shape,0,No Code Smell
400,29148454,55,"# splitting the data into train and test
X_train, X_test, y_train, y_test = train_test_split(X,y, train_size = 0.7, test_size = 0.3, random_state = 100)",0,No Code Smell
401,29148454,56,"X_orig = X.copy()
y_orig = y.copy()",0,No Code Smell
402,29148454,57,"from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics

X = X_orig.copy()
y = y_orig.copy()

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=100)

model_rf = RandomForestClassifier()
model_rf.fit(X_train, y_train)

# Make predictions
prediction_test = model_rf.predict(X_test)
print(classification_report(y_test,prediction_test))",0,No Code Smell
403,29148454,58,"# Printing confusion matrix
print(confusion_matrix(y_test, prediction_test))",0,No Code Smell
404,29148454,59,"# Defining a genric function to calculate sensitivity
def sensitivity_score(inp_y_test, inp_y_pred):
    positives = confusion_matrix(inp_y_test,inp_y_pred)[1]
    # print('True Positives: ', positives[1], ' False Positives: ', positives[0])
    return (positives[1]/(positives[1] + positives[0]))",0,No Code Smell
405,29148454,60,"print ('Random Forest Accuracy with Default Hyperparameter', metrics.accuracy_score(y_test, prediction_test))
print ('Random Forest Sensitivity with Default Hyperparameter', sensitivity_score(y_test, prediction_test))",0,No Code Smell
406,29148454,61,"import lightgbm as lgb
from sklearn.model_selection import RandomizedSearchCV
clf = lgb.LGBMClassifier(silent=True, random_state = 304, metric='roc_auc', n_jobs=4)",0,No Code Smell
407,29148454,62,"from scipy.stats import randint as sp_randint
from scipy.stats import uniform as sp_uniform
params ={'cat_smooth' : sp_randint(1, 100), 'min_data_per_group': sp_randint(1,1000), 'max_cat_threshold': sp_randint(1,100)}",0,No Code Smell
408,29148454,63,"fit_params={""early_stopping_rounds"":2, 
            ""eval_metric"" : 'auc', 
            ""eval_set"" : [(X_train, y_train),(X_test,y_test)],
            'eval_names': ['train','valid'],
            'verbose': 300,
            'categorical_feature': 'auto'}",0,No Code Smell
409,29148454,64,"gs = RandomizedSearchCV( estimator=clf, param_distributions=params, scoring='roc_auc',cv=3, refit=True,random_state=304,verbose=True)",0,No Code Smell
410,29148454,65,"gs.fit(X_train, y_train, **fit_params)
print('Best score reached: {} with params: {} '.format(gs.best_score_, gs.best_params_))",0,No Code Smell
411,29148454,66,"gs.best_params_, gs.best_score_",0,No Code Smell
412,29148454,67,"# clf2 = lgb.LGBMClassifier(random_state=304, metric = 'roc_auc', n_jobs=4)
clf2 = lgb.LGBMClassifier(random_state=304, metric = 'roc_auc', cat_smooth = 32, max_cat_threshold = 75, min_data_per_group = 82, n_jobs=4)",0,No Code Smell
413,29148454,68,"params_2 = {'learning_rate': [0.04, 0.05, 0.08],   
            'num_iterations': [1400,1600, 1800]}",0,No Code Smell
414,29148454,69,"gs2 = GridSearchCV(clf2,params_2, scoring='roc_auc',cv=3)",0,No Code Smell
415,29148454,70,"gs2.fit(X_train, y_train, **fit_params)
print('Best score reached: {} with params: {} '.format(gs2.best_score_, gs2.best_params_))",0,No Code Smell
416,29148454,71,"gs2.best_params_, gs2.best_score_",0,No Code Smell
417,29148454,72,"params_2 = {
 'bagging_fraction': 0.4,
 'boosting': 'dart',
 'num_iterations': 1400, 
 'learning_rate': 0.04,
 'colsample_bytree': 0.5048747931447324,
 'cat_smooth': 32, 
 'max_cat_threshold':75, 
 'min_data_per_group': 82,
 'max_bin': 1312,
 'max_depth': 12,
 'num_leaves': 4090,
 'min_child_samples': 407,
 'min_child_weight': 0.1,
 'min_data_in_leaf': 2420,
 'reg_alpha': 0.1,
 'reg_lambda': 20,
 'scale_pos_weight': 3,
 'subsample': 0.7340872997512691,
 'subsample_for_bin': 512,
 'scoring': 'roc_auc',
 'metric': 'auc',
 'objective': 'binary'}",0,No Code Smell
418,29148454,73,"lgbm_train2 = lgb.Dataset(X_train, y_train)
lgbm_val2 = lgb.Dataset(X_test, y_test)",0,No Code Smell
419,29148454,74,"evals_result = {}  # to record eval results for plotting
model_lgbm_2 = lgb.train(params_2,
                lgbm_train2,
                num_boost_round=250,
                valid_sets=[lgbm_train2, lgbm_val2],
                feature_name=['f' + str(i + 1) for i in range(X_train.shape[-1])],
                categorical_feature= [182],
                evals_result=evals_result,
                verbose_eval=100)",0,No Code Smell
420,29148454,75,"ax = lgb.plot_metric(evals_result, metric='auc', figsize=(15, 8))
plt.show()",0,No Code Smell
421,29148454,76,"df_unlabel[""hospital_death""] = model_lgbm_2.predict(test, pred_contrib=False)",0,No Code Smell
422,29148454,77,df_unlabel.shape,0,No Code Smell
423,29148454,78,df_unlabel.head(),0,No Code Smell
424,29148454,79,"df_unlabel[['encounter_id','hospital_death']].to_csv('submission.csv',index = False)
df_unlabel[['encounter_id','hospital_death']].head()",0,No Code Smell
425,29148454,80,,0,No Code Smell
426,28395914,0,"# Necessary libraries
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import LabelEncoder
import statsmodels.api as sm
from tqdm.notebook import tqdm
from scipy import stats",1,Code Smell
427,28395914,1,"# Import dataframes
df_train = pd.read_csv(""/kaggle/input/widsdatathon2020/training_v2.csv"")
df_test = pd.read_csv(""/kaggle/input/widsdatathon2020/unlabeled.csv"")",1,Code Smell
428,28395914,2,"# Convert categorical labels into numerical values
categorical_columns = ['gender', 'apache_2_bodysystem', 'ethnicity', 'apache_3j_bodysystem', 
    'icu_admit_source', 'icu_stay_type', 'apache_2_diagnosis', 'apache_3j_diagnosis', 'icu_type']

cat_labenc_mapping = {
    col: LabelEncoder()
    for col in categorical_columns
}

for col in tqdm(categorical_columns):
    df_train[col] = df_train[col].astype('str')
    cat_labenc_mapping[col] = cat_labenc_mapping[col].fit(
        np.unique(df_train[col].unique().tolist() + df_test[col].unique().tolist())
    )
    df_train[col] = cat_labenc_mapping[col].transform(df_train[col])
    
    df_test[col] = df_test[col].astype('str')
    df_test[col] = cat_labenc_mapping[col].transform(df_test[col])",0,No Code Smell
429,28395914,3,"predictives = ['height','diabetes_mellitus']
dependents = ['weight']

#Load in the data columns we need and drop NA rows
test = df_train[(predictives+dependents)].dropna()

#Add the intercept to the model
X2 = sm.add_constant(test[predictives])

#create regression object and fit it
estWeight = sm.OLS(test[dependents], X2).fit()
print(estWeight.summary())",0,No Code Smell
430,28395914,4,"cw = estWeight.params[0] # constant
h = estWeight.params[1] # height
db = estWeight.params[2] # diabetes mellitus

index = df_train['weight'].isna() & ~df_train['height'].isna() & ~df_train['diabetes_mellitus'].isna()
n = 0
for idx,row in df_train[index].iterrows():
    df_train.loc[idx,'weight'] = cw + df_train.loc[idx,'height'] * h + df_train.loc[idx,'diabetes_mellitus'] * db
    n+=1
print('Filled up '+str(n)+' weight values')",0,No Code Smell
431,28395914,5,"predictives = ['weight','gender','ethnicity']
dependents = ['height']

#Load in the data columns we need and drop NA rows
test = df_train[(predictives+dependents)].dropna()

#Add the intercept to the model
X2 = sm.add_constant(test[predictives])

#create regression object and fit it
estHeight = sm.OLS(test[dependents], X2).fit()
print(estHeight.summary())",0,No Code Smell
432,28395914,6,"ch = estHeight.params[0] # constant
w = estHeight.params[1] # weight
g = estHeight.params[2] # gender
e = estHeight.params[3] # ethnicity

index = df_train['height'].isna() & ~df_train['weight'].isna() & ~df_train['gender'].isna() & ~df_train['ethnicity'].isna()
n = 0
for idx,row in df_train[index].iterrows():
    df_train.loc[idx,'height'] = ch + df_train.loc[idx,'weight'] * w + df_train.loc[idx,'gender'] * g + df_train.loc[idx,'ethnicity'] * e
    n+=1
print('Filled up '+str(n)+' height values')",0,No Code Smell
433,28395914,7,"index = df_train['bmi'].isna()
for idx,row in df_train[index].iterrows():
    df_train.loc[idx,'bmi'] = df_train.loc[idx,'weight'] / (df_train.loc[idx,'height']/100)**2

print('Calculated '+str(len(df_train[index]))+' bmi values')",0,No Code Smell
434,28577122,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the ""../input/"" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# Any results you write to the current directory are saved as output.

import warnings
warnings.filterwarnings(""ignore"")

import seaborn as sns
import math
import matplotlib as p
import matplotlib.pyplot as plt
%matplotlib inline
import scipy.stats as sps
import re",1,Code Smell
435,28577122,1,"train = pd.read_csv('../input/widsdatathon2020/training_v2.csv')
test = pd.read_csv('../input/widsdatathon2020/unlabeled.csv')
st = pd.read_csv('../input/widsdatathon2020/solution_template.csv')
ss = pd.read_csv('../input/widsdatathon2020/samplesubmission.csv')
dictionary = pd.read_csv('../input/widsdatathon2020/WiDS Datathon 2020 Dictionary.csv')

pd.set_option('display.max_columns', 500)
print('solution template shape', st.shape)
display(st.head())
print('dictionary shape', dictionary.shape)
display(dictionary.T.head())
print('train shape', train.shape)
display(train.head())
print('test shape', test.shape)
display(test.head())",1,Code Smell
436,28577122,2,"# Dropping patient_id for now
train = train.copy().drop('patient_id', axis = 1)
test = test.copy().drop('patient_id', axis = 1)",0,No Code Smell
437,28577122,3,"from sklearn.model_selection import train_test_split

Train, Validation = train_test_split(train, test_size = 0.3)",0,No Code Smell
438,28577122,4,"X_train = Train.copy().drop('hospital_death', axis = 1)
y_train = Train[['encounter_id','hospital_death']]
X_val = Validation.copy().drop('hospital_death', axis = 1)
y_val = Validation[['encounter_id','hospital_death']]",0,No Code Smell
439,28577122,5,"X_test = test.copy().drop('hospital_death', axis = 1)
y_test = test[['encounter_id','hospital_death']]",0,No Code Smell
440,28577122,6,"sns.catplot('hospital_death', data= train, kind='count', alpha=0.7, height=6, aspect=1)

# Get current axis on current figure
ax = plt.gca()

# Max value to be set
y_max = train['hospital_death'].value_counts().max() 

# Iterate through the list of axes' patches
for p in ax.patches:
    ax.text(p.get_x() + p.get_width()/5., p.get_height(),'%d' % int(p.get_height()),
            fontsize=13, color='blue', ha='center', va='bottom')
plt.title('Frequency plot of Hospital Deaths', fontsize = 20, color = 'black')
plt.show()",0,No Code Smell
441,28577122,7,"plt.figure(figsize=(30,15))
ethnicity_vs_death = sns.catplot(x='ethnicity', col='hospital_death', kind='count', data=train, 
                                 order = train['ethnicity'].value_counts().index, height = 7, aspect = 1);
ethnicity_vs_death.set_xticklabels(rotation=90);",0,No Code Smell
442,28577122,8,"plt.figure(figsize=(30,15))
has_vs_death = sns.catplot(x='hospital_admit_source', col='hospital_death', kind='count', data=train, 
                           order = train['hospital_admit_source'].value_counts().index, height = 7, aspect = 1.5);
has_vs_death.set_xticklabels(rotation=90);",0,No Code Smell
443,28577122,9,"plt.figure(figsize=(30,15))
ias_vs_death = sns.catplot(x='icu_admit_source', col='hospital_death', kind='count', data=train, 
                           order = train['icu_admit_source'].value_counts().index, height = 7, aspect = 1.5);
ias_vs_death.set_xticklabels(rotation=90);",0,No Code Smell
444,28577122,10,"# Freq plot of Hospital ID for hospital_death = 0
train_hID_0 = train[train['hospital_death'] == 0]
plt.figure(figsize=(30,15))
hID_vs_death = sns.catplot(y='hospital_id',  orient = ""v"", kind='count', data=train_hID_0, order = train_hID_0['hospital_id'].value_counts().index, 
                           height = 30, aspect = 1)",0,No Code Smell
445,28577122,11,"# Freq plot of Hospital ID for hospital_death = 1
train_hID_1 = train[train['hospital_death'] != 0]
plt.figure(figsize=(30,20))
hID_vs_death = sns.catplot(y='hospital_id',  orient = ""v"", kind='count', data=train_hID_1, order = train_hID_1['hospital_id'].value_counts().index, 
                           height = 30, aspect = 1);",0,No Code Smell
446,28577122,12,"# Freq plot of Hospital ID for hospital_death = 0 & 1
plt.figure(figsize=(30,40))
hID_vs_death = sns.catplot(x = 'hospital_id', col='hospital_death', kind='count', data=train, order = train['hospital_id'].value_counts().index, 
                           height = 5, aspect = 2.8);
hID_vs_death.set_xticklabels(rotation=90);",0,No Code Smell
447,28577122,13,"plt.figure(figsize = (15,5))
sns.kdeplot(train_hID_1['age'], shade=True, color=""r"")
sns.kdeplot(train_hID_0['age'], shade=True, color=""b"")",0,No Code Smell
448,28577122,14,"sns.jointplot(x=""age"", y=""bmi"", data=train, kind = ""kde"")
sns.jointplot(x=""age"", y=""height"", data=train, kind = ""kde"")
sns.jointplot(x=""age"", y=""weight"", data=train, kind = ""kde"")",0,No Code Smell
449,28577122,15,"dataset = pd.concat(objs=[X_train, X_val], axis=0)",0,No Code Smell
450,28577122,16,col_1 = dataset.columns,0,No Code Smell
451,28577122,17,"for i in col_1:
    if X_train[i].nunique() == 1:
        print('in Train', i)
    if X_val[i].nunique() == 1:
        print('in Val', i)
    if X_test[i].nunique() == 1:
        print('in Test', i)
    ",0,No Code Smell
452,28577122,18,"# Dropping 'readmission_status'
X_train = X_train.drop(['readmission_status'], axis=1)
X_val = X_val.drop(['readmission_status'], axis=1)
X_test = X_test.drop(['readmission_status'], axis=1)",0,No Code Smell
453,28577122,19,"print('For Train')
d1 = X_train.nunique()
print(sorted(d1))
print(""=============================="")
print('For Validation')
d2 = X_val.nunique()
print(sorted(d2))

# Considering columns with <= 15 unique values for conversion",0,No Code Smell
454,28577122,20,"d = pd.concat(objs=[X_train, X_val], axis=0)",0,No Code Smell
455,28577122,21,col = d.columns ,0,No Code Smell
456,28577122,22,"# For Train data
l1 = []
for i in col:
    if X_train[i].nunique() <= 15:
        l1.append(i)
        
l1",0,No Code Smell
457,28577122,23,"# For Val data
l2 = []
for i in col:
    if X_val[i].nunique() <= 15:
        l2.append(i)
        
l2",0,No Code Smell
458,28577122,24,"# For Test data
l3 = []
for i in col:
    if X_test[i].nunique() <= 15:
        l3.append(i)
        
l3",0,No Code Smell
459,28577122,25,"# Checking for columns in X_train and X_validation
set(l1) & set(l2)",0,No Code Smell
460,28577122,26,"# Checking for columns in X_train and X_test
set(l1) & set(l3)",0,No Code Smell
461,28577122,27,"print('Train', len(l1))
print('Validation', len(l2))
print('Common', len(set(l1) & set(l2)))",0,No Code Smell
462,28577122,28,"print('Train', len(l1))
print('Test', len(l3))
print('Common', len(set(l1) & set(l3)))",0,No Code Smell
463,28577122,29,X_train[l1].dtypes,0,No Code Smell
464,28577122,30,"X_val[l2].dtypes
# Not a necessary step since we already confirmed the common columns. Included just for reference. ",0,No Code Smell
465,28577122,31,"X_train[l1] = pd.Categorical(X_train[l1])
X_val[l2] = pd.Categorical(X_val[l2])
X_test[l3] = pd.Categorical(X_test[l3])
print('Train dtypes:')
print(X_train[l1].dtypes)
print('======================================')
print('Validation dtypes:')
print(X_val[l2].dtypes)
print('======================================')
print('Test dtypes:')
print(X_test[l3].dtypes)",0,No Code Smell
466,28577122,32,"# On train data
pd.set_option('display.max_rows', 500)
NA_col = pd.DataFrame(X_train.isna().sum(), columns = ['NA_Count'])
NA_col['% of NA'] = (NA_col.NA_Count/len(X_train))*100
NA_col.sort_values(by = ['% of NA'], ascending = False, na_position = 'first')",0,No Code Smell
467,28577122,33,"# On val data
pd.set_option('display.max_rows', 500)
NA_col = pd.DataFrame(X_val.isna().sum(), columns = ['NA_Count'])
NA_col['% of NA'] = (NA_col.NA_Count/len(X_val))*100
NA_col.sort_values(by = ['% of NA'], ascending = False, na_position = 'first')",0,No Code Smell
468,28577122,34,"# On test data
pd.set_option('display.max_rows', 500)
NA_col = pd.DataFrame(X_test.isna().sum(), columns = ['NA_Count'])
NA_col['% of NA'] = (NA_col.NA_Count/len(X_test))*100
NA_col.sort_values(by = ['% of NA'], ascending = False, na_position = 'first')",0,No Code Smell
469,28577122,35,"cols = X_train.columns
num_cols = X_train._get_numeric_data().columns
cat_cols = list(set(cols) - set(num_cols))
cat_cols",0,No Code Smell
470,28577122,36,"# Courtesy: https://www.kaggle.com/jayjay75/wids2020-lgb-starter-script
from sklearn.preprocessing import LabelEncoder,OneHotEncoder
for usecol in cat_cols:
    X_train[usecol] = X_train[usecol].astype('str')
    X_val[usecol] = X_val[usecol].astype('str')
    X_test[usecol] = X_test[usecol].astype('str')
    
    #Fit LabelEncoder
    le = LabelEncoder().fit(
            np.unique(X_train[usecol].unique().tolist()+
                      X_val[usecol].unique().tolist()+
                     X_test[usecol].unique().tolist()))

    #At the end 0 will be used for dropped values
    X_train[usecol] = le.transform(X_train[usecol])+1
    X_val[usecol]  = le.transform(X_val[usecol])+1
    X_test[usecol]  = le.transform(X_test[usecol])+1
    
    X_train[usecol] = X_train[usecol].replace(np.nan, 0).astype('int').astype('category')
    X_val[usecol]  = X_val[usecol].replace(np.nan, 0).astype('int').astype('category')
    X_test[usecol]  = X_test[usecol].replace(np.nan, 0).astype('int').astype('category')",0,No Code Smell
471,28577122,37,"X_train.set_index('encounter_id', inplace = True)
y_train.set_index('encounter_id', inplace = True)
X_val.set_index('encounter_id', inplace = True)
y_val.set_index('encounter_id', inplace = True)
X_test.set_index('encounter_id', inplace = True)
y_test.set_index('encounter_id', inplace = True)",0,No Code Smell
472,28577122,38,# y_test.hospital_death = y_test.hospital_death.fillna(0),0,No Code Smell
473,28577122,39,"# y_train['hospital_death'] = pd.Categorical(y_train['hospital_death'])
# y_train.dtypes",0,No Code Smell
474,28577122,40,"# y_test['hospital_death'] = pd.Categorical(y_test['hospital_death'])
# y_test.dtypes",0,No Code Smell
475,28577122,41,"# from sklearn.preprocessing import LabelEncoder,OneHotEncoder
# from pandas import Series
# l=LabelEncoder() 
# l.fit(y_train['hospital_death']) 
# l.classes_ 
# y_train['hospital_death']=Series(l.transform(y_train['hospital_death']))  #label encoding our target variable 
# y_train['hospital_death'].value_counts() ",0,No Code Smell
476,28577122,42,"# l.fit(y_test['hospital_death']) 
# l.classes_ 
#y_test['hospital_death'].fillna(0.0, inplace = True)
# y_test['hospital_death']=Series(l.transform(y_test['hospital_death']))  #label encoding our target variable 
# y_test['hospital_death'].value_counts() ",0,No Code Smell
477,28577122,43,"import lightgbm as lgbm

lgbm_train = lgbm.Dataset(X_train, y_train, categorical_feature=cat_cols)
# lgbm_test = lgbm.Dataset(X_test, y_test, categorical_feature=cat_cols)
lgbm_val = lgbm.Dataset(X_val, y_val, reference = lgbm_train)",0,No Code Smell
478,28577122,44,"params = {'feature_fraction': 0.9,
          'lambda_l1': 1,
          'lambda_l2': 1,
          'learning_rate': 0.01,
          'max_depth': 10,
          'metric': 'auc',
          'num_leaves': 500,
          'min_data_in_leaf': 100,
          'subsample_freq': 1,
          'scale_pos_weight':1,
          'metric': 'auc',
          'is_unbalance': 'true',
          'boosting': 'gbdt',
          'bagging_fraction': 0.5,
          'bagging_freq': 10,}",0,No Code Smell
479,28577122,45,"evals_result = {}  # to record eval results for plotting
model_lgbm = lgbm.train(params,
                lgbm_train,
                num_boost_round=100,
                valid_sets=[lgbm_train, lgbm_val],
                feature_name=['f' + str(i + 1) for i in range(X_train.shape[-1])],
                categorical_feature= [182],
                evals_result=evals_result,
                verbose_eval=10)",0,No Code Smell
480,28577122,46,"ax = lgbm.plot_metric(evals_result, metric='auc', figsize=(15, 8))
plt.show()",0,No Code Smell
481,28577122,47,"test[""hospital_death""] = model_lgbm.predict(X_test, predition_type = 'Probability')
test[[""encounter_id"",""hospital_death""]].to_csv(""submission_lgbm.csv"",index=False)",0,No Code Smell
482,28577122,48,"test[[""encounter_id"",""hospital_death""]].head()",0,No Code Smell
483,28722761,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
from sklearn.preprocessing import LabelEncoder
import os
from keras import regularizers
import tensorflow as tf
from sklearn import preprocessing
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import SGD, Adam
from tensorflow.keras.callbacks import ModelCheckpoint
from keras.callbacks import Callback, EarlyStopping
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import seaborn as sns
# roc curve and auc score
from sklearn import preprocessing
from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score",1,Code Smell
484,28722761,1,"# Input data files are available in the ""../input/"" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

trainData = pd.read_csv(""../input/widsdatathon2020/training_v2.csv"")
testData = pd.read_csv(""../input/widsdatathon2020/unlabeled.csv"")

# Any results you write to the current directory are saved as output.",1,Code Smell
485,28722761,2,"categorical_cols = [c for c in trainData.columns if (trainData[c].dtype != np.number)& (trainData[c].dtype != int) ]
Categorical_df= trainData[categorical_cols]
# for col in categorical_cols:
#     print(col, ""*****************"")
#     print(Categorical_df[col].value_counts(), Categorical_df[col].unique())",0,No Code Smell
486,28722761,3,"# frequency encoder
for col in ['ethnicity', 'hospital_admit_source', 'icu_admit_source', 'apache_3j_bodysystem', 'apache_2_bodysystem', 'icu_type']:
    trainData[col] = trainData[col].astype('str')
    freq = trainData.groupby(col).size()/len(trainData[col])
    trainData[col] = trainData[col].map(freq)
    
    testData[col] = testData[col].astype('str')
    freq = testData.groupby(col).size()/len(testData[col])
    testData[col] = testData[col].map(freq)

trainData.head()",0,No Code Smell
487,28722761,4,"# ordinal encoder
icu_st_dict={'admit':0,'readmit':1,'transfer':2}

trainData['icu_stay_type'] = trainData['icu_stay_type'].astype('str')
trainData['icu_stay_type'] = trainData['icu_stay_type'].map(icu_st_dict)

testData['icu_stay_type'] = testData['icu_stay_type'].astype('str')
testData['icu_stay_type'] = testData['icu_stay_type'].map(icu_st_dict)",0,No Code Smell
488,28722761,5,"trainData['icu_stay_type'].tail(10)
trainData['icu_type'].tail(10)",0,No Code Smell
489,28722761,6,"# label encoding the data gender
le = LabelEncoder()
for col in ['gender']:
    trainData[col] = trainData[col].astype('str')

    #Fit LabelEncoder
    le.fit(np.unique(trainData[col].unique()))

    #At the end 0 will be used for null values so we start at 1 
    trainData[col] = le.transform(trainData[col])+1
    trainData[col] = trainData[col].replace(np.nan, 0).astype('int')
    
    testData[col] = testData[col].astype('str')

    #Fit LabelEncoder
    le.fit(np.unique(testData[col].unique()))

    #At the end 0 will be used for null values so we start at 1 
    testData[col] = le.transform(testData[col])+1
    testData[col] = testData[col].replace(np.nan, 0).astype('int')

testData.head()",0,No Code Smell
490,28722761,7,"# replace na with mean for following categories

for col in ['age', 'bmi', 'weight', 'height']:
    mean = trainData[col].mean()
    trainData[col] = trainData[col].replace(np.nan, mean).astype('int')
    
    mean = testData[col].mean()
    testData[col] = testData[col].replace(np.nan, mean).astype('int')",0,No Code Smell
491,28722761,8,"x = testData.isnull().sum(axis=0)
x",0,No Code Smell
492,28722761,9,"trainData = trainData.replace(np.nan, 0).astype('int')
testData = testData.replace(np.nan, 0).astype('int')
testData.head(5)",0,No Code Smell
493,28722761,10,"to_drop = ['gender','ethnicity' ,'encounter_id', 'patient_id',  'hospital_death', 'hospital_id']
testDataOld = testData
trainLabel = trainData['hospital_death']
for col in to_drop:
    trainData = trainData.drop(col, axis = 1)
    testData = testData.drop(col, axis = 1)

trainData.head()",0,No Code Smell
494,28722761,11,"cols_with_missing = (col for col in y_test.columns if y_test[col].isnull().any())
for col in cols_with_missing:
    y_test[col + '_was_missing'] = y_test[col].isnull()
    y_test[col + '_was_missing'] = y_test[col].isnull()",0,No Code Smell
495,28722761,12,"x_train, y_test ,x_label, y_label = train_test_split(trainData, trainLabel, test_size=0.3, random_state=1)
std_scale = preprocessing.StandardScaler().fit(x_train)
x_train = std_scale.transform(x_train)
y_test  = std_scale.transform(y_test)
testData = std_scale.transform(testData)
x_train.shape",0,No Code Smell
496,28722761,13,"# from sklearn.linear_model import LinearRegression
# from sklearn.feature_selection import RFE
# from sklearn.feature_selection import SelectKBest, SelectFpr, f_classif

# reg = LinearRegression()
# x_train_new = SelectFpr(f_classif, alpha=0.01).fit_transform(x_train, x_label)
# fit = reg.fit(x_train_new, x_label)
# pred = fit.predict(y_test)
# print(testData.shape, pred.shape)

# auc = roc_auc_score(y_label, pred)
# fpr, tpr, thresholds = roc_curve(y_label, pred)
# plot_roc_curve(fpr, tpr)
# print(auc)
# pred = fit.predict(testData)
# testDataOld[""hospital_death""] = pred
# testDataOld[[""encounter_id"",""hospital_death""]].to_csv(""submission.csv"",index=False)
# testDataOld[[""encounter_id"",""hospital_death""]].head()",0,No Code Smell
497,28722761,14,"def plot_roc_curve(fpr, tpr):
    plt.plot(fpr, tpr, color='orange', label='ROC')
    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic (ROC) Curve')
    plt.legend()
    plt.show()",0,No Code Smell
498,28722761,15,"checkpoint_callback = ModelCheckpoint(""model.h5"", monitor='accuracy', save_best_only=True, save_freq=2)
y_test.shape",0,No Code Smell
499,28722761,16,"model = Sequential()
model.add(Dense(64, input_shape=(180,),kernel_initializer='normal', activation='sigmoid', name='fc1'))
model.add(Dense(32, activation='sigmoid',kernel_initializer='normal', name='fc3'))
model.add(Dense(1, name='output'))
optimizer = tf.keras.optimizers.RMSprop(0.0001)",0,No Code Smell
500,28722761,17,"model.compile(optimizer, loss='mse', metrics=['accuracy', 'mse'])",0,No Code Smell
501,28722761,18,"model.fit(x_train, x_label, batch_size=60, epochs=50, callbacks=[checkpoint_callback])",0,No Code Smell
502,28722761,19,"probs = model.predict_proba(y_test).flatten()
auc = roc_auc_score(y_label, probs)
fpr, tpr, thresholds = roc_curve(y_label, probs)
plot_roc_curve(fpr, tpr)
print(""AUC-ROC :"",auc)
probs",0,No Code Smell
503,28722761,20,"probstest = model.predict_proba(testData)
probstest = probstest[:]
print(probstest)
testDataOld[""hospital_death""] = probstest
testDataOld[[""encounter_id"",""hospital_death""]].to_csv(""submission3.csv"",index=False)
testDataOld[[""encounter_id"",""hospital_death""]].head()
testDataOld[[""encounter_id"",""hospital_death""]].head()",0,No Code Smell
504,28722761,21,"from IPython.display import FileLink
import os
os.chdir(r'/kaggle/working')
FileLink(r'submission3.csv')",0,No Code Smell
505,29100312,0,"
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt
import seaborn as sns


# Input data files are available in the ""../input/"" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))
         import os
    os.chdir(r'kaggle/working')

# Any results you write to the current directory are saved as output.",1,Code Smell
506,29100312,1,"import h2o
from h2o.automl import H2OAutoML
h2o.init()
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt
%matplotlib inline

import plotly as py
#plotly.offline doesn't push your charts to the clouds
import plotly.offline as pyo
import plotly.graph_objs as go
pyo.offline.init_notebook_mode()
import plotly.express as px

from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier ,AdaBoostClassifier,VotingClassifier
from sklearn.model_selection import train_test_split

import matplotlib.pyplot as plt
import seaborn as sns
# roc curve and auc score
from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score
from sklearn import preprocessing

pd.set_option('display.max_columns', None)",0,No Code Smell
507,29100312,2,"# loading dataset 
df= pd.read_csv(""../input/widsdatathon2020/training_v2.csv"")
test=pd.read_csv(""..//input/widsdatathon2020/unlabeled.csv"")",1,Code Smell
508,29100312,3,"df.sample(5)

",0,No Code Smell
509,29100312,4,"#By using the above code, maybe I can check how missing values vary by thresholds?
for x in range(30):
    df_check = df.dropna(thresh=x)
    print(x,"" variables = "",df_check.shape)",0,No Code Smell
510,29100312,5,"
# Drop columns based on threshold limit
threshold = len(df) * 0.60
df_thresh=df.dropna(axis=1, thresh=threshold)

# View columns in the dataset
#We can see that 74 columns have been dropped as they cant be used for predictions as they are missing lots of data
df_thresh.shape",0,No Code Smell
511,29100312,6,"# Drop columns based on threshold limit
threshold = len(test) * 0.50
test=test.dropna(axis=1, thresh=threshold)

# View columns in the dataset
#We can see that 74 columns have been dropped as they cant be used for predictions as they are missing lots of data
test.shape",0,No Code Smell
512,29100312,7,"#Getting the numerical data from the dataset
df_thresh._get_numeric_data()

",1,Code Smell
513,29100312,8,"#Checking for missing values in IDs
df_thresh[['encounter_id','hospital_id','patient_id','icu_id']].isnull().sum()",0,No Code Smell
514,29100312,9,"#Applying skelarn imputer for numerical values 
#Using mean as the imputation value as most of the numerical data are clinical data that can be approximated to mean
imputer_skdf = df_thresh._get_numeric_data()
colNames=imputer_skdf.columns;
from sklearn.impute import SimpleImputer
imputer = SimpleImputer(missing_values=np.nan, strategy='mean')
# Fit and transform to the parameters
imputer_skdf = pd.DataFrame(imputer.fit_transform(imputer_skdf))
imputer_skdf.columns = colNames;
# Checking for any null values
imputer_skdf.isna().sum()
imputer_skdf.info()",0,No Code Smell
515,29100312,10,"#Applying skelarn imputer for numerical values 
#Using mean as the imputation value as most of the numerical data are clinical data that can be approximated to mean
imputer_sktest =test._get_numeric_data()
colNames1=imputer_sktest.columns;
from sklearn.impute import SimpleImputer
imputer1 = SimpleImputer(missing_values=np.nan, strategy='mean')
# Fit and transform to the parameters
imputer_sktest = pd.DataFrame(imputer.fit_transform(imputer_skdf))
imputer_sktest.columns = colNames1;
# Checking for any null values
imputer_sktest.isna().sum()
imputer_sktest.info()",0,No Code Smell
516,29100312,11,"#categorical values
categ_df=df_thresh.select_dtypes(exclude=['int','float'])
column_names=categ_df.columns
column_names",0,No Code Smell
517,29100312,12,"#categorical values for test
categ_df1=test.select_dtypes(exclude=['int','float'])
column_names1=categ_df1.columns
column_names1

",0,No Code Smell
518,29100312,13,"# Replacing null values in categorical data with most frequent value for test set
imputer2 = SimpleImputer(missing_values=np.nan, strategy='most_frequent')
categ_df1 = pd.DataFrame(imputer2.fit_transform(categ_df1))
categ_df1.columns =column_names1;",0,No Code Smell
519,29100312,14,"# Replacing null values in categorical data with most frequent value
imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')
categ_df = pd.DataFrame(imputer.fit_transform(categ_df))
categ_df.columns =column_names;",0,No Code Smell
520,29100312,15,"#merging Categorical and numerical data to a single dataset for test set
categ_df1['encounter_id']=imputer_sktest.encounter_id

test=pd.merge(imputer_sktest,categ_df1,on='encounter_id')
test.shape",0,No Code Smell
521,29100312,16,"#merging Categorical and numerical data to a single dataset 
categ_df['encounter_id']=imputer_skdf.encounter_id

df1=pd.merge(imputer_skdf,categ_df,on='encounter_id')
df1.shape

",0,No Code Smell
522,29100312,17,"#Exploratory data anlaysis and feature engineering 
#Checking the gender distribution using a pie chart
labels = df1['gender'].value_counts().index
values = df1['gender'].value_counts().values

colors = ['#eba796', '#96ebda']

fig = {'data' : [{'type' : 'pie',
                  'name' : ""Patients by Gender: Pie chart"",
                 'labels' : df1['gender'].value_counts().index,
                 'values' : df1['gender'].value_counts().values,
                 'direction' : 'clockwise',
                 'marker' : {'colors' : ['#9cc359', '#e96b5c']}}], 'layout' : {'title' : 'Patients by Gender'}}

pyo.iplot(fig)",0,No Code Smell
523,29100312,18,"#Visualizing the ICU admit source Distribution 
#we could see that accident and emergency, operating room and floor are major contributors for ICU admissions
colors = ['#eba796', '#96ebda']


fig = {'data' : [{'type' : 'pie',
                  'name' : ""ICU admit source"",
                 'labels' : df1['icu_admit_source'].value_counts().index,
                 'values' : df1['icu_admit_source'].value_counts().values,
                 'direction' : 'clockwise',
                 'marker' : {'colors' : ['#9cc359', '#e96b5c']}}], 'layout' : {'title' : 'ICU admit source distribution'}}

pyo.iplot(fig)",0,No Code Smell
524,29100312,19,"#Visualizing the Hospital admit source Distribution 
#we could see that accident and emergency are major contributors for ICU admissions also floor constitute 11% of the hospital admissions 
colors = ['#eba796', '#96ebda']


fig = {'data' : [{'type' : 'pie',
                  'name' : ""Hospital admit source"",
                 'labels' : df1['hospital_admit_source'].value_counts().index,
                 'values' : df['hospital_admit_source'].value_counts().values,
                 'direction' : 'clockwise',
                 'marker' : {'colors' : ['#9cc359', '#e96b5c']}}], 'layout' : {'title' : 'hospital admit source distribution'}}

pyo.iplot(fig)",0,No Code Smell
525,29100312,20,"#Visualizing the Hospital admit source Distribution 
#we could see that accident and emergency are major contributors for ICU admissions also floor constitute 11% of the hospital admissions 
colors = ['#eba796', '#96ebda']


fig = {'data' : [{'type' : 'pie',
                  'name' : ""Hospital admit source"",
                 'labels' : df1['hospital_admit_source'].value_counts().index,
                 'values' : df['hospital_admit_source'].value_counts().values,
                 'direction' : 'clockwise',
                 'marker' : {'colors' : ['#9cc359', '#e96b5c']}}], 'layout' : {'title' : 'hospital admit source distribution'}}

pyo.iplot(fig)",0,No Code Smell
526,29100312,21,"#taking a subset of the data to understand the effect of features on dataset
df1['hospital_death'].value_counts(normalize=True)


",0,No Code Smell
527,29100312,22,"#Checking the distribution of the icu admit source
df1['icu_admit_source'].value_counts()",0,No Code Smell
528,29100312,23,"#Checking the distribution of icu_type
df1['icu_type'].value_counts()",0,No Code Smell
529,29100312,24,"#Visualizing the age 
#We can see that most hospital death occur for patients in the 60 -70 age group
fig=plt.figure() #Plots in matplotlib reside within a figure object, use plt.figure to create new figure
#Create one or more subplots using add_subplot, because you can't create blank figure
ax = fig.add_subplot(1,1,1)
#Variable
ax.hist(df1['age'],bins = 7) # Here you can play with number of bins

plt.title('Age distribution')
plt.xlabel('Age')
plt.ylabel('Patient')
plt.show()",0,No Code Smell
530,29100312,25,"#gender and #age distrubution
import seaborn as sns 
sns.violinplot(df1['age'], df1['gender']) #Variable Plot
sns.despine()",0,No Code Smell
531,29100312,26,"var = df1.groupby('gender').hospital_death.sum() #grouped sum of sales at Gender level
fig = plt.figure()
ax1 = fig.add_subplot(1,1,1)
ax1.set_xlabel('gender')
ax1.set_ylabel('Sum of deaths')
ax1.set_title(""Gender wise Sum of deaths"")
var.plot(kind='bar')",0,No Code Smell
532,29100312,27,"#Visualizing ethnicity
var = df1.groupby('ethnicity').hospital_death.sum() 
fig = plt.figure()
ax1 = fig.add_subplot(1,1,1)
ax1.set_xlabel('ethnicity')
ax1.set_ylabel('Sum of deaths')
ax1.set_title(""Ethnicity wise Sum of deaths"")
var.plot(kind='bar')",0,No Code Smell
533,29100312,28,"var = df1.groupby('bmi').hospital_death.sum()
fig = plt.figure()
ax1 = fig.add_subplot(1,1,1)
ax1.set_xlabel('bmi')
ax1.set_ylabel('Sum of deaths')
ax1.set_title(""BMI wise Sum of deaths"")
var.plot(kind='line')",0,No Code Smell
534,29100312,29,df1['apache_3j_diagnosis'].value_counts(),0,No Code Smell
535,29100312,30,df1['apache_2_bodysystem'].value_counts(),0,No Code Smell
536,29100312,31,"
#Visualizing hospital death by apache_2_bodySystem
#We could see that cardio vascular conditions account for the most hospital deaths
var = df1.groupby('apache_2_bodysystem').hospital_death.sum() 
fig = plt.figure()
ax1 = fig.add_subplot(1,1,1)
ax1.set_xlabel('apache_2_bodysystem')
ax1.set_ylabel('Sum of deaths')
ax1.set_title(""apache bodysystem wise Sum of deaths"")
var.plot(kind='bar')",0,No Code Smell
537,29100312,32,test.columns=df1.columns,0,No Code Smell
538,29100312,33,"df_encoded=pd.get_dummies(df1, columns=['ethnicity', 'gender', 'hospital_admit_source', 'icu_admit_source',
       'icu_stay_type', 'icu_type', 'apache_3j_bodysystem',
       'apache_2_bodysystem'])
df_encoded.columns",0,No Code Smell
539,29100312,34,"# creating independent features X and dependent feature Y
y =df_encoded['hospital_death']
X = df_encoded
X = df_encoded.drop(columns=['hospital_death'],axis='columns')
test=pd.get_dummies(test, columns=['ethnicity', 'gender', 'hospital_admit_source', 'icu_admit_source',
       'icu_stay_type', 'icu_type', 'apache_3j_bodysystem',
       'apache_2_bodysystem'])",0,No Code Smell
540,29100312,35,"# Split into training and validation set
X_train, valid_features, Y_train, valid_y = train_test_split(X, y, test_size = 0.25, random_state = 1)",0,No Code Smell
541,29100312,36,"# Gradient Boosting Classifier
GBC = GradientBoostingClassifier(random_state=1)",0,No Code Smell
542,29100312,37,"# Random Forest Classifier
RFC = RandomForestClassifier(n_estimators=100)",0,No Code Smell
543,29100312,38,"# Voting Classifier with soft voting 
votingC = VotingClassifier(estimators=[('rfc', RFC),('gbc',GBC)], voting='soft')
votingC = votingC.fit(X_train, Y_train)",0,No Code Smell
544,29100312,39,predict_y = votingC.predict(valid_features),0,No Code Smell
545,29100312,40,"def plot_roc_curve(fpr, tpr):
    plt.plot(fpr, tpr, color='orange', label='ROC')
    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic (ROC) Curve')
    plt.legend()
    plt.show()",0,No Code Smell
546,29100312,41,"probs = votingC.predict_proba(valid_features)
probs = probs[:, 1]
auc = roc_auc_score(valid_y, probs)
fpr, tpr, thresholds = roc_curve(valid_y, probs)
plot_roc_curve(fpr, tpr)
print(""AUC-ROC :"",auc)",0,No Code Smell
547,29100312,42,"test1 = test.copy()

test1[""hospital_death""] = votingC.predict(test)
test1.hospital_death =test1.hospital_death.astype(int)
test1.encounter_id =test1.encounter_id.astype(int)
test1[[""encounter_id"",""hospital_death""]].to_csv(""..//output/kaggle/working/submission5.csv"",index=False)
test1[[""encounter_id"",""hospital_death""]].head()
from IPython.display import FileLink
FileLink(r'submission5.csv')",0,No Code Smell
548,29100312,43,"
",0,No Code Smell
549,29100312,44,,0,No Code Smell
550,29100312,45,,0,No Code Smell
551,29100312,46,,0,No Code Smell
552,29100312,47,,0,No Code Smell
553,29100312,48,,0,No Code Smell
554,29100312,49,,0,No Code Smell
555,29100312,50,,0,No Code Smell
556,29100312,51,"
",0,No Code Smell
557,29100312,52,,0,No Code Smell
558,29100312,53,,0,No Code Smell
559,29100312,54,"
",0,No Code Smell
560,29100312,55,,0,No Code Smell
561,29100312,56,,0,No Code Smell
562,29100312,57,,0,No Code Smell
563,29100312,58,"
",0,No Code Smell
564,29100312,59,"
",0,No Code Smell
565,29100312,60,,0,No Code Smell
566,29100312,61,,0,No Code Smell
567,29100312,62,,0,No Code Smell
568,29100312,63,,0,No Code Smell
569,29100312,64,,0,No Code Smell
570,29100312,65,,0,No Code Smell
571,29100312,66,"
",0,No Code Smell
572,35177928,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the ""../input/"" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# Any results you write to the current directory are saved as output.",1,Code Smell
573,35177928,1,"import seaborn as sns
import lightgbm
import matplotlib.pyplot as plt
from decimal import *
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import train_test_split, cross_val_score,cross_validate
from xgboost import XGBClassifier
from sklearn import ensemble
from sklearn.ensemble import (RandomForestClassifier, ExtraTreesClassifier, VotingClassifier, AdaBoostClassifier, BaggingRegressor)
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, classification_report
from sklearn.metrics import mean_absolute_error
",1,Code Smell
574,35177928,2,"train_data = pd.read_csv('/kaggle/input/widsdatathon2020/training_v2.csv')
test_data = pd.read_csv('/kaggle/input/widsdatathon2020/unlabeled.csv')",1,Code Smell
575,35177928,3,"print(train_data.shape)
print(test_data.shape)",0,No Code Smell
576,35177928,4,train_data.describe(),0,No Code Smell
577,35177928,5,test_data.describe(),0,No Code Smell
578,35177928,6,"plt.figure(figsize=(15,9))
plt.xticks(rotation=90)
sns.countplot(x='age', hue='hospital_death', data= train_data);",0,No Code Smell
579,35177928,7,"sns.countplot(x='gender', hue='hospital_death', data=train_data);",0,No Code Smell
580,35177928,8,"sns.countplot(x='hospital_death', data=train_data);
",0,No Code Smell
581,35177928,9,"np.round(train_data['hospital_death'].value_counts()/train_data.shape[0]*100,2)",0,No Code Smell
582,35177928,10,train_data.head(),0,No Code Smell
583,35177928,11,"drop_columns = ['encounter_id','patient_id', 'hospital_id', 'icu_id']",0,No Code Smell
584,35177928,12,"train_data = train_data.drop(drop_columns, axis=1)
test_data = test_data.drop(drop_columns, axis=1)",0,No Code Smell
585,35177928,13,"# How much of the data is missing 
np.round(train_data.isna().sum()/train_data.shape[0]*100,2).sort_values(ascending=False)
",0,No Code Smell
586,35177928,14,"np.round(test_data.isna().sum()/train_data.shape[0]*100,2)
",0,No Code Smell
587,35177928,15,"null_values = train_data.isnull().sum()/len(train_data)*100
missing_features = null_values[null_values > 30].index
",0,No Code Smell
588,35177928,16,"train_data.drop(missing_features, axis=1, inplace=True)
train_data.shape",0,No Code Smell
589,35177928,17,"test_data.drop(missing_features, axis=1, inplace=True)
test_data.shape",0,No Code Smell
590,35177928,18,len(missing_features),0,No Code Smell
591,35177928,19,"#list of features to dummy
todummy_list = []

# Check how many unique categories I have 
for col_name in train_data.columns:
    if train_data[col_name].dtype == 'object':
        unique_cat = len(train_data[col_name].unique())
        todummy_list.append(col_name)
        print('Feature {col_name} has {unique_cat} unique categories'.format(col_name=col_name,unique_cat=unique_cat ))",0,No Code Smell
592,35177928,20,"print(train_data['ethnicity'].value_counts(normalize=True, ascending=False)*100)",0,No Code Smell
593,35177928,21,"train_data['ethnicity'] = ['Caucasian' if value == 'Caucasian' else 'Others' for value in train_data['ethnicity']]
test_data['ethnicity'] = ['Caucasian' if value == 'Caucasian' else 'Others' for value in test_data['ethnicity']]

print(train_data['ethnicity'].value_counts(normalize=True, ascending=False)*100)",0,No Code Smell
594,35177928,22,"print(train_data['hospital_admit_source'].value_counts(normalize=True, ascending=False)*100)",0,No Code Smell
595,35177928,23,"train_data['hospital_admit_source'] = ['Emergency Department' if value == 'Emergency Department' else 'Others' for value in train_data['hospital_admit_source']]
test_data['hospital_admit_source'] = ['Emergency Department' if value == 'Emergency Department' else 'Others' for value in test_data['hospital_admit_source']]

print(train_data['hospital_admit_source'].value_counts(normalize=True, ascending=False)*100)",0,No Code Smell
596,35177928,24,"print(train_data['icu_admit_source'].value_counts(normalize=True, ascending=False)*100)",0,No Code Smell
597,35177928,25,"train_data['icu_admit_source'] = ['Accident & Emergency' if value == 'Accident & Emergency' else 'Others' for value in train_data['icu_admit_source']]
test_data['icu_admit_source'] = ['Accident & Emergency' if value == 'Accident & Emergency' else 'Others' for value in test_data['icu_admit_source']]

print(train_data['icu_admit_source'].value_counts(normalize=True, ascending=False)*100)",0,No Code Smell
598,35177928,26,"print(train_data['icu_stay_type'].value_counts(normalize=True, ascending=False)*100)",0,No Code Smell
599,35177928,27,"train_data['icu_stay_type'] = ['admit' if value == 'admit' else 'Others' for value in train_data['icu_stay_type']]
test_data['icu_stay_type'] = ['admit' if value == 'admit' else 'Others' for value in test_data['icu_stay_type']]

print(train_data['icu_stay_type'].value_counts(normalize=True, ascending=False)*100)",0,No Code Smell
600,35177928,28,"print(train_data['icu_type'].value_counts(normalize=True, ascending=False)*100)",0,No Code Smell
601,35177928,29,"train_data['icu_type'] = ['Med-Surg ICU' if value == 'Med-Surg ICU' else 'Others' for value in train_data['icu_type']]
test_data['icu_type'] = ['Med-Surg ICU' if value == 'Med-Surg ICU' else 'Others' for value in test_data['icu_type']]

print(train_data['icu_type'].value_counts(normalize=True, ascending=False)*100)",0,No Code Smell
602,35177928,30,"print(train_data['apache_3j_bodysystem'].value_counts(normalize=True, ascending=False)*100)",0,No Code Smell
603,35177928,31,"print(train_data['apache_2_bodysystem'].value_counts(normalize=True, ascending=False)*100)",0,No Code Smell
604,35177928,32,"#function to dummy all the categorical variables used for modeling
def dummy_df(df, todummy_list):
    for feature in todummy_list:
        dummies = pd.get_dummies(df[feature], prefix= feature, dummy_na = False)
        df = df.drop(feature, 1)
        df = pd.concat([df, dummies], axis =1)
    return df",0,No Code Smell
605,35177928,33,"train_data = dummy_df(train_data, todummy_list)
test_data = dummy_df(test_data, todummy_list)",0,No Code Smell
606,35177928,34,"print(train_data.shape)
print(test_data.shape)",0,No Code Smell
607,35177928,35,"#Assign X a DataFrame of features and y as a Series of outcome variable
X = train_data.drop('hospital_death', 1)
y = train_data.hospital_death",0,No Code Smell
608,35177928,36,"X_test = test_data.drop('hospital_death', 1)
y_test = test_data.hospital_death",0,No Code Smell
609,35177928,37,"pd.DataFrame(X).fillna(X.median(), inplace=True)
pd.DataFrame(X_test).fillna(X_test.median(), inplace=True)",0,No Code Smell
610,35177928,38,X.isna().sum().sort_values(ascending=False),0,No Code Smell
611,35177928,39,"scaler = StandardScaler()
scaler.fit(X)
X_scaled = scaler.transform(X)",0,No Code Smell
612,35177928,40,"print('XGBClassifier Model:')
XGB_CV = pd.DataFrame(cross_validate(XGBClassifier(), X_scaled, y, cv = 3, return_train_score=True, scoring = ['accuracy', 'precision', 'recall', 'f1']))
XGB_CV.mean()",0,No Code Smell
613,35177928,41,import sklearn.feature_selection,1,Code Smell
614,35177928,42,"select = sklearn.feature_selection.SelectKBest(k=50)
selected_features = select.fit(X,y)
indices_selected = selected_features.get_support(indices=True)
col_names_selected = [X.columns[i] for i in indices_selected]

X_selected = X[col_names_selected]
",0,No Code Smell
615,35177928,43,col_names_selected,0,No Code Smell
616,35177928,44,"scaler_2 = StandardScaler()
scaler.fit(X_scaled)
X_scaled_2 = scaler.transform(X_scaled)
",0,No Code Smell
617,35177928,45,"print('LogisticRegression Model:')
log_CV = pd.DataFrame(cross_validate(LogisticRegression(), X_scaled_2, y, cv = 3, return_train_score=True, scoring = ['accuracy', 'precision', 'recall', 'f1']))
log_CV.mean()",0,No Code Smell
618,35177928,46,"print('XGBClassifier Model:')
XGB_CV = pd.DataFrame(cross_validate(XGBClassifier(), X_scaled_2, y, cv = 3, return_train_score=True, scoring = ['accuracy', 'precision', 'recall', 'f1']))
XGB_CV.mean()",0,No Code Smell
619,35177928,47,"print('AdaBoostClassifier Model:')
ada_model = pd.DataFrame(cross_validate(ensemble.AdaBoostClassifier(), X_selected, y, cv = 3, return_train_score=True, scoring = ['accuracy', 'precision', 'recall', 'f1']))
ada_model.mean()",0,No Code Smell
620,35177928,48,from imblearn.over_sampling import SMOTE,1,Code Smell
621,35177928,49,"# transform the dataset
oversample = SMOTE()
X, y = oversample.fit_resample(X, y)",0,No Code Smell
622,35177928,50,"sns.countplot(x=y, data=train_data);",0,No Code Smell
623,35177928,51,"np.round(y.value_counts()/len(y)*100,2)",0,No Code Smell
624,35177928,52,"scaler_3 = StandardScaler()
scaler.fit(X)
X_scaled_3= scaler.transform(X)
X_test_scaled = scaler.transform(X_test)",0,No Code Smell
625,35177928,53,"print('XGBClassifier Model:')
XGB_CV = pd.DataFrame(cross_validate(XGBClassifier(), X_scaled_3, y, cv = 3, return_train_score=True, scoring = ['accuracy', 'precision', 'recall', 'f1']))
XGB_CV.mean()",0,No Code Smell
626,35177928,54,"def print_confusion_matrix(confusion_matrix, class_names, figsize = (10,7), fontsize=18):
    """"""Prints a confusion matrix, as returned by sklearn.metrics.confusion_matrix, as a heatmap.
    
    Arguments
    ---------
    confusion_matrix: numpy.ndarray
        The numpy.ndarray object returned from a call to sklearn.metrics.confusion_matrix. 
        Similarly constructed ndarrays can also be used.
    class_names: list
        An ordered list of class names, in the order they index the given confusion matrix.
    figsize: tuple
        A 2-long tuple, the first value determining the horizontal size of the ouputted figure,
        the second determining the vertical size. Defaults to (10,7).
    fontsize: int
        Font size for axes labels. Defaults to 14.
        
    Returns
    -------
    matplotlib.figure.Figure
        The resulting confusion matrix figure
    """"""
    df_cm = pd.DataFrame(confusion_matrix, index=class_names, columns=class_names, )
    fig = plt.figure(figsize=figsize)
    try:
        heatmap = sns.heatmap(df_cm, annot=True, fmt=""d"")
    except ValueError:
        raise ValueError(""Confusion matrix values must be integers."")
    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)
    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)
    plt.xlim(0,len(class_names))
    plt.ylim(len(class_names),0)
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    return fig",0,No Code Smell
627,35177928,55,"def model_test(model, X, y):
    # perform train/val split
    X, X_test, y, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    model.fit(X, y,  eval_metric='auc')
    pred = model.predict(X_test)
    
    print('Mean Absolute Error:', mean_absolute_error(y_test,pred))
    print('Accuracy score:', accuracy_score(y_test, pred))
    conf = confusion_matrix(y_test, pred)
    print(classification_report(y_test, pred))
    print_confusion_matrix(conf, ['1', '0'])",0,No Code Smell
628,35177928,56,"model = XGBClassifier()
model_test(model, X_scaled_3, y)
#model.fit(X_scaled_3, y)",0,No Code Smell
629,35177928,57,y_test = model.predict(X_test_scaled),0,No Code Smell
630,35177928,58,"solution_template = pd.read_csv(""/kaggle/input/widsdatathon2020/solution_template.csv"")",1,Code Smell
631,35177928,59,"print(y_test.shape)
print(solution_template.shape)",0,No Code Smell
632,35177928,60,"solution_template.hospital_death = y_test
solution_template.to_csv(""Version_2.csv"", index=0)",0,No Code Smell
633,35177928,61,"from keras.models import Sequential
from keras.layers import Dense, Dropout",1,Code Smell
634,35177928,62,"# define the keras model
nn_model = Sequential()
nn_model.add(Dense(15, input_dim=X.shape[1], activation='relu'))
nn_model.add(Dense(10, activation='relu'))
nn_model.add(Dense(8, activation='relu'))
nn_model.add(Dropout(.2))
nn_model.add(Dense(1, activation='sigmoid'))",0,No Code Smell
635,35177928,63,"# compile the keras model
nn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
",0,No Code Smell
636,35177928,64,"# fit the keras model on the dataset
nn_model.fit(X, y, epochs=150, batch_size=10)",0,No Code Smell
637,35177928,65,"# save the model
nn_model.save('nn_model.h5')",0,No Code Smell
638,35177928,66,"# make class predictions with the model
predictions = nn_model.predict_classes(X_test)
",0,No Code Smell
639,35177928,67,"solution_template.hospital_death = predictions
solution_template.to_csv(""Version_3.csv"", index=0)",0,No Code Smell
640,35177928,68,,0,No Code Smell
641,35177928,69,,0,No Code Smell
642,29626958,0,"# data analysis and wrangling
import pandas as pd
import numpy as np
import random as rnd

# visualization
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline 

# machine learning
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler, normalize",0,No Code Smell
643,29626958,1,"import warnings
warnings.filterwarnings('ignore')",0,No Code Smell
644,29626958,2,"#Load the training and test dataset
train_df = pd.read_csv('/kaggle/input/widsdatathon2020/training_v2.csv')
test_df = pd.read_csv('/kaggle/input/widsdatathon2020/unlabeled.csv')",1,Code Smell
645,29626958,3,"#Know the features of the dataset
print(train_df.info())
print(train_df.shape)",1,Code Smell
646,29626958,4,"# Train missing values (in percent)
train_missing = (train_df.isnull().sum() / len(train_df)).sort_values(ascending = False)
train_missing.head()
train_missing = train_missing.index[train_missing > 0.75]
print('There are %d columns with more than 75%% missing values' % len(train_missing))
print('The missing columns are %s' % train_missing)
df_train = train_df.drop(columns = train_missing)
df_test = test_df.drop(columns = train_missing)
df_train.shape
df_test.shape",0,No Code Smell
647,29626958,5,"columns_to_drop = train_df[['patient_id', 'hospital_id','icu_id','readmission_status','hospital_death']]
target = train_df['hospital_death']
df_train = df_train.drop(columns = columns_to_drop)
df_test = df_test.drop(columns = columns_to_drop)",0,No Code Smell
648,29626958,6,"print(df_train.shape)
print(df_test.shape)",0,No Code Smell
649,29626958,7,"# Remove duplicates from training and test data
df_train = df_train.drop_duplicates(subset=None, keep='first', inplace=False).copy()
df_test = df_test.drop_duplicates(subset=None, keep='first', inplace=False).copy()
print(df_train.shape)
print(df_test.shape)",0,No Code Smell
650,29626958,8,"# Identifying the datatypes of the variables
continous_attrib = df_train.select_dtypes(include=np.number).columns
binary_attrib = df_train[['apache_post_operative', 'arf_apache', 'cirrhosis', 'diabetes_mellitus', 'immunosuppression',
'hepatic_failure', 'leukemia', 'lymphoma', 'solid_tumor_with_metastasis', 'gcs_unable_apache',
'intubated_apache', 'ventilated_apache']].columns
continous_attrib = continous_attrib.drop(binary_attrib)
categorical_attrib =   df_train.select_dtypes(include=['object']).columns
selected_attributes = list(set(continous_attrib)) + list(set(categorical_attrib)) + list(set(binary_attrib))
print(len(selected_attributes))
df_train,y_train = df_train[selected_attributes],target
df_test= df_test[selected_attributes]
df_train.head()
df_test.head()
",0,No Code Smell
651,29626958,9,"#Imputing features in train and test data
from sklearn.impute import SimpleImputer
# Replacing NAN values in numerical columns with mean

imputer_Num = SimpleImputer(missing_values=np.nan, strategy='mean')
imputer_Cat_Bin = SimpleImputer(missing_values=np.nan, strategy='most_frequent')

continous_attrib = list(set(continous_attrib)) 
categorical_attrib = list(set(categorical_attrib)) 
binary_attrib = list(set(binary_attrib))

#One hot coding for train data
df_Cat_attrib = pd.DataFrame(df_train[categorical_attrib])
df_Cat_OneHotCoded = pd.get_dummies(df_Cat_attrib)
df_Cat_OneHotCoded.head()

# Fit and transform to the parameters
df_imputed_Num = pd.DataFrame(imputer_Num.fit_transform(df_train[continous_attrib]))
df_imputed_Num.columns = continous_attrib

df_imputed_Cat = pd.DataFrame(imputer_Cat_Bin.fit_transform(df_Cat_OneHotCoded[df_Cat_OneHotCoded.columns]))
df_imputed_Cat.columns = df_Cat_OneHotCoded.columns

df_imputed_binary = pd.DataFrame(imputer_Num.fit_transform(df_train[binary_attrib]))
df_imputed_binary.columns = binary_attrib

train_imputed_df = pd.concat([df_imputed_Num,df_imputed_Cat,df_imputed_binary], axis=1).dropna()


#One hot coding for test data
df_Cat_attrib = pd.DataFrame(df_test[categorical_attrib])
df_Cat_OneHotCoded = pd.get_dummies(df_Cat_attrib)
df_Cat_OneHotCoded.head()

# Fit and transform to the parameters
df_imputed_Num = pd.DataFrame(imputer_Num.fit_transform(df_test[continous_attrib]))
df_imputed_Num.columns = continous_attrib

df_imputed_Cat = pd.DataFrame(imputer_Cat_Bin.fit_transform(df_Cat_OneHotCoded[df_Cat_OneHotCoded.columns]))
df_imputed_Cat.columns = df_Cat_OneHotCoded.columns

df_imputed_binary = pd.DataFrame(imputer_Num.fit_transform(df_test[binary_attrib]))
df_imputed_binary.columns = binary_attrib

test_imputed_df = pd.concat([df_imputed_Num,df_imputed_Cat,df_imputed_binary], axis=1).dropna()

train_imputed_df.head(5)
test_imputed_df.head(5)

",0,No Code Smell
652,29626958,10,"# The number of features in the train and test data are different.  Concat and use the get_dummies to even 
# them out such that training and test data have the same number of attributes
train_objs_num = len(train_imputed_df)
dataset = pd.concat(objs=[train_imputed_df, test_imputed_df], axis=0,sort='False')
train_X_df = dataset[:train_objs_num].copy()
test_X_df = dataset[train_objs_num:].copy()

train_X_df = train_X_df.round(decimals=2)
test_X_df = test_X_df.round(decimals=2)

train_X_df.columns.values
test_X_df.head(5)",0,No Code Smell
653,29626958,11,"#Distribution of train data
fig = plt.figure(figsize=(20,30))
for i in range(int(len(train_X_df.columns)-1)):
    fig.add_subplot(40,5,i+1)
    sns.distplot(train_X_df.iloc[:,i+1].dropna())
    plt.xlabel(train_X_df.columns[i])
plt.show()",0,No Code Smell
654,29626958,12,"threshold = 0.9
# Absolute value correlation matrix - train data
corr_matrix_train = train_X_df.corr().abs()
corr_matrix_train.head()

# test data
corr_matrix_test = test_X_df.corr().abs()
corr_matrix_test.head()",1,Code Smell
655,29626958,13,"# Upper triangle of correlations - train data
upper = corr_matrix_train.where(np.triu(np.ones(corr_matrix_train.shape), k=1).astype(np.bool))
# Select columns with correlations above threshold
to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]
print(to_drop)
print('There are %d columns to remove.' % (len(to_drop)))
#Drop the columns with high correlations
train_X_df = train_X_df.drop(columns = to_drop)",0,No Code Smell
656,29626958,14,"# Upper triangle of correlations - test data
upper = corr_matrix_test.where(np.triu(np.ones(corr_matrix_test.shape), k=1).astype(np.bool))
# Select columns with correlations above threshold
to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]
print(to_drop)
print('There are %d columns to remove.' % (len(to_drop)))
#Drop the columns with high correlations
test_X_df = test_X_df.drop(columns = to_drop)
train_X_df = train_X_df.drop(columns = 'd1_hematocrit_max')",0,No Code Smell
657,29626958,15,"print(train_X_df.shape)
print(test_X_df.shape)
",0,No Code Smell
658,29626958,16,"from sklearn.model_selection import train_test_split
X_train, X_eval, Y_train, Y_eval = train_test_split(train_X_df, y_train, test_size=0.15, stratify=y_train)
X_train.shape, X_eval.shape, Y_train.shape, Y_eval.shape",0,No Code Smell
659,29626958,17,"from sklearn.model_selection import  GridSearchCV, RandomizedSearchCV
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import KFold
from xgboost import XGBClassifier
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import StratifiedKFold, KFold

from sklearn.metrics import (confusion_matrix, precision_recall_curve, auc,
                             roc_curve, recall_score, classification_report, f1_score,
                             precision_recall_fscore_support, roc_auc_score)

gkf = KFold(n_splits=3, shuffle=True, random_state=42).split(X=X_train, y=Y_train)
fit_params_of_xgb = {
    ""early_stopping_rounds"":100, 
    ""eval_metric"" : 'auc', 
    ""eval_set"" : [(X_eval,Y_eval)],
    # 'callbacks': [lgb.reset_parameter(learning_rate=learning_rate_010_decay_power_099)],
    'verbose': 500,
}
params = {
    'booster': [""gbtree""],
    'learning_rate': [0.01],
    'n_estimators': [3000],#range(1000, 2000, 3000),#range(100, 500, 100)
    'min_child_weight': [1],#1
    'gamma': [0],
    'subsample': [0.4],
    'colsample_bytree': [0.8],
    'max_depth': [4],
    ""scale_pos_weight"": [1],
    ""reg_alpha"":[1],#0.08
}
xgb_estimator = XGBClassifier(
    objective='binary:logistic',
    silent=True,
)

gsearch = GridSearchCV(
    estimator=xgb_estimator,
    param_grid=params,
    scoring='roc_auc',
    n_jobs=-1,
    cv=gkf, verbose=3
)
xgb_model = gsearch.fit(X=X_train, y=Y_train, **fit_params_of_xgb)
gsearch.best_params_, gsearch.best_score_",0,No Code Smell
660,29626958,18,"fit_params_of_xgb = {
    ""early_stopping_rounds"":100, 
    ""eval_metric"" : 'auc', 
    ""eval_set"" : [(X_eval,Y_eval)],
    # 'callbacks': [lgb.reset_parameter(learning_rate=learning_rate_010_decay_power_099)],
    'verbose': 500
}
xgb_estimator = XGBClassifier(
    objective='binary:logistic',
    silent=True,    
    booster= ""gbtree"",
    learning_rate= 0.01,
    n_estimators=3000,#range(1000, 2000, 3000),#range(100, 500, 100)
    min_child_weight= 1,#1
    gamma= 0,
    subsample= 0.4,
    colsample_bytree= 0.8,
    max_depth= 4,
    scale_pos_weight=1,
    reg_alpha=1,#0.08
)
xgb_estimator.fit(X=X_train, y=Y_train, **fit_params_of_xgb)
gsearch.best_params_, gsearch.best_score_",0,No Code Smell
661,29626958,19,"Y_pred = xgb_estimator.predict(test_X_df)
submission = pd.DataFrame({
        ""encounter_id"": test_df[""encounter_id""],
        ""hospital_death"": Y_pred
    })
submission.to_csv(""hospital_death.csv"",index=False)",0,No Code Smell
662,28701986,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the ""../input/"" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# Any results you write to the current directory are saved as output.",1,Code Smell
663,28701986,1,!pip install fastai==0.7.0,0,No Code Smell
664,28701986,2,"from fastai.imports import *
from fastai.structured import *
from pandas_summary import DataFrameSummary
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
from IPython.display import display
from sklearn import metrics",1,Code Smell
665,28701986,3,!pip install ggplot,0,No Code Smell
666,28701986,4,"import pathlib
PATH = '../input/fifa2019wages'
working_path = '/kaggle/working/'

path = pathlib.Path(PATH)
path_w = pathlib.Path(working_path)",0,No Code Smell
667,28701986,5,!head -n 100000 {path}/FifaTrainNew.csv > {path_w}/FifaTrainNew.csv,0,No Code Smell
668,28701986,6,"df_raw = pd.read_csv(f'{working_path}/FifaTrainNew.csv', low_memory=False, 
                     parse_dates=[""Joined"",'Contract Valid Until'])",1,Code Smell
669,28701986,7,df_raw.head(),0,No Code Smell
670,28701986,8,df_raw.tail().T,0,No Code Smell
671,28701986,9,"def display_all(df):
    with pd.option_context(""display.max_rows"", 1000, ""display.max_columns"", 1000): 
        display(df)",0,No Code Smell
672,28701986,10,display_all(df_raw.tail().T),0,No Code Smell
673,28701986,11,df_raw['Contract Valid Until'].unique(),0,No Code Smell
674,28701986,12,display_all(df_raw.describe(include='all').T),1,Code Smell
675,28701986,13,"df_raw = df_raw.drop('Ob' , axis = 1)",0,No Code Smell
676,28701986,14,train_cats(df_raw),0,No Code Smell
677,28701986,15,df_raw.Club.unique(),0,No Code Smell
678,28701986,16,display_all(df_raw.head()),0,No Code Smell
679,28701986,17,display_all(df_raw.isnull().sum().sort_index()/len(df_raw)),0,No Code Smell
680,28701986,18,"c=0
for col in df_raw.columns:
    if(str(df_raw[col].dtype)!=""category""):
        print(""'""+col+""',"")",0,No Code Smell
681,28701986,19,df_raw['LongPassing'],0,No Code Smell
682,28701986,20,"add_datepart(df_raw, 'Joined')
add_datepart(df_raw, 'Contract Valid Until')",0,No Code Smell
683,28701986,21,"df_trn, y_trn, nas= proc_df(df_raw,y_fld= 'WageNew')",0,No Code Smell
684,28701986,22,df_raw,0,No Code Smell
685,28701986,23,"m = RandomForestRegressor(n_jobs=-1)
m.fit(df_trn, y)
m.score(df_trn,y)",0,No Code Smell
686,28701986,24,?proc_df,0,No Code Smell
687,28701986,25,len(df),0,No Code Smell
688,28701986,26,"def split_vals(a,n): return a[:n], a[n:]
n_valid = 6000
n_trn = len(df_trn)-n_valid
X_train, X_valid = split_vals(df_trn, n_trn)
y_train, y_valid = split_vals(y_trn, n_trn)
raw_train, raw_valid = split_vals(df_raw, n_trn)",0,No Code Smell
689,28701986,27,"def print_score(m,imp_cols=None):
    if(imp_cols is not None):
        res = [ m.score(X_train[imp_cols], y_train), m.score(X_valid[imp_cols], y_valid)]
    else:
        res = [ m.score(X_train, y_train), m.score(X_valid, y_valid)]
    if hasattr(m, 'oob_score_'): res.append(m.oob_score_)
    print(res)",0,No Code Smell
690,28701986,28,len(df_raw),0,No Code Smell
691,28701986,29,"#set_rf_samples(6000)
reset_rf_samples()",0,No Code Smell
692,28701986,30,??set_rf_samples,0,No Code Smell
693,28701986,31,"m = RandomForestRegressor(n_estimators=60, min_samples_leaf=3,max_features=0.5, n_jobs=-1, oob_score=True)
m.fit(X_train, y_train)
print_score(m)",0,No Code Smell
694,28701986,32,"%time preds = np.stack([t.predict(X_valid) for t in m.estimators_])
np.mean(preds[:,0]), np.std(preds[:,0])",0,No Code Smell
695,28701986,33,"def get_preds(t): return t.predict(X_valid)
%time preds = np.stack(parallel_trees(m, get_preds))
np.mean(preds[:,0]), np.std(preds[:,0])",0,No Code Smell
696,28701986,34,display_all(df_trn),0,No Code Smell
697,28701986,35,"fi = rf_feat_importance(m, df_trn); fi[:10]",0,No Code Smell
698,28701986,36,"fi.plot('cols', 'imp', figsize=(10,6), legend=False);",0,No Code Smell
699,28701986,37,"def plot_fi(fi): return fi.plot('cols', 'imp', 'barh', figsize=(12,7), legend=False)",0,No Code Smell
700,28701986,38,plot_fi(fi[:30]);,0,No Code Smell
701,28701986,39,plot_fi(fi[:12]);,0,No Code Smell
702,28701986,40,to_keep = fi[fi.imp>0.005].cols; len(to_keep),0,No Code Smell
703,28701986,41,to_keep,0,No Code Smell
704,28701986,42,"df_keep = df_trn[to_keep].copy()
X_train, X_valid = split_vals(df_keep, n_trn)",0,No Code Smell
705,28701986,43,"m = RandomForestRegressor(n_estimators=80, min_samples_leaf=3, max_features=0.5,
                          n_jobs=-1, oob_score=True)
m.fit(X_train, y_train)
print_score(m)",0,No Code Smell
706,28701986,44,"fi = rf_feat_importance(m, df_keep)
plot_fi(fi);",0,No Code Smell
707,28701986,45,fi,0,No Code Smell
708,28701986,46,"df_trn2, y_trn, nas = proc_df(df_raw, 'WageNew', max_n_cat=7)
X_train, X_valid = split_vals(df_trn2, n_trn)

m = RandomForestRegressor(n_estimators=80, min_samples_leaf=3, max_features=0.6, n_jobs=-1, oob_score=True)
m.fit(X_train, y_train)
print_score(m)",0,No Code Smell
709,28701986,47,"fi = rf_feat_importance(m, df_trn2)
plot_fi(fi[:25]);",0,No Code Smell
710,28701986,48,from scipy.cluster import hierarchy as hc,1,Code Smell
711,28701986,49,?scipy.stats.spearmanr,0,No Code Smell
712,28701986,50,"corr = np.round(scipy.stats.spearmanr(df_keep).correlation, 4)
corr_condensed = hc.distance.squareform(1-corr)
z = hc.linkage(corr_condensed, method='average')
fig = plt.figure(figsize=(16,10))
dendrogram = hc.dendrogram(z, labels=df_keep.columns, orientation='left', leaf_font_size=16)
plt.show()",0,No Code Smell
713,28701986,51,"def get_oob(df):
    m = RandomForestRegressor(n_estimators=80, min_samples_leaf=5, max_features=0.6, n_jobs=-1, oob_score=True)
    x, _ = split_vals(df, n_trn)
    m.fit(x, y_train)
    return m.oob_score_",0,No Code Smell
714,28701986,52,get_oob(df_keep),0,No Code Smell
715,28701986,53,df_keep.columns,0,No Code Smell
716,28701986,54,"for c in ( 'LCM', 'RCM', 'RAM', 'CAM', 'RW' , 'LM'):
    print(c, get_oob(df_keep.drop(c, axis=1)))",0,No Code Smell
717,28701986,55,"to_drop = ['CM', 'CAM', 'RW']
get_oob(df_keep.drop(to_drop, axis=1))",0,No Code Smell
718,28701986,56,"df_keep.drop(to_drop, axis=1, inplace=True)
X_train, X_valid = split_vals(df_keep, n_trn)",0,No Code Smell
719,28701986,57,"np.save('/kaggle/working/keep_cols.npy', np.array(df_keep.columns))",0,No Code Smell
720,28701986,58,"keep_cols = np.load('/kaggle/working/keep_cols.npy' , allow_pickle=True)
df_keep = df_trn[keep_cols]",1,Code Smell
721,28701986,59,reset_rf_samples(),0,No Code Smell
722,28701986,60,"m = RandomForestRegressor(n_estimators=80, min_samples_leaf=4, max_features=0.5, n_jobs=-1, oob_score=True)
m.fit(X_train, y_train)
print_score(m)",0,No Code Smell
723,28701986,61,"from pdpbox import pdp
from plotnine import *",1,Code Smell
724,28701986,62,reset_rf_samples(),0,No Code Smell
725,28701986,63,"df_trn2, y_trn, nas = proc_df(df_raw, 'WageNew', max_n_cat=7)
X_train, X_valid = split_vals(df_trn2, n_trn)
m = RandomForestRegressor(n_estimators=80, min_samples_leaf=3, max_features=0.6, n_jobs=-1, oob_score=True)
m.fit(X_train, y_train);",0,No Code Smell
726,28701986,64,print_score(m),0,No Code Smell
727,28701986,65,"plot_fi(rf_feat_importance(m, df_trn2)[:10]);",0,No Code Smell
728,28701986,66,"df_raw.plot('Reactions', 'JoinedElapsed', 'scatter', alpha=0.01, figsize=(10,8));",0,No Code Smell
729,28701986,67,sum(df_raw['Reactions']>45),0,No Code Smell
730,28701986,68,"x_all = get_sample(df_raw[df_raw.Reactions>40], 500)",0,No Code Smell
731,28701986,69,??get_sample,0,No Code Smell
732,28701986,70,!pip install scikit-misc,0,No Code Smell
733,28701986,71,"ggplot(x_all, aes('Reactions', 'WageNew'))+stat_smooth(se=True, method='loess')",0,No Code Smell
734,28701986,72,"x = get_sample(X_train[X_train.Reactions>45], 500)
def plot_pdp(feat, clusters=None, feat_name=None):
    feat_name = feat_name or feat
    p = pdp.pdp_isolate(m, x, x.columns, feat)
    return pdp.pdp_plot(p, feat_name, plot_lines=True,
                        cluster=clusters is not None,
                        n_cluster_centers=clusters)",0,No Code Smell
735,28701986,73,plot_pdp('Reactions'),0,No Code Smell
736,28701986,74,"feats = ['JoinedElapsed', 'Reactions']
p = pdp.pdp_interact(m, x, x.columns, feats)
pdp.pdp_interact_plot(p, feats)",0,No Code Smell
737,28701986,75,df_keep.describe,0,No Code Smell
738,28701986,76,df_keep.Age.describe,0,No Code Smell
739,28701986,77,!pip install treeinterpreter,0,No Code Smell
740,28701986,78,from treeinterpreter import treeinterpreter as ti,1,Code Smell
741,28701986,79,,0,No Code Smell
742,28701986,80,"df_raw = pd.read_csv(f'{working_path}/FifaTrainNew.csv', low_memory=False, 
                     parse_dates=[""Joined"",'Contract Valid Until'])",1,Code Smell
743,28701986,81,obj_cols = df_raw.dtypes[df_raw.dtypes == object].index.tolist(),0,No Code Smell
744,28701986,82,"for col in obj_cols:
    print(f'{col}\t\t{df_raw[col].unique()}')",0,No Code Smell
745,28701986,83,"df_raw.drop('Ob',axis=1,inplace=True)",0,No Code Smell
746,28701986,84,train_cats(df_raw),0,No Code Smell
747,28701986,85,"for col in obj_cols:
    print(f'{col}\t{df_raw[col].unique()}')",0,No Code Smell
748,28701986,86,"add_datepart(df_raw, 'Joined')
add_datepart(df_raw, 'Contract Valid Until')
",0,No Code Smell
749,28701986,87,"def split_vals(a,n): return a[:n],a[n:]

df_trn, y_trn, nas = proc_df(df_raw,y_fld='WageNew', max_n_cat=10)

n_valid = 7500
n_train = len(df_trn) - n_valid

X_train, X_valid = split_vals(df_trn,n_train)
y_train, y_valid = split_vals(y_trn, n_train)
train_raw, valid_raw = split_vals(df_raw, n_train)",0,No Code Smell
750,28701986,88,set_rf_samples(5000),0,No Code Smell
751,28701986,89,"def print_score(m):
    res = [ m.score(X_train, y_train), m.score(X_valid, y_valid)]
    if hasattr(m, 'oob_score_'): res.append(m.oob_score_)
    print(res)",0,No Code Smell
752,28701986,90,"def print_score(m,imp_cols=None):
    if(imp_cols is not None):
        res = [ m.score(X_train[imp_cols], y_train), m.score(X_valid[imp_cols], y_valid)]
    else:
        res = [ m.score(X_train, y_train), m.score(X_valid, y_valid)]
    if hasattr(m, 'oob_score_'): res.append(m.oob_score_)
    print(res)",0,No Code Smell
753,28701986,91,"m = RandomForestRegressor(n_estimators=1000,min_samples_leaf=3,max_features=0.555,n_jobs=-1,warm_start=True,oob_score=True)
m.fit(X_train,y_train)
print_score(m)",0,No Code Smell
754,28701986,92,?RandomForestRegressor,0,No Code Smell
755,28701986,93,"fi = rf_feat_importance(m,df_trn)
def plot_fi(fi): return fi.plot('cols','imp','barh',figsize=(12,10),legend=True)",0,No Code Smell
756,28701986,94,plot_fi(fi[fi['imp'] > 0.005]),0,No Code Smell
757,28701986,95,"imps_cols = fi[fi['imp'] > 0.002]['cols'].tolist()
imps_cols",0,No Code Smell
758,28701986,96,len(imps_cols),0,No Code Smell
759,28701986,97,X_train.columns.tolist(),0,No Code Smell
760,28701986,98,"m = RandomForestRegressor(n_estimators=1000,min_samples_leaf=3,max_features=0.555,n_jobs=-1,warm_start=True,oob_score=True)
m.fit(X_train[imps_cols],y_train)
print_score(m,imps_cols)",0,No Code Smell
761,28701986,99,"fi = rf_feat_importance(m,X_train[imps_cols])
def plot_fi(fi): return fi.plot('cols','imp','barh',figsize=(12,10),legend=True)
plot_fi(fi)",0,No Code Smell
762,28701986,100,plot_fi(fi[:25]),0,No Code Smell
763,28701986,101,"imps_cols = fi[:25]['cols'].tolist()
df_trn[imps_cols].dtypes",0,No Code Smell
764,28701986,102,from scipy.cluster import hierarchy as hc,1,Code Smell
765,28701986,103,"corr = np.round(scipy.stats.spearmanr(X_train[imps_cols]).correlation, 4)
corr_condensed = hc.distance.squareform(1-corr)
z = hc.linkage(corr_condensed, method='average')
fig = plt.figure(figsize=(16,10))
dendrogram = hc.dendrogram(z, labels=X_train[imps_cols].columns, orientation='left', leaf_font_size=16)
plt.show()",0,No Code Smell
766,28701986,104,"def get_oob(df):
    m = RandomForestRegressor(n_estimators=1000,min_samples_leaf=3,max_features=0.555,n_jobs=-1,warm_start=True,oob_score=True)
    x,_ = split_vals(df,n_train)
    m.fit(x,y_train)
    return m.oob_score_",0,No Code Smell
767,28701986,105,get_oob(X_train[imps_cols]),0,No Code Smell
768,28701986,106,"for col in ['RCM','LCM','CM','RDM','LDM','CDM']:
    print(col,get_oob(X_train[imps_cols].drop(col,axis=1)))",0,No Code Smell
769,28701986,107,"to_drop = ['RCM','LCM','CM','RDM','LDM','CDM']
get_oob(X_train[imps_cols].drop(to_drop,axis=1))
",0,No Code Smell
770,28701986,108,"m = RandomForestRegressor(n_estimators=1000,min_samples_leaf=3,max_features=0.555,n_jobs=-1,warm_start=True,oob_score=True)
cols = X_train[imps_cols].drop(to_drop,axis=1).columns
m.fit(X_train[cols],y_train)
print_score(m,imp_cols=cols)",0,No Code Smell
771,28701986,109,"df_test = pd.read_csv(f'{path}/FifaNoY.csv', low_memory=False, 
                     parse_dates=[""Joined"",'Contract Valid Until'])",1,Code Smell
772,28701986,110,train_cats(df_test),0,No Code Smell
773,28701986,111,"add_datepart(df_test, 'Joined')
add_datepart(df_test, 'Contract Valid Until')
",0,No Code Smell
774,28701986,112,"df_test1, y_trn, nas = proc_df(df_test, max_n_cat=10)
",0,No Code Smell
775,28701986,113,predictions = m.predict(df_test1[cols]),0,No Code Smell
776,28701986,114,?proc_df,0,No Code Smell
777,28701986,115,"submission = pd.DataFrame({'Ob':df_test1['Ob'],'WageNew':predictions})",0,No Code Smell
778,28701986,116,submission,0,No Code Smell
779,28701986,117,"filename = '/kaggle/working/FIFA2019Wages.csv'

submission.to_csv(filename,index=False)

print('Saved file: ' + filename)",0,No Code Smell
780,28701986,118,ls,1,Code Smell
781,28701986,119,pwd,0,No Code Smell
782,28701986,120,,0,No Code Smell
783,28537757,0,"# Data manipulation packages import.
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt # Data visualization
import seaborn as sns # More data visualization (pretty)

# Supress warnings.
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

# Sklearn and XGBoost import.
from sklearn.decomposition import PCA
from sklearn.linear_model import Lasso, Ridge, LogisticRegression
from xgboost import XGBClassifier, XGBRegressor
from sklearn.model_selection import RandomizedSearchCV
from sklearn.model_selection import StratifiedKFold
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, AdaBoostClassifier
from sklearn.svm import SVC
from sklearn.metrics import roc_auc_score as auc
from sklearn.cluster import AgglomerativeClustering as LinkCluster
from sklearn.cluster import KMeans

# Tensorflow import.
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras import Model

# File Download.
import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))",1,Code Smell
784,28537757,1,"def normalise(data, test_data):
    mean = data.mean()
    std = data.std()
    
    data = (data - mean) / std
    test_data = (test_data - mean) / std
    
    return data, test_data

def prep(data, mean=None, std=None, fancy_nan=False):
    '''
    Separate the data into X and Y values, normalise the X data, and handle NaN values.
    '''
    
    # Handle the NaN values based on the algorithm.
    data = handle_nan(data, fancy=fancy_nan)
    
    # Separate the y-values if the exist.
    if 'y' in data.columns:
        features = data.drop(['y'], axis=1)
        labels = data['y']
    else:
        features = data
    
    if 'y' in data.columns:
        return features, labels
    else:
        return features

def handle_nan(data, fancy=False):
    '''
    Handle NaN values.
    '''
    
    if fancy:  # If we want to use Lasso regression to fill in NaN values.
        # Deal with NaN numbers.
        fill_X_open = data[['d_open_interest', 'transacted_qty', 'opened_position_qty ']].dropna(axis=0)
        fill_X_closed = data[['d_open_interest', 'transacted_qty', 'closed_position_qty']].dropna(axis=0)

        # The scale is a bit off (i.e. max d_open_interestis 1361 vs 42 of opened_position_qty), but it should be okay
        lasso_fill_open = Lasso()
        lasso_fill_open.fit(fill_X_open.drop(['opened_position_qty '], axis=1), fill_X_open['opened_position_qty '])

        lasso_fill_closed = Lasso()
        lasso_fill_closed.fit(fill_X_closed.drop(['closed_position_qty'], axis=1), fill_X_closed['closed_position_qty'])

        # Fill in the NaN values for the training set.
        elements = data.loc[data[['closed_position_qty', 'opened_position_qty ']].isna().any(axis=1), ['d_open_interest', 'transacted_qty']]

        data.loc[elements.index, 'opened_position_qty '] = lasso_fill_open.predict(elements)
        data.loc[elements.index, 'closed_position_qty'] = lasso_fill_closed.predict(elements)
    else:  # If we just want to drop the columns with NaN values.
        data = data.fillna(data.mean())
        pass # data = data.dropna(axis=1)
        
    return data


def pca(data, test_data, run=True, n_components=None):
    '''
    Run PCA on the data set.
    '''
    
    if run:
        # Run PCA on the data set.
        transform = PCA(n_components=n_components)
        transform.fit(data)

        # Transform the data set.
        data = transform.fit_transform(data)
        test_data = transform.fit_transform(test_data)
        
        # Convert data back to DataFrame
        data = pd.DataFrame(data)
        test_data = pd.DataFrame(test_data)
    
    return data, test_data


def logistic(array):
    '''
    Convert raw scores to probabilities.
    '''
    
    array = (array - array.mean()) / array.std()
    return np.exp(array) / (1 + np.exp(array))


def export(predictions, train_data=None, path='preds.csv'):
    '''
    Export a list of predictions to a CSV file.
    '''
    
    # Convert the predictions to the proper format.
    predictions = pd.DataFrame(predictions)
    predictions[1] = predictions.index + train_data.shape[0] if train_data is not None else 0
    predictions = predictions[[1, 0]]
    predictions.columns = ['id', 'Predicted']
    predictions = predictions.set_index(predictions.index + 1)
    
    # Load the predictions to a CSV file.
    predictions.to_csv(path, index=False)",0,No Code Smell
785,28537757,2,"def predict(model, data):
    return model.predict(data)[:, 0]",0,No Code Smell
786,28537757,3,"def add_features(data):
    data.loc[:, 'diff'] = data.loc[:, 'ask1'] - data.loc[:, 'bid1']
    data.loc[:, 'bid1/ask1'] = data.loc[:, 'bid1'] / data.loc[:, 'ask1']
    data.loc[:, 'bid_spread'] = data.loc[:, 'bid1'] / data.loc[:, 'bid5']
    data.loc[:, 'ask_spread'] = data.loc[:, 'ask1'] / data.loc[:, 'ask5']
    data.loc[:, 'imbalance1'] = data.loc[:, 'ask1vol'] / data.loc[:, 'bid1vol']
    data.loc[:, 'imbalance2'] = data.loc[:, 'ask2vol'] / data.loc[:, 'bid2vol']
    data.loc[:, 'imbalance3'] = data.loc[:, 'ask3vol'] / data.loc[:, 'bid3vol']
    data.loc[:, 'imbalance4'] = data.loc[:, 'ask4vol'] / data.loc[:, 'bid4vol']
    data.loc[:, 'imbalance5'] = data.loc[:, 'ask5vol'] / data.loc[:, 'bid5vol']
    data.loc[:, 'spread'] = data.loc[:, 'ask1vol'] - data.loc[:, 'bid1vol']
    data.loc[:, 'imb*spread'] = data.loc[:, 'imbalance1'] * data.loc[:, 'spread']
    data.loc[:, 'askvol*ask_spread'] = data.loc[:, 'ask1vol'] * data.loc[:, 'ask_spread']
    data.loc[:, 'askvol*diff'] = data.loc[:, 'ask1vol'] * data.loc[:, 'diff']
    data.loc[:, 'bidvol*diff'] = data.loc[:, 'bid1vol'] * data.loc[:, 'diff']
    
    return data

def add_clusters(data, test_data, clusters=5):
    # K means clustering.
    for c in clusters:
        print(c)
        cluster = KMeans(n_clusters=c, n_jobs=-1).fit(data)
    
        data.loc[:, f'kmeans/{c}'] = cluster.labels_
        test_data.loc[:, f'kmeans/{c}'] = cluster.predict(test_data)
    
    return data, test_data",0,No Code Smell
787,28537757,4,"CLUSTERS = [10, 15, 20]

testing = False

if testing:
    # Load the data, and split it into training and testing.
    input_data = pd.read_csv('/kaggle/input/caltech-cs155-2020/train.csv').set_index('id')
    input_data = input_data.iloc[np.random.permutation(len(input_data))].reset_index(drop=True)

    # Split the data into training and testing.
    t = 4 * input_data.shape[0] // 5

    train = input_data.iloc[:t]
    test = input_data.iloc[t:]
else:
    train = pd.read_csv('/kaggle/input/caltech-cs155-2020/train.csv').set_index('id')
    test = pd.read_csv('/kaggle/input/caltech-cs155-2020/test.csv').set_index('id')

# Let's add some features!!
train = add_features(train)
test = add_features(test)

# Prep the data and split into X and Y.
mean = train.drop(['y'], axis=1).mean(axis=0)
std = train.drop(['y'], axis=1).std(axis=0)

X, Y = prep(train, mean=mean, std=std)

if testing:
    Xt, Yt = prep(test, mean=mean, std=std)
else:
    Xt = prep(test, mean=mean, std=std)

# Add clusters.
# X, Xt = add_clusters(X, Xt, clusters=CLUSTERS)

# Normalise the data.
X, Xt = normalise(X, Xt)",1,Code Smell
788,28537757,5,"N_MODELS = 1

# Keep track of predictions for each model.
train_preds = pd.DataFrame()
test_preds = pd.DataFrame()

# Add the XGB models to the ensemble.
for i in range(N_MODELS):
    print(f'XGB {i + 1}')
    # Randomly sample the columns
    cols = np.random.choice(X.columns, size=np.random.randint(3, 7), replace=False)
    
    # Train the model.
    model = XGBRegressor(n_jobs=-1, tree_method='gpu_hist', objective='reg:squarederror')
    model.fit(X[cols], Y)

    # Make the predictions.
    train_preds.loc[:, f'xgb{i}'] = model.predict(X[cols])
    test_preds.loc[:, f'xgb{i}'] = model.predict(Xt[cols])

# Add the Logistic models to the ensemble.
for i in range(N_MODELS):
    print(f'Log {i + 1}')
    # Randomly sample the columns.
    cols = np.random.choice(X.columns, size=np.random.randint(3, 7), replace=False)
    
    # Train the model.
    model = LogisticRegression(C=30)
    model.fit(X[cols], Y)

    # Make the predictions.
    train_preds.loc[:, f'log{i}'] = model.predict_proba(X[cols])[:, 1]
    test_preds.loc[:, f'log{i}'] = model.predict_proba(Xt[cols])[:, 1]
    

# One hot encode the labels so that the probability of each can be attained.
Y_1hc = pd.DataFrame(Y)
Y_1hc.columns = ['1']
Y_1hc['0'] = 1 - Y_1hc['1']  

# Add Neural Nets to the ensemble.
for i in range(1):
    print(f'Neural Net {i + 1}')
    # Create Neural Network.
    model = Sequential()
    model.add(Dense(100, input_dim=X.shape[1], activation='relu'))
    model.add(Dense(100, activation='relu'))
    model.add(Dense(70, activation='relu'))
    model.add(Dense(50, activation='relu'))
    model.add(Dense(30, activation='relu'))
    model.add(Dense(20, activation='relu'))
    model.add(Dense(10, activation='relu'))
    model.add(Dense(5, activation='relu'))
    model.add(Dense(2, activation='softmax'))

    # Train the model.
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', tf.keras.metrics.AUC()])
    model.fit(X, Y_1hc, epochs=1, batch_size=32)

    # Make the predictions.
    train_preds.loc[:, f'nn{i}'] = predict(model, X)
    test_preds.loc[:, f'nn{i}'] = predict(model, Xt)

print('Complete.')",0,No Code Smell
789,28537757,6,"# Normalise the pooled predictions.
mean = train_preds.mean()
std = train_preds.std()

train_preds = (train_preds - mean) / std
test_preds = (test_preds - mean) / std

# If we want, we can pass the original data to the nn as well.
original = False
if original:
    train_preds = pd.concat([train_preds, X.reset_index(drop=True)], axis=1)
    test_preds = pd.concat([test_preds, Xt.reset_index(drop=True)], axis=1)",0,No Code Smell
790,28537757,7,"# One hot encode the labels so that the probability of each can be attained.
Y_1hc = pd.DataFrame(Y)
Y_1hc.columns = ['1']
Y_1hc['0'] = 1 - Y_1hc['1']

# Create Neural Network.
nn_model = Sequential()
nn_model.add(Dense(200, input_dim=train_preds.shape[1], activation='relu'))
nn_model.add(Dense(300, activation='relu'))
nn_model.add(Dense(200, activation='relu'))
nn_model.add(Dense(50, activation='relu'))
nn_model.add(Dense(2, activation='softmax'))

# Train the model.
nn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', tf.keras.metrics.AUC()])
nn_model.fit(train_preds, Y_1hc, epochs=1, batch_size=32)

# In-sample error
preds_in = predict(nn_model, train_preds)
train_acc = auc(Y, preds_in)
print(f'Train Acc: {train_acc:.8f}')

# Out-of-sample error.
preds = predict(nn_model, test_preds)

if testing:
    test_acc = auc(Yt, preds)
    print(f'Test Acc:  {test_acc:.8f}')",0,No Code Smell
791,28537757,8,"if not testing:
    export(preds, X)",0,No Code Smell
1843,29007121,0,"import pandas as pd
from keras.models import Sequential
from keras.layers import Dense, Dropout, LeakyReLU
from keras.wrappers.scikit_learn import KerasClassifier
from keras.utils import np_utils
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold
from sklearn.preprocessing import LabelEncoder
from sklearn.pipeline import Pipeline
import numpy as np",1,Code Smell
1844,29007121,1,"data=pd.read_csv('/kaggle/input/bitsf312-lab1/train.csv',index_col=0)",1,Code Smell
1845,29007121,2,"data = data.replace({'?': np.nan})
data = data.fillna(data.mean())
data.fillna(value = data.mode().loc[0], inplace = True)",0,No Code Smell
1846,29007121,3,data.head(20),0,No Code Smell
1847,29007121,4,"data = pd.get_dummies(data, columns= ['Size'], prefix = ['Size'])
data.head(100)",0,No Code Smell
1848,29007121,5,data = data.astype('float64'),0,No Code Smell
1849,29007121,6,data.dtypes,0,No Code Smell
1850,29007121,7,# data = data.fillna(value = data.mean()),0,No Code Smell
1851,29007121,8,"# from sklearn.preprocessing import StandardScaler

# data_scaled = data.copy()
# scaler = StandardScaler()

# data_scaled = pd.DataFrame(scaler.fit_transform(data_scaled), columns=data.columns)",0,No Code Smell
1852,29007121,9,"
data.head()
",0,No Code Smell
1853,29007121,10,"X = data.drop('Class', axis= 1)
#X = X.drop('ID', axis = 1)
y=data['Class']
y.value_counts()",0,No Code Smell
1854,29007121,11,"# encoder = LabelEncoder()
# encoder.fit(y)
# encoded_Y = encoder.transform(y)
# # convert integers to dummy variables (i.e. one hot encoded)
# dummy_y = np_utils.to_categorical(encoded_Y)
 
# # define baseline model
# def baseline_model():
# 	# create model
# 	model = Sequential()
# 	model.add(Dense(8, input_dim=13, activation='relu'))
# 	model.add(Dense(7, activation='softmax'))
# 	# Compile model
# 	model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
# 	return model
 
# estimator = KerasClassifier(build_fn=baseline_model, epochs=200, batch_size=5, verbose=0)
# kfold = KFold(n_splits=10, shuffle=True)
# results = cross_val_score(estimator, X, dummy_y, cv=kfold)
# print(""Baseline: %.2f%% (%.2f%%)"" % (results.mean()*100, results.std()*100))

",0,No Code Smell
1855,29007121,12,"encoder = LabelEncoder()
encoder.fit(y)
encoded_Y = encoder.transform(y)
# convert integers to dummy variables (i.e. one hot encoded)
dummy_y = np_utils.to_categorical(encoded_Y)
 
# define baseline model

# create model
model = Sequential()
model.add(Dense(64, input_dim=13, activation='relu'))
model.add(Dropout(rate = 0.2))
model.add(Dense(32, activation='relu'))
model.add(Dropout(rate = 0.2))
model.add(Dense(32, activation='tanh'))
model.add(LeakyReLU(alpha=0.3))
model.add(Dropout(rate = 0.2))
model.add(Dense(6, activation='softmax'))
# Compile model
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
history=model.fit(X, dummy_y, validation_split=0.2, epochs=200,batch_size=15)
model.summary()

 
# estimator = KerasClassifier(build_fn=baseline_model, epochs=200, batch_size=5, verbose=0)
# kfold = KFold(n_splits=10, shuffle=True)
# results = cross_val_score(estimator, X, dummy_y, cv=kfold)
# print(""Baseline: %.2f%% (%.2f%%)"" % (results.mean()*100, results.std()*100))",0,No Code Smell
1856,29007121,13,"test = pd.read_csv('/kaggle/input/bitsf312-lab1/test.csv')
test = test.replace({'?': np.nan})
test = test.fillna(data.mean())
test.fillna(value = test.mode().loc[0], inplace = True)",1,Code Smell
1857,29007121,14,test.head(),0,No Code Smell
1858,29007121,15,"test = pd.get_dummies(test, columns= ['Size'], prefix = ['size'])",0,No Code Smell
1859,29007121,16,"y2 = test['ID']
test = test.drop(['ID'], axis = 1)",0,No Code Smell
1860,29007121,17,"
# test_scaled = test.copy()
# scaler = StandardScaler()

# test_scaled = pd.DataFrame(scaler.fit_transform(test_scaled), columns=test.columns)",0,No Code Smell
1861,29007121,18,test.head(),0,No Code Smell
1862,29007121,19,,0,No Code Smell
1863,29007121,20,ans = model.predict(test),0,No Code Smell
1864,29007121,21,labels = ans.argmax(-1),0,No Code Smell
1865,29007121,22,len(labels),0,No Code Smell
1866,29007121,23,"labels = pd.DataFrame(data = labels, index = y2)",0,No Code Smell
1867,29007121,24,labels.head(20),0,No Code Smell
1868,29007121,25,labels.to_csv('nnfl4.csv'),0,No Code Smell
1869,29007121,26,"from IPython.display import HTML
import pandas as pd
import numpy as np
import base64def create_download_link(df, title = ""Download CSV file"", filename = ""data.csv""):    csv = df.to_csv(index=False)    b64 = base64.b64encode(csv.encode())    payload = b64.decode()html='<adownload=""{filename}""href=""data:text/csv;base64,{payload}""target=""_blank"">{title}</a>'    html = html.format(payload=payload,title=title,filename=filename)    
return HTML(html)create_download_link(​<submission_DataFrame_name>​)",0,No Code Smell
1870,29007121,27,,0,No Code Smell
2551,32052230,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the ""../input/"" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# Any results you write to the current directory are saved as output.",1,Code Smell
2552,32052230,1,"trainurl = ""/kaggle/input/hecmontrealdeeplearningcourse/train.csv""
testurl = ""/kaggle/input/hecmontrealdeeplearningcourse/test.csv""
referenceurl = ""/kaggle/input/hecmontrealdeeplearningcourse/reference.csv""
texturl = ""/kaggle/input/hecmontrealdeeplearningcourse/text.csv""
sampleurl = ""/kaggle/input/hecmontrealdeeplearningcourse/sample.csv""
train1 = pd.read_csv(trainurl)
test1 = pd.read_csv(testurl)
reference = pd.read_csv(referenceurl)
text = pd.read_csv(texturl)
sample = pd.read_csv(sampleurl)
test1.tail(5)",1,Code Smell
2553,32052230,2,"newtest = pd.merge(test1, text, how = 'left', on = 'id')
newtrain = pd.merge(train1, text, how = 'left', on = 'id')
newtrain.head()
newtrain['label'].max()
print(text[""title""].map(len).max()) #Finding max length of sentence",0,No Code Smell
2554,32052230,3,classes =  list(set(newtrain.title)),0,No Code Smell
2555,32052230,4,"train = newtrain[['title', 'label']]",0,No Code Smell
2556,32052230,5,"train = pd.get_dummies(train, prefix='', prefix_sep='', columns=['label',])
",0,No Code Smell
2557,32052230,6,"import pandas as pd, numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer",1,Code Smell
2558,32052230,7,"import re, string
re_tok = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')
def tokenize(s): return re_tok.sub(r' \1 ', s).split()",0,No Code Smell
2559,32052230,8,"label_cols = ['0','1', '2','3','4']",0,No Code Smell
2560,32052230,9,"n = train.shape[0]
vec = TfidfVectorizer(ngram_range=(1,2), tokenizer=tokenize,
               min_df=3, max_df=0.9, strip_accents='unicode', use_idf=1,
               smooth_idf=1, sublinear_tf=1 )
trn_term_doc = vec.fit_transform(train.title)
test_term_doc = vec.transform(newtest.title)",0,No Code Smell
2561,32052230,10,"#A sparse matrix is created 
trn_term_doc, test_term_doc",0,No Code Smell
2562,32052230,11,"def pr(y_i, y):
    p = x[y==y_i].sum(0)
    return (p+1) / ((y==y_i).sum()+1)",0,No Code Smell
2563,32052230,12,"x = trn_term_doc
test_x = test_term_doc
",0,No Code Smell
2564,32052230,13,"def get_mdl(y):
    y = y.values
    r = np.log(pr(1,y) / pr(0,y))
    m = LogisticRegression(C=4, dual=True, solver = 'liblinear')
    x_nb = x.multiply(r)
    return m.fit(x_nb, y), r",0,No Code Smell
2565,32052230,14,"preds = np.zeros((len(newtest), len(label_cols)))

for i, j in enumerate(label_cols):
    print('fit', j)
    m,r = get_mdl(train[j])
    preds[:,i] = m.predict_proba(test_x.multiply(r))[:,1]",0,No Code Smell
2566,32052230,15,"submid = pd.DataFrame({'id': newtest[""id""]})",0,No Code Smell
2567,32052230,16,"answer1 = pd.concat([submid, pd.DataFrame(preds, columns = label_cols)], axis=1)
answer1.to_csv('answer1.csv', index=False)",0,No Code Smell
2568,32052230,17,answer1.head(),0,No Code Smell
2569,32052230,18,answer1.shape,0,No Code Smell
2570,32052230,19,"import sys, os, re, csv, codecs, numpy as np, pandas as pd

from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation
from keras.layers import Bidirectional, GlobalMaxPool1D
from keras.models import Model
from keras import initializers, regularizers, constraints, optimizers, layers",1,Code Smell
2571,32052230,20,"EMBEDDING_FILE= ""/kaggle/input/glovedataset/glove.6B.50d.txt""",0,No Code Smell
2572,32052230,21,"embed_size = 50 # how big is each word vector
max_features = 20000 # how many unique words to use (i.e num rows in embedding vector)
maxlen = 100 # max number of words in a title",0,No Code Smell
2573,32052230,22,"list_classes = [""0"", ""1"", ""2"", ""3"", ""4""]",0,No Code Smell
2574,32052230,23,y = train[list_classes].values,0,No Code Smell
2575,32052230,24,"list_sentences_train = train[""title""].values
list_sentences_test = newtest['title'].values",0,No Code Smell
2576,32052230,25,"tokenizer = Tokenizer(num_words=max_features)
tokenizer.fit_on_texts(list(list_sentences_train))
list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)
list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)
X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)
X_te = pad_sequences(list_tokenized_test, maxlen=maxlen)",0,No Code Smell
2577,32052230,26,"def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')
embeddings_index = dict(get_coefs(*o.strip().split()) for o in open(EMBEDDING_FILE))",0,No Code Smell
2578,32052230,27,"all_embs = np.stack(embeddings_index.values())
emb_mean,emb_std = all_embs.mean(), all_embs.std()
emb_mean,emb_std",1,Code Smell
2579,32052230,28,"word_index = tokenizer.word_index
nb_words = min(max_features, len(word_index))+1
embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))
for word, i in word_index.items():
    if i >= max_features: continue
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None: embedding_matrix[i] = embedding_vector",0,No Code Smell
2580,32052230,29,embedding_matrix.shape,0,No Code Smell
2581,32052230,30,"inp = Input(shape=(maxlen,))
inp.shape",0,No Code Smell
2582,32052230,31,"inp = Input(shape=(maxlen,))

x = Embedding(9322, embed_size, weights=[embedding_matrix])(inp)
x = Bidirectional(LSTM(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)
x = GlobalMaxPool1D()(x)
x = Dense(50, activation=""relu"")(x)
x = Dropout(0.1)(x)
x = Dense(5, activation=""softmax"")(x)
model = Model(inputs=inp, outputs=x)
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])",0,No Code Smell
2583,32052230,32,"model.fit(X_t, y, batch_size=32, epochs=8, validation_split=0.1);",0,No Code Smell
2584,32052230,33,"y_test = model.predict([X_te], batch_size=1024, verbose=1)",0,No Code Smell
2585,32052230,34,y_test.shape,0,No Code Smell
2586,32052230,35,print(y_test),0,No Code Smell
2587,32052230,36,"answer2 = pd.concat([submid, pd.DataFrame(y_test, columns = label_cols)], axis=1)
answer2.to_csv('answer2.csv', index=False)",0,No Code Smell
2588,32052230,37,answer2.head(),0,No Code Smell
2589,32052230,38,"answer3 = answer2.copy()
answer3[label_cols] = (answer1[label_cols] + answer1[label_cols]) / 2",0,No Code Smell
2590,32052230,39,answer3.head(),0,No Code Smell
2591,32052230,40,df = answer3[label_cols],0,No Code Smell
2592,32052230,41,type(df),0,No Code Smell
2593,32052230,42,print(df),0,No Code Smell
2594,32052230,43,df = df.to_numpy(),0,No Code Smell
2595,32052230,44,"df2 = np.argmax(df,axis=1)",0,No Code Smell
2596,32052230,45,,0,No Code Smell
2597,32052230,46,"prediction10 = pd.DataFrame({'id':newtest.id,'label':df2},columns=['id', 'label'])
prediction10.to_csv('prediction10.csv', index=False)
prediction10.shape",0,No Code Smell
2598,32052230,47,prediction10.head(),0,No Code Smell
2599,30296882,0,"import os
import numpy as np 
import pandas as pd 
import json",1,Code Smell
2600,30296882,1,"%%time

with open('../input/herbarium-2020-fgvc7/nybg2020/train/metadata.json', ""r"", encoding=""ISO-8859-1"") as file:
    train = json.load(file)

train_img = pd.DataFrame(train['images'])
train_ann = pd.DataFrame(train['annotations']).drop(columns='image_id')
train_df = train_img.merge(train_ann, on='id')
train_df.head()",0,No Code Smell
2601,30296882,2,"%%time

with open('../input/herbarium-2020-fgvc7/nybg2020/test/metadata.json', ""r"", encoding=""ISO-8859-1"") as file:
    test = json.load(file)

test_df = pd.DataFrame(test['images'])
test_df.head()",0,No Code Smell
2602,30296882,3,train_df['category_id'].value_counts(),0,No Code Smell
2603,30296882,4,"from sklearn import preprocessing

le = preprocessing.LabelEncoder()
le.fit(train_df['category_id'])
train_df['category_id'] = le.transform(train_df['category_id'])",0,No Code Smell
2604,30296882,5,"# ====================================================
# Library
# ====================================================

import sys

import gc
import os
import random
import time
from contextlib import contextmanager
from pathlib import Path

import cv2
from PIL import Image
import numpy as np
import pandas as pd
import scipy as sp

import sklearn.metrics

from functools import partial

import torch
import torch.nn as nn
from torch.optim import Adam, SGD
from torch.optim.lr_scheduler import CosineAnnealingLR
from torch.utils.data import DataLoader, Dataset

from albumentations import Compose, Normalize, Resize
from albumentations.pytorch import ToTensorV2


device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device",1,Code Smell
2605,30296882,6,"# ====================================================
# Utils
# ====================================================

@contextmanager
def timer(name):
    t0 = time.time()
    LOGGER.info(f'[{name}] start')
    yield
    LOGGER.info(f'[{name}] done in {time.time() - t0:.0f} s.')

    
def init_logger(log_file='train.log'):
    from logging import getLogger, DEBUG, FileHandler,  Formatter,  StreamHandler
    
    log_format = '%(asctime)s %(levelname)s %(message)s'
    
    stream_handler = StreamHandler()
    stream_handler.setLevel(DEBUG)
    stream_handler.setFormatter(Formatter(log_format))
    
    file_handler = FileHandler(log_file)
    file_handler.setFormatter(Formatter(log_format))
    
    logger = getLogger('Herbarium')
    logger.setLevel(DEBUG)
    logger.addHandler(stream_handler)
    logger.addHandler(file_handler)
    
    return logger

LOG_FILE = 'train.log'
LOGGER = init_logger(LOG_FILE)


def seed_torch(seed=777):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True

SEED = 777
seed_torch(SEED)",0,No Code Smell
2606,30296882,7,"N_CLASSES = 32093


class TrainDataset(Dataset):
    def __init__(self, df, labels, transform=None):
        self.df = df
        self.labels = labels
        self.transform = transform
        
    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        file_name = self.df['file_name'].values[idx]
        file_path = f'../input/herbarium-2020-fgvc7/nybg2020/train/{file_name}'
        image = cv2.imread(file_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        label = self.labels.values[idx]
        
        if self.transform:
            augmented = self.transform(image=image)
            image = augmented['image']
        
        return image, label",0,No Code Smell
2607,30296882,8,"HEIGHT = 128
WIDTH = 128


def get_transforms(*, data):
    
    assert data in ('train', 'valid')
    
    if data == 'train':
        return Compose([
            Resize(HEIGHT, WIDTH),
            Normalize(
                mean=[0.485, 0.456, 0.406],
                std=[0.229, 0.224, 0.225],
            ),
            ToTensorV2(),
        ])
    
    elif data == 'valid':
        return Compose([
            Resize(HEIGHT, WIDTH),
            Normalize(
                mean=[0.485, 0.456, 0.406],
                std=[0.229, 0.224, 0.225],
            ),
            ToTensorV2(),
        ])",0,No Code Smell
2608,30296882,9,"from sklearn.model_selection import StratifiedKFold

DEBUG = False

if DEBUG:
    folds = train_df.sample(n=10000, random_state=0).reset_index(drop=True).copy()
else:
    folds = train_df.copy()
train_labels = folds['category_id'].values
kf = StratifiedKFold(n_splits=2)
for fold, (train_index, val_index) in enumerate(kf.split(folds.values, train_labels)):
    folds.loc[val_index, 'fold'] = int(fold)
folds['fold'] = folds['fold'].astype(int)
folds.to_csv('folds.csv', index=None)
folds.head()",0,No Code Smell
2609,30296882,10,"FOLD = 0
trn_idx = folds[folds['fold'] != FOLD].index
val_idx = folds[folds['fold'] == FOLD].index
print(trn_idx.shape, val_idx.shape)",0,No Code Smell
2610,30296882,11,"train_dataset = TrainDataset(folds.loc[trn_idx].reset_index(drop=True), 
                             folds.loc[trn_idx]['category_id'], 
                             transform=get_transforms(data='train'))
valid_dataset = TrainDataset(folds.loc[val_idx].reset_index(drop=True), 
                             folds.loc[val_idx]['category_id'], 
                             transform=get_transforms(data='valid'))",0,No Code Smell
2611,30296882,12,"batch_size = 512

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)",1,Code Smell
2612,30296882,13,"import torchvision.models as models

model = models.resnet18(pretrained=True)
model.avgpool = nn.AdaptiveAvgPool2d(1)
model.fc = nn.Linear(model.fc.in_features, N_CLASSES)",0,No Code Smell
2613,30296882,14,"from torch.optim.lr_scheduler import ReduceLROnPlateau
from sklearn.metrics import f1_score
from tqdm import tqdm


with timer('Train model'):
    
    n_epochs = 1
    lr = 4e-4
    
    model.to(device)
    
    optimizer = Adam(model.parameters(), lr=lr, amsgrad=False)
    scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.75, patience=5, verbose=True, eps=1e-6)
    
    criterion = nn.CrossEntropyLoss()
    best_score = 0.
    best_loss = np.inf
    
    for epoch in range(n_epochs):
        
        start_time = time.time()

        model.train()
        avg_loss = 0.

        optimizer.zero_grad()

        for i, (images, labels) in tqdm(enumerate(train_loader)):

            images = images.to(device)
            labels = labels.to(device)
            
            y_preds = model(images)
            loss = criterion(y_preds, labels)
            
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()

            avg_loss += loss.item() / len(train_loader)
            
        model.eval()
        avg_val_loss = 0.
        preds = np.zeros((len(valid_dataset)))

        for i, (images, labels) in enumerate(valid_loader):
            
            images = images.to(device)
            labels = labels.to(device)
            
            with torch.no_grad():
                y_preds = model(images)
            
            preds[i * batch_size: (i+1) * batch_size] = y_preds.argmax(1).to('cpu').numpy()

            loss = criterion(y_preds, labels)
            avg_val_loss += loss.item() / len(valid_loader)
        
        scheduler.step(avg_val_loss)
            
        score = f1_score(folds.loc[val_idx]['category_id'].values, preds, average='macro')

        elapsed = time.time() - start_time

        LOGGER.debug(f'  Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  F1: {score:.6f}  time: {elapsed:.0f}s')

        if score>best_score:
            best_score = score
            LOGGER.debug(f'  Epoch {epoch+1} - Save Best Score: {best_score:.6f} Model')
            torch.save(model.state_dict(), f'fold{FOLD}_best_score.pth')

        if avg_val_loss<best_loss:
            best_loss = avg_val_loss
            LOGGER.debug(f'  Epoch {epoch+1} - Save Best Loss: {best_loss:.4f} Model')
            torch.save(model.state_dict(), f'fold{FOLD}_best_loss.pth')",0,No Code Smell
2614,33188284,0,"import matplotlib.pyplot as plt
from PIL import Image
import seaborn as sns
import pandas as pd
import numpy as np
import math
import sklearn
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error,r2_score,mean_absolute_error
import matplotlib.pyplot as plt
import seaborn as sb
from sklearn import preprocessing
from sklearn.feature_extraction.text import CountVectorizer
import time
import warnings 
warnings.filterwarnings('ignore')",0,No Code Smell
2615,33188284,1,"import json, codecs
with codecs.open(""../input/herbarium-2020-fgvc7/nybg2020/train/metadata.json"", 'r',
                 encoding='utf-8', errors='ignore') as f:
    train= json.load(f)
    
with codecs.open(""../input/herbarium-2020-fgvc7/nybg2020/test/metadata.json"", 'r',
                 encoding='utf-8', errors='ignore') as f:
    test = json.load(f)",0,No Code Smell
2616,33188284,2,"display(train.keys())
",1,Code Smell
2617,33188284,3,"train_data = pd.DataFrame(train['annotations'])
display(train_data)",0,No Code Smell
2618,33188284,4,"Cat = pd.DataFrame(train['categories'])
display(Cat)",0,No Code Smell
2619,33188284,5,"train_img = pd.DataFrame(train['images'])
train_img.columns = ['file_name', 'height', 'image_id', 'license', 'width']
display(train_img)",0,No Code Smell
2620,33188284,6,"licenses = pd.DataFrame(train['licenses'])
display(licenses)",0,No Code Smell
2621,33188284,7,"regions = pd.DataFrame(train['regions'])
display(regions)",0,No Code Smell
2622,33188284,8,"train_data = train_data.merge(Cat, on='id', how='outer')
train_data = train_data.merge(train_img, on='image_id', how='outer')
train_data = train_data.merge(regions, on='id', how='outer')",0,No Code Smell
2623,33188284,9,"print(train_data.info())

display(train_data)",1,Code Smell
2624,33188284,10,"test_data = pd.DataFrame(test['images'])
test_data.columns = ['file_name', 'height', 'image_id', 'license', 'width']
print(test_data.info())
display(test_data)",0,No Code Smell
2625,33188284,11,"print(len(train_data.id.unique()))
",0,No Code Smell
2626,33188284,12,"sub = pd.DataFrame()
sub['Id'] = test_data.image_id
sub['Predicted'] = list(map(int, np.random.randint(1, 32093, (test_data.shape[0]))))
display(sub)
sub.to_csv('submission.csv', index=False)",0,No Code Smell
2627,30313596,0,"import time
start_time = time.time()",0,No Code Smell
2628,30313596,1,"import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        if filename.endswith('.jpg'):
            break
        print(os.path.join(dirname, filename))",1,Code Smell
2629,30313596,2,"sample_sub = pd.read_csv('../input/herbarium-2020-fgvc7/sample_submission.csv')
display(sample_sub)",1,Code Smell
2630,30313596,3,"import json, codecs
with codecs.open(""../input/herbarium-2020-fgvc7/nybg2020/train/metadata.json"", 'r',
                 encoding='utf-8', errors='ignore') as f:
    train_meta = json.load(f)
    
with codecs.open(""../input/herbarium-2020-fgvc7/nybg2020/test/metadata.json"", 'r',
                 encoding='utf-8', errors='ignore') as f:
    test_meta = json.load(f)",0,No Code Smell
2631,30313596,4,display(train_meta.keys()),1,Code Smell
2632,30313596,5,"train_df = pd.DataFrame(train_meta['annotations'])
display(train_df)",0,No Code Smell
2633,30313596,6,"train_cat = pd.DataFrame(train_meta['categories'])
train_cat.columns = ['family', 'genus', 'category_id', 'category_name']
display(train_cat)",0,No Code Smell
2634,30313596,7,"train_img = pd.DataFrame(train_meta['images'])
train_img.columns = ['file_name', 'height', 'image_id', 'license', 'width']
display(train_img)",0,No Code Smell
2635,30313596,8,"train_reg = pd.DataFrame(train_meta['regions'])
train_reg.columns = ['region_id', 'region_name']
display(train_reg)",0,No Code Smell
2636,30313596,9,"train_df = train_df.merge(train_cat, on='category_id', how='outer')
train_df = train_df.merge(train_img, on='image_id', how='outer')
train_df = train_df.merge(train_reg, on='region_id', how='outer')",0,No Code Smell
2637,30313596,10,"print(train_df.info())
display(train_df)",1,Code Smell
2638,30313596,11,"na = train_df.file_name.isna()
keep = [x for x in range(train_df.shape[0]) if not na[x]]
train_df = train_df.iloc[keep]",0,No Code Smell
2639,30313596,12,"dtypes = ['int32', 'int32', 'int32', 'int32', 'object', 'object', 'object', 'object', 'int32', 'int32', 'int32', 'object']
for n, col in enumerate(train_df.columns):
    train_df[col] = train_df[col].astype(dtypes[n])
print(train_df.info())
display(train_df)",0,No Code Smell
2640,30313596,13,"test_df = pd.DataFrame(test_meta['images'])
test_df.columns = ['file_name', 'height', 'image_id', 'license', 'width']
print(test_df.info())
display(test_df)",0,No Code Smell
2641,30313596,14,"train_df.to_csv('full_train_data.csv', index=False)
test_df.to_csv('full_test_data.csv', index=False)",0,No Code Smell
2642,30313596,15,"print(""Total Unique Values for each columns:"")
print(""{0:10s} \t {1:10d}"".format('train_df', len(train_df)))
for col in train_df.columns:
    print(""{0:10s} \t {1:10d}"".format(col, len(train_df[col].unique())))",0,No Code Smell
2643,30313596,16,"family = train_df[['family', 'genus', 'category_name']].groupby(['family', 'genus']).count()
display(family.describe())",0,No Code Smell
2644,30313596,17,"from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPool2D, Flatten, BatchNormalization, Input, concatenate
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.utils import plot_model
from sklearn.model_selection import train_test_split as tts

in_out_size = (120*120) + 3 #We will resize the image to 120*120 and we have 3 outputs
def xavier(shape, dtype=None):
    return np.random.rand(*shape)*np.sqrt(1/in_out_size)

def fg_model(shape, lr=0.001):
    '''Family-Genus model receives an image and outputs two integers indicating both the family and genus index.'''
    i = Input(shape)
    
    x = Conv2D(3, (3, 3), activation='relu', padding='same', kernel_initializer=xavier)(i)
    x = Conv2D(3, (5, 5), activation='relu', padding='same', kernel_initializer=xavier)(x)
    x = MaxPool2D(pool_size=(3, 3), strides=(3,3))(x)
    x = BatchNormalization()(x)
    x = Dropout(0.5)(x)
    x = Conv2D(16, (5, 5), activation='relu', padding='same', kernel_initializer=xavier)(x)
    #x = Conv2D(16, (5, 5), activation='relu', padding='same', kernel_initializer=xavier)(x)
    x = MaxPool2D(pool_size=(5, 5), strides=(5,5))(x)
    x = BatchNormalization()(x)
    x = Dropout(0.5)(x)
    x = Flatten()(x)
    
    o1 = Dense(310, activation='softmax', name='family', kernel_initializer=xavier)(x)
    
    o2 = concatenate([o1, x])
    o2 = Dense(3678, activation='softmax', name='genus', kernel_initializer=xavier)(o2)
    
    o3 = concatenate([o1, o2, x])
    o3 = Dense(32094, activation='softmax', name='category_id', kernel_initializer=xavier)(o3)
    
    x = Model(inputs=i, outputs=[o1, o2, o3])
    
    opt = Adam(lr=lr, amsgrad=True)
    x.compile(optimizer=opt, loss=['sparse_categorical_crossentropy', 
                                   'sparse_categorical_crossentropy', 
                                   'sparse_categorical_crossentropy'],
                 metrics=['accuracy'])
    return x

model = fg_model((120, 120, 3))
model.summary()
plot_model(model, to_file='full_model_plot.png', show_shapes=True, show_layer_names=True)",0,No Code Smell
2645,30313596,18,"from tensorflow.keras.preprocessing.image import ImageDataGenerator

train_datagen = ImageDataGenerator(featurewise_center=False,
                                     featurewise_std_normalization=False,
                                     rotation_range=180,
                                     width_shift_range=0.1,
                                     height_shift_range=0.1,
                                     zoom_range=0.2)",0,No Code Smell
2646,30313596,19,"m = train_df[['file_name', 'family', 'genus', 'category_id']]
fam = m.family.unique().tolist()
m.family = m.family.map(lambda x: fam.index(x))
gen = m.genus.unique().tolist()
m.genus = m.genus.map(lambda x: gen.index(x))
display(m)",0,No Code Smell
2647,30313596,20,"train, verif = tts(m, test_size=0.2, shuffle=True, random_state=17)
train = train[:40000]
verif = verif[:10000]
shape = (120, 120, 3)
epochs = 2
batch_size = 32

model = fg_model(shape, 0.007)

#Disable the last two output layers for training the Family
for layers in model.layers:
    if layers.name == 'genus' or layers.name=='category_id':
        layers.trainable = False

#Train Family for 2 epochs
model.fit_generator(train_datagen.flow_from_dataframe(dataframe=train,
                                                      directory='../input/herbarium-2020-fgvc7/nybg2020/train/',
                                                      x_col=""file_name"",
                                                      y_col=[""family"", ""genus"", ""category_id""],
                                                      target_size=(120, 120),
                                                      batch_size=batch_size,
                                                      class_mode='multi_output'),
                    validation_data=train_datagen.flow_from_dataframe(
                        dataframe=verif,
                        directory='../input/herbarium-2020-fgvc7/nybg2020/train/',
                        x_col=""file_name"",
                        y_col=[""family"", ""genus"", ""category_id""],
                        target_size=(120, 120),
                        batch_size=batch_size,
                        class_mode='multi_output'),
                    epochs=epochs,
                    steps_per_epoch=len(train)//batch_size,
                    validation_steps=len(verif)//batch_size,
                    verbose=1,
                    workers=8,
                    use_multiprocessing=False)

#Reshuffle the inputs
train, verif = tts(m, test_size=0.2, shuffle=True, random_state=17)
train = train[:40000]
verif = verif[:10000]

#Make the Genus layer Trainable
for layers in model.layers:
    if layers.name == 'genus':
        layers.trainable = True
        
#Train Family and Genus for 2 epochs
model.fit_generator(train_datagen.flow_from_dataframe(dataframe=train,
                                                      directory='../input/herbarium-2020-fgvc7/nybg2020/train/',
                                                      x_col=""file_name"",
                                                      y_col=[""family"", ""genus"", ""category_id""],
                                                      target_size=(120, 120),
                                                      batch_size=batch_size,
                                                      class_mode='multi_output'),
                    validation_data=train_datagen.flow_from_dataframe(
                        dataframe=verif,
                        directory='../input/herbarium-2020-fgvc7/nybg2020/train/',
                        x_col=""file_name"",
                        y_col=[""family"", ""genus"", ""category_id""],
                        target_size=(120, 120),
                        batch_size=batch_size,
                        class_mode='multi_output'),
                    epochs=epochs,
                    steps_per_epoch=len(train)//batch_size,
                    validation_steps=len(verif)//batch_size,
                    verbose=1,
                    workers=8,
                    use_multiprocessing=False)

#Reshuffle the inputs
train, verif = tts(m, test_size=0.2, shuffle=True, random_state=17)
train = train[:40000]
verif = verif[:10000]

#Make the category_id layer Trainable
for layers in model.layers:
    if layers.name == 'category_id':
        layers.trainable = True
        
#Train them all for 2 epochs
model.fit_generator(train_datagen.flow_from_dataframe(dataframe=train,
                                                      directory='../input/herbarium-2020-fgvc7/nybg2020/train/',
                                                      x_col=""file_name"",
                                                      y_col=[""family"", ""genus"", ""category_id""],
                                                      target_size=(120, 120),
                                                      batch_size=batch_size,
                                                      class_mode='multi_output'),
                    validation_data=train_datagen.flow_from_dataframe(
                        dataframe=verif,
                        directory='../input/herbarium-2020-fgvc7/nybg2020/train/',
                        x_col=""file_name"",
                        y_col=[""family"", ""genus"", ""category_id""],
                        target_size=(120, 120),
                        batch_size=batch_size,
                        class_mode='multi_output'),
                    epochs=epochs,
                    steps_per_epoch=len(train)//batch_size,
                    validation_steps=len(verif)//batch_size,
                    verbose=1,
                    workers=8,
                    use_multiprocessing=False)

'''
for i in range(epochs):
    n = 1
    for X, Y in train_datagen.flow_from_dataframe(dataframe=train,
                                                  directory='../input/herbarium-2020-fgvc7/nybg2020/train/',
                                                  x_col=""file_name"",
                                                  y_col=[""family"", ""genus"", ""category_id""],
                                                  target_size=(120, 120),
                                                  batch_size=batch_size,
                                                  class_mode='multi_output'):
        model.train_on_batch(X, Y, reset_metrics=False)
        loss, fam_loss, gen_loss, cat_loss, fam_acc, gen_acc, cat_acc = model.evaluate(X, Y, verbose=False)
        if n%10==0:
            print(f""For epoch {i} batch {n}: {loss}, {fam_loss}, {gen_loss}, {cat_loss}, {fam_acc}, {gen_acc}, {cat_acc}"")
            for layers in model.layers:
                if layers.name == 'family' and fam_acc>0.90:
                    layers.trainable=False
                elif layers.name == 'genus':
                    if fam_acc>0.75:
                        layers.trainable=True
                    else:
                        layers.trainable=False
                elif layers.name == 'category_id':
                    if fam_acc>0.75 and gen_acc>0.5:
                        layers.trainable=True
                    else:
                        layers.trainable=False
        n += 1
'''",0,No Code Smell
2648,30313596,21,model.save('fg_model.h5'),0,No Code Smell
2649,30313596,22,"batch_size = 32
test_datagen = ImageDataGenerator(featurewise_center=False,
                                  featurewise_std_normalization=False)

generator = test_datagen.flow_from_dataframe(
        dataframe = test_df.iloc[:10000], #Limiting the test to the first 10,000 items
        directory = '../input/herbarium-2020-fgvc7/nybg2020/test/',
        x_col = 'file_name',
        target_size=(120, 120),
        batch_size=batch_size,
        class_mode=None,  # only data, no labels
        shuffle=False)

family, genus, category = model.predict_generator(generator, verbose=1)",0,No Code Smell
2650,30313596,23,"sub = pd.DataFrame()
sub['Id'] = test_df.image_id
sub['Id'] = sub['Id'].astype('int32')
sub['Predicted'] = np.concatenate([np.argmax(category, axis=1), 23718*np.ones((len(test_df.image_id)-len(category)))], axis=0)
sub['Predicted'] = sub['Predicted'].astype('int32')
display(sub)
sub.to_csv('category_submission.csv', index=False)",0,No Code Smell
2651,30313596,24,"sub['Predicted'] = np.concatenate([np.argmax(family, axis=1), np.zeros((len(test_df.image_id)-len(family)))], axis=0)
sub['Predicted'] = sub['Predicted'].astype('int32')
display(sub)
sub.to_csv('family_submission.csv', index=False)",0,No Code Smell
2652,30313596,25,"sub['Predicted'] = np.concatenate([np.argmax(genus, axis=1), np.zeros((len(test_df.image_id)-len(genus)))], axis=0)
sub['Predicted'] = sub['Predicted'].astype('int32')
display(sub)
sub.to_csv('genus_submission.csv', index=False)",0,No Code Smell
2653,30313596,26,"end_time = time.time()
total = end_time - start_time
h = total//3600
m = (total%3600)//60
s = total%60
print(""Total time spent: %i hours, %i minutes, and %i seconds"" %(h, m, s))",0,No Code Smell
2654,30042790,0,"import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        if filename.endswith('.jpg'):
            break
        print(os.path.join(dirname, filename))",1,Code Smell
2655,30042790,1,"sample_sub = pd.read_csv('../input/herbarium-2020-fgvc7/sample_submission.csv')
display(sample_sub)",1,Code Smell
2656,30042790,2,"import json, codecs
with codecs.open(""../input/herbarium-2020-fgvc7/nybg2020/train/metadata.json"", 'r',
                 encoding='utf-8', errors='ignore') as f:
    train_meta = json.load(f)
    
with codecs.open(""../input/herbarium-2020-fgvc7/nybg2020/test/metadata.json"", 'r',
                 encoding='utf-8', errors='ignore') as f:
    test_meta = json.load(f)",0,No Code Smell
2657,30042790,3,display(train_meta.keys()),1,Code Smell
2658,30042790,4,"train_df = pd.DataFrame(train_meta['annotations'])
display(train_df)",0,No Code Smell
2659,30042790,5,"train_cat = pd.DataFrame(train_meta['categories'])
train_cat.columns = ['family', 'genus', 'category_id', 'categort_name']
display(train_cat)",0,No Code Smell
2660,30042790,6,"train_img = pd.DataFrame(train_meta['images'])
train_img.columns = ['file_name', 'height', 'image_id', 'license', 'width']
display(train_img)",0,No Code Smell
2661,30042790,7,"train_reg = pd.DataFrame(train_meta['regions'])
train_reg.columns = ['region_id', 'region_name']
display(train_reg)",0,No Code Smell
2662,30042790,8,"train_df = train_df.merge(train_cat, on='category_id', how='outer')
train_df = train_df.merge(train_img, on='image_id', how='outer')
train_df = train_df.merge(train_reg, on='region_id', how='outer')",0,No Code Smell
2663,30042790,9,"print(train_df.info())

display(train_df)
",1,Code Smell
2664,30042790,10,"na = train_df.file_name.isna()
keep = [x for x in range(train_df.shape[0]) if not na[x]]
train_df = train_df.iloc[keep]",0,No Code Smell
2665,30042790,11,"dtypes = ['int32', 'int32', 'int32', 'int32', 'object', 'object', 'object', 'object', 'int32', 'int32', 'int32', 'object']
for n, col in enumerate(train_df.columns):
    train_df[col] = train_df[col].astype(dtypes[n])
print(train_df.info())
display(train_df)",0,No Code Smell
2666,30042790,12,"test_df = pd.DataFrame(test_meta['images'])
test_df.columns = ['file_name', 'height', 'image_id', 'license', 'width']
print(test_df.info())
display(test_df)",0,No Code Smell
2667,30042790,13,"#train_df.to_csv('full_train_data.csv', index=False)
#test_df.to_csv('full_test_data.csv', index=False)",0,No Code Smell
2668,30042790,14,print(len(train_df.category_id.unique())),0,No Code Smell
2669,30042790,15,"sub = pd.DataFrame()
sub['Id'] = test_df.image_id
sub['Predicted'] = list(map(int, np.random.randint(1, 32093, (test_df.shape[0]))))
display(sub)
sub.to_csv('submission.csv', index=False)",0,No Code Smell
2670,30646081,0,"import os, json, time, sys, math
import numpy as np
import pandas as pd 
from matplotlib import pyplot as plt
from PIL import Image

if 'google.colab' in sys.modules:
    %tensorflow_version 2.x
import tensorflow as tf

print(""Tensorflow version "" + tf.__version__)
AUTO = tf.data.experimental.AUTOTUNE 

start_time = time.time()",1,Code Smell
2671,30646081,1,"df = pd.read_csv('../input/data-for-datatse-herbarium/data.csv')
df1 = df.copy()
df1['file_name'] = df['file_name'].map(lambda x: x.split('/')[-1])
df1.head()
",1,Code Smell
2672,30646081,2,"TRAIN_PATTERN = '/kaggle/input/herbarium-2020-fgvc7/nybg2020/train/images/*/*/*.jpg'
TEST_PATTERN = '/kaggle/input/herbarium-2020-fgvc7/nybg2020/test/images/*/*/*.jpg'",0,No Code Smell
2673,30646081,3,shard_size = 32,0,No Code Smell
2674,30646081,4,"def display_from_dataset(dataset):
    plt.figure(figsize=(13,13))
    subplot=331
    for i, (image, label) in enumerate(dataset):
        plt.subplot(subplot)
        plt.axis('off')
        plt.imshow(image.numpy().astype(np.uint8))
        plt.title(label.numpy().decode(""utf-8""), fontsize=16)
        subplot += 1
        if i==8:
            break
    plt.tight_layout()
    plt.subplots_adjust(wspace=0.1, hspace=0.1)
    plt.show()
    
def decode_jpeg_and_label(filename):
    bits = tf.io.read_file(filename)
    image = tf.image.decode_jpeg(bits)
    label = tf.strings.split(tf.expand_dims(filename, axis=-1), sep='/')
    label = label.values[-1]
    return image, label

def resize_and_crop_image(image, label):
    w = tf.shape(image)[0]
    h = tf.shape(image)[1]
    tw = w//5
    th = h//5
    resize_crit = (w * th) / (h * tw)
    image = tf.cond(resize_crit < 1,
                    lambda: tf.image.resize(image, [w*tw/w, h*tw/w]), # if true
                    lambda: tf.image.resize(image, [w*th/h, h*th/h])  # if false
                   )
    nw = tf.shape(image)[0]
    nh = tf.shape(image)[1]
    image = tf.image.crop_to_bounding_box(image, (nw - tw) // 2, (nh - th) // 2, tw, th)
    return image, label

def recompress_image(image, label):
    height = tf.shape(image)[0]
    width = tf.shape(image)[1]
    image = tf.cast(image, tf.uint8)
    image = tf.image.encode_jpeg(image, optimize_size=True, chroma_downsampling=False)
    return image, label, height, width",0,No Code Smell
2675,30646081,5,"filenames = tf.data.Dataset.list_files(TRAIN_PATTERN) 
dataset   = filenames.map(decode_jpeg_and_label, num_parallel_calls=AUTO)
dataset   = dataset.map(resize_and_crop_image, num_parallel_calls=AUTO) 
dataset   = dataset.map(recompress_image, num_parallel_calls=AUTO)
dataset   = dataset.batch(shard_size)
dataset   = dataset.prefetch(AUTO)",0,No Code Smell
2676,30646081,6,!mkdir tfrecords,0,No Code Smell
2677,30646081,7,"
def _bytestring_feature(list_of_bytestrings):
    return tf.train.Feature(bytes_list=tf.train.BytesList(value=list_of_bytestrings))

def _int_feature(list_of_ints): # int64
    return tf.train.Feature(int64_list=tf.train.Int64List(value=list_of_ints))

def _float_feature(list_of_floats): # float32
    return tf.train.Feature(float_list=tf.train.FloatList(value=list_of_floats))
  

def to_tfrecord(tfrec_filewriter, img_bytes, label, family, genus, category_id, width, height):
    one_hot_family = np.eye(310)[family]
    one_hot_genus = np.eye(3678)[genus]
    one_hot_category_id = np.eye(32094)[category_id]

    feature = {
      ""image"": _bytestring_feature([img_bytes]),
      ""label"":  _bytestring_feature([label]),
        
      ""family"": _int_feature([family]),
      ""genus"": _int_feature([genus]),
      ""category_id"": _int_feature([category_id]),
        
      ""one_hot_family"": _float_feature(one_hot_family.tolist()),
      ""one_hot_genus"": _float_feature(one_hot_genus.tolist()),
      ""one_hot_category_id"": _float_feature(one_hot_category_id.tolist()),
        
      ""size"":  _int_feature([width, height])
    }
    return tf.train.Example(features=tf.train.Features(feature=feature))

print(""Writing TFRecords"")
stoped = 0
for shard, (image, label, height, width) in enumerate(dataset):
    if stoped > 0:
        break
    stoped +=1
    shard_size = image.numpy().shape[0]
    filename   = './tfrecords/' + ""{:02d}-{}.tfrec"".format(shard, shard_size)
  
    with tf.io.TFRecordWriter(filename) as out_file:
        for i in range(shard_size):
            lbl         = label.numpy()[i]
            family      = df1.loc[df1.file_name == lbl.decode('utf-8')]['family'].values[0]
            genus       = df1.loc[df1.file_name == lbl.decode('utf-8')]['genus'].values[0]
            category_id = df1.loc[df1.file_name == lbl.decode('utf-8')]['category_id'].values[0]
            
            example = to_tfrecord(out_file,
                            image.numpy()[i],
                            lbl,
                            family, 
                            genus, 
                            category_id,
                            height.numpy()[i],
                            width.numpy()[i])
            out_file.write(example.SerializeToString())
        print(""Wrote file {} containing {} records"".format(filename, shard_size))",0,No Code Smell
2678,30646081,8,"def read_tfrecord(example):
    features = {
        ""image"": tf.io.FixedLenFeature([], tf.string), 
        ""label"": tf.io.FixedLenFeature([], tf.string),
        
        ""family"": tf.io.FixedLenFeature([], tf.int64), 
        ""genus"": tf.io.FixedLenFeature([], tf.int64), 
        ""category_id"": tf.io.FixedLenFeature([], tf.int64), 
        
        ""one_hot_family"": tf.io.VarLenFeature(tf.float32) ,
        ""one_hot_genus"": tf.io.VarLenFeature(tf.float32) ,
        ""one_hot_category_id"": tf.io.VarLenFeature(tf.float32) ,

        ""size"": tf.io.FixedLenFeature([2], tf.int64) 
    }
    example = tf.io.parse_single_example(example, features)
    width = example['size'][0]
    height  = example['size'][1]
    image = tf.image.decode_jpeg(example['image'], channels=3)
    image = tf.reshape(image, [width,height, 3])
    
    label = example['label']
    
  
    return image, label

option_no_order = tf.data.Options()
option_no_order.experimental_deterministic = False

filenames = tf.io.gfile.glob('/kaggle/working/tfrecords/' + ""*.tfrec"")
dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO)
dataset = dataset.with_options(option_no_order)
dataset = dataset.map(read_tfrecord, num_parallel_calls=AUTO)
dataset = dataset.shuffle(300)",0,No Code Smell
2679,30646081,9,display_from_dataset(dataset),1,Code Smell
2680,30646081,10,"end_time = time.time()
total = end_time - start_time
h = total//3600
m = (total%3600)//60
s = total%60
print(""Total time spent: %i hours, %i minutes, and %i seconds"" %(h, m, s))",0,No Code Smell
2681,34782705,0,"## Herbarium 2020 - FGVC7
# read and prerpocess the json file.
import json
import codecs
import pandas as pd

# split the dataset.
from sklearn.model_selection import train_test_split

# deeplearning framework.
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Dense, Input, concatenate, Flatten, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.utils import plot_model
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import tensorflow as tf

# other python package.
from PIL import Image
import numpy as np
import os

## I. Data Preprocessing define function :
#-------------------<Data Preprocessing>----------------------------------------#
##   Given path of json file, return the DataFrame type data 
##   ** (merge all info into 1 DataFrame) **
def load_data(train_path, test_path):
    
    def load_jsonfile(json_path):
        with codecs.open(json_path,'r',encoding='utf-8',errors='ignore') as file:
            meta_data = json.load(file)  ## return dictionary type data.
        return meta_data 
    
    train_meta = load_jsonfile(train_path)
    test_meta = load_jsonfile(test_path)
    
    ## Extract from each information into corresponding DataFrame.
    ## 1. Annotation information.
    annotations = pd.DataFrame(train_meta['annotations'])
    ## Debug view : 
    #print(""\n\nAnnotations dataframe infomation : \n"")
    #print(annotations.info())
    
    ## 2. Categories information.
    categories = pd.DataFrame(train_meta['categories'])
    categories.columns = ['family', 'genus', 'category_id', 'category_name'] ## replace columns name.
    ## Debug view :
    #print(""\n\nCategories dataframe infomation : \n"")
    #print(categories.info())
    
    ## 3. Images information.
    images = pd.DataFrame(train_meta['images'])
    images.columns = ['file_name', 'height', 'image_id','license','width']
    ## Debug view :
    #print(""\n\nImages dataframe infomation : \n"")
    #print(images.info())
   
    ## 4. Regions information.
    regions = pd.DataFrame(train_meta['regions'])
    regions.columns = ['region_id','name']
    ## Debug view :
    #print(""\n\nRegions dataframe infomation : \n"")
    #print(regions.info())
    
    ## Merge all dataframe to get raw training dataframe.
    raw_train_df = annotations.merge(categories,on='category_id', how=""left""
                                     ).merge(images, on=""image_id"", how=""outer""
                                            ).merge(regions, on=""region_id"", how=""outer"")
    
    raw_train_df = raw_train_df[['file_name','family','genus','category_id']]
    ## Debug view :
    #print(""\n\nRaw dataframe of training set : \n"")
    #print(raw_train_df.info())
    #print(""\n\n The family, genus data type is object, which should be transformed into int type \
    #       via indicator (use index of list to present the corresponding content)"")
    
    ## Preprocess the dataframe.
    ## 1. unique the element in the frame, \
    ##           and replace the 'category type' data into corresponding 'index type' data in the list.
    name_list = raw_train_df['family'].unique().tolist()
    raw_train_df.loc[:,'family'] = raw_train_df['family'].map(lambda x:name_list.index(x))
    genus_list = raw_train_df['genus'].unique().tolist()
    raw_train_df.loc[:,'genus'] = raw_train_df['genus'].map(lambda x:genus_list.index(x))
    
    
    ## 2. redeclare the data type to shrink the memory usage.
    train_df = raw_train_df.astype({'family':'int16','genus':'int16','category_id':'int16'})
    
    ## Extract the test dataframe information.
    raw_test_df = pd.DataFrame(test_meta['images'])
    raw_test_df.columns = ['file_name', 'height', 'image_id','license','width']
    test_df = raw_test_df[['image_id','file_name']]
    return train_df, test_df

def crop(batch_x):
    cut1 = int(0.1*batch_x.shape[1])
    cut2 = int(0.05*batch_x.shape[2])
    return batch_x[:,cut1:-cut1,cut2:-cut2]
## II. Build Classifier :
#----------------------<self-build>------------------------#
def get_cnn_classifier(img_shape):
    '''
    def prerpocess_img(x):
        if isinstance(x, np.ndarray):  ## Training phase, instance input.
            return x/127.5 -1 
        else:  ## Symbolic preprocess.
            return tf.sub(tf.div(x, 127.5), 1)
    '''
    def build_classifier(img_shape):
        model = Sequential({
            Conv2D(filters=64, kernel_size=5, activation='relu',\
                   stride=2, input_shape=img_shape),
            MaxPooling2D(2),
            Conv2D(128, 3, activation='relu', padding='same'),
            Conv2D(128, 3, activation='relu', padding='same'),
            MaxPooling2D(2),
            Conv2D(256, 3, activation='relu', padding='same'),
            Conv2D(256, 3, activation='relu', padding='same'),
            MaxPooling2D(2),
            Flatten(),
            Dense(128, activation='relu'),
            Droupout(0.25),
            Dense(64, activation='relu'),
            Droupout(0.2),
            Dense(32094, activation='softmax')      
        })
        return model
    
#-------------------<keras applications>-------------------#
## VGG19
def get_VGG_based_Classifier(img_shape):
    ## Closure function : 
    def __preprocess_vgg(x):
        ## Take a HR image [-1, 1], convert to [0, 255], then to input for VGG network

        if isinstance(x, np.ndarray):  ## Training phase, instance input.
            return preprocess_input(x) 
        else:  ## Symbolic preprocess.
            return Lambda(lambda x: preprocess_input(x))(x)
    
    def use_raw_VGG():
        pass
        
    ## Build Basic VGG19 network :
    #prepro_VGG19 = prepro_no_top_VGG()
    #prepro_VGG19()
    ## The modification of output prediction :  output 320000 specimens
    ## Please take the reference of keras VGG application.
    #VGG_classifier = Model(inputs=prepro_VGG19.input, output=)
    
    VGG_classifier = VGG19(weights=None, include_top=True, \
                      input_shape=img_shape, classes=32000)
    VGG_classifier.compile(loss='mae',optimizer='adam')
    return VGG_classifier
    
## ResNet50
in_out_size = (120*120) + 3
def xavier(shape, dtype=None):
    return np.random.rand(*shape)*np.sqrt(1/in_out_size)
def create_model():
    actual_shape = (crop(np.zeros((1,img_shape[0],img_shape[1],img_shape[2]))).shape)[1:]
    i = Input(actual_shape)
    x = ResNet50(weights='imagenet', include_top=False, input_shape=actual_shape, pooling='max')(i)
    x = Dropout(0.5)(x)
    x = Flatten()(x)
    o1 = Dense(310, activation='softmax', name='family', kernel_initializer=xavier)(x)
    
    o2 = concatenate([o1, x])
    o2 = Dense(3678, activation='softmax', name='genus', kernel_initializer=xavier)(o2)
    
    o3 = concatenate([o1, o2, x])
    o3 = Dense(32094, activation='softmax', name='category_id', kernel_initializer=xavier)(o3)
    model = Model(inputs=i,outputs=[o1,o2,o3])
    model.layers[1].trainable = False
    model.get_layer('genus').trainable = False
    model.get_layer('category_id').trainable = False
    return model

def compile(model,learning_rate=0.005):
    model.compile(optimizer=Adam(learning_rate=0.005),loss=[""sparse_categorical_crossentropy"",
                                     ""sparse_categorical_crossentropy"",
                                     ""sparse_categorical_crossentropy""],
                                metrics=['accuracy'])
    


#TRAINSTEPS = (X_train.shape[0]//batchsize)+1
#VALSTEPS = (X_dev.shape[0]//batchsize)+1

## Main part :
# self-define params : *(argparse)
batch_size=64
learning_rate=0.01
epochs=1000
img_shape = (200, 200, 3) # original height = 1000, width= 682    219673 667    212347 676    190476
train_path = '/kaggle/input/herbarium-2020-fgvc7/nybg2020/train/metadata.json'
test_path = '/kaggle/input/herbarium-2020-fgvc7/nybg2020/test/metadata.json'

## I. Load & Preprocess data :
train_df, test_df = load_data(train_path, test_path)

nmb_cat = train_df['category_id'].max()+1
nmb_gen = train_df['genus'].max()+1
nmb_fam = train_df['family'].max()+1

train_data, test_data = train_test_split(train_df, test_size=0.05, shuffle=True, random_state=13)
print(train_data.info())

## II. Build classifier :
#VGG_classifier = get_VGG_based_Classifier(img_shape)
ResNet_classifier = create_model()

## III. Image Augumentation and Build the data flow :
                ## Image Augumentation.
train_datagen = ImageDataGenerator(horizontal_flip = True, vertical_flip = True, \
                                   rotation_range = 180, zoom_range=0.3, fill_mode='nearest')
                                   
            ## setting data flow extract from DataFrame type data.
data_flow = train_datagen.flow_from_dataframe(dataframe=train_data, \
                                  directory='../input/herbarium-2020-fgvc7/nybg2020/train/', \
                                  x_col=""file_name"", y_col=[""family"", ""genus"", ""category_id""], \
                                  target_size=img_shape, batch_size=batch_size, \
                                  class_mode='multi_output')

## IV. Training phase :
## In the training phase, we'll first predict the family, and then use the family to predict the genus,
##    and so on, use the predicted family & genus to predict category_id.
#VGG_classifier.fit_generator(data_flow, epochs=epochs, verbose=2, workers=4, use_multiprocessing=False)
#VGG_classifier.save('./classifier.h5')
model = create_model()
compile(model,learning_rate)
#model.summary()
filename=""classifier.h5""
model.save_weights(filename)
print(""Weights saved to {}"".format(filename))
plot_model(model, show_shapes=True, show_layer_names=True)",0,No Code Smell
2682,34782705,1,"## V. Prediction phase :
batch_size = 32
test_datagen = ImageDataGenerator(featurewise_center=False, 
                                    featurewise_std_normalization=False)

generator = test_datagen.flow_from_dataframe(
        dataframe = test_df.iloc[:10000], #Limiting the test to the first 10,000 items
        directory = '../input/herbarium-2020-fgvc7/nybg2020/test/',
        x_col = 'file_name',
        target_size=(120, 120),
        batch_size=batch_size,
        class_mode=None,  # only data, no labels
        shuffle=False)

family, genus, category = model.predict_generator(generator, verbose=1)",0,No Code Smell
2683,34782705,2,"sub = pd.DataFrame()
sub['Id'] = test_df.image_id
sub['Id'] = sub['Id'].astype('int32')
sub['Predicted'] = np.concatenate([np.argmax(category, axis=1), 23718*np.ones((len(test_df.image_id)-len(category)))], axis=0)
sub['Predicted'] = sub['Predicted'].astype('int32')
display(sub)
sub.to_csv('category_submission.csv', index=False)",0,No Code Smell
2684,34782705,3,"sub['Predicted'] = np.concatenate([np.argmax(family, axis=1), np.zeros((len(test_df.image_id)-len(family)))], axis=0)
sub['Predicted'] = sub['Predicted'].astype('int32')
display(sub)
sub.to_csv('family_submission.csv', index=False)",0,No Code Smell
2685,34782705,4,"sub['Predicted'] = np.concatenate([np.argmax(genus, axis=1), np.zeros((len(test_df.image_id)-len(genus)))], axis=0)
sub['Predicted'] = sub['Predicted'].astype('int32')
display(sub)
sub.to_csv('genus_submission.csv', index=False)",0,No Code Smell
2686,34373570,0,"''' This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only ""../input/"" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        if filename.endswith('.jpg'):
            break
        print(os.path.join(dirname, filename))

# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session'''",1,Code Smell
2687,34373570,1,"#The kernal does not have the memory to train this model. But here's what I had done anyways.

'''import cv2
import re
from PIL import Image
import matplotlib.pyplot as plt
from keras.preprocessing.image import img_to_array, image
from keras.applications.resnet50 import preprocess_input, decode_predictions, resnet50
from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D
from keras.layers import Dropout, Flatten, Dense, Activation
from keras.models import Sequential 
from sklearn.model_selection import train_test_split
from keras import optimizers
from keras.applications.vgg16 import VGG16
from keras.applications.vgg19 import VGG19
from keras.models import Model
from keras.preprocessing.image import ImageDataGenerator
from keras import backend as K'''",1,Code Smell
2688,34373570,2,"#%matplotlib inline
#import matplotlib.pyplot as plt
import numpy as np  
import pandas as pd
import torch

from torch import nn
from torch import optim
#import torch.nn.functional as F
from torchvision import datasets, transforms, models",0,No Code Smell
2689,34373570,3,"import json

with open('/kaggle/input/herbarium-2020-fgvc7/nybg2020/train/metadata.json', 'r', errors='ignore') as f:
    train_metadata = json.load(f)",0,No Code Smell
2690,34373570,4,#train_metadata.keys(),1,Code Smell
2691,34373570,5,"with open('/kaggle/input/herbarium-2020-fgvc7/nybg2020/test/metadata.json', 'r', errors='ignore') as f:
    test_metadata = json.load(f)",0,No Code Smell
2692,34373570,6,#test_metadata.keys(),1,Code Smell
2693,34373570,7,sub_samp = pd.read_csv('/kaggle/input/herbarium-2020-fgvc7/sample_submission.csv'),1,Code Smell
2694,34373570,8,sub_samp,0,No Code Smell
2695,34373570,9,"test_dir = '/kaggle/input/herbarium-2020-fgvc7/nybg2020/test/'
train_dir = '/kaggle/input/herbarium-2020-fgvc7/nybg2020/train/'",0,No Code Smell
2696,34373570,10,train_ann = pd.DataFrame(train_metadata['annotations']),0,No Code Smell
2697,34373570,11,#train_ann,0,No Code Smell
2698,34373570,12,#train_cat = pd.DataFrame(train_metadata['categories']),0,No Code Smell
2699,34373570,13,#train_cat,0,No Code Smell
2700,34373570,14,#train_imgs = pd.DataFrame(train_metadata['images']),0,No Code Smell
2701,34373570,15,#train_imgs,0,No Code Smell
2702,34373570,16,#train_metadata['info'],1,Code Smell
2703,34373570,17,#test_metadata['info'],1,Code Smell
2704,34373570,18,#train_metadata['licenses'],0,No Code Smell
2705,34373570,19,#test_metadata['licenses'],0,No Code Smell
2706,34373570,20,#train_metadata['regions'],0,No Code Smell
2707,34373570,21,test_imgs = pd.DataFrame(test_metadata['images']),0,No Code Smell
2708,34373570,22,#test_imgs,0,No Code Smell
2709,34373570,23,#train_files_names = train_imgs['file_name'].values,0,No Code Smell
2710,34373570,24,#train_files_names,0,No Code Smell
2711,34373570,25,#test_file_names = test_imgs['file_name'].values,0,No Code Smell
2712,34373570,26,#test_file_names,0,No Code Smell
2713,34373570,27,X = train_ann['image_id'].values,0,No Code Smell
2714,34373570,28,#X,0,No Code Smell
2715,34373570,29,y = train_ann['category_id'].values,0,No Code Smell
2716,34373570,30,#y.size,0,No Code Smell
2717,34373570,31,"train_torch_y = torch.tensor(y, dtype=torch.long ).view(35543, 29)",0,No Code Smell
2718,34373570,32,#train_torch_y[0].shape ,0,No Code Smell
2719,34373570,33,#train_ann['category_id'].nunique(),0,No Code Smell
2720,34373570,34,test_X = test_imgs['id'].values,0,No Code Smell
2721,34373570,35,#test_X,0,No Code Smell
2722,34373570,36,"'''def convert_to_tensor(path_img):
    img = image.load_img(path_img, target_size = (224,224))
    img_arr = image.img_to_array(img)
    return np.expand_dims(img_arr, axis = 0)'''",0,No Code Smell
2723,34373570,37,"'''def convert_all_tensor(paths_imgs):
    tensor_list = [convert_to_tensor(i) for i in paths_imgs]
    return np.vstack(tensor_list)'''",0,No Code Smell
2724,34373570,38,"#train_tensors = convert_all_tensor(train_dir+train_files_names).astype('float64')/255 #took too much memory
#test_tensors = convert_all_tensor(train_dir+train_files_names).astype('float64')/255",0,No Code Smell
2725,34373570,39,"#train_tensors.shape, test_tensors.shape",0,No Code Smell
2726,34373570,40,,0,No Code Smell
2727,34373570,41,,0,No Code Smell
2728,34373570,42,,0,No Code Smell
2729,34373570,43,"data_transforms_train = transforms.Compose([transforms.RandomRotation(30),
                                       transforms.RandomResizedCrop(224),
                                       transforms.RandomHorizontalFlip(),
                                       transforms.ToTensor(),
                                       transforms.Normalize([0.485, 0.456, 0.406], 
                                                            [0.229, 0.224, 0.225])])



image_datasets_train =  datasets.ImageFolder(train_dir, transform=data_transforms_train)



dataloaders_train = torch.utils.data.DataLoader(image_datasets_train, batch_size=29)


",0,No Code Smell
2730,34373570,44,#next(iter(dataloaders_train))[0].shape,0,No Code Smell
2731,34373570,45,"data_transforms_test = transforms.Compose([transforms.Resize(256),
                                      transforms.CenterCrop(224),
                                      transforms.ToTensor(),
                                      transforms.Normalize([0.485, 0.456, 0.406], 
                                                           [0.229, 0.224, 0.225])])

image_datasets_test = datasets.ImageFolder(test_dir, transform=data_transforms_test)

dataloaders_test = torch.utils.data.DataLoader(image_datasets_test, batch_size=44)",0,No Code Smell
2732,34373570,46,"device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")
model = models.vgg16_bn(pretrained=True)

for param in model.parameters():
    param.requires_grad = False",0,No Code Smell
2733,34373570,47,"model.classifier = nn.Sequential(nn.Linear(25088, 12544),
                                 nn.ReLU(),
                                 nn.Dropout(0.2),
                                 nn.Linear(12544, 32093),
                                 nn.LogSoftmax(dim=1))",0,No Code Smell
2734,34373570,48,"
criterion = nn.NLLLoss()
optimizer = optim.Adam(model.classifier.parameters(), lr=0.003)",0,No Code Smell
2735,34373570,49,#train_torch_y[0],0,No Code Smell
2736,34373570,50,model.to(device),0,No Code Smell
2737,34373570,51,"epochs = 2
steps = 0
running_loss = 0
print_every = 100
for epoch in range(epochs):
    for inputs, labels in zip(dataloaders_train,train_torch_y):
        steps += 1
        inputs, labels = inputs[0].to(device), labels.to(device)
        
        optimizer.zero_grad()
        
        logps = model.forward(inputs)
        loss = criterion(logps, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        
        if steps % print_every == 0:
               
            print(f""Epoch {epoch+1}/{epochs}.. ""
                  f""Train loss: {running_loss/print_every:.3f}.. "")
            running_loss = 0
        if steps % 2000 == 0:
            break #to get out of loop before memory is full",0,No Code Smell
2738,34373570,52,#torch.cuda.is_available(),0,No Code Smell
2739,34373570,53,,0,No Code Smell
2740,34373570,54,,0,No Code Smell
2741,34373570,55,,0,No Code Smell
2742,34373570,56,,0,No Code Smell
2743,34373570,57,"model.eval()
top_c = []
top_probs = []
with torch.no_grad():
    for inputs, labels in dataloaders_test:
        inputs  = inputs.to(device) 
        logps = model.forward(inputs)
                           
        ps = torch.exp(logps)
        top_p, top_class = ps.topk(1, dim=1)
        top_c.append(top_class)
        top_probs.append(top_p)
model.train()",0,No Code Smell
2744,34373570,58,"

#print(top_c)",0,No Code Smell
2745,34373570,59,#print(top_probs),0,No Code Smell
2746,34373570,60,sub = sub_samp.copy(),0,No Code Smell
2747,34373570,61,topc_list = [torch.Tensor.cpu(i).numpy().tolist() for i in top_c],0,No Code Smell
2748,34373570,62,"from functools import reduce
topc_list1 = reduce(lambda x,y: x+y, topc_list)
topc_list2 = reduce(lambda x,y: x+y, topc_list1)",0,No Code Smell
2749,34373570,63,#topc_list2,0,No Code Smell
2750,34373570,64,,0,No Code Smell
2751,34373570,65,"sub['Predicted'] = list(map(int,topc_list2))",0,No Code Smell
2752,34373570,66,"sub.to_csv('submission.csv', index=False)",0,No Code Smell
2753,34373570,67,sub,0,No Code Smell
2754,34373570,68,,0,No Code Smell
2755,31847564,0,"import numpy as np                                    # Array, Linear Algebra
from torch.utils.data.dataset import random_split     # spliting inTrain Val
import pandas as pd                                   # handling CSV
import os                                             # For File handling
import random                                         # Choosing from images dataset
import time                                           # timing Epochs  
from tqdm.notebook import tqdm                        # Testing
from os.path import join                              # File Handling
from torchvision import transforms                    # Data Aug
import torch                                          # Framework
from PIL import Image                                 # Loading Image
from torch.utils.data import Dataset, DataLoader      # Dataset
import torch.nn.functional as F                       # Function
import json                                           # Loading Metadat
from PIL import  ImageOps                             # Data Aug 
from PIL.Image import open as openIm                  # Image Handling
import matplotlib.pyplot  as plt                      # Ploting Image
import cv2",1,Code Smell
2756,31847564,1,"TRAIN       = ""../input/herbarium-2020-fgvc7/nybg2020/train/""
TEST        = ""../input/herbarium-2020-fgvc7/nybg2020/test/""
META        = ""metadata.json""
BATCH_SIZE  = 7
NUM_WORKERS = 2
BATCH_EVAL  = 1
SHUFFLE     = True
EPOCHS      = 3
RESIZE      = (800, 600)
CLASSES     = 32094
LENGTH      = 2*CLASSES",0,No Code Smell
2757,31847564,2,"with open(join(TRAIN,META),""r"", encoding = ""ISO-8859-1"") as file:
    metadata = json.load(file)
print(""Metadata has {} sections. These section has all the Information regarding Images in dataset like class, id, size etc. "".format(len(list(metadata.keys()))))
print(""Let us see al the sections in metadata:- "", [print("" - "",i) for i in list(metadata.keys())])

print(""Number of Images in our Training set is:- "", len(metadata[""images""]))
print(""\n Let us see how every section of Dataset Looks like:-\n"")
for i in list(metadata.keys()):
    print("" - sample and number of elements in {} :- "".format(i),len(list(metadata[i])))
    print(""\t"",list(metadata[i])[0], end = ""\n\n"")",0,No Code Smell
2758,31847564,3,"with open(join(TEST,META),""r"", encoding = ""ISO-8859-1"") as file:
    metadata_test = json.load(file)
print(""Metadata has {} sections. These section has all the Information regarding Images in dataset like class, id, size etc. "".format(len(list(metadata_test.keys()))))
print(""Let us see al the sections in metadata:- "", [print("" - "",i) for i in list(metadata_test.keys())])

print(""Number of Images in our Training set is:- "", len(metadata_test[""images""]))
print(""\n Let us see how every section of Dataset Looks like:-\n"")
for i in list(metadata_test.keys()):
    print("" - sample and number of elements in {} :- "".format(i),len(list(metadata_test[i])))
    print(""\t"",list(metadata_test[i])[0], end = ""\n\n"")",0,No Code Smell
2759,31847564,4,"train_img = pd.DataFrame(metadata['images'])
train_ann = pd.DataFrame(metadata['annotations'])
train_df = pd.merge(train_ann, train_img, left_on='image_id', right_on='id', how='left').drop('image_id', axis=1).sort_values(by=['category_id'])
train_df.head()",0,No Code Smell
2760,31847564,5,"im = Image.open(""../input/herbarium-2020-fgvc7/nybg2020/train/images/156/72/354106.jpg"")
print(""Category Id is 15672 and Image Id is 354106 is shown below"")
im",0,No Code Smell
2761,31847564,6,"size_of_img = (40, 40)
fig=plt.figure(figsize=(80,80))
for i in range(60):
    ax=fig.add_subplot(20,20,i+1)
    img = cv2.imread(TRAIN + metadata[""images""][i][""file_name""])
    img = cv2.resize(img,size_of_img)
    ax.imshow(img)
plt.show()",0,No Code Smell
2762,31847564,7,"import time
start_time = time.time()",0,No Code Smell
2763,31847564,8,"import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        if filename.endswith('.jpg'):
            break
        print(os.path.join(dirname, filename))",1,Code Smell
2764,31847564,9,"sample_sub = pd.read_csv('../input/herbarium-2020-fgvc7/sample_submission.csv')
display(sample_sub)",1,Code Smell
2765,31847564,10,"import json, codecs
with codecs.open(""../input/herbarium-2020-fgvc7/nybg2020/train/metadata.json"", 'r',
                 encoding='utf-8', errors='ignore') as f:
    train_meta = json.load(f)
    
with codecs.open(""../input/herbarium-2020-fgvc7/nybg2020/test/metadata.json"", 'r',
                 encoding='utf-8', errors='ignore') as f:
    test_meta = json.load(f)",0,No Code Smell
2766,31847564,11,display(train_meta.keys()),1,Code Smell
2767,31847564,12,"train_df = pd.DataFrame(train_meta['annotations'])
display(train_df)",0,No Code Smell
2768,31847564,13,"train_cat = pd.DataFrame(train_meta['categories'])
train_cat.columns = ['family', 'genus', 'category_id', 'category_name']
display(train_cat)",0,No Code Smell
2769,31847564,14,"train_img = pd.DataFrame(train_meta['images'])
train_img.columns = ['file_name', 'height', 'image_id', 'license', 'width']
display(train_img)",0,No Code Smell
2770,31847564,15,"train_reg = pd.DataFrame(train_meta['regions'])
train_reg.columns = ['region_id', 'region_name']
display(train_reg)",0,No Code Smell
2771,31847564,16,"train_df = train_df.merge(train_cat, on='category_id', how='outer')
train_df = train_df.merge(train_img, on='image_id', how='outer')
train_df = train_df.merge(train_reg, on='region_id', how='outer')",0,No Code Smell
2772,31847564,17,"print(train_df.info())
display(train_df)",1,Code Smell
2773,31847564,18,"na = train_df.file_name.isna()
keep = [x for x in range(train_df.shape[0]) if not na[x]]
train_df = train_df.iloc[keep]",0,No Code Smell
2774,31847564,19,"dtypes = ['int32', 'int32', 'int32', 'int32', 'object', 'object', 'object', 'object', 'int32', 'int32', 'int32', 'object']
for n, col in enumerate(train_df.columns):
    train_df[col] = train_df[col].astype(dtypes[n])
print(train_df.info())
display(train_df)",0,No Code Smell
2775,31847564,20,"test_df = pd.DataFrame(test_meta['images'])
test_df.columns = ['file_name', 'height', 'image_id', 'license', 'width']
print(test_df.info())
display(test_df)",0,No Code Smell
2776,31847564,21,"train_df.to_csv('full_train_data.csv', index=False)
test_df.to_csv('full_test_data.csv', index=False)",0,No Code Smell
2777,31847564,22,"print(""Total Unique Values for each columns:"")
print(""{0:10s} \t {1:10d}"".format('train_df', len(train_df)))
for col in train_df.columns:
    print(""{0:10s} \t {1:10d}"".format(col, len(train_df[col].unique())))",0,No Code Smell
2778,31847564,23,"family = train_df[['family', 'genus', 'category_name']].groupby(['family', 'genus']).count()
display(family.describe())",0,No Code Smell
2779,31847564,24,"from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPool2D, Flatten, BatchNormalization, Input, concatenate
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.utils import plot_model
from sklearn.model_selection import train_test_split as tts

in_out_size = (120*120) + 3 #We will resize the image to 120*120 and we have 3 outputs
def xavier(shape, dtype=None):
    return np.random.rand(*shape)*np.sqrt(1/in_out_size)

def fg_model(shape, lr=0.001):
    '''Family-Genus model receives an image and outputs two integers indicating both the family and genus index.'''
    i = Input(shape)
    
    x = Conv2D(3, (3, 3), activation='relu', padding='same', kernel_initializer=xavier)(i)
    x = Conv2D(3, (5, 5), activation='relu', padding='same', kernel_initializer=xavier)(x)
    x = MaxPool2D(pool_size=(3, 3), strides=(3,3))(x)
    x = BatchNormalization()(x)
    x = Dropout(0.5)(x)
    x = Conv2D(16, (5, 5), activation='relu', padding='same', kernel_initializer=xavier)(x)
    #x = Conv2D(16, (5, 5), activation='relu', padding='same', kernel_initializer=xavier)(x)
    x = MaxPool2D(pool_size=(5, 5), strides=(5,5))(x)
    x = BatchNormalization()(x)
    x = Dropout(0.5)(x)
    x = Flatten()(x)
    
    o1 = Dense(310, activation='softmax', name='family', kernel_initializer=xavier)(x)
    
    o2 = concatenate([o1, x])
    o2 = Dense(3678, activation='softmax', name='genus', kernel_initializer=xavier)(o2)
    
    o3 = concatenate([o1, o2, x])
    o3 = Dense(32094, activation='softmax', name='category_id', kernel_initializer=xavier)(o3)
    
    x = Model(inputs=i, outputs=[o1, o2, o3])
    
    opt = Adam(lr=lr, amsgrad=True)
    x.compile(optimizer=opt, loss=['sparse_categorical_crossentropy', 
                                   'sparse_categorical_crossentropy', 
                                   'sparse_categorical_crossentropy'],
                 metrics=['accuracy'])
    return x

model = fg_model((120, 120, 3))
model.summary()
plot_model(model, to_file='full_model_plot.png', show_shapes=True, show_layer_names=True)",0,No Code Smell
2780,31847564,25,"from tensorflow.keras.preprocessing.image import ImageDataGenerator

train_datagen = ImageDataGenerator(featurewise_center=False,
                                     featurewise_std_normalization=False,
                                     rotation_range=180,
                                     width_shift_range=0.1,
                                     height_shift_range=0.1,
                                     zoom_range=0.2)",0,No Code Smell
2781,31847564,26,"m = train_df[['file_name', 'family', 'genus', 'category_id']]
fam = m.family.unique().tolist()
m.family = m.family.map(lambda x: fam.index(x))
gen = m.genus.unique().tolist()
m.genus = m.genus.map(lambda x: gen.index(x))
display(m)",0,No Code Smell
2782,31847564,27,"train, verif = tts(m, test_size=0.2, shuffle=True, random_state=17)
train = train[:49000]
verif = verif[:1000]
shape = (120, 120, 3)
epochs = 2
batch_size = 32

model = fg_model(shape, 0.007)

#Disable the last two output layers for training the Family
for layers in model.layers:
    if layers.name == 'genus' or layers.name=='category_id':
        layers.trainable = False

#Train Family for 2 epochs
model.fit_generator(train_datagen.flow_from_dataframe(dataframe=train,
                                                      directory='../input/herbarium-2020-fgvc7/nybg2020/train/',
                                                      x_col=""file_name"",
                                                      y_col=[""family"", ""genus"", ""category_id""],
                                                      target_size=(120, 120),
                                                      batch_size=batch_size,
                                                      class_mode='multi_output'),
                    validation_data=train_datagen.flow_from_dataframe(
                        dataframe=verif,
                        directory='../input/herbarium-2020-fgvc7/nybg2020/train/',
                        x_col=""file_name"",
                        y_col=[""family"", ""genus"", ""category_id""],
                        target_size=(120, 120),
                        batch_size=batch_size,
                        class_mode='multi_output'),
                    epochs=epochs,
                    steps_per_epoch=len(train)//batch_size,
                    validation_steps=len(verif)//batch_size,
                    verbose=1,
                    workers=8,
                    use_multiprocessing=False)

#Reshuffle the inputs
train, verif = tts(m, test_size=0.2, shuffle=True, random_state=17)
train = train[:40000]
verif = verif[:10000]

#Make the Genus layer Trainable
for layers in model.layers:
    if layers.name == 'genus':
        layers.trainable = True
        
#Train Family and Genus for 2 epochs
model.fit_generator(train_datagen.flow_from_dataframe(dataframe=train,
                                                      directory='../input/herbarium-2020-fgvc7/nybg2020/train/',
                                                      x_col=""file_name"",
                                                      y_col=[""family"", ""genus"", ""category_id""],
                                                      target_size=(120, 120),
                                                      batch_size=batch_size,
                                                      class_mode='multi_output'),
                    validation_data=train_datagen.flow_from_dataframe(
                        dataframe=verif,
                        directory='../input/herbarium-2020-fgvc7/nybg2020/train/',
                        x_col=""file_name"",
                        y_col=[""family"", ""genus"", ""category_id""],
                        target_size=(120, 120),
                        batch_size=batch_size,
                        class_mode='multi_output'),
                    epochs=epochs,
                    steps_per_epoch=len(train)//batch_size,
                    validation_steps=len(verif)//batch_size,
                    verbose=1,
                    workers=8,
                    use_multiprocessing=False)

#Reshuffle the inputs
train, verif = tts(m, test_size=0.2, shuffle=True, random_state=17)
train = train[:40000]
verif = verif[:10000]

#Make the category_id layer Trainable
for layers in model.layers:
    if layers.name == 'category_id':
        layers.trainable = True
        
#Train them all for 2 epochs
model.fit_generator(train_datagen.flow_from_dataframe(dataframe=train,
                                                      directory='../input/herbarium-2020-fgvc7/nybg2020/train/',
                                                      x_col=""file_name"",
                                                      y_col=[""family"", ""genus"", ""category_id""],
                                                      target_size=(120, 120),
                                                      batch_size=batch_size,
                                                      class_mode='multi_output'),
                    validation_data=train_datagen.flow_from_dataframe(
                        dataframe=verif,
                        directory='../input/herbarium-2020-fgvc7/nybg2020/train/',
                        x_col=""file_name"",
                        y_col=[""family"", ""genus"", ""category_id""],
                        target_size=(120, 120),
                        batch_size=batch_size,
                        class_mode='multi_output'),
                    epochs=epochs,
                    steps_per_epoch=len(train)//batch_size,
                    validation_steps=len(verif)//batch_size,
                    verbose=1,
                    workers=8,
                    use_multiprocessing=False)

",0,No Code Smell
2783,31847564,28,model.save('fg_model.h5'),0,No Code Smell
2784,31847564,29,"batch_size = 32
test_datagen = ImageDataGenerator(featurewise_center=False,
                                  featurewise_std_normalization=False)

generator = test_datagen.flow_from_dataframe(
        dataframe = test_df.iloc[:10000], #Limiting the test to the first 10,000 items
        directory = '../input/herbarium-2020-fgvc7/nybg2020/test/',
        x_col = 'file_name',
        target_size=(120, 120),
        batch_size=batch_size,
        class_mode=None,  # only data, no labels
        shuffle=False)

family, genus, category = model.predict_generator(generator, verbose=1)",0,No Code Smell
2785,31847564,30,"sub = pd.DataFrame()
sub['Id'] = test_df.image_id
sub['Id'] = sub['Id'].astype('int32')
sub['Predicted'] = np.concatenate([np.argmax(category, axis=1), 23718*np.ones((len(test_df.image_id)-len(category)))], axis=0)
sub['Predicted'] = sub['Predicted'].astype('int32')
display(sub)
sub.to_csv('category_submission.csv', index=False)",0,No Code Smell
2786,31847564,31,"sub['Predicted'] = np.concatenate([np.argmax(family, axis=1), np.zeros((len(test_df.image_id)-len(family)))], axis=0)
sub['Predicted'] = sub['Predicted'].astype('int32')
display(sub)
sub.to_csv('family_submission.csv', index=False)",0,No Code Smell
2787,31847564,32,"sub['Predicted'] = np.concatenate([np.argmax(genus, axis=1), np.zeros((len(test_df.image_id)-len(genus)))], axis=0)
sub['Predicted'] = sub['Predicted'].astype('int32')
display(sub)
sub.to_csv('genus_submission.csv', index=False)",0,No Code Smell
2788,31847564,33,"end_time = time.time()
total = end_time - start_time
h = total//3600
m = (total%3600)//60
s = total%60
print(""Total time spent: %i hours, %i minutes, and %i seconds"" %(h, m, s))",0,No Code Smell
2789,30800274,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the ""../input/"" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# Any results you write to the current directory are saved as output.",1,Code Smell
2790,30800274,1,"import matplotlib.pyplot as plt
import seaborn as sns",1,Code Smell
2791,30800274,2,"data = pd.read_csv(""/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_train.csv"")",1,Code Smell
2792,30800274,3,data.tail(),0,No Code Smell
2793,30800274,4,data.info(),1,Code Smell
2794,30800274,5,"data.set_index('Date',inplace=True)",0,No Code Smell
2795,30800274,6,data.index = pd.to_datetime(data.index),0,No Code Smell
2796,30800274,7,"data.asfreq = ""D""",0,No Code Smell
2797,30800274,8,PositiveCase =  data[data['ConfirmedCases']>0],0,No Code Smell
2798,30800274,9,"
PositiveCase['ConfirmedCases'].plot()",0,No Code Smell
2799,30800274,10,"
PositiveCase['Fatalities'].plot()",0,No Code Smell
2800,30800274,11,"PositiveCase[""ConfirmedCases""][0]",0,No Code Smell
2801,30800274,12,"PositiveCase['FirstDerivative']= 0
for i in range(len(PositiveCase)):
    if i==0:
        PositiveCase[""FirstDerivative""][i]= 0
    else:
        PositiveCase['FirstDerivative'][i]= (PositiveCase['ConfirmedCases'][i]-PositiveCase['ConfirmedCases'][0])/i
    ",0,No Code Smell
2802,30800274,13,PositiveCase.head(),0,No Code Smell
2803,30800274,14,PositiveCase['FirstDerivative'].plot(),0,No Code Smell
2804,30800274,15,"def adf_test(series,title=''):
    """"""
    Pass in a time series and an optional title, returns an ADF report
    """"""
    print(f'Augmented Dickey-Fuller Test: {title}')
    result = adfuller(series.dropna(),autolag='AIC') # .dropna() handles differenced data
    
    labels = ['ADF test statistic','p-value','# lags used','# observations']
    out = pd.Series(result[0:4],index=labels)

    for key,val in result[4].items():
        out[f'critical value ({key})']=val
        
    print(out.to_string())          # .to_string() removes the line ""dtype: float64""
    
    if result[1] <= 0.05:
        print(""Strong evidence against the null hypothesis"")
        print(""Reject the null hypothesis"")
        print(""Data has no unit root and is stationary"")
    else:
        print(""Weak evidence against the null hypothesis"")
        print(""Fail to reject the null hypothesis"")
        print(""Data has a unit root and is non-stationary"")",0,No Code Smell
2805,30800274,16,"Positiveseries = PositiveCase[['ConfirmedCases','Fatalities']]",0,No Code Smell
2806,30800274,17,Positiveseries.head(),0,No Code Smell
2807,30800274,18,!pip install pmdarima,0,No Code Smell
2808,30800274,19,"from statsmodels.tsa.statespace.varmax import VARMAX, VARMAXResults
from statsmodels.tsa.stattools import adfuller
# from pmdarima import auto_arima
from statsmodels.tools.eval_measures import rmse",1,Code Smell
2809,30800274,20,"adf_test(Positiveseries['ConfirmedCases'],title=""Confirmed Cases"")",0,No Code Smell
2810,30800274,21,Positiveseries_tr = Positiveseries.diff(),0,No Code Smell
2811,30800274,22,"Positiveseries_tred = Positiveseries_tr.dropna()
adf_test(Positiveseries_tred['ConfirmedCases'],title=""Confirmed Cases"")",0,No Code Smell
2812,30800274,23,Positive = Positiveseries_tred.diff(),0,No Code Smell
2813,30800274,24,"Positived =  Positive.dropna()
adf_test(Positived['ConfirmedCases'],title=""Confirmed Cases"")",0,No Code Smell
2814,30800274,25,Positived1 = Positived.diff(),0,No Code Smell
2815,30800274,26,"Positived1 =  Positived1.dropna()
adf_test(Positived1['ConfirmedCases'],title=""Confirmed Cases"")",0,No Code Smell
2816,30800274,27,Positived1.head(),0,No Code Smell
2817,30800274,28,"from statsmodels.tsa.arima_model import ARMA,ARMAResults,ARIMA,ARIMAResults
",1,Code Smell
2818,30800274,29,# Trying varius ARIMA model for 3 differencing level,0,No Code Smell
2819,30800274,30,"ARIMA(1,3,1)",0,No Code Smell
2820,30800274,31,"model = ARIMA(Positiveseries['ConfirmedCases'],order=(1,0,1))
results = model.fit()
results.summary()",0,No Code Smell
2821,30800274,32,Positiveseries['ConfirmedCasesDiff1']= Positiveseries['ConfirmedCases'].diff(),0,No Code Smell
2822,30800274,33,Positiveseries.head(),0,No Code Smell
2823,30800274,34,"Positiveseries['Fatalitiesdiff1']= Positiveseries['Fatalities'].diff()
Positiveseries['ConfirmedCasesDiff2'] = Positiveseries['ConfirmedCasesDiff1'].diff()
Positiveseries['Fatalitiesdiff2']= Positiveseries['Fatalitiesdiff1'].diff()
Positiveseries['ConfirmedCasesDiff3'] = Positiveseries['ConfirmedCasesDiff2'].diff()
Positiveseries['Fatalitiesdiff3']= Positiveseries['Fatalitiesdiff2'].diff()",0,No Code Smell
2824,30800274,35,Positiveseries,0,No Code Smell
2825,30800274,36,Positiveseries.plot(),0,No Code Smell
2826,30800274,37,"Positiveseries[['ConfirmedCasesDiff3','Fatalitiesdiff3']].plot()",0,No Code Smell
2827,30800274,38,"#Adding rows for testing set
test = pd.read_csv(""/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_test.csv"",index_col='Date',parse_dates=True)",1,Code Smell
2828,30800274,39,test.head(),0,No Code Smell
2829,30800274,40,len(test),0,No Code Smell
2830,30800274,41,test = test.loc['25-03-2020':],0,No Code Smell
2831,30800274,42,,0,No Code Smell
2832,30800274,43,"for i in list(Positiveseries.columns):
    test[i] = 0",0,No Code Smell
2833,30800274,44,"test.drop(['ForecastId','Province/State','Lat','Long'],inplace=True,axis=1)",0,No Code Smell
2834,30800274,45,"test.drop('Country/Region',axis=1,inplace=True)",0,No Code Smell
2835,30800274,46,test.head(),0,No Code Smell
2836,30800274,47,Positiveseries = Positiveseries.append(test),0,No Code Smell
2837,30800274,48,Positiveseries.tail(),0,No Code Smell
2838,30800274,49,Positiveseries,0,No Code Smell
2839,30800274,50,"# Positiveseries.asfreq =""D""
for i in range(15,len(Positiveseries)):
    Positiveseries.iloc[i]['ConfirmedCasesDiff3'] = (Positiveseries.iloc[i-1]['ConfirmedCasesDiff3'] + Positiveseries.iloc[i-2]['ConfirmedCasesDiff3']+Positiveseries.iloc[i-3]['ConfirmedCasesDiff3']+Positiveseries.iloc[i-4]['ConfirmedCasesDiff3'])/4
    Positiveseries.iloc[i]['Fatalitiesdiff3'] = (Positiveseries.iloc[i-1]['Fatalitiesdiff3'] + Positiveseries.iloc[i-2]['Fatalitiesdiff3'])/2
    Positiveseries.iloc[i]['ConfirmedCasesDiff2'] = Positiveseries.iloc[i-1]['ConfirmedCasesDiff2']+ Positiveseries.iloc[i]['ConfirmedCasesDiff3']
# #     Positiveseries.iloc[i]['ConfirmedCasesDiff2'] = Positiveseries.iloc[i-1]['ConfirmedCasesDiff2']+ Positiveseries.iloc[i]['ConfirmedCasesDiff3']
    Positiveseries.iloc[i]['ConfirmedCasesDiff1'] = Positiveseries.iloc[i-1]['ConfirmedCasesDiff1']+ Positiveseries.iloc[i]['ConfirmedCasesDiff2']
    Positiveseries.iloc[i]['ConfirmedCases'] = Positiveseries.iloc[i-1]['ConfirmedCases']+ Positiveseries.iloc[i]['ConfirmedCasesDiff1']
    Positiveseries.iloc[i]['Fatalitiesdiff2']= Positiveseries.iloc[i-1]['Fatalitiesdiff2'] + Positiveseries.iloc[i]['Fatalitiesdiff3']
    Positiveseries.iloc[i]['Fatalitiesdiff1']= Positiveseries.iloc[i-1]['Fatalitiesdiff1'] + Positiveseries.iloc[i]['Fatalitiesdiff2']
    Positiveseries.iloc[i]['Fatalities']= Positiveseries.iloc[i-1]['Fatalities'] + Positiveseries.iloc[i]['Fatalitiesdiff1']
    
# Positiveseries['ConfirmedCasesDiff3']['24-03-2020']",0,No Code Smell
2840,30800274,51,"Output = Positiveseries[['ConfirmedCases','Fatalities']]",0,No Code Smell
2841,30800274,52,Output.loc['2020-04-03' :] =0,0,No Code Smell
2842,30800274,53,Output.head(),0,No Code Smell
2843,30800274,54,"submission = pd.read_csv(""/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_submission.csv"")",1,Code Smell
2844,30800274,55,submission.head(),0,No Code Smell
2845,30800274,56,submission['ConfirmedCases']= list(Output.loc['2020-03-12':]['ConfirmedCases']),0,No Code Smell
2846,30800274,57,submission['Fatalities']= list(Output.loc['2020-03-12':]['Fatalities']),0,No Code Smell
2847,30800274,58,"submission.to_csv(""submission.csv"",index=False)",0,No Code Smell
2848,30568127,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the ""../input/"" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# Any results you write to the current directory are saved as output.",1,Code Smell
2849,30568127,1,"import numpy as np
import pandas as pd
import scipy.optimize as opt
%matplotlib inline
import matplotlib.pyplot as plt
from tqdm import tqdm_notebook
from datetime import datetime, timedelta
from sklearn.metrics import mean_squared_log_error",0,No Code Smell
2850,30568127,2,"train = pd.read_csv('/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_train.csv')
train['Date'] = pd.to_datetime(train['Date'])
test = pd.read_csv('/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_test.csv')
test['Date'] = pd.to_datetime(test['Date'])
sub = pd.read_csv('/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_submission.csv')
# sub = sub.set_index('ForecastId').reset_index(drop=True)
print(test.shape)
print(train.shape)
print(sub.shape)",1,Code Smell
2851,30568127,3,train.head(5),0,No Code Smell
2852,30568127,4,test.head(5),0,No Code Smell
2853,30568127,5,sub,0,No Code Smell
2854,30568127,6,"def sigmoid(t, M, beta, alpha):
    return M / (1 + np.exp(-beta * (t - alpha)))",0,No Code Smell
2855,30568127,7,"NUMBER_UNCASEc= 47 
MAXIMUM = 1000
BOUNDS=(0, [2000, 1.0, 100])

x = list(range(len(train)))
min_date = min(train['Date'])
max_date = max(test['Date'])
cases = train['ConfirmedCases'].values
plt.plot(x, cases, label=""Train Data"")
print(""Case:"", cases)
popt, pcov = opt.curve_fit(sigmoid, x, cases, bounds=BOUNDS)
print(popt)
print(pcov)
M, beta, alpha = popt
plt.plot(x, sigmoid(x, M, beta, alpha), label=""Predict"")
# Place a legend to the right of this smaller subplot.
plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)

plt.show()",0,No Code Smell
2856,30568127,8,"M, beta, alpha = popt
x_forcast = list(range(train.shape[0], train.shape[0]+ test.shape[0]))
case_forecast = sigmoid(x_forcast, M, beta, alpha)
print(""case_forecast"",case_forecast)
sub[""ConfirmedCases""] = [int(i) for i in case_forecast]
plt.plot(x_forcast, case_forecast)",0,No Code Smell
2857,30568127,9,"deaths = train['Fatalities'].values
popt, pcov = opt.curve_fit(sigmoid, list(range(len(deaths))), deaths, bounds=BOUNDS)
M, beta, alpha = popt
death_forecast = sigmoid(x_forcast, M, beta, alpha)
print(""death_forecast"",death_forecast)
plt.plot(x_forcast, death_forecast)
sub[""Fatalities""] = [int(i) for i in death_forecast]
sub.to_csv('submission.csv', index=False)",0,No Code Smell
2858,30568127,10,sub,0,No Code Smell
2859,30568127,11,,0,No Code Smell
2860,30568127,12,,0,No Code Smell
2861,30790194,0,"import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

from sklearn.linear_model import LinearRegression

import warnings
warnings.filterwarnings('ignore')

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))",1,Code Smell
2862,30790194,1,"sub=pd.read_csv(""../input/covid19-local-us-ca-forecasting-week-1/ca_submission.csv"")
test=pd.read_csv(""../input/covid19-local-us-ca-forecasting-week-1/ca_test.csv"")
train=pd.read_csv(""../input/covid19-local-us-ca-forecasting-week-1/ca_train.csv"")",1,Code Smell
2863,30790194,2,"print(train.shape)
train.head()",0,No Code Smell
2864,30790194,3,"#Only taking data with confirmed cases
train=train[train.ConfirmedCases>0]
print(train.shape)
train.head()",0,No Code Smell
2865,30790194,4,"sns.lineplot(train.Id, train.ConfirmedCases)",0,No Code Smell
2866,30790194,5,"sns.regplot(train.Id, np.log(train.ConfirmedCases))",0,No Code Smell
2867,30790194,6,"model_1= LinearRegression()
x1=np.array(train.Id).reshape(-1,1)
y1=np.log(train.ConfirmedCases)
model_1.fit(x1,y1)
print(""R-squared score : "",model_1.score(x1,y1))

gr=np.power(np.e, model_1.coef_[0])
print(""Growth Factor : "", gr)
print(f""Growth Rate : {round((gr-1)*100,2)}%"")",0,No Code Smell
2868,30790194,7,"sns.regplot(train.ConfirmedCases,train.Fatalities)",0,No Code Smell
2869,30790194,8,"model_2= LinearRegression()
x2=np.array(train.ConfirmedCases).reshape(-1,1)
y2=train.Fatalities
model_2.fit(x2,y2)
print(""R-Squared Score= "",model_2.score(x2,y2))",0,No Code Smell
2870,30790194,9,test.head(),0,No Code Smell
2871,30790194,10,"#Making Id as unique key between test and train
test[""Id""]=50+test.ForecastId
test.head()",0,No Code Smell
2872,30790194,11,"test[""LogConf""]=model_1.predict(np.array(test.Id).reshape(-1,1))
test[""ConfirmedCases""]=np.exp(test.LogConf)//1
test[""Fatalities""]=model_2.predict(np.array(test.ConfirmedCases).reshape(-1,1))//1
test",0,No Code Smell
2873,30790194,12,"#Wherever confirmed cases and fatalities are available in train data, update it into test data
for id in train.Id:
    test.ConfirmedCases[test.Id==id]=train.ConfirmedCases[train.Id==id].sum()
    test.Fatalities[test.Id==id]=train.Fatalities[train.Id==id].sum()
test",0,No Code Smell
2874,30790194,13,### Prepare submission file,0,No Code Smell
2875,30790194,14,"sub.ConfirmedCases=test.ConfirmedCases
sub.Fatalities=test.Fatalities
sub.to_csv(""submission.csv"", index=False)",0,No Code Smell
2876,30790194,15,,0,No Code Smell
2877,30840463,0,"## Importing packages

# This R environment comes with all of CRAN and many other helpful packages preinstalled.
# You can see which packages are installed by checking out the kaggle/rstats docker image: 
# https://github.com/kaggle/docker-rstats

library(tidyverse) # metapackage with lots of helpful functions
library(dplyr)
library(readxl)
library(stats)
library(forecast)
library(fGarch)
library(tseries)
library(vars)
library(timeSeries)
library(readr)
## Running code

# In a notebook, you can run a single code cell by clicking in the cell and then hitting 
# the blue arrow to the left, or by clicking in the cell and pressing Shift+Enter. In a script, 
# you can run code by highlighting the code you want to run and then clicking the blue arrow
# at the bottom of this window.

## Reading in files

# You can access files from datasets you've added to this kernel in the ""../input/"" directory.
# You can see the files added to this kernel by running the code below. 

list.files(path = ""../input"")

## Saving data

# If you save any files or images, these will be put in the ""output"" directory. You 
# can see the output directory by committing and running your kernel (using the 
# Commit & Run button) and then checking out the compiled version of your kernel.",0,No Code Smell
2878,30840463,1,"train <- read.csv('../input/covid19-local-us-ca-forecasting-week-1/ca_train.csv')
test <- read.csv('../input/covid19-local-us-ca-forecasting-week-1/ca_test.csv')
sample <- read.csv('../input/covid19-local-us-ca-forecasting-week-1/ca_submission.csv')",0,No Code Smell
2879,30840463,2,head(train),0,No Code Smell
2880,30840463,3,"cases <- ts(train$ConfirmedCases)
fatal <- ts(train$Fatalities)",0,No Code Smell
2881,30840463,4,auto.arima(cases),0,No Code Smell
2882,30840463,5,"forecast:::forecast.Arima(auto.arima(cases), 
                          h = 30, 
                          level = c(68, 90))$mean",0,No Code Smell
2883,30840463,6,auto.arima(fatal),0,No Code Smell
2884,30840463,7,"forecast:::forecast.Arima(auto.arima(fatal), 
                          h = 30, 
                          level = c(68, 90))$mean",0,No Code Smell
2885,30840463,8,"test[which(as.Date(test$Date) > as.Date('2020-03-24')),]",0,No Code Smell
2886,30840463,9,"train[which(as.Date(train$Date) > as.Date('2020-03-11')),]",0,No Code Smell
2887,30840463,10,"new_test <- merge(test, train, by = 'Date', all.x = TRUE)",0,No Code Smell
2888,30840463,11,"new_test[which(as.Date(new_test$Date) > as.Date('2020-03-24')),]$ConfirmedCases <- forecast:::forecast.Arima(auto.arima(cases), 
                          h = 30, 
                          level = c(68, 90))$mean",0,No Code Smell
2889,30840463,12,"new_test[which(as.Date(new_test$Date) > as.Date('2020-03-24')),]$Fatalities <- forecast:::forecast.Arima(auto.arima(fatal), 
                          h = 30, 
                          level = c(68, 90))$mean",0,No Code Smell
2890,30840463,13,new_test,0,No Code Smell
2891,30840463,14,"sample$ConfirmedCases <- new_test$ConfirmedCases
sample$Fatalities <- new_test$Fatalities",0,No Code Smell
2892,30840463,15,"write.csv(sample, file = ""submission.csv"", row.names = FALSE)",0,No Code Smell
2893,30840463,16,,0,No Code Smell
2894,30810228,0,"import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

import warnings
warnings.filterwarnings('ignore')

from sklearn.linear_model import LinearRegression 

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))",1,Code Smell
2895,30810228,1,"sub=pd.read_csv(""../input/covid19-local-us-ca-forecasting-week-1/ca_submission.csv"")
test=pd.read_csv(""../input/covid19-local-us-ca-forecasting-week-1/ca_test.csv"")
train=pd.read_csv(""../input/covid19-local-us-ca-forecasting-week-1/ca_train.csv"")",1,Code Smell
2896,30810228,2,"print(train.shape)
train.head()",0,No Code Smell
2897,30810228,3,"#Only taking data with confirmed cases
train=train[train.ConfirmedCases>0]
print(train.shape)
train.head()",0,No Code Smell
2898,30810228,4,"sns.lineplot(train.Id, train.ConfirmedCases)",0,No Code Smell
2899,30810228,5,"sns.regplot(train.Id, np.log(train.ConfirmedCases))",0,No Code Smell
2900,30810228,6,"model_1= LinearRegression()
x1=np.array(train.Id).reshape(-1,1)
y1=np.log(train.ConfirmedCases)
model_1.fit(x1,y1)
print(""R-squared score : "",model_1.score(x1,y1))

gr=np.power(np.e, model_1.coef_[0])
print(""Growth Factor : "", gr)
print(f""Growth Rate : {round((gr-1)*100,2)}%"")",0,No Code Smell
2901,30810228,7,"sns.regplot(train.ConfirmedCases,train.Fatalities)",0,No Code Smell
2902,30810228,8,"model_2= LinearRegression()
x2=np.array(train.ConfirmedCases).reshape(-1,1)
y2=train.Fatalities
model_2.fit(x2,y2)
model_2.score(x2,y2)",0,No Code Smell
2903,30810228,9,test.head(),0,No Code Smell
2904,30810228,10,"#Making Id as unique key between test and train
test[""Id""]=50+test.ForecastId
test.head()",0,No Code Smell
2905,30810228,11,"test[""LogConf""]=model_1.predict(np.array(test.Id).reshape(-1,1))
test[""ConfirmedCases""]=np.exp(test.LogConf)//1
test[""Fatalities""]=model_2.predict(np.array(test.ConfirmedCases).reshape(-1,1))//1",0,No Code Smell
2906,30810228,12,"#Wherever confirmed cases and fatalities are available in train data, update it into test data
for id in train.Id:
    test.ConfirmedCases[test.Id==id]=train.ConfirmedCases[train.Id==id].sum()
    test.Fatalities[test.Id==id]=train.Fatalities[train.Id==id].sum()",0,No Code Smell
2907,30810228,13,"test[""Conf_d1""]=test.ConfirmedCases
test[""Fat_d1""]=test.Fatalities

rate=gr-1
for i in range(train.shape[0]-2,test.shape[0]):
    rate*=0.99
    test.Conf_d1[i]=(1+rate)*test.Conf_d1[i-1]//1
    test.Fat_d1[i]=model_2.predict(np.array(test.Conf_d1[i]).reshape(-1,1))[0]//1",0,No Code Smell
2908,30810228,14,"test[""Conf_d5""]=test.ConfirmedCases
test[""Fat_d5""]=test.Fatalities

rate=gr-1
for i in range(train.shape[0]-2,test.shape[0]):
    rate*=0.95
    test.Conf_d5[i]=(1+rate)*test.Conf_d5[i-1]//1
    test.Fat_d5[i]=model_2.predict(np.array(test.Conf_d5[i]).reshape(-1,1))[0]//1",0,No Code Smell
2909,30810228,15,"test[""Conf_i1""]=test.ConfirmedCases
test[""Fat_i1""]=test.Fatalities

rate=gr-1
for i in range(train.shape[0]-2,test.shape[0]):
    rate*=1.01
    test.Conf_i1[i]=(1+rate)*test.Conf_i1[i-1]//1
    test.Fat_i1[i]=model_2.predict(np.array(test.Conf_i1[i]).reshape(-1,1))[0]//1",0,No Code Smell
2910,30810228,16,"test[""Conf_15""]=test.ConfirmedCases
test[""Fat_15""]=test.Fatalities

rate=1.15
for i in range(train.shape[0]-2,test.shape[0]):
    test.Conf_15[i]=rate*test.Conf_15[i-1]//1
    test.Fat_15[i]=model_2.predict(np.array(test.Conf_15[i]).reshape(-1,1))[0]//1",0,No Code Smell
2911,30810228,17,"test[""Conf_20""]=test.ConfirmedCases
test[""Fat_20""]=test.Fatalities

rate=1.20
for i in range(train.shape[0]-2,test.shape[0]):
    test.Conf_20[i]=rate*test.Conf_20[i-1]//1
    test.Fat_20[i]=model_2.predict(np.array(test.Conf_20[i]).reshape(-1,1))[0]//1",0,No Code Smell
2912,30810228,18,"test[""Conf_25""]=test.ConfirmedCases
test[""Fat_25""]=test.Fatalities

rate=1.25
for i in range(train.shape[0]-2,test.shape[0]):
    test.Conf_25[i]=rate*test.Conf_25[i-1]//1
    test.Fat_25[i]=model_2.predict(np.array(test.Conf_25[i]).reshape(-1,1))[0]//1",0,No Code Smell
2913,30810228,19,"test[""Conf_30""]=test.ConfirmedCases
test[""Fat_30""]=test.Fatalities

rate=1.30
for i in range(train.shape[0]-2,test.shape[0]):
    test.Conf_30[i]=rate*test.Conf_30[i-1]//1
    test.Fat_30[i]=model_2.predict(np.array(test.Conf_30[i]).reshape(-1,1))[0]//1",0,No Code Smell
2914,30810228,20,"plt.figure(figsize=(12,8))
sns.lineplot(test.Id, test.Conf_20, label=""Constant 20%"")
sns.lineplot(test.Id, test.Conf_25, label=""Constant 25%"")
sns.lineplot(test.Id, test.Conf_30, label=""Constant 30%"", dashes=True)
sns.lineplot(test.Id, test.Conf_15, label=""Constant 15%"")
sns.lineplot(test.Id, test.Conf_i1, label=""Increasing by 1%"")
sns.lineplot(test.Id, test.Conf_d1, label=""Decreasing by 1%"")
sns.lineplot(test.Id, test.Conf_d5, label=""Decreasing by 5%"")
sns.lineplot(test.Id, test.ConfirmedCases, label=""Current Rate"")
sns.lineplot(train.Id, train.ConfirmedCases, label=""Train Data"")
plt.legend()
plt.show()",0,No Code Smell
2915,30810228,21,"plt.figure(figsize=(12,8))
sns.lineplot(test.Id, test.Fat_25, label=""Constant 25%"")
sns.lineplot(test.Id, test.Fat_20, label=""Constant 20%"")
sns.lineplot(test.Id, test.Fat_30, label=""Constant 30%"")
sns.lineplot(test.Id, test.Fat_15, label=""Constant 15%"")
sns.lineplot(test.Id, test.Fat_i1, label=""Increasing by 1%"")
sns.lineplot(test.Id, test.Fat_d1, label=""Decreasing by 1%"")
sns.lineplot(test.Id, test.Fat_d5, label=""Decreasing by 5%"")
sns.lineplot(test.Id, test.Fatalities, label=""Current Rate"")
sns.lineplot(train.Id, train.Fatalities, label=""Train Data"")
plt.legend()
plt.show()",0,No Code Smell
2916,30810228,22,"sub.ConfirmedCases=test.ConfirmedCases
sub.Fatalities=test.Fatalities
sub.to_csv(""submission.csv"", index=False)",0,No Code Smell
2917,30845748,0,"## Importing packages

library(tidyverse)
library(ggplot2)
library(ggthemes)
library(forcats)
library(readxl)
library(forecast)
library(tseries)

## Reading in tables

ca_submissions <- read.csv(""../input/covid19-local-us-ca-forecasting-week-1/ca_submission.csv"")
ca_test <- read.csv(""../input/covid19-local-us-ca-forecasting-week-1/ca_test.csv"")
ca_train <- read.csv(""../input/covid19-local-us-ca-forecasting-week-1/ca_train.csv"")

##external datasets
cases_03_22_2020 <- read.csv(""../input/california-county-level-data/cases_03_22_2020.csv"")
cases_03_23_2020 <- read.csv(""../input/california-county-level-data/cases_03_23_2020.csv"")
cases_03_24_2020 <- read.csv(""../input/california-county-level-data//cases_03_24_2020.csv"")
early_cases_cali <- read.csv(""../input/california-county-level-data/California Cases by County Xtime.csv"")
county_health_info <- read.csv(""../input/california-county-level-data/California County Health Rankings 2020.csv"")
county_health_addtnl_info <- read.csv(""../input/california-county-level-data/California County Health Rankings 2020 - AdditionalInfo.csv"")
health_info_with_cases <- read.csv(""../input/california-county-level-data/health_info_with_cases.csv"")
health_addtln_info_with_cases <- read.csv(""../input/california-county-level-data/health_addtln_info_with_cases.csv"")
ca_train_new <- read.csv(""../input/california-county-level-data/ca_train_new.csv"")

ca_submission <- read.csv(""../input/california-county-level-data/ca_submission1.csv"")

write_csv(ca_submission, 'submission.csv')",1,Code Smell
2918,30845748,1,"head(ca_train)
head(ca_test)
head(ca_submissions)",0,No Code Smell
2919,30845748,2,"head(early_cases_cali)
head(cases_03_22_2020)
head(cases_03_23_2020)
head(cases_03_24_2020)",0,No Code Smell
2920,30845748,3,"head(county_health_info)
head(county_health_addtnl_info)",0,No Code Smell
2921,30845748,4,"ggplot(data = health_info_with_cases) +
  geom_point(aes(Average.Daily.PM2.5...Air.Pollution, Case, color = 'red')) +
  theme(legend.position = ""none"", title = element_text(size = 10)) +
  labs(x = ""Average Daily Particulate Matter"", y = ""Cases"", title = ""Average Daily Particulate Matter vs Cases"", color = 'Legend') +
  theme(plot.title = element_text(hjust = 0.5))",0,No Code Smell
2922,30845748,5,"ggplot(data = health_info_with_cases) +
  geom_point(aes(X..Severe.Housing.Problems, Case, color = 'red')) +
  theme(legend.position = ""none"", title = element_text(size = 10)) +
  labs(x = ""% Severe Housing Problems"", y = ""Cases"", title = ""% Severe Housing Problems vs Cases"", color = 'Legend') +
  theme(plot.title = element_text(hjust = 0.5))

ggplot(data = health_info_with_cases) +
  geom_point(aes(Overcrowding, Case, color = 'red')) +
  theme(legend.position = ""none"", title = element_text(size = 10)) +
  labs(x = ""Overcrowding"", y = ""Cases"", title = ""Overcrowding vs Cases"", color = 'Legend') +
  theme(plot.title = element_text(hjust = 0.5))",0,No Code Smell
2923,30845748,6,"ggplot(data = health_info_with_cases) +
  geom_point(aes(Presence.of.Water.Violation, Case, color = 'red')) +
  theme(legend.position = ""none"", title = element_text(size = 10)) +
  labs(x = ""Presence of Water Violation"", y = ""Cases"", title = ""Presence of Water Violation vs Cases"", color = 'Legend') +
  theme(plot.title = element_text(hjust = 0.5))",0,No Code Smell
2924,30845748,7,"ggplot(data = health_addtln_info_with_cases) +
  geom_point(aes(Average.Traffic.Volume.per.Meter.of.Major.Roadways, Case, color = 'red')) +
  theme(legend.position = ""none"", title = element_text(size = 10)) +
  labs(x = ""Average Traffic Volume/Meter of Major Roadways"", y = ""Cases"", title = ""Average Traffic Volume/Meter of Major Roadways vs Cases"", color = 'Legend') +
  theme(plot.title = element_text(hjust = 0.5))",0,No Code Smell
2925,30845748,8,"ggplot(data = health_addtln_info_with_cases) +
  geom_point(aes(X..65.and.over, Deaths, color = 'red')) +
  theme(legend.position = ""none"", title = element_text(size = 10)) +
  labs(x = ""% 65 and over"", y = ""Cases"", title = ""% 65 and over vs Cases"", color = 'Legend') +
  theme(plot.title = element_text(hjust = 0.5))",0,No Code Smell
2926,30845748,9,"##ARIMA for Cases
##identify optimal p,d,q values
auto.arima(ca_train_new$ConfirmedCases)

#use arima model ARIMA(2,0,2)
arima_model_cases <- Arima(ca_train_new$ConfirmedCases, order = c(2,0,2))
summary(arima_model_cases)

plot(forecast(arima_model_cases, h = 30))
",0,No Code Smell
2927,30845748,10,"##ARIMAX for fatality
##identify optimal p,d,q values
auto.arima(ca_train_new$Fatalities)

#use arima model ARIMA(0,2,1)
arima_model_deaths <- Arima(ca_train_new$Fatalities, order = c(0,2,1), xreg = ca_train_new$ConfirmedCases)
summary(arima_model_deaths)

plot(forecast(arima_model_deaths, xreg = forecast(arima_model_cases, h = 30)[[""mean""]][1:30], h = 30))",0,No Code Smell
2928,30845748,11,"#my submission!
head(ca_submission)",0,No Code Smell
2929,32669954,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the ""../input/"" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# Any results you write to the current directory are saved as output.",1,Code Smell
2930,32669954,1,"train=pd.read_csv(""/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_train.csv"")
test=pd.read_csv(""/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_test.csv"")
submission=pd.read_csv(""/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_submission.csv"")",1,Code Smell
2931,32669954,2,"import matplotlib.pyplot as plt
%matplotlib inline",0,No Code Smell
2932,32669954,3,"train['Date']=pd.to_datetime(train['Date'])
train['Date'] = train['Date'].apply(lambda x:x.date().strftime('%m-%d'))
test['Date']=pd.to_datetime(test['Date'])
test['Date'] = test['Date'].apply(lambda x:x.date().strftime('%m-%d'))",0,No Code Smell
2933,32669954,4,"hor=train['Date']
ver=train['ConfirmedCases']
plt.figure(figsize=(20,10))
plt.plot(hor, ver)
plt.title('Time Series Confirmed Cases')
plt.show()",0,No Code Smell
2934,32669954,5,"hor=train['Date']
ver=train['Fatalities']
plt.figure(figsize=(20,10))
plt.plot(hor, ver)
plt.title('Time Series Fatalities')
plt.show()",0,No Code Smell
2935,32669954,6,"train1=train[48:]  #excluding first 48 values from train dataset as they are all zero
#train1=train
train1.head()",0,No Code Smell
2936,32669954,7,X_test1=test[['ForecastId']]+50 #matching the test data Id in line to training ID's,0,No Code Smell
2937,32669954,8,"X1=train1[['Id']]
y_con=train1[['ConfirmedCases']]
y_fat=train1[['Fatalities']]",0,No Code Smell
2938,32669954,9,"from sklearn.preprocessing import PolynomialFeatures
poly=PolynomialFeatures(7) #Polynomial Feature with degree 7
X=poly.fit_transform(X1)
X_test=poly.fit_transform(X_test1)",0,No Code Smell
2939,32669954,10,"from sklearn.linear_model import Ridge, Lasso, SGDRegressor, LinearRegression
model_con=Lasso()
model_con.fit(X, y_con)",0,No Code Smell
2940,32669954,11,y_pred_con=model_con.predict(X_test),0,No Code Smell
2941,32669954,12,"model_fat=Lasso()
model_fat.fit(X, y_fat)",0,No Code Smell
2942,32669954,13,y_pred_fat=model_fat.predict(X_test),0,No Code Smell
2943,32669954,14,"y_pred_con1=y_pred_con.ravel()
y_pred_fat1=y_pred_fat.ravel()
",1,Code Smell
2944,32669954,15,"y_pred_con1=y_pred_con1[13:]  #replacing 13 test prediction with training label as they overlap
y_con_t=train1['ConfirmedCases']
y_con_t=y_con_t[2:].ravel()  #getting those 13 labels from training set to put into prediction
#y_con_t=y_con_t[50:].ravel()
y_pred_con_final=np.round(np.append(y_con_t, y_pred_con1))
y_pred_con_final",0,No Code Smell
2945,32669954,16,"y_pred_fat1=y_pred_fat1[13:] #replacing 13 test prediction with training label as they overlap
y_fat_t=train1['Fatalities']
y_fat_t=y_fat_t[2:].ravel() #getting those 13 labels from training set to put into prediction
#y_fat_t=y_fat_t[50:].ravel()
y_pred_fat_final=np.round(np.append(y_fat_t, y_pred_fat1))
y_pred_fat_final",0,No Code Smell
2946,32669954,17,"data={'ForecastId':submission.ForecastId,'ConfirmedCases':y_pred_con_final, 'Fatalities':y_pred_fat_final}
result=pd.DataFrame(data, index=submission.index)
result.to_csv('/kaggle/working/submission.csv', index=False)
m1=pd.read_csv('/kaggle/working/submission.csv')
m1.head()",0,No Code Smell
2947,32669954,18,"hor=test.Date
ver=y_pred_con_final
plt.figure(figsize=(20,10))
plt.plot(hor, ver)
plt.title('Confirmed Cases Prediction')
plt.show()",0,No Code Smell
2948,32669954,19,"hor=test.Date
ver=y_pred_fat_final
plt.figure(figsize=(20,10))
plt.plot(hor, ver)
plt.title('Fatalities Prediction')
plt.show()",0,No Code Smell
2949,30831873,0,"# CORONA VIRUS - GROWTH RATE PREDICTION VIA TAYLOR SERIES MODEL
# REF: https://www.kaggle.com/rnglol/simple-taylor-series-model",0,No Code Smell
2950,30831873,1,"%%time
# IMPORTS
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# LOAD TRAIN DATA
train = pd.read_csv('/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_train.csv')

# SCRUB DATA
junk =['Id','Country/Region','Lat','Long','Province/State']
train.drop(junk, axis=1, inplace=True)",1,Code Smell
2951,30831873,2,"# PREP TRAIN DATA 
X_train = train[48:]
X_train.reset_index(inplace = True, drop = True) 
print(X_train)",0,No Code Smell
2952,30831873,3,"# CALCULATE EXPANSION TABLE
diff_conf, conf_old = [], 0 
diff_fat, fat_old = [], 0
dd_conf, dc_old = [], 0
dd_fat, df_old = [], 0

for row in X_train.values:
    diff_conf.append(row[1]-conf_old)
    conf_old=row[1]
    diff_fat.append(row[2]-fat_old)
    fat_old=row[2]
    dd_conf.append(diff_conf[-1]-dc_old)
    dc_old=diff_conf[-1]
    dd_fat.append(diff_fat[-1]-df_old)
    df_old=diff_fat[-1]
    
print(len(diff_conf),X_train.shape)",0,No Code Smell
2953,30831873,4,"# SAMPLES
samples = len(diff_conf)
answer = samples - 1
key = answer - 1",0,No Code Smell
2954,30831873,5,"# POPULATE DATAFRAME FEATURES
pd.options.mode.chained_assignment = None  # default='warn'

X_train['diff_confirmed'] = diff_conf
X_train['diff_fatalities'] = diff_fat
X_train['dd_confirmed'] = dd_conf
X_train['dd_fatalities'] = dd_fat
    
X_train",0,No Code Smell
2955,30831873,6,"# CALCULATE SERIES AVERAGES
d_c = X_train.diff_confirmed.drop(0).mean()
dd_c = X_train.dd_confirmed.drop(0).drop(1).mean()
d_f = X_train.diff_fatalities.drop(0).mean()
dd_f = X_train.dd_fatalities.drop(0).drop(1).mean()


print(d_c, dd_c, d_f, dd_f)",0,No Code Smell
2956,30831873,7,"# ITERATE TAYLOR SERIES
pred_c, pred_f = list(X_train.ConfirmedCases.loc[2:answer]), list(X_train.Fatalities.loc[2:answer])

for i in range(1, 44 - key):
    pred_c.append(int((X_train.ConfirmedCases[answer] + (d_c + dd_c*i) * i) * 1.3))
    pred_f.append(int((X_train.Fatalities[answer] + (d_f + dd_f*i) * i)))",0,No Code Smell
2957,30831873,8,"# WRITE SUBMISSION
my_submission = pd.DataFrame({'ForecastId': list(range(1,44)), 'ConfirmedCases': pred_c, 'Fatalities': pred_f})

my_submission.to_csv('submission.csv', index=False)",0,No Code Smell
2958,30831873,9,print(my_submission),0,No Code Smell
2959,30790196,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

# import numpy as np # linear algebra
# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the ""../input/"" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

# import os
# for dirname, _, filenames in os.walk('/kaggle/input'):
#     for filename in filenames:
#         print(os.path.join(dirname, filename))

# Any results you write to the current directory are saved as output.

# -*- coding: utf-8 -*-
""""""KaggleTest.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rV0sSw8fMw2DkVnWwzV8_cnfQDYKhz9E
""""""
# -*- coding: utf-8 -*-
""""""KaggleTest.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rV0sSw8fMw2DkVnWwzV8_cnfQDYKhz9E
""""""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

from sklearn.linear_model import LinearRegression

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

submission = pd.read_csv(""../input/covid19-local-us-ca-forecasting-week-1/ca_submission.csv"")
test = pd.read_csv(""../input/covid19-local-us-ca-forecasting-week-1/ca_test.csv"")
train = pd.read_csv(""../input/covid19-local-us-ca-forecasting-week-1/ca_train.csv"")

train = train[train.ConfirmedCases>0]

model_1 = LinearRegression()
x1 = np.array(train.Id).reshape(-1,1)
y1 = np.log(train.ConfirmedCases)
model_1.fit(x1,y1)
print(""R-squared score : "",model_1.score(x1,y1))

model_2 = LinearRegression()
x2 = np.array(train.ConfirmedCases).reshape(-1,1)
y2 = train.Fatalities
model_2.fit(x2,y2)
print(""R-Squared Score= "",model_2.score(x2,y2))

test[""Id""] = 50 + test.ForecastId

test[""LogConf""] = model_1.predict(np.array(test.Id).reshape(-1,1))
test[""ConfirmedCases""] = np.exp(test.LogConf)//1
test[""Fatalities""] = model_2.predict(np.array(test.ConfirmedCases).reshape(-1,1))//1

# from google.colab import files

submission.ConfirmedCases = test.ConfirmedCases
submission.Fatalities = test.Fatalities
submission.to_csv(""submission.csv"", index=False)
# files.download(""submission.csv"")",1,Code Smell
2960,30898457,0,"library(tidyverse)
library(ggplot2) 
library(deSolve)
options(scipen = 999)

path_target <- ""../input/covid19-local-us-ca-forecasting-week-1""

df_data_train <- read.csv(file = paste(path_target, ""ca_train.csv"", sep = ""/""))

df_data_test <- read.csv(file = paste(path_target, ""ca_test.csv"", sep = ""/""))
df_data_submission <- read.csv(file = paste(path_target, ""ca_submission.csv"", sep = ""/""))

df_train_eda <- df_data_train %>% 
  pivot_longer(cols = c(ConfirmedCases, Fatalities), names_to = ""metric_name"", values_to = ""metric_value"") %>% 
  mutate(metric_period = as.Date(as.character(Date)))

plot_cases <- ggplot(data = df_train_eda) + theme_bw() +
  geom_line(aes(x = metric_period, y = metric_value, group = metric_name, colour = metric_name), size = 1.2) +
  scale_x_date(date_breaks = ""1 week"", labels = scales::date_format(""%b %d"")) +
  scale_colour_manual(values = c(""ConfirmedCases"" = ""steelblue"", ""Fatalities"" = ""orangered"")) +
  theme(legend.position = ""bottom"") +
  labs(title = ""Confirmed Cases and Fatalities"",
       subtitle = ""California"",
       x = ""Week Commencing"", y = ""Cases"", colour = ""Case Type"")

print(plot_cases) 

df_growth <- df_data_train %>% 
  mutate(metric_period = as.Date(as.character(Date)),  
         cum_conf = cumsum(ConfirmedCases), 
         cum_fatal = cumsum(Fatalities), 
         cum_fatal_prop = cum_fatal / cum_conf, 
         cum_conf_lag = cumsum(dplyr::lag(ConfirmedCases, default = 0)), 
         grwth = ConfirmedCases/dplyr::lag(ConfirmedCases),
         grwth_cum =cum_conf/cum_conf_lag)

rate_growth <- df_growth %>% 
  filter(metric_period == max(metric_period)) %>% 
  pull(grwth_cum) 

rate_death <- df_growth %>% 
  filter(metric_period == max(metric_period)) %>% 
  pull(cum_fatal_prop) 

#--- Susceptible
fun_dS_dt <- function(S, I, beta_const, n_pop) {
  -beta_const * S * I / n_pop 
}

#--- Exposed 
fun_dE_dt <- function(S, E, I, beta_const, epsilon_const, n_pop) {
  beta_const * S * I / n_pop - epsilon_const * E
}

#--- Infected 
fun_dI_dt <- function(I, E, epsilon_const, gamma_const) {
  epsilon_const * E - gamma_const * I
}

#--- Recovered 
fun_dR_dt <- function(I, gamma_const) {
  gamma_const * I 
}

fun_seir <- function(t, x, parms) {  
  
  if(t <= parms[""beta_day""]) {
    beta_const <- parms[""beta_0""]
  } else {
    beta_const <- parms[""beta_1""]
  }  
  
  if(t <= parms[""gamma_day""]) {
    gamma_const <- parms[""gamma_0""]
  } else {
    gamma_const <- parms[""gamma_1""]
  } 
  
  S <- x[""S""]
  E <- x[""E""]
  I <- x[""I""] 
  
  epsilon_const <- parms[""epsilon_const""]
  n_pop <- parms[""n_pop""]
  
  dS <- fun_dS_dt(S, I, beta_const, n_pop) 
  dE <- fun_dE_dt(S, E, I, beta_const, epsilon_const, n_pop)
  dI <- fun_dI_dt(I, E, epsilon_const, gamma_const) 
  dR <- fun_dR_dt(I, gamma_const) 
  res <- c(dS, dE, dI, dR)
  list(res)
}

#--- model run length 
run_days = 100

#--- model initial parameters 
mdl_init <- c(S = 36000000, 
              E = 0, 
              I = 12, 
              R = 0)
n_pop <- sum(mdl_init)

#--- infection rate
beta_0 <- 1.01
beta_intervene_day = 60
beta_1 = 0.9

#--- days infectous 
infectious_days_0 <- 2.2 # days 
infectious_days_1 <- 2.0 # days 

gamma_0 <- 1 / infectious_days_0
gamma_intervene_day = 40
gamma_1 = 1 / infectious_days_1

#--- exposed hosts, who are latently infected but not yet infectious
latency_days <- 2 # days 

this_time_steps <- seq(from = 1, to = run_days) 

#--- Parameters 
params <- c(beta_0 = beta_0,
            beta_1 = beta_1, 
            beta_day = beta_intervene_day, 
            gamma_0 = gamma_0,
            gamma_1 = gamma_1, 
            gamma_day = gamma_intervene_day,  
            epsilon_const = 1 / latency_days,
            n_pop = n_pop) 

mdl_seir <- lsoda(y = mdl_init, times = this_time_steps, func = fun_seir, parms = params) 

df_output <- as.data.frame(mdl_seir) %>% 
  rename(Susceptable = S, Exposed = E,  Infected = I, Recovered = R) %>% 
  pivot_longer(cols = c(Susceptable, Exposed, Infected, Recovered), names_to = ""metric_name"", values_to = ""metric_value"") %>% 
  mutate(metric_name = factor(metric_name, levels = c(""Susceptable"", ""Exposed"", ""Infected"", ""Recovered"")))

metric_colours <- c(""Susceptable"" = ""black"", ""Exposed"" = ""steelblue"", ""Infected"" = ""tomato"", ""Recovered"" = ""darkolivegreen3"")

plot_output <- ggplot(data = df_output) + theme_bw() + 
  geom_line(aes(x = time, y = metric_value, group = metric_name, colour = metric_name), size = 1.2) +  
  scale_colour_manual(values = metric_colours) + 
  scale_x_continuous(breaks = seq(from = 0, to = run_days, by = 10),
                     minor_breaks = seq(from = 0, to = run_days, by = 1)) + 
  scale_y_continuous(breaks = seq(from = 0, to = n_pop, by = 2e6),
                     labels = scales::comma) + 
  theme(legend.position = ""bottom"") + 
  labs(title = ""SEIR"",
       x = ""Days"",
       y = ""Cases"",
       colour = ""Status"")
print(plot_output)


df_t <- df_data_train %>% 
  select(id = Id, train_cases = ConfirmedCases)

sync_val <- 36

df_m <- df_output %>% 
  filter(metric_name == ""Infected"") %>% 
  select(id_x = time, mdl_cases = metric_value) %>% 
  mutate(id = id_x + sync_val)

df_x <- left_join(df_t, df_m, by = ""id"") %>% 
  filter(train_cases > 0)


plot_x <- ggplot(data = df_x, aes(x = id)) + theme_bw() + 
  geom_line(aes(y = train_cases), colour = ""blue"") + 
  geom_line(aes(y = mdl_cases), colour = ""red"")
print(plot_x)

start_point <- 13
submission <- df_output %>%
  filter(metric_name == ""Infected"" &
           time >= start_point &
           time <= start_point + 42) %>% 
  mutate(ForecastID = 1:43) %>% 
  select(ForecastID,
         ConfirmedCases = metric_value) %>%
  mutate(ConfirmedCases = round(ConfirmedCases, 0),
         Fatalities = round(ConfirmedCases * rate_death, 0))

write.csv(x = submission, file = ""submission.csv"", row.names = FALSE)

",0,No Code Smell
2961,30801003,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the ""../input/"" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# Any results you write to the current directory are saved as output.",1,Code Smell
2962,30801003,1,"import matplotlib.pyplot as plt
%matplotlib inline",0,No Code Smell
2963,30801003,2,"df_train = pd.read_csv(""/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_train.csv"")
df_train.head(5)",1,Code Smell
2964,30801003,3,"df_test = pd.read_csv(""/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_test.csv"")",1,Code Smell
2965,30801003,4,"def from_date_to_day(train, test):
    date_train = pd.to_datetime(train[""Date""])
    
    beginning = date_train.min()
    
    days_train = date_train - beginning
    train[""Day""] = days_train.dt.days
    
    date_test = pd.to_datetime(test[""Date""])
    days_test = date_test - beginning
    test[""Day""] = days_test.dt.days",0,No Code Smell
2966,30801003,5,"from_date_to_day(df_train, df_test)",0,No Code Smell
2967,30801003,6,"# get rid of log(0) problem
def log_rectified(x):
    return np.log(max(x, 0) + 1.0)

np_log_rectified = np.vectorize(log_rectified)

def inv_log_rectified(x):
    return np.exp(x) - 1.0

np_inv_log_rectified = np.vectorize(inv_log_rectified)",0,No Code Smell
2968,30801003,7,"def plot_log_cases(data):
    
    X = data[""Day""].unique()
    y = np_log_rectified(data.groupby(""Day"").ConfirmedCases.sum())
    
    plt.plot(X, y, 'bo')",0,No Code Smell
2969,30801003,8,plot_log_cases(df_train),0,No Code Smell
2970,30801003,9,"from scipy import interpolate
class Model:
    def __init__(self, train, target):
        X = np.arange(0, train.Day.max() + 1)
        y_log = np_log_rectified(train.groupby(""Day"")[target].sum()[X])
        
        self.f1 = interpolate.interp1d(X, y_log, fill_value=""extrapolate"", kind=""linear"")
    
    def predict(self, test):
        return self.f1(test)",0,No Code Smell
2971,30801003,10,"def plot_prediction(data, model, target):
    X = np.linspace(0,65,100)
    y_pred = np_inv_log_rectified(model.predict(X))
    
    days = data[""Day""].unique()
    y = data.groupby(""Day"")[target].sum()
    
    plt.plot(days, y, 'bo')
    plt.plot(X, y_pred)",0,No Code Smell
2972,30801003,11,"def plot_error(data, model, target):
    days = data[""Day""].unique()
    y = data.groupby(""Day"")[target].sum()
    
    y_pred = np_inv_log_rectified(model.predict(days))
    
    plt.plot(days, abs(y - y_pred))",0,No Code Smell
2973,30801003,12,"model_case = Model(df_train, ""ConfirmedCases"")
plot_prediction(df_train, model_case, ""ConfirmedCases"")",0,No Code Smell
2974,30801003,13,"plot_error(df_train, model_case, ""ConfirmedCases"")",0,No Code Smell
2975,30801003,14,"model_fatal = Model(df_train, ""Fatalities"")
plot_prediction(df_train, model_fatal, ""Fatalities"")",0,No Code Smell
2976,30801003,15,"plot_error(df_train, model_fatal, ""Fatalities"")",0,No Code Smell
2977,30801003,16,"pred_cases = np_inv_log_rectified(model_case.predict(df_test.Day)).astype(int)
pred_fatal = np_inv_log_rectified(model_fatal.predict(df_test.Day)).astype(int)",0,No Code Smell
2978,30801003,17,"submission = pd.concat(
    [ pd.Series(np.arange(1, df_test.ForecastId.max() + 1)),
     pd.Series(pred_cases),
     pd.Series(pred_fatal)],
    axis=1)",0,No Code Smell
2979,30801003,18,submission.head(5),0,No Code Smell
2980,30801003,19,"submission.to_csv('submission.csv', header=['ForecastId', 'ConfirmedCases', 'Fatalities'], index=False)",0,No Code Smell
2981,30848066,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the ""../input/"" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# Any results you write to the current directory are saved as output.",1,Code Smell
2982,30848066,1,"import pandas as pd
sample_submission = pd.read_csv(""/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_submission.csv"")
test = pd.read_csv(""/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_test.csv"")
train = pd.read_csv(""/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_train.csv"")",1,Code Smell
2983,30848066,2,"len(train)
",0,No Code Smell
2984,30848066,3,sample_submission.head(),0,No Code Smell
2985,30848066,4,test.head(),0,No Code Smell
2986,30848066,5,train.tail(),0,No Code Smell
2987,30848066,6,"#make a heatmap

import folium
from folium import Choropleth, Marker
from folium.plugins import HeatMap, MarkerCluster
m = folium.Map(location=[37, -115], zoom_start=6) 
def embed_map(m, file_name):
    from IPython.display import IFrame
    m.save(file_name)
    return IFrame(file_name, width='100%', height='750px')

#merge test and training data
Full_data = pd.merge(test, train, on=['Lat','Long','Date'])

# Add a heatmap to the base map
HeatMap(data=Full_data[['Lat', 'Long']], radius=11).add_to(m)

# Show the map
embed_map(m, ""q_1.html"")",0,No Code Smell
2988,30848066,7,"#rename therefor the data columns
train.rename(columns={'Province/State':'Province'}, inplace=True)
train.rename(columns={'Country/Region':'Country'}, inplace=True)
train.rename(columns={'ConfirmedCases':'Confirmed'}, inplace=True)",0,No Code Smell
2989,30848066,8,"#and we do the same for test set
test.rename(columns={'Province/State':'Province'}, inplace=True)
test.rename(columns={'Country/Region':'Country'}, inplace=True)",0,No Code Smell
2990,30848066,9,"from sklearn.preprocessing import LabelEncoder
# creating initial dataframe
bridge_types = ('Lat', 'Date', 'Province', 'Country', 'Long', 'Confirmed',
       'ForecastId', 'Id')
countries = pd.DataFrame(train, columns=['Country'])
# creating instance of labelencoder
labelencoder = LabelEncoder()
# Assigning numerical values and storing in another column
train['Countries'] = labelencoder.fit_transform(train['Country'])

#do the same for test set
test['Countries'] = labelencoder.fit_transform(test['Country'])

#check label encoding 
train['Countries'].head()
",0,No Code Smell
2991,30848066,10,"train['Date']= pd.to_datetime(train['Date']) 
test['Date']= pd.to_datetime(test['Date']) ",0,No Code Smell
2992,30848066,11,"train = train.set_index(['Date'])
test = test.set_index(['Date'])",0,No Code Smell
2993,30848066,12,"def create_time_features(df):
    """"""
    Creates time series features from datetime index
    """"""
    df['date'] = df.index
    df['hour'] = df['date'].dt.hour
    df['dayofweek'] = df['date'].dt.dayofweek
    df['quarter'] = df['date'].dt.quarter
    df['month'] = df['date'].dt.month
    df['year'] = df['date'].dt.year
    df['dayofyear'] = df['date'].dt.dayofyear
    df['dayofmonth'] = df['date'].dt.day
    df['weekofyear'] = df['date'].dt.weekofyear
    
    X = df[['hour','dayofweek','quarter','month','year',
           'dayofyear','dayofmonth','weekofyear']]
    return X",0,No Code Smell
2994,30848066,13,"create_time_features(train).head()
create_time_features(test).head()",0,No Code Smell
2995,30848066,14,train.head(),0,No Code Smell
2996,30848066,15,"train.drop(""date"", axis=1, inplace=True)
test.drop(""date"", axis=1, inplace=True)",0,No Code Smell
2997,30848066,16,# train.isnull().sum(),0,No Code Smell
2998,30848066,17,"#drop useless columns for train and test set
train.drop(['Country'], axis=1, inplace=True)
train.drop(['Province'], axis=1, inplace=True)",0,No Code Smell
2999,30848066,18,"test.drop(['Country'], axis=1, inplace=True)
test.drop(['Province'], axis=1, inplace=True)",0,No Code Smell
3000,30848066,19,"from sklearn.tree import DecisionTreeRegressor  
regressor = DecisionTreeRegressor(random_state = 0) ",0,No Code Smell
3001,30848066,20,"# import xgboost as xgb
# from xgboost import plot_importance, plot_tree
from sklearn.metrics import mean_squared_error, mean_absolute_error

# reg= xgb.XGBRegressor(n_estimators=1000)",0,No Code Smell
3002,30848066,21,train.head(),0,No Code Smell
3003,30848066,22,"# features that will be used in the model
x = train[['Lat', 'Long','Countries','dayofweek','month','dayofyear','weekofyear']]
y1 = train[['Confirmed']]
y2 = train[['Fatalities']]
x_test = test[['Lat', 'Long','Countries','dayofweek','month','dayofyear','weekofyear']]",0,No Code Smell
3004,30848066,23,x.head(),0,No Code Smell
3005,30848066,24,"#use model on data 
regressor.fit(x,y1)
predict_1 = regressor.predict(x_test)
predict_1 = pd.DataFrame(predict_1)
predict_1.columns = [""Confirmed_predict""]",0,No Code Smell
3006,30848066,25,predict_1.head(),0,No Code Smell
3007,30848066,26,"#use model on data 
regressor.fit(x,y2)
predict_2 = regressor.predict(x_test)
predict_2 = pd.DataFrame(predict_2)
predict_2.columns = [""Death_prediction""]
predict_2.head()",0,No Code Smell
3008,30848066,27,"# plot = plot_importance(regressor, height=0.9, max_num_features=20)",0,No Code Smell
3009,30848066,28,"Samle_submission = pd.read_csv(""/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_submission.csv"")
Samle_submission.columns
submission = Samle_submission[[""ForecastId""]]",1,Code Smell
3010,30848066,29,"Final_submission = pd.concat([predict_1,predict_2,submission],axis=1)
Final_submission.head()",0,No Code Smell
3011,30848066,30,"Final_submission.columns = ['ConfirmedCases', 'Fatalities', 'ForecastId']
Final_submission = Final_submission[['ForecastId','ConfirmedCases', 'Fatalities']]

Final_submission[""ConfirmedCases""] = Final_submission[""ConfirmedCases""].astype(int)
Final_submission[""Fatalities""] = Final_submission[""Fatalities""].astype(int)",0,No Code Smell
3012,30848066,31,Final_submission.head(),0,No Code Smell
3013,30848066,32,"Final_submission.to_csv(""submission.csv"",index=False)
print('Model ready for submission!')",0,No Code Smell
3014,30823052,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_squared_log_error
from sklearn.compose import TransformedTargetRegressor
from sklearn.linear_model import LinearRegression,  Ridge
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline

# Input data files are available in the ""../input/"" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# Any results you write to the current directory are saved as output.",1,Code Smell
3015,30823052,1,"train = pd.read_csv('/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_train.csv')
train.head()",1,Code Smell
3016,30823052,2,"# Clean the data. only look at the data from the first confirmed case
train = train[train.ConfirmedCases>0]
train",0,No Code Smell
3017,30823052,3,"whole_world_data = pd.read_csv('/kaggle//input/covid19-global-forecasting-week-1/train.csv')
us_data = whole_world_data.loc[whole_world_data['Country/Region'] == 'US']
us_data",1,Code Smell
3018,30823052,4,"possible_states = us_data['Province/State'].unique()[(us_data.groupby('Province/State').max().ConfirmedCases>144 ) &  (us_data.groupby('Province/State').min().ConfirmedCases<6)]
possible_states",0,No Code Smell
3019,30823052,5,"COLUMNS = ['ConfirmedCases', 'Fatalities', 'Province/State']
possible_starts = pd.DataFrame(columns=COLUMNS)
for country in possible_states:
    possible_starts = pd.concat([possible_starts, us_data[(us_data['Province/State'] == country) & (us_data['ConfirmedCases']<144) & (us_data['ConfirmedCases']>0)][COLUMNS]])

possible_starts",0,No Code Smell
3020,30823052,6,"# First, find which start is the best for an exponential pattern, and what is the lowest cross validation we get with it
def test_exponential_accuraccy(y):
    accuraccy = 0
    X = np.asarray(list(range(len(y)))) +1
    tscv = TimeSeriesSplit(len(X)-1)
    for train_index, test_index in tscv.split(X):
        X_train, X_test = X[train_index], X[test_index]
        y_train, y_test = y[train_index], y[test_index]

        # Transform the data with a log function and after prediction, apply the exp() function.
        regressor = TransformedTargetRegressor(regressor=LinearRegression(),
                                                         func=np.log1p,
                                                         inverse_func=np.expm1)
        regressor.fit(np.array(X_train).reshape(-1,1), np.array(y_train).reshape(-1,1))
        y_pred = regressor.predict(np.array(X_test).reshape(-1,1))
        accuraccy += mean_squared_log_error(y_pred, y_test)
        
        
    
    return accuraccy",0,No Code Smell
3021,30823052,7,"# For every state, compute its sliding window error.
best_accuraccy = 100
best_start_state = ''
for state in possible_starts['Province/State'].unique():
    possible_train_data = pd.concat([possible_starts[possible_starts['Province/State']==state], train])
    
    state_accuracy = test_exponential_accuraccy(possible_train_data['ConfirmedCases'].values) 
    if  state_accuracy < best_accuraccy:
        best_start_state = state
        best_accuraccy = state_accuracy
                                
print(best_start_state, best_accuraccy)",0,No Code Smell
3022,30823052,8,"# Now, lets find which start is the best for a polynomial pattern, what is the degree of the polynom,
# and what is the lowest cross validation we get with it
def test_polynomial_accuraccy(y, degree):
    accuraccy = 0
    X = np.asarray(list(range(len(y)))) +1
    tscv = TimeSeriesSplit(len(X)-1)
    for train_index, test_index in tscv.split(X):
        X_train, X_test = X[train_index], X[test_index]
        y_train, y_test = y[train_index], y[test_index]
        
        # Fit the data with a polynom of the given degree.
        model = make_pipeline(PolynomialFeatures(degree), Ridge())
        model.fit(X_train.reshape(-1,1), y_train.reshape(-1,1))
        y_pred = model.predict(X_test.reshape(-1,1))
#         print(type(y_pred))
        if y_pred < 0: 
            y_pred = np.array([0]) # this case can happen in the beggining of the sliding window, and a high degree polynom. y_pred=0 is a big enough error
        accuraccy += mean_squared_log_error(y_pred, y_test)
        
        
    
    return accuraccy",0,No Code Smell
3023,30823052,9,"# For every state and polynom degree, compute its sliding window error
# What we are going to do is look at a few options and decide where we get a low enough error, and not over generalize polynom.

degrees = [2,3,4,5,6]
lowest_errors = pd.DataFrame(columns=['accuraccy', 'state', 'degree'])
for degree in degrees:
    best_accuraccy = 100
    best_start_state = ''
    for state in possible_starts['Province/State'].unique():
        possible_train_data = pd.concat([possible_starts[possible_starts['Province/State']==state], train])
    
        state_accuracy = test_polynomial_accuraccy(possible_train_data['ConfirmedCases'].values, degree)
        if  state_accuracy < best_accuraccy:
            best_start_state = state
            best_accuraccy = state_accuracy
    
    lowest_errors = lowest_errors.append(pd.DataFrame({'accuraccy': [best_accuraccy], 'state': [best_start_state], 'degree': [degree]}))",0,No Code Smell
3024,30823052,10,lowest_errors,0,No Code Smell
3025,30823052,11,possible_starts[possible_starts['Province/State'] == 'Massachusetts'],0,No Code Smell
3026,30823052,12,"train = pd.concat([possible_starts[possible_starts['Province/State'] == 'Massachusetts'], train])[['ConfirmedCases', 'Fatalities']]
train = train.loc[train.ConfirmedCases>0]",0,No Code Smell
3027,30823052,13,train.reset_index(),0,No Code Smell
3028,30823052,14,"days_to_predict = 43 # Change to 29
public_leader_board_first_column=7 # Change to 26
model = make_pipeline(PolynomialFeatures(3), Ridge())

#ConfirmedCases predictions
X_train = np.array(range(len(train))) + 1
X_test = np.array(range(public_leader_board_first_column,public_leader_board_first_column+days_to_predict)) + 1
y_train = train.ConfirmedCases.values
model.fit(X_train.reshape(-1,1), y_train.reshape(-1,1))
confirmed_cases_predictions = model.predict(X_test.reshape(-1,1))
confirmed_cases_predictions = list(map(lambda x: x[0], confirmed_cases_predictions.tolist()))

#Fatalities predictions
y_train = train.Fatalities.values
model.fit(X_train.reshape(-1,1), y_train.reshape(-1,1))
fatalities_predictions = model.predict(X_test.reshape(-1,1))
fatalities_predictions = list(map(lambda x: x[0], fatalities_predictions.tolist()))

submissions = pd.DataFrame({'ConfirmedCases': confirmed_cases_predictions, 'Fatalities': fatalities_predictions})
submissions.to_csv('submission.csv', index=False)

",0,No Code Smell
3029,30823052,15,,0,No Code Smell
3030,30844920,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt

# Input data files are available in the ""../input/"" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# Any results you write to the current directory are saved as output.",1,Code Smell
3031,30844920,1,"def load_training_csv(path):
    """"""helper function to regularize the preprocessing of dataframes""""""
    df = pd.read_csv(path, header=0, parse_dates=['Date'])
    #df.drop(df[((df['ConfirmedCases'] == 0) & (df['Fatalities'] == 0))].index, inplace=True)
    df['ConfirmedCases_log1p'] = df['ConfirmedCases'].map(np.log1p)
    df['Fatalities_log1p'] = df['Fatalities'].map(np.log1p)
    df.drop(['Province/State', 'Country/Region','Lat','Long'], axis=1, inplace=True)
    return df

def load_case_data_from_csv(path):
    """"""load case count data where it's > 0 only and drop unneeded columns""""""
    df = pd.read_csv(path, header=0, parse_dates=['Date'])
    df.drop(df[(df['ConfirmedCases'] == 0)].index, inplace=True)
    df['ConfirmedCases_log1p'] = df['ConfirmedCases'].map(np.log1p)
    df.drop(['Fatalities', 'Province/State', 'Country/Region','Lat','Long'], axis=1, inplace=True)
    return df

def load_fatality_data_from_csv(path):
    """"""load fatality count data where it's > 0 only and drop unneeded columns""""""
    df = pd.read_csv(path, header=0, parse_dates=['Date'])
    df.drop(df[(df['Fatalities'] == 0)].index, inplace=True)
    df['Fatalities_log1p'] = df['Fatalities'].map(np.log1p)
    df.drop(['ConfirmedCases', 'Province/State', 'Country/Region','Lat','Long'], axis=1, inplace=True)
    return df

def rmsle(y_true, y_pred):
    """"""return the root mean squared logarithmic error: square root of the mean squared error of the natural log 
    of (value plus 1): sqrt(mean(power(log1p(p)-log1p(a),2)))
    """"""
    return np.sqrt(np.mean(np.power(np.log1p(y_true) - np.log1p(y_pred),2)))



from scipy.stats import linregress

full_df = load_training_csv(""../input/covid19-local-us-ca-forecasting-week-1/ca_train.csv"") # for reference
cdf = load_case_data_from_csv(""../input/covid19-local-us-ca-forecasting-week-1/ca_train.csv"") # just case counts
fdf = load_fatality_data_from_csv(""../input/covid19-local-us-ca-forecasting-week-1/ca_train.csv"") # just fatality counts

y_cases_train = cdf['ConfirmedCases_log1p'].values
y_cases_true = cdf['ConfirmedCases'].values
y_fatalities_train = fdf['Fatalities_log1p'].values
y_fatalities_true = fdf['Fatalities'].values

# make some x values for regression purposes
case_xs = range(0, len(y_cases_train))
fat_xs = range(0, len(y_fatalities_train))

#
# linear regressionon against log1p transformed data
#

cslope, cintercept, cr, cp, csterr = linregress(case_xs, y_cases_train)
fslope, fintercept, fr, fp, fsterr = linregress(fat_xs, y_fatalities_train)
print(""case log1p(y) = mx + b: {0:.4f}x + {1:.4f} sterr {2:.4f} r={3:.4f}"".format(cslope, cintercept, csterr, cr))
print(""fatality log1p(y) = mx + b:{0:.4f}x + {1:.4f} sterr {2:.4f} r={3:.4f}"".format(fslope, fintercept, fsterr, fr))

#
# polyfit the transformed data
#

poly_degrees = 5
zcase = np.polyfit(case_xs, y_cases_train, deg=poly_degrees)
zfat = np.polyfit(fat_xs, y_fatalities_train, deg=poly_degrees)
case_poly = np.poly1d(zcase)
fat_poly = np.poly1d(zfat)

#
# check linear fit against existing data
#

y_cases_pred_linear = np.round(np.expm1(case_xs * cslope + cintercept), 0)
y_fatalities_pred_linear = np.round(np.expm1(fat_xs * fslope + fintercept), 0)
case_rmsle = rmsle(y_cases_true, y_cases_pred_linear)
fat_rmsle = rmsle(y_fatalities_true, y_fatalities_pred_linear)
plt.plot(case_xs, y_cases_true,'o')
plt.plot(case_xs, y_cases_pred_linear, '-', label=""RMSLE={0:.4f}"".format(case_rmsle))
plt.plot(fat_xs, y_fatalities_true, 'o')
plt.plot(fat_xs, y_fatalities_pred_linear, '-', label=""RMSLE={0:.4f}"".format(fat_rmsle))
plt.legend()
plt.show()

#
# check poly fit against existing data
#

y_cases_pred_poly = np.round(np.expm1(case_poly(case_xs)))
y_fatalities_pred_poly = np.round(np.expm1(fat_poly(fat_xs)))
case_poly_rmsle = rmsle(y_cases_true, y_cases_pred_poly)
fat_poly_rmsle = rmsle(y_fatalities_true, y_fatalities_pred_poly)
plt.plot(case_xs, y_cases_true, 'o')
plt.plot(case_xs, y_cases_pred_poly, '-', label=""RMSLE={0:.4f}"".format(case_poly_rmsle))
plt.plot(fat_xs, y_fatalities_true, 'o')
plt.plot(fat_xs, y_fatalities_pred_poly, '-', label='RMSLE={0:.4f}'.format(fat_poly_rmsle))
plt.legend()
plt.show()
                                      
",1,Code Smell
3032,30844920,2,"#print(full_df.tail())

# load test data and predict based on above regression
test_df = pd.read_csv(""../input/covid19-local-us-ca-forecasting-week-1/ca_test.csv"")
print(test_df.head())
print(test_df.shape)
test_xs = range(2,test_df.shape[0]+2)
print(len(test_xs))
#print(list(test_xs))
test_df['xs'] = test_xs
#test_df['ConfirmedCases'] = test_df.apply(lambda x:np.round(np.expm1(x['xs']*cslope + cintercept)), axis=1)
#test_df['Fatalities'] = test_df.apply(lambda x: np.round(np.expm1(x['xs']*fslope + fintercept)), axis=1)
test_df['ConfirmedCases'] = test_df.apply(lambda x: np.round(np.expm1(case_poly(case_xs))), axis=1)
test_df['Fatalities'] = test_df.apply(lambda x: np.round(np.expm1(fat_poly(fat_xs))), axis=1)

print(test_df[test_df['Date'] > '2020-03-18'])

sub_df = test_df[['ForecastId','ConfirmedCases','Fatalities']]
sub_df.to_csv('submission.csv', index=False, header=True)",0,No Code Smell
3033,30919301,0,"import pandas as pd
import plotly.graph_objects as go
import plotly.express as px
# Plotly installation: https://plot.ly/python/getting-started/#jupyterlab-support-python-35",0,No Code Smell
3034,30919301,1,"df = pd.read_csv('../input/covid19-local-us-ca-forecasting-week-1/ca_train.csv')
df=df[df[""Date""]>""2020-03-09""] # Only keep dates with confirmed cases
df.head()",1,Code Smell
3035,30919301,2,"# Reference: https://plot.ly/python/time-series/
fig = go.Figure(
    [go.Scatter(x=df['Date'], y=df['ConfirmedCases'])],
    layout_title_text=""Confirmed Cases in California""
)
fig.update_layout(
    yaxis_type=""log"",
    margin=dict(l=20, r=20, t=50, b=20),
    template=""plotly_white"")
fig.show()",0,No Code Smell
3036,30919301,3,"df_test = pd.read_csv('../input/covid19-local-us-ca-forecasting-week-1/ca_test.csv')
print(df_test.shape)
df_test.head()",1,Code Smell
3037,30919301,4,"public_leaderboard_start_date = ""2020-03-12""
last_public_leaderboard_train_date = ""2020-03-11""
public_leaderboard_end_date  = ""2020-03-26""

cases  = df[df[""Date""]==last_public_leaderboard_train_date][""ConfirmedCases""].values[0] * (2**(1/4))
df_test.insert(1, ""ConfirmedCases"", 0)",0,No Code Smell
3038,30919301,5,"for i in range(15):
    df_test.loc[i, ""ConfirmedCases""] = cases
    cases = cases * (2**(1/4))    
df_test.head()",0,No Code Smell
3039,30919301,6,"# Reference: https://plot.ly/python/time-series/
fig = go.Figure(
    [
        go.Scatter(x=df['Date'], y=df['ConfirmedCases'], name=""actual""),
        go.Scatter(x=df_test['Date'].iloc[:15], y=df_test['ConfirmedCases'], name=""predicted""),
    ],
    layout_title_text=""Confirmed Cases in California""
)
fig.update_layout(
    yaxis_type=""log"",
    margin=dict(l=20, r=20, t=50, b=20),
    template=""plotly_white"")
fig.show()",0,No Code Smell
3040,30919301,7,"df_growth = pd.DataFrame({
    ""Date"": df[""Date""].iloc[1:].values,
    ""Rate"": df[""ConfirmedCases""].iloc[1:].values / df[""ConfirmedCases""].iloc[:-1].values * 100
})",0,No Code Smell
3041,30919301,8,"# Reference: https://plot.ly/python/bar-charts/
fig = px.bar(df_growth, x='Date', y='Rate', width=600, height=400)
fig.update_layout(
    margin=dict(l=20, r=20, t=50, b=20),
    template=""plotly_white"",
    title=""Empirical Growth Rate"",
    yaxis_title=""Rate (%)""
)
fig.update_yaxes(range=[100, 135])
fig.show()",0,No Code Smell
3042,30919301,9,"# rate = df_growth[""Rate""].mean() / 100
rate = df_growth[""Rate""].iloc[:7].median() / 100
print(f""Rate used: {rate:.4f}"")
df_test = pd.read_csv('../input/covid19-local-us-ca-forecasting-week-1/ca_test.csv')
public_leaderboard_start_date = ""2020-03-12""
last_public_leaderboard_train_date = ""2020-03-11""
public_leaderboard_end_date  = ""2020-03-26""

cases  = df[df[""Date""]==last_public_leaderboard_train_date][""ConfirmedCases""].values[0] * (rate)
df_test.insert(1, ""ConfirmedCases"", 0)
for i in range(15):
    df_test.loc[i, ""ConfirmedCases""] = cases
    cases = cases * rate  
df_test.head()",1,Code Smell
3043,30919301,10,"# Reference: https://plot.ly/python/time-series/
fig = go.Figure(
    [
        go.Scatter(x=df['Date'], y=df['ConfirmedCases'], name=""actual""),
        go.Scatter(x=df_test['Date'].iloc[:15], y=df_test['ConfirmedCases'], name=""predicted""),
    ],
    layout_title_text=""Confirmed Cases in California""
)
fig.update_layout(
    yaxis_type=""log"",
    margin=dict(l=20, r=20, t=50, b=20),
    template=""plotly_white"")
fig.show()",0,No Code Smell
3044,30919301,11,"# Reference: https://plot.ly/python/time-series/
fig = go.Figure(
    [go.Scatter(x=df['Date'], y=df['Fatalities'])],
    layout_title_text=""Fatalities in California""
)
fig.update_layout(
    yaxis_type=""log"",
    margin=dict(l=20, r=20, t=50, b=20),
    template=""plotly_white"")
fig.show()",0,No Code Smell
3045,30919301,12,"df_growth = pd.DataFrame({
    ""Date"": df[""Date""].iloc[1:].values,
    ""Rate"": df[""Fatalities""].iloc[1:].values / df[""Fatalities""].iloc[:-1].values * 100
})
# Reference: https://plot.ly/python/bar-charts/
fig = px.bar(df_growth, x='Date', y='Rate', width=600, height=400)
fig.update_layout(
    margin=dict(l=20, r=20, t=50, b=20),
    template=""plotly_white"",
    title=""Empirical Growth Rate"",
    yaxis_title=""Rate (%)""
)
fig.update_yaxes(range=[100, 180])
fig.show()",0,No Code Smell
3046,30919301,13,"rate = df_growth[""Rate""].iloc[:7].median() / 100
print(f""Rate used: {rate:.4f}"")
df_test = pd.read_csv('../input/covid19-local-us-ca-forecasting-week-1/ca_test.csv')
public_leaderboard_start_date = ""2020-03-12""
last_public_leaderboard_train_date = ""2020-03-11""
public_leaderboard_end_date  = ""2020-03-26""

cases  = df[df[""Date""]==last_public_leaderboard_train_date][""Fatalities""].values[0] * (rate)
df_test.insert(1, ""Fatalities"", 0)
for i in range(15):
    df_test.loc[i, ""Fatalities""] = cases
    cases = cases * rate  
df_test.head()",1,Code Smell
3047,30919301,14,"# Reference: https://plot.ly/python/time-series/
fig = go.Figure(
    [
        go.Scatter(x=df['Date'], y=df['Fatalities'], name=""actual""),
        go.Scatter(x=df_test['Date'].iloc[:15], y=df_test['Fatalities'], name=""predicted""),
    ],
    layout_title_text=""Fatalities in California""
)
fig.update_layout(
    yaxis_type=""log"",
    margin=dict(l=20, r=20, t=50, b=20),
    template=""plotly_white"")
fig.show()",0,No Code Smell
3048,30842974,0,"import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))",1,Code Smell
3049,30842974,1,"import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

import tensorflow as tf
from tensorflow import keras
from keras.models import Sequential
from keras.layers import Dense, LSTM, Dropout",1,Code Smell
3050,30842974,2,"train = pd.read_csv('/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_train.csv')
train.Date = pd.to_datetime(train.Date)
train.info()",1,Code Smell
3051,30842974,3,train,0,No Code Smell
3052,30842974,4,"cc = train.ConfirmedCases.values
f = train.Fatalities.values
plt.plot(cc, color = 'blue', label = 'Confirmed Cases')
plt.plot(f, color = 'orange', label = 'Fatalities')
plt.legend()
plt.show()",0,No Code Smell
3053,30842974,5,"plot_acf(cc)
plot_pacf(cc)
plt.show()",0,No Code Smell
3054,30842974,6,"plot_acf(f)
plot_pacf(f)
plt.show()",0,No Code Smell
3055,30842974,7,"ccf = train[['ConfirmedCases', 'Fatalities']]
ccf = ccf[ccf.ConfirmedCases > 0].values
ccf_max, ccf_min = np.max(ccf), np.min(ccf)
ccf_norm = (ccf - ccf_min) / (ccf_max - ccf_min)

X, y = [], []
for i in range(len(ccf_norm)):
    end = i+2
    if end > len(ccf_norm)-1:
        break
    X.append(ccf_norm[i:end])
    y.append(ccf_norm[end])
    
X, y = np.array(X).reshape(-1, 2, 2), np.array(y)
print(X.shape, y.shape)",0,No Code Smell
3056,30842974,8,"tf.random.set_seed(1)

m = Sequential()
m.add(LSTM(100, input_shape = (X.shape[1], X.shape[2]), activation = 'relu'))
m.add(Dense(2))
m.compile(loss = 'mse', optimizer = 'adam')
h = m.fit(X, y, epochs = 100, verbose = 0)",0,No Code Smell
3057,30842974,9,"plt.plot(h.history['loss'], label = 'Loss')
plt.legend()
plt.show()",0,No Code Smell
3058,30842974,10,"plt.figure(figsize = (10, 4))
plt.subplot(121)
plt.title('Confirmed Cases')
plt.plot(np.pad((m.predict(X) * (ccf_max - ccf_min) + ccf_min)[:,0], (2,0)), 'r--', label = 'Predict')
plt.plot(ccf[:,0], label = 'Actual')
plt.legend()

plt.subplot(122)
plt.title('Fatalities')
plt.plot(np.pad((m.predict(X) * (ccf_max - ccf_min) + ccf_min)[:,1], (2,0)), 'r--', label = 'Predict')
plt.plot(ccf[:,1], label = 'Actual')
plt.legend()
plt.show()",0,No Code Smell
3059,30842974,11,"test = pd.read_csv('/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_test.csv')
pd.concat([test.head(1), test.tail(1)])",1,Code Smell
3060,30842974,12,"test_pred = y[-2:]
n = len(test) - len(train[train.Date >= '2020-03-12'])
for i in range(n):
    p = m.predict(test_pred[-2:].reshape(-1, 2, 2))
    test_pred = np.append(test_pred, p).reshape(-1, 2)
    i += 1",0,No Code Smell
3061,30842974,13,"test_pred_round = np.round(test_pred * (ccf_max - ccf_min) + ccf_min, 0)[:-2]
plt.plot(test_pred_round[:,0], label = 'Conf Cases Pred')
plt.plot(test_pred_round[:,1], label = 'Fatal Pred')
plt.legend()
plt.show()",0,No Code Smell
3062,30842974,14,sub = pd.read_csv('/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_submission.csv'),1,Code Smell
3063,30842974,15,"cc_sub = np.append(train[train.Date >= '2020-03-12'].ConfirmedCases.values, test_pred_round[:, 0])
f_sub = np.append(train[train.Date >= '2020-03-12'].Fatalities.values, test_pred_round[:, 1])",0,No Code Smell
3064,30842974,16,"sub = sub.assign(ConfirmedCases = cc_sub, Fatalities = f_sub)
sub.to_csv('submission.csv', index = False)",0,No Code Smell
3065,30776229,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the ""../input/"" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# Any results you write to the current directory are saved as output.",1,Code Smell
3066,30776229,1,"import pandas as pd
import numpy as np",1,Code Smell
3067,30776229,2,"train = pd.read_csv('/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_train.csv')
test = pd.read_csv('/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_test.csv')",1,Code Smell
3068,30776229,3,train.info(),1,Code Smell
3069,30776229,4,test.info(),1,Code Smell
3070,30776229,5,"train['Date'] = pd.to_datetime(train['Date'])
test['Date'] = pd.to_datetime(test['Date'])",0,No Code Smell
3071,30776229,6,train.head(5),0,No Code Smell
3072,30776229,7,train.tail(5),0,No Code Smell
3073,30776229,8,"train['Date'] = train['Date'].astype('int64')
test['Date'] = test['Date'].astype('int64')",0,No Code Smell
3074,30776229,9,train.tail(5),0,No Code Smell
3075,30776229,10,train.info(),1,Code Smell
3076,30776229,11,"train.iloc[:,-3].sample(3)",0,No Code Smell
3077,30776229,12,"X = train.iloc[:,-3]
print(X.shape)
X = np.array(X).reshape(-1,1)
print(X.shape)",0,No Code Smell
3078,30776229,13,"Y = train.iloc[:,-2:]
print(Y.shape)
Y.sample(3)",0,No Code Smell
3079,30776229,14,"from sklearn.model_selection import train_test_split 
trainX , valX, trainY, valY = train_test_split(X, Y, random_state=1)",0,No Code Smell
3080,30776229,15,"y1Train = trainY.iloc[:,0]
print(y1Train.shape)
y1Train.sample(3)",0,No Code Smell
3081,30776229,16,"y2Train = trainY.iloc[:,1]
y2Train.sample(3)",0,No Code Smell
3082,30776229,17,"y1Val = valY.iloc[:,0]
y1Val.sample(3)",0,No Code Smell
3083,30776229,18,"y2Val = valY.iloc[:,1]
y2Val.sample(3)",0,No Code Smell
3084,30776229,19,print(trainX.shape),0,No Code Smell
3085,30776229,20,"from sklearn.tree import DecisionTreeRegressor
lrModel1 = DecisionTreeRegressor(random_state = 27)
%time lrModel1.fit(trainX, y1Train)",0,No Code Smell
3086,30776229,21,"%time y1Pred = lrModel1.predict(valX)
print(y1Pred[:,])",0,No Code Smell
3087,30776229,22,"from sklearn.metrics import mean_absolute_error

print(""Accuracy in train set : "", lrModel1.score(trainX, y1Train))
print(""RMSE : "", mean_absolute_error(y1Val, y1Pred)**(0.5))",0,No Code Smell
3088,30776229,23,"lrModel2 = DecisionTreeRegressor(random_state = 27)
%time lrModel2.fit(trainX.reshape(-1, 1), y2Train)

%time y2Pred = lrModel2.predict(valX)

print(""Accuracy in train set : "", lrModel2.score(trainX, y2Train))
print(""RMSE : "", mean_absolute_error(y2Val, y2Pred)**(0.5))",0,No Code Smell
3089,30776229,24,"print(test.shape)
test.sample(3)",0,No Code Smell
3090,30776229,25,"forecastID = test.iloc[:,0]",0,No Code Smell
3091,30776229,26,"test.iloc[:,-1].sample(3)",0,No Code Smell
3092,30776229,27,"test = np.array(test.iloc[:,-1]).reshape(-1,1)",0,No Code Smell
3093,30776229,28,"%time finalPred1 = lrModel1.predict(test)
print(finalPred1[:,])",0,No Code Smell
3094,30776229,29,"%time finalPred2 = lrModel2.predict(test)
print(finalPred2[:,])",0,No Code Smell
3095,30776229,30,"outputFile = pd.DataFrame({""ForecastId"": forecastID,
                           ""ConfirmedCases"": (finalPred1+0.5).astype('int'),
                           ""Fatalities"": (finalPred2+0.5).astype('int')})",0,No Code Smell
3096,30776229,31,outputFile.sample(3),0,No Code Smell
3097,30776229,32,"outputFile.to_csv(""submission.csv"", index=False)",0,No Code Smell
3098,30772485,0,"library(data.table)
library(dplyr)
library(ggplot2)
library(leaflet)

dt<-fread(""../input/novel-corona-virus-2019-dataset/covid_19_data.csv"",stringsAsFactors = F)
dti<-fread(""../input/covid19-in-italy/covid19_italy_region.csv"",stringsAsFactors = F)
colnames(dt)<-c(""SNo"",""Date"", ""Province"",""Country"",""Update"",""Confirmed"",""Deaths"",""Recovered"")
dt<-select(dt,-SNo,-Update)
dt$Province[dt$Province==""France""|dt$Province==""United Kingdom""|dt$Province==""UK""|dt$Province==""Netherlands""|dt$Province==""Denmark""]<-""""
dt$Date<-as.Date(dt$Date, ""%m/%d/%Y"")",0,No Code Smell
3099,30772485,1,"dt$day_cont<-as.numeric(as.Date(dt$Date)-as.Date(min(dt$Date)))
",0,No Code Smell
3100,30772485,2,"dt$province_country<-paste(dt$Province,dt$Country,sep = "", "")
dt$province_country[is.na(dt$Province)|dt$Province==""""]<-dt$Country[is.na(dt$Province)|dt$Province==""""]
dti$Date<-as.Date(dti$Date, ""%Y-%m-%d"")
dti$day_cont<-as.numeric(as.Date(dti$Date)-as.Date(min(dt$Date)))
dti<-dti[order(dti$day_cont,decreasing = T),]
dti_last<-dti[!duplicated(dti$RegionName),]
dti_last<-select(dti_last,Latitude,Longitude,TotalPositiveCases)

dti<-select(dti,RegionName,TotalPositiveCases,Deaths,Recovered,day_cont)
dti$province_country<-paste0(dti$RegionName,"", Italy"")
dti<-select(dti,-RegionName)
dt<-dt[!dt$Country==""Italy""]
dt<-select(dt,-Date,-Province,-Country)
setnames(dti, ""TotalPositiveCases"",""Confirmed"")

dt<-rbind(dt,dti)

last<-dt[order(dt$day_cont,decreasing = T),]
last<-last[!duplicated(last$province_country),]
last<-last[order(last$Confirmed,decreasing = T),]",0,No Code Smell
3101,30772485,3,"print(last[1:20,])",0,No Code Smell
3102,30772485,4,"regions_to_compare<-c(""Hubei, Mainland China"",""Lombardia, Italy"",""Spain"",""Emilia Romagna, Italy"",
                     ""New York, US"",""Washington, US"",""California, US"" )

dd<-dt[dt$province_country %in% regions_to_compare]
",0,No Code Smell
3103,30772485,5,"dd[,day_100_cases:=min(day_cont[Confirmed>100]), by=province_country]
dd$day_100<-dd$day_cont-dd$day_100_cases
dd<-dd[dd$day_100>0]

dd_100<-select(dd,province_country,day_100,Confirmed)

ggplot(dd_100, aes(x = day_100, y = Confirmed, colour = province_country)) + 
  geom_line() + 
  ylab(label=""Cummulative cases"") + 
  xlab(""Days since 100th reported case"") + 
  scale_colour_manual(values=c(""red"", ""blue"",""green"",""orange"",""black"",""yellow"",""brown"",""pink""))",0,No Code Smell
3104,30772485,6,"dd_log<-dd_100
dd_log$Confirmed<-log10(dd_log$Confirmed)
ggplot(dd_log, aes(x = day_100, y = Confirmed, colour = province_country)) + 
  geom_line() + 
  ylab(label=""log of Cummulative cases"") + 
  xlab(""Days since 100th reported case"") + 
  scale_colour_manual(values=c(""red"", ""blue"",""green"",""orange"",""black"",""yellow"",""brown"",""pink""))",0,No Code Smell
3105,30772485,7,"dd$deaths_to_cases<-dd$Deaths/dd$Confirmed*100
ggplot(dd, aes(x = day_100, y = deaths_to_cases, colour = province_country)) + 
  geom_line() + 
  ylab(label=""deaths per 100 cases"") + 
  xlab(""Days since 100th reported case"") + 
  scale_colour_manual(values=c(""red"", ""blue"",""green"",""orange"",""black"",""yellow"",""brown"",""pink""))",0,No Code Smell
3106,30772485,8,"dd$recovered_to_confirmed<-dd$Recovered/dd$Confirmed
ggplot(dd, aes(x = day_100, y = recovered_to_confirmed, colour = province_country)) + 
  geom_line() + 
  ylab(label=""revovered to confirmed per 100 cases"") + 
  xlab(""Days since 100th reported case"") + 
  scale_colour_manual(values=c(""red"", ""blue"",""green"",""orange"",""black"",""yellow"",""brown"",""pink""))
",0,No Code Smell
3107,30902241,0,!pip install hy > /dev/null,0,No Code Smell
3108,30902241,1,"# Hy Magic
",0,No Code Smell
3109,30902241,2,"import IPython
",1,Code Smell
3110,30902241,3,"def hy_eval(*args):import hy;return hy.eval(hy.read_str(""(do\n""+"""".join(map(lambda s:s or """",args))+""\n)\n""),globals())
",0,No Code Smell
3111,30902241,4,"@IPython.core.magic.register_line_cell_magic
",0,No Code Smell
3112,30902241,5,"def h(*args):hy_eval(*args) # Silent: Does not print result.
",0,No Code Smell
3113,30902241,6,"@IPython.core.magic.register_line_cell_magic
",0,No Code Smell
3114,30902241,7,"def hh(*args): return hy_eval(*args) # Verbose: Prints result.
",0,No Code Smell
3115,30902241,8,"del h, hh",0,No Code Smell
3116,30902241,9,"%%h
",0,No Code Smell
3117,30902241,10,"(import  [useful [*]])
",1,Code Smell
3118,30902241,11,(require [useful [*]]),0,No Code Smell
3119,30902241,12,"%%h
",0,No Code Smell
3120,30902241,13,"; Figure out location of data
",0,No Code Smell
3121,30902241,14,"(s covid-root-kaggle ""/kaggle/input"")
",0,No Code Smell
3122,30902241,15,"(s covid-root-laptop ""$HOME/d"")
",0,No Code Smell
3123,30902241,16,"(s covid-root 
",0,No Code Smell
3124,30902241,17,"   (-> ""/kaggle"" 
",0,No Code Smell
3125,30902241,18,"       (os.path.exists) 
",1,Code Smell
3126,30902241,19,"       (if covid-root-kaggle covid-root-laptop)
",0,No Code Smell
3127,30902241,20,"       (os.path.expandvars)))
",1,Code Smell
3128,30902241,21,"(s covid-prefix (+ covid-root ""/covid19-local-us-ca-forecasting-week-1/ca_""))
",0,No Code Smell
3129,30902241,22,"(s covid-train  (+ covid-prefix ""train.csv""))
",1,Code Smell
3130,30902241,23,"(s covid-test   (+ covid-prefix ""test.csv""))
",0,No Code Smell
3131,30902241,24,"(s covid-submit (+ covid-prefix ""submission.csv""))",0,No Code Smell
3132,30902241,25,"%%hh
",0,No Code Smell
3133,30902241,26,"; Find out day of week
",0,No Code Smell
3134,30902241,27,"(-> covid-train
",0,No Code Smell
3135,30902241,28,"  (pd.read-csv) 
",1,Code Smell
3136,30902241,29,"  (pd-keep [""Date"" ""ConfirmedCases"" ""Fatalities""]) 
",0,No Code Smell
3137,30902241,30,"  (.assign :Date (fn [x] (pd.to-datetime :yearfirst True x.Date)))
",0,No Code Smell
3138,30902241,31,"  (.assign :Day  (fn [x] (-> (x.Date.dt.day_name))))
",0,No Code Smell
3139,30902241,32,"  (.head)
",0,No Code Smell
3140,30902241,33,),0,No Code Smell
3141,30902241,34,"%%h
",0,No Code Smell
3142,30902241,35,"; Plot the data
",0,No Code Smell
3143,30902241,36,"
",0,No Code Smell
3144,30902241,37,"(import math)
",1,Code Smell
3145,30902241,38,"(-> 39.94 (* 1000) (* 1000) (math.log1p) (s1 ca-pop-log))
",0,No Code Smell
3146,30902241,39,"
",0,No Code Smell
3147,30902241,40,"(-> covid-train
",0,No Code Smell
3148,30902241,41,"  (pd.read-csv) 
",1,Code Smell
3149,30902241,42,"  (pd-keep [""Date"" ""ConfirmedCases"" ""Fatalities""]) 
",0,No Code Smell
3150,30902241,43,"  (.where (fn1-> (. ConfirmedCases) (> 0.0))) (.dropna)
",0,No Code Smell
3151,30902241,44,"  (.assign :ConfirmedCases (fn1-> (. ConfirmedCases) (np.log1p))) 
",0,No Code Smell
3152,30902241,45,"  (.assign :Fatalities     (fn1-> (. Fatalities)     (np.log1p)))
",0,No Code Smell
3153,30902241,46,"  (.assign :Date           (fn1-> (. Date)           (pd.to-datetime :yearfirst True)))
",0,No Code Smell
3154,30902241,47,"  (.assign :DayOfYear      (fn1-> (. Date) (. dt) (. dayofyear)))
",0,No Code Smell
3155,30902241,48,"  (.assign :Day            (fn1-> (. Date) (. dt) ( .day_name))) 
",0,No Code Smell
3156,30902241,49,"  (.dropna)
",0,No Code Smell
3157,30902241,50,"  (s1 df1))
",0,No Code Smell
3158,30902241,51,"
",0,No Code Smell
3159,30902241,52,"(-> df1
",0,No Code Smell
3160,30902241,53,"  (pd-keep [""Date"" ""ConfirmedCases"" ""Fatalities""])
",0,No Code Smell
3161,30902241,54,"  (.set-index ""Date"")
",0,No Code Smell
3162,30902241,55,"  (pd-plot ""Log1p"")
",0,No Code Smell
3163,30902241,56,  (display)),0,No Code Smell
3164,30902241,57,"%%h
",0,No Code Smell
3165,30902241,58,"; Define data frames.
",0,No Code Smell
3166,30902241,59,"
",0,No Code Smell
3167,30902241,60,"(s MILLION    (-> 1000 (* 1000)))
",0,No Code Smell
3168,30902241,61,"(s population (-> 39.94 (* MILLION)))
",0,No Code Smell
3169,30902241,62,"
",0,No Code Smell
3170,30902241,63,"(-> covid-train
",0,No Code Smell
3171,30902241,64,"  (pd.read-csv) 
",1,Code Smell
3172,30902241,65,"  (pd-keep [""Date"" ""ConfirmedCases"" ""Fatalities""]) 
",0,No Code Smell
3173,30902241,66,"  (.rename :columns {""ConfirmedCases"" ""Conf"" ""Fatalities"" ""Dead"" })
",0,No Code Smell
3174,30902241,67,"  (.assign :Conf (fn1-> (. Conf) (/ population))) 
",0,No Code Smell
3175,30902241,68,"  (.assign :Dead (fn1-> (. Dead) (/ population)))
",0,No Code Smell
3176,30902241,69,"  (.dropna)
",0,No Code Smell
3177,30902241,70,"  (s1 df))
",0,No Code Smell
3178,30902241,71,"
",0,No Code Smell
3179,30902241,72,"(setv conf-actual (df.Conf.rename ""Actual""))
",0,No Code Smell
3180,30902241,73,"(setv dead-actual (df.Dead.rename ""Actual""))",0,No Code Smell
3181,30902241,74,"%%h
",0,No Code Smell
3182,30902241,75,"(import [scipy.optimize [minimize]])
",1,Code Smell
3183,30902241,76,"(import [math [exp]])
",1,Code Smell
3184,30902241,77,"
",0,No Code Smell
3185,30902241,78,"; Define conf model then run it.
",0,No Code Smell
3186,30902241,79,"
",0,No Code Smell
3187,30902241,80,"(defn conf-model [a alpha t0 t]
",0,No Code Smell
3188,30902241,81,"  (setv t-delta (- t t0))
",0,No Code Smell
3189,30902241,82,"  (if (< t-delta 0)
",0,No Code Smell
3190,30902241,83,"    0.0
",0,No Code Smell
3191,30902241,84,"    (** (- 1 (exp (* (- a) t-delta))) alpha)))
",0,No Code Smell
3192,30902241,85,"
",0,No Code Smell
3193,30902241,86,"(defn conf-model-loss [x df]
",0,No Code Smell
3194,30902241,87,"  (setv (, a alpha t0) x)
",0,No Code Smell
3195,30902241,88,"  (setv r 0)
",0,No Code Smell
3196,30902241,89,"  (for [t (range (len df))]
",0,No Code Smell
3197,30902241,90,"    (+= r (-> (conf-model a alpha t0 t) (- (get df t)) (** 2))))
",0,No Code Smell
3198,30902241,91,"  r)
",0,No Code Smell
3199,30902241,92,"
",0,No Code Smell
3200,30902241,93,"(-> conf-model-loss 
",0,No Code Smell
3201,30902241,94,"    (minimize :x0 (np.array [0.1 1.0 5]) 
",0,No Code Smell
3202,30902241,95,"              :args conf-actual 
",0,No Code Smell
3203,30902241,96,"              :method ""Nelder-Mead"" :tol 1e-6)
",0,No Code Smell
3204,30902241,97,    (s1 conf-opt)),0,No Code Smell
3205,30902241,98,"%%h
",0,No Code Smell
3206,30902241,99,"
",0,No Code Smell
3207,30902241,100,"; Define dead model then run it.
",0,No Code Smell
3208,30902241,101,"
",0,No Code Smell
3209,30902241,102,"(defn dead-model [death-rate lag t]
",0,No Code Smell
3210,30902241,103,"  (s (, a alpha t0) conf-opt.x)
",0,No Code Smell
3211,30902241,104,"  (s t (- t lag))
",0,No Code Smell
3212,30902241,105,"  (s conf (conf-model a alpha t0 t))
",0,No Code Smell
3213,30902241,106,"  (s dead (* conf death-rate)))
",0,No Code Smell
3214,30902241,107,"
",0,No Code Smell
3215,30902241,108,"(defn dead-model-loss [x df]
",0,No Code Smell
3216,30902241,109,"  (s (, death-rate lag) x)
",0,No Code Smell
3217,30902241,110,"  (s (, a alpha t0) conf-opt.x)
",0,No Code Smell
3218,30902241,111,"  (s r 0)
",0,No Code Smell
3219,30902241,112,"  (for [t (range (len df))]
",0,No Code Smell
3220,30902241,113,"    (+= r (-> (dead-model death-rate lag t) (- (get df t)) (** 2))))
",0,No Code Smell
3221,30902241,114,"  r)
",0,No Code Smell
3222,30902241,115,"
",0,No Code Smell
3223,30902241,116,"(-> dead-model-loss 
",0,No Code Smell
3224,30902241,117,"    (minimize :x0 (np.array [0.01 15]) 
",0,No Code Smell
3225,30902241,118,"              :args dead-actual
",0,No Code Smell
3226,30902241,119,"              :method ""Nelder-Mead"" :tol 1e-6)
",0,No Code Smell
3227,30902241,120,"    (s1 dead-opt))
",0,No Code Smell
3228,30902241,121,"
",0,No Code Smell
3229,30902241,122,[conf-opt dead-opt],0,No Code Smell
3230,30902241,123,"%%h
",0,No Code Smell
3231,30902241,124,"(defn model-to-fn [model opt] 
",0,No Code Smell
3232,30902241,125,"  (fn [&rest args]
",0,No Code Smell
3233,30902241,126,"    (setv params (-> opt (. x) (list) (+ (list args))))
",0,No Code Smell
3234,30902241,127,"    (-> model (apply params))))
",0,No Code Smell
3235,30902241,128,"
",0,No Code Smell
3236,30902241,129,"(-> conf-model (model-to-fn conf-opt) (s1 conf-fn))
",0,No Code Smell
3237,30902241,130,(-> dead-model (model-to-fn dead-opt) (s1 dead-fn)),0,No Code Smell
3238,30902241,131,"%%h
",0,No Code Smell
3239,30902241,132,"; Compare actual vs predictions
",0,No Code Smell
3240,30902241,133,"(-> conf-actual (len) (range) (map1 conf-fn) (pd.Series :name ""Predict"") (s1 conf-predict))
",0,No Code Smell
3241,30902241,134,"(-> (pd.concat [conf-actual conf-predict] :axis 1) (s1 conf-eval))
",0,No Code Smell
3242,30902241,135,"(-> conf-eval (* population) (.plot :title ""Confirmed Cases""))
",0,No Code Smell
3243,30902241,136,"
",0,No Code Smell
3244,30902241,137,"(-> dead-actual (len) (range) (map1 dead-fn) (pd.Series :name ""Predict"") (s1 dead-predict))
",0,No Code Smell
3245,30902241,138,"(-> (pd.concat [dead-actual dead-predict] :axis 1) (s1 dead-eval))
",0,No Code Smell
3246,30902241,139,"(-> dead-eval (* population) (.plot :title ""Fatalities""))",0,No Code Smell
3247,30902241,140,"%%h
",0,No Code Smell
3248,30902241,141,"(import [sklearn [metrics]])
",1,Code Smell
3249,30902241,142,"
",0,No Code Smell
3250,30902241,143,"; Calculate conf errors.
",0,No Code Smell
3251,30902241,144,"(print ""Confirmed Cases Errors"")
",0,No Code Smell
3252,30902241,145,"(print ""Conf MSE ="" (metrics.mean-squared-error (conf-eval.Actual.to-numpy) (conf-eval.Predict.to-numpy)))
",0,No Code Smell
3253,30902241,146,"(print ""Conf MAE ="" (metrics.mean-absolute-error (conf-eval.Actual.to-numpy) (conf-eval.Predict.to-numpy)))
",0,No Code Smell
3254,30902241,147,"(print ""Conf RMSE ="" (np.sqrt (metrics.mean-squared-error (conf-eval.Actual.to-numpy) (conf-eval.Predict.to-numpy))))
",0,No Code Smell
3255,30902241,148,"
",0,No Code Smell
3256,30902241,149,"; Calculate dead errors.
",0,No Code Smell
3257,30902241,150,"(print ""Fatalities Errors"")
",0,No Code Smell
3258,30902241,151,"(print ""Dead MSE ="" (metrics.mean-squared-error (dead-eval.Actual.to-numpy) (dead-eval.Predict.to-numpy)))
",0,No Code Smell
3259,30902241,152,"(print ""Dead MAE ="" (metrics.mean-absolute-error (dead-eval.Actual.to-numpy) (dead-eval.Predict.to-numpy)))
",0,No Code Smell
3260,30902241,153,"(print ""Dead RMSE ="" (np.sqrt (metrics.mean-squared-error (dead-eval.Actual.to-numpy) (dead-eval.Predict.to-numpy))))",0,No Code Smell
3261,30902241,154,"%%h
",0,No Code Smell
3262,30902241,155,"; Next lets build out the test
",0,No Code Smell
3263,30902241,156,"(defn pd-head-tail [df]
",0,No Code Smell
3264,30902241,157,"  (print ""Rows = ""(-> df (len)))
",0,No Code Smell
3265,30902241,158,"  (-> df (.head 1) (display))
",0,No Code Smell
3266,30902241,159,"  (-> df (.tail 1) (display)))
",0,No Code Smell
3267,30902241,160,"
",0,No Code Smell
3268,30902241,161,"(-> covid-train  (pd.read-csv) (s1 df-train))
",1,Code Smell
3269,30902241,162,"(-> covid-test   (pd.read-csv) (s1 df-test))
",0,No Code Smell
3270,30902241,163,"
",0,No Code Smell
3271,30902241,164,";(pd-head-tail df-train)
",0,No Code Smell
3272,30902241,165,;(pd-head-tail df-test),0,No Code Smell
3273,30902241,166,"%%h
",0,No Code Smell
3274,30902241,167,"; Compute date0 of training data.
",0,No Code Smell
3275,30902241,168,"(-> df-train (. Date) (get 0) (dateparser.parse) (s1 date0-train))
",0,No Code Smell
3276,30902241,169,"
",0,No Code Smell
3277,30902241,170,"; Useful functions.
",0,No Code Smell
3278,30902241,171,"(defn date-string->t [d] 
",0,No Code Smell
3279,30902241,172,"  (-> d (dateparser.parse) (- date0-train) (. days)))
",0,No Code Smell
3280,30902241,173,"(defn date-string->confirmed-cases [d]
",0,No Code Smell
3281,30902241,174,"  (-> d (date-string->t) (conf-fn) (* population) (int)))
",0,No Code Smell
3282,30902241,175,"(defn date-string->fatalities [d]
",0,No Code Smell
3283,30902241,176,"  (-> d (date-string->t) (dead-fn) (* population) (int)))
",0,No Code Smell
3284,30902241,177,"
",0,No Code Smell
3285,30902241,178,"; Submission.
",0,No Code Smell
3286,30902241,179,"(-> df-test 
",0,No Code Smell
3287,30902241,180,"    (.to-dict ""records"") 
",0,No Code Smell
3288,30902241,181,"    (map1 (fn [r] 
",0,No Code Smell
3289,30902241,182,"            (-> r
",0,No Code Smell
3290,30902241,183,"              (assoc1 ""ConfirmedCases"" (-> r (get ""Date"") (date-string->confirmed-cases)))
",0,No Code Smell
3291,30902241,184,"              (assoc1 ""Fatalities""     (-> r (get ""Date"") (date-string->fatalities))))))
",0,No Code Smell
3292,30902241,185,"    (pd.DataFrame)
",0,No Code Smell
3293,30902241,186,"    (pd-keep [""ForecastId"" ""ConfirmedCases"" ""Fatalities""])
",0,No Code Smell
3294,30902241,187,"    (.to-csv ""submission.csv"" :index False))",0,No Code Smell
3295,30633398,0,"import numpy as np
import pandas as pd
train_data = pd.read_csv('/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_train.csv')
# we don't need the test data
# test_data = pd.read_csv('/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_test.csv')",1,Code Smell
3296,30633398,1,print(train_data),0,No Code Smell
3297,30633398,2,"useless_cols = []
for col in train_data.columns:
    entries_num = len(set(train_data[col]))
    if entries_num < 2:
        useless_cols.append(col)
    print(f'{col} -- has {entries_num} unic entries')",0,No Code Smell
3298,30633398,3,"train_data.drop(useless_cols, axis=1, inplace=True)
train_data.drop(['Id', 'Date'], axis=1, inplace=True)",0,No Code Smell
3299,30633398,4,"X_train = train_data[48:]
print(X_train)",0,No Code Smell
3300,30633398,5,"# Approximation of the confirmed cases first and second derivatives
diff_conf, conf_old = [], 0 
dd_conf, dc_old = [], 0

# Approximation of the fatalities first and second derivatives
diff_fat, fat_old = [], 0
dd_fat, df_old = [], 0

# Approximation of exponential grow coefficient
exp_conf, exp_fat = [], []

# Approximation of the ration between fatalities and confirmed cases
ratio = []

# Calc the features' arrays
for row in X_train.values:
    diff_conf.append(row[0]-conf_old)
    conf_old=row[0]
    
    diff_fat.append(row[1]-fat_old)
    fat_old=row[1]
    
    exp_conf.append(diff_conf[-1]/row[0])
    exp_fat.append(diff_fat[-1]/row[1])
    
    ratio.append(row[1]/row[0])
    
    dd_conf.append(diff_conf[-1]-dc_old)
    dc_old=diff_conf[-1]
    
    dd_fat.append(diff_fat[-1]-df_old)
    df_old=diff_fat[-1]

# Insert features into training set
X_train['diff_confirmed'] = diff_conf
X_train['diff_fatalities'] = diff_fat
X_train['exp_confirmed'] = exp_conf
X_train['exp_fatalities'] = exp_fat
X_train['fatalities_to_confirmed'] = ratio
X_train['dd_confirmed'] = dd_conf
X_train['dd_fatalities'] = dd_conf
print(X_train)",0,No Code Smell
3301,30633398,6,"exp_c = X_train.exp_confirmed.drop(48).mean()
print(f'exp_c: {exp_c}')
exp_f = X_train.exp_fatalities.drop(48).mean()
print(f'exp_f: {exp_f}')
ratio = X_train.fatalities_to_confirmed.drop(48).mean()
print(f'ratio: {ratio}')
d_c = X_train.diff_confirmed.drop(48).mean()
print(f'd_c: {d_c}')
dd_c = X_train.dd_confirmed.drop(48).drop(49).mean()
print(f'dd_c: {dd_c}')
d_f = X_train.diff_fatalities.drop(48).mean()
print(f'd_f: {d_f}')
dd_f = X_train.dd_fatalities.drop(48).drop(49).mean()
print(f'dd_f: {dd_f}')",0,No Code Smell
3302,30633398,7,"pred_c, pred_f = list(X_train.ConfirmedCases.loc[50:56]), list(X_train.Fatalities.loc[50:56])",0,No Code Smell
3303,30633398,8,"for i in range(1, 44 - 7):
    # use taylor series to predict confirmed cases
    pred_c.append((X_train.ConfirmedCases[56] + d_c*i + 0.5*dd_c*(i**2)))
    # use taylor series to predict fatalities
    # pred_f.append((X_train.Fatalities[56] + d_f*i + 0.5*dd_f*(i**2)))
    
    # We can also try to use ratio of fatalities to cases instead
    pred_f.append(pred_c[-1] * ratio)",0,No Code Smell
3304,30633398,9,"# for i in range(1, 44 - 7):
#     pred_c.append(X_train.ConfirmedCases[56] * ((1 + exp_c) ** i ))
#     pred_f.append(pred_c[-1] * ratio)",0,No Code Smell
3305,30633398,10,"my_submission = pd.DataFrame({'ForecastId': list(range(1,44)), 'ConfirmedCases': pred_c, 'Fatalities': pred_f})
my_submission.to_csv('submission.csv', index=False)
print(my_submission)",0,No Code Smell
3306,30633398,11,,0,No Code Smell
3307,41203950,0,"# Input data files are available in the ""../input/"" directory.

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt
%matplotlib inline

# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# Any results you write to the current directory are saved as output.

",1,Code Smell
3308,41203950,1,"global_data = pd.read_csv(""../input/novel-corona-virus-2019-dataset/covid_19_data.csv"")
global_data.head()",1,Code Smell
3309,41203950,2,"#We will use it to smooth the data for growth factor..
def smoother(inputdata,w,imax):
    data = 1.0*inputdata
    data = data.replace(np.nan,1)
    data = data.replace(np.inf,1)
    #print(data)
    smoothed = 1.0*data
    normalization = 1
    for i in range(-imax,imax+1):
        if i==0:
            continue
        smoothed += (w**abs(i))*data.shift(i,axis=0)
        normalization += w**abs(i)
    smoothed /= normalization
    return smoothed
# function to compute growth factor
def growth_factor(confirmed):
    confirmed_iminus1 = confirmed.shift(1, axis=0)
    confirmed_iminus2 = confirmed.shift(2, axis=0)
    return (confirmed-confirmed_iminus1)/(confirmed_iminus1-confirmed_iminus2)
#function to compute growth ratio
def growth_ratio(confirmed):
    confirmed_iminus1 = confirmed.shift(1, axis=0)
    return (confirmed/confirmed_iminus1)

# We don't need a function for growth rate since we can use the np.gradient() function.

# This is a function which plots (for in input country) the active, confirmed, and recovered cases, deaths, and the growth factor.
def plot_country_active_confirmed_recovered_growth_metrics(country):
    
    # Plots Active, Confirmed, and Recovered Cases. Also plots deaths.
    country_data = global_data[global_data['Country/Region']==country]
    table = country_data.drop(['SNo','Province/State', 'Last Update'], axis=1)
    table['ActiveCases'] = table['Confirmed'] - table['Recovered'] - table['Deaths']
    table2 = pd.pivot_table(table, values=['ActiveCases','Confirmed', 'Recovered','Deaths'], index=['ObservationDate'], aggfunc=np.sum)
    table3 = table2.drop(['Deaths'], axis=1)
   
    # Growth Factor
    w = 0.5
    table2['GrowthFactor'] = growth_factor(table2['Confirmed'])
    table2['GrowthFactor'] = smoother(table2['GrowthFactor'],w,5)

    # 2nd Derivative
    table2['2nd_Derivative'] = np.gradient(np.gradient(table2['Confirmed'])) #2nd derivative
    table2['2nd_Derivative'] = smoother(table2['2nd_Derivative'],w,7)


    #Plot confirmed[i]/confirmed[i-1], this is called the growth ratio
    table2['GrowthRatio'] = growth_ratio(table2['Confirmed'])
    table2['GrowthRatio'] = smoother(table2['GrowthRatio'],w,5)
    
    #Plot the growth rate, we will define this as k in the logistic function presented at the beginning of this notebook.
    table2['GrowthRate']=np.gradient(np.log(table2['Confirmed']))
    table2['GrowthRate'] = smoother(table2['GrowthRate'],0.5,3)
    
    # horizontal line at growth rate 1.0 for reference
    x_coordinates = [1, 100]
    y_coordinates = [1, 1]
    #plots
    table2['Deaths'].plot(title='Deaths')
    plt.show()
    table3.plot() 
    plt.show()
    table2['GrowthFactor'].plot(title='Growth Factor')
    plt.plot(x_coordinates, y_coordinates) 
    plt.show()
    table2['2nd_Derivative'].plot(title='2nd_Derivative')
    plt.show()
    table2['GrowthRatio'].plot(title='Growth Ratio')
    plt.plot(x_coordinates, y_coordinates)
    plt.show()
    table2['GrowthRate'].plot(title='Growth Rate')
    plt.show()

    
   # import plotly.express as px
   # table3 = table3.melt(id_vars=""ObservationDate"", value_vars=['ActiveCases','Confirmed','Recovered'],
   #              var_name='case', value_name='count')
   # table3.reindex()
   # fig = px.area(table3, x=""ObservationDate"", y=""count"", color='case',
   #              title='Confirmed Cases', color_discrete_sequence = ['cyan', 'red', 'orange'])
   # fig.show()

    return 
",0,No Code Smell
3310,41203950,3,plot_country_active_confirmed_recovered_growth_metrics('Mainland China'),0,No Code Smell
3311,41203950,4,plot_country_active_confirmed_recovered_growth_metrics('South Korea'),0,No Code Smell
3312,41203950,5,"plot_country_active_confirmed_recovered_growth_metrics('US')
",0,No Code Smell
3313,41203950,6,plot_country_active_confirmed_recovered_growth_metrics('Germany'),0,No Code Smell
3314,41203950,7,plot_country_active_confirmed_recovered_growth_metrics('Italy'),0,No Code Smell
3315,41203950,8,plot_country_active_confirmed_recovered_growth_metrics('Netherlands'),0,No Code Smell
3316,41203950,9,plot_country_active_confirmed_recovered_growth_metrics('Russia'),0,No Code Smell
3317,41203950,10,plot_country_active_confirmed_recovered_growth_metrics('New Zealand'),0,No Code Smell
3318,41203950,11,"restofworld_data = global_data
for country in restofworld_data['Country/Region']:
    if country != 'US': 
        restofworld_data['Country/Region'] = restofworld_data['Country/Region'].replace(country, ""RestOfWorld"")

plot_country_active_confirmed_recovered_growth_metrics('RestOfWorld')",0,No Code Smell
3319,41203950,12,"from scipy.optimize import curve_fit
",1,Code Smell
3320,41203950,13,"# We want number of confirmed for each date for each country
#country_data = global_data[global_data['Country/Region']=='Mainland China']
global_data2 = pd.read_csv(""../input/novel-corona-virus-2019-dataset/covid_19_data.csv"")
country_data = global_data2[global_data2['Country/Region']=='US']
#country_data = country_data[country_data['ObservationDate']<""07/05/2020""]
#country_data = country_data.drop(['SNo','Province/State', 'Last Update.'], axis=1)
country_data = pd.pivot_table(country_data, values=['Confirmed', 'Recovered','Deaths'], index=['ObservationDate'], aggfunc=np.sum)
country_data.tail()",1,Code Smell
3321,41203950,14,"#country_data['GrowthFactor'] = growth_factor(country_data['Confirmed'])

# we will want x_data to be the number of days since first confirmed and the y_data to be the confirmed data. This will be the data we use to fit a logistic curve
x_data = range(len(country_data.index))
y_data = country_data['Confirmed']

def log_curve(x, k, x_0, ymax):
    return ymax / (1 + np.exp(-k*(x-x_0)))

# Fit the curve
popt, pcov = curve_fit(log_curve, x_data, y_data, bounds=([0,0,0],np.inf), maxfev=50000)
estimated_k, estimated_x_0, ymax= popt


# Plot the fitted curve
k = estimated_k
x_0 = estimated_x_0
y_fitted = log_curve(range(0,350), k, x_0, ymax)
print(k, x_0, ymax)
#print(y_fitted)
y_data.tail()",0,No Code Smell
3322,41203950,15,"# Plot everything for illustration
fig = plt.figure()
ax = fig.add_subplot(111)
ax.plot(range(0,350), y_fitted, '--', label='fitted')
ax.plot(x_data, y_data, 'o', label='Confirmed Data')
",0,No Code Smell
3323,30799422,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
from fbprophet import Prophet

# Input data files are available in the ""../input/"" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# Any results you write to the current directory are saved as output.

# Read Data

cal_df = pd.read_csv(""/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_train.csv"")
cal_test= pd.read_csv(""/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_test.csv"")

## prepare data for FB-Prophet 
## Confirm data
cal_dat_fb = pd.DataFrame({'ds':cal_df['Date'],'y':cal_df['ConfirmedCases']})
cal_dat_fb['ds'] = cal_dat_fb['ds'].apply(pd.to_datetime)

## Fatalities
cal_dat_fat = pd.DataFrame({'ds':cal_df['Date'],'y':cal_df['Fatalities']})
cal_dat_fat['ds'] = cal_dat_fat['ds'].apply(pd.to_datetime)
    
#future data
future = pd.DataFrame({'ds':cal_test['Date']})

## FB prophet code

### Confirmed cases
m1 = Prophet(mcmc_samples=1000,interval_width=0.01,yearly_seasonality=0.4)
m2 = Prophet(mcmc_samples=1000,interval_width=0.01,yearly_seasonality=0.4)
Confirm_forecast = m1.fit(cal_dat_fb[48:]).predict(future)
Fatality_forecast = m2.fit(cal_dat_fat[48:]).predict(future)

#cf= pd.DataFrame(columns=['ConfirmedCases'])
#ff=pd.DataFrame(columns=['Fatalities'])
cf = Confirm_forecast[['yhat']].apply(np.ceil)
ff=Fatality_forecast[['yhat']].apply(np.ceil)
cf= cf.rename(columns={""yhat"": ""ConfirmedCases""})
ff= ff.rename(columns={""yhat"": ""Fatalities""})

print(Fatality_forecast[['yhat']])

## submission code 
dlist = [cal_test['ForecastId'],cf['ConfirmedCases'],ff['Fatalities']]
#names = ['ForecastId','ConfirmedCases','Fatalities']
submission = pd.concat(dlist,axis=1)

print(submission)
submission.to_csv('submission.csv',index=False)",1,Code Smell
3324,30755960,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import seaborn as sns

# Input data files are available in the ""../input/"" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# Any results you write to the current directory are saved as output.",1,Code Smell
3325,30755960,1,"df_train = pd.read_csv('/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_train.csv')
df_test = pd.read_csv('/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_test.csv')
df_submit = pd.read_csv('/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_submission.csv')
df_train.head()",1,Code Smell
3326,30755960,2,df_train.info(),1,Code Smell
3327,30755960,3,"print(df_train['Province/State'].unique())
print(df_train['Country/Region'].unique())",0,No Code Smell
3328,30755960,4,"import matplotlib.pyplot as plt
plt.figure(figsize=(14,6))
plt.hist(df_train['ConfirmedCases'],bins=10,color='green')
plt.xlabel('Confirmed Cases')
plt.ylabel('Count')
plt.title('Count of Confirmed Cases')",0,No Code Smell
3329,30755960,5,"import matplotlib.pyplot as plt
plt.figure(figsize=(15,10))
plt.plot(df_train['Fatalities'])
plt.xlabel('Fatalities')
plt.ylabel('Count')
plt.title('Graph of Fatalities ')",0,No Code Smell
3330,30755960,6,"df_train = df_train[['Date','ConfirmedCases','Fatalities']]
df_train.head()",0,No Code Smell
3331,30755960,7,"plt.figure(figsize=(15,10))
sns.barplot(x=df_train['Date'] , y = df_train['ConfirmedCases'])
plt.xticks(rotation=90)
",0,No Code Smell
3332,30755960,8,"plt.figure(figsize=(15,10))
sns.barplot(x=df_train['Date'] , y = df_train['Fatalities'])
plt.xticks(rotation=90)",0,No Code Smell
3333,30755960,9,"df_train_new = df_train.query('ConfirmedCases > 0')
df_train_new",0,No Code Smell
3334,30755960,10,"plt.figure(figsize=(15,10))
#sns.barplot(x=df_train_new['Date'] , y = df_train_new['Fatalities'])
sns.barplot(x=df_train_new['Date'] , y = df_train_new['ConfirmedCases'])
plt.xticks(rotation=45)
plt.title('ConfirmedCases as per Date')",0,No Code Smell
3335,30755960,11,"df_train['Date'] = pd.to_datetime(df_train['Date'])
df_train.insert(1,'Week',df_train['Date'].dt.week)
df_train.insert(2,'Day',df_train['Date'].dt.day)
df_train.insert(3,'DayofWeek',df_train['Date'].dt.dayofweek)
df_train.insert(4,'DayofYear',df_train['Date'].dt.dayofyear)",0,No Code Smell
3336,30755960,12,df_train,0,No Code Smell
3337,30755960,13,"from sklearn.linear_model import LinearRegression
from sklearn.linear_model import BayesianRidge 
from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import r2_score
from sklearn.model_selection import train_test_split",1,Code Smell
3338,30755960,14,"X = df_train.drop(['Date', 'ConfirmedCases', 'Fatalities'], axis=1)
y = df_train[['ConfirmedCases', 'Fatalities']]",0,No Code Smell
3339,30755960,15,"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)",0,No Code Smell
3340,30755960,16,y_train.head(),0,No Code Smell
3341,30755960,17,"#Function that predicts the scores of models.
def predict_scores(reg_alg):
    m = reg_alg()
    m.fit(X_train, y_train['ConfirmedCases'])
    y_pred = m.predict(X_test)
    m_r = r2_score(y_test['ConfirmedCases'], y_pred)
    sc_Cases.append(m_r)
    
    m.fit(X_train, y_train['Fatalities'])
    y_pred = m.predict(X_test)
    m_r2 = r2_score(y_test['Fatalities'], y_pred)
    sc_Fatalities.append(m_r2)


    
reg_models = [KNeighborsRegressor, LinearRegression, RandomForestRegressor, GradientBoostingRegressor, DecisionTreeRegressor,BayesianRidge]

sc_Cases = []
sc_Fatalities = []

for x in reg_models:
    predict_scores(x)",0,No Code Smell
3342,30755960,18,sc_Cases,0,No Code Smell
3343,30755960,19,sc_Fatalities,0,No Code Smell
3344,30755960,20,"models = pd.DataFrame({
    'Model': ['KNeighborsRegressor', 'LinearRegression', 'RandomForestRegressor', 'GradientBoostingRegressor', 'DecisionTreeRegressor','BayesianRidge' ],
    'ConfirmedCase_r2': sc_Cases,
    'Fatalities_r2' : sc_Fatalities
})

models",0,No Code Smell
3345,30755960,21,df_test.head(),0,No Code Smell
3346,30755960,22,df_test.info(),1,Code Smell
3347,30755960,23,"df_test = df_test[['ForecastId', 'Date']]

df_test['Date'] = pd.to_datetime(df_test['Date'])
df_test.insert(1,'Week',df_test['Date'].dt.week)
df_test.insert(2,'Day',df_test['Date'].dt.day)
df_test.insert(3,'DayofWeek',df_test['Date'].dt.dayofweek)
df_test.insert(4,'DayofYear',df_test['Date'].dt.dayofyear)

df_test.head()",0,No Code Smell
3348,30755960,24,"model1 = RandomForestRegressor()
model1.fit(X_train, y_train['ConfirmedCases'])

model2 = RandomForestRegressor()
model2.fit(X_train, y_train['Fatalities'])

df_test['ConfirmedCases'] = model1.predict(df_test.drop(['Date', 'ForecastId'], axis=1))
df_test['Fatalities'] = model2.predict(df_test.drop(['Date', 'ForecastId', 'ConfirmedCases'], axis=1))",0,No Code Smell
3349,30755960,25,"import warnings
warnings.filterwarnings('ignore')
df_results = df_test[['ForecastId', 'ConfirmedCases', 'Fatalities']] 
df_results['ConfirmedCases'] = df_results['ConfirmedCases'].astype(int)
df_results['Fatalities'] = df_results['Fatalities'].astype(int)

df_results.head()",0,No Code Smell
3350,30755960,26,"df_results.to_csv('submission.csv', index=False)",0,No Code Smell
3351,30592516,0,"import pandas as pd
import matplotlib.pyplot as plt
from statsmodels.tsa.arima_model import ARIMA",0,No Code Smell
3352,30592516,1,"df = pd.read_csv('../input/covid19-local-us-ca-forecasting-week-1/ca_train.csv')
df.head()",1,Code Smell
3353,30592516,2,"data_confirm = df['ConfirmedCases'].to_list()
print(""Length of initial data_confim = "",len(data_confirm)) 
data_Fat = df['Fatalities'].to_list()
print(""Length of initial data_Fat = "",len(data_Fat)) ",0,No Code Smell
3354,30592516,3,"def ARIMA_FIT(data,num_prediction):
    temp_list = data
    for i in range(num_prediction):
        model = ARIMA(temp_list, order=(0, 2, 1))
        model_fit = model.fit(disp=False)
        yhat = model_fit.predict(len(temp_list), len(temp_list), typ='levels')
        temp_list.append(round(yhat[0]))
    return temp_list",0,No Code Smell
3355,30592516,4,"ConfirmedCases = ARIMA_FIT(data_confirm,36)
print(""Length of predict data_confim = "",len(ConfirmedCases))

Fatalities = ARIMA_FIT(data_Fat,36)
print(""Length of predict data_Fat = "",len(Fatalities))",0,No Code Smell
3356,30592516,5,"plt.plot(ConfirmedCases)
plt.plot(Fatalities)",0,No Code Smell
3357,30592516,6,"df_submission = pd.read_csv('../input/covid19-local-us-ca-forecasting-week-1/ca_submission.csv')
submission = pd.DataFrame({""ForecastId"": df_submission['ForecastId'], ""ConfirmedCases"": ConfirmedCases[50:],""Fatalities"":Fatalities[50:]})
submission.to_csv('submission.csv', index=False)",0,No Code Smell
3358,30592516,7,,0,No Code Smell
3359,30715869,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the ""../input/"" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# Any results you write to the current directory are saved as output.",1,Code Smell
3360,30715869,1,"# Load data into Pandas dataframes
df_train = pd.read_csv('/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_train.csv')
df_test = pd.read_csv('/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_test.csv')
df_submission = pd.read_csv('/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_submission.csv')",1,Code Smell
3361,30715869,2,"# Check a preview of the data
df_train.tail()",0,No Code Smell
3362,30715869,3,"# Check the properties of the data

print(df_train['Province/State'].unique())
print(df_train['Country/Region'].unique())
print(df_train['Lat'].unique())
print(df_train['Long'].unique())
print(df_train.dtypes)",0,No Code Smell
3363,30715869,4,df_train.describe(),0,No Code Smell
3364,30715869,5,"# Check the distribution of the confirmed cases

df_train.hist(column='ConfirmedCases')",0,No Code Smell
3365,30715869,6,"# Check the distribution of the fatalities

df_train.hist(column='Fatalities')",0,No Code Smell
3366,30715869,7,"# Take only what we need: date, confirmed cases and fatalities

df_train = df_train[['Date', 'ConfirmedCases', 'Fatalities']]",0,No Code Smell
3367,30715869,8,"# Convert Date column to Pandas date and orther to get chronological data

df_train['Date'] = pd.to_datetime(df_train['Date'])
df_train = df_train.sort_values(by=['Date'])",0,No Code Smell
3368,30715869,9,"# Check the trend in a chart

df_train.plot.bar(x='Date', y=['ConfirmedCases','Fatalities'])",0,No Code Smell
3369,30715869,10,"# As the confirmed cases are far away from the start we will focus in that time

df_train2 = df_train.query('ConfirmedCases != 0.0')

df_train2.plot.bar(x='Date', y=['ConfirmedCases', 'Fatalities'])",0,No Code Smell
3370,30715869,11,"df_train['Week'] = df_train['Date'].dt.week
df_train['Day'] = df_train['Date'].dt.day
df_train['WeekDay'] = df_train['Date'].dt.dayofweek
df_train['YearDay'] = df_train['Date'].dt.dayofyear",0,No Code Smell
3371,30715869,12,"from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import Lasso, ElasticNet
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import r2_score

from sklearn.model_selection import train_test_split

predictors = df_train.drop(['Date', 'ConfirmedCases', 'Fatalities'], axis=1)
target = df_train[['ConfirmedCases', 'Fatalities']]
x_train, x_test, y_train, y_test = train_test_split(predictors, target, test_size=0.2, random_state=1)

def scores(alg):
    lin = alg()
    lin.fit(x_train, y_train['ConfirmedCases'])
    y_pred = lin.predict(x_test)
    lin_r = r2_score(y_test['ConfirmedCases'], y_pred)
    s.append(lin_r)
    
    lin.fit(x_train, y_train['Fatalities'])
    y_pred = lin.predict(x_test)
    lin_r = r2_score(y_test['Fatalities'], y_pred)
    s2.append(lin_r)
    
algos = [KNeighborsRegressor, LinearRegression, RandomForestRegressor, GradientBoostingRegressor, Lasso, ElasticNet, DecisionTreeRegressor]

s = []
s2 = []

for algo in algos:
    scores(algo)
    
models = pd.DataFrame({
    'Method': ['KNeighborsRegressor', 'LinearRegression', 'RandomForestRegressor', 'GradientBoostingRegressor', 'Lasso', 'ElasticNet', 'DecisionTreeRegressor'],
    'ScoreCC': s,
    'ScoreF' : s2
})

models.sort_values(by=['ScoreCC', 'ScoreF'], ascending=False)",0,No Code Smell
3372,30715869,13,"# Now let's try for last an ARIMA model

# First we see that data is not stationary, so we need to check the autocorrelation of the time series

from pandas.plotting import autocorrelation_plot

autocorrelation_plot(df_train['ConfirmedCases'])",0,No Code Smell
3373,30715869,14,"from statsmodels.tsa.arima_model import ARIMA
from matplotlib import*

arima_model = ARIMA(df_train['ConfirmedCases'], order=(4,1,0)).fit(disp=0, transparams=True, trend='c')
print(arima_model.summary())

residuals = pd.DataFrame(arima_model.resid)
residuals.plot()
pyplot.show()
residuals.plot(kind='kde')
pyplot.show()
print(residuals.describe())

arima_model2 = ARIMA(df_train['Fatalities'], order=(4,1,0)).fit(disp=0, transparams=True, trend='c')
print(arima_model2.summary())

residuals2 = pd.DataFrame(arima_model2.resid)
residuals2.plot()
pyplot.show()
residuals2.plot(kind='kde')
pyplot.show()
print(residuals2.describe())",0,No Code Smell
3374,30715869,15,"predictions_arima = list(arima_model.predict())
predictions_arima.append(arima_model.forecast()[0][0])
predictions_arima.append(arima_model.forecast()[0][0])

df_train['arima'] = predictions_arima

predictions_arima2 = list(arima_model2.predict())
predictions_arima2.append(arima_model2.forecast()[0][0])
predictions_arima2.append(arima_model2.forecast()[0][0])

df_train['arima2'] = predictions_arima2

df_train.plot.bar(x='Date', y=['ConfirmedCases', 'arima'])
df_train.plot.bar(x='Date', y=['Fatalities', 'arima2'])",0,No Code Smell
3375,30715869,16,df_submission.head(),0,No Code Smell
3376,30715869,17,"print(df_test['Date'].values)
print(len(df_test['Date']))",0,No Code Smell
3377,30715869,18,"df_test = df_test[['ForecastId', 'Date']]

df_test['Date'] = pd.to_datetime(df_test['Date'])
df_test['Week'] = df_test['Date'].dt.week
df_test['Day'] = df_test['Date'].dt.day
df_test['WeekDay'] = df_test['Date'].dt.dayofweek
df_test['YearDay'] = df_test['Date'].dt.dayofyear

df_test.head()",0,No Code Smell
3378,30715869,19,"model = RandomForestRegressor()
model.fit(x_train, y_train['ConfirmedCases'])

model2 = RandomForestRegressor()
model2.fit(x_train, y_train['Fatalities'])


df_test['ConfirmedCases'] = model.predict(df_test.drop(['Date', 'ForecastId'], axis=1))
df_test['Fatalities'] = model2.predict(df_test.drop(['Date', 'ForecastId', 'ConfirmedCases'], axis=1))",0,No Code Smell
3379,30715869,20,"df_final = df_test[['ForecastId', 'ConfirmedCases', 'Fatalities']] 
df_final['ConfirmedCases'] = df_final['ConfirmedCases'].astype(int)
df_final['Fatalities'] = df_final['Fatalities'].astype(int)

df_final.head()",0,No Code Smell
3380,30715869,21,"df_final.plot.bar(x='ForecastId', y=['ConfirmedCases', 'Fatalities'])",0,No Code Smell
3381,30715869,22,"df_final.to_csv('submission.csv', index=False)",0,No Code Smell
3382,30715869,23,,0,No Code Smell
3383,30827911,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the ""../input/"" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# Any results you write to the current directory are saved as output.",1,Code Smell
3384,30827911,1,"#import IPython
#IPython.display.IFrame(<iframe width=""650"" height=""400"" frameborder=""0"" scrolling=""no"" marginheight=""0"" marginwidth=""0"" title=""2019-nCoV"" src=""/gisanddata.maps.arcgis.com/apps/Embed/index.html?webmap=14aa9e5660cf42b5b4b546dec6ceec7c&extent=77.3846,11.535,163.5174,52.8632&zoom=true&previewImage=false&scale=true&disable_scroll=true&theme=light""></iframe>)",0,No Code Smell
3385,30827911,2,"from IPython.display import HTML

HTML('<div style=""position:relative;height:0;padding-bottom:56.25%""><iframe src=""https://www.youtube.com/embed/jmHbS8z57yI?ecver=2"" width=""640"" height=""360"" frameborder=""0"" style=""position:absolute;width:100%;height:100%;left:0"" allowfullscreen></iframe></div>')",0,No Code Smell
3386,30827911,3,"## install calmap
#! pip install calmap",0,No Code Smell
3387,30827911,4,"# essential libraries
import json
import random
from urllib.request import urlopen

# storing and anaysis
import numpy as np
import pandas as pd

# visualization
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objs as go
import plotly.figure_factory as ff
#import calmap
import folium
import plotly.io as pio
pio.templates.default = ""plotly_dark""
from plotly.subplots import make_subplots

# color pallette
cnf = '#393e46' # confirmed - grey
dth = '#ff2e63' # death - red
rec = '#21bf73' # recovered - cyan
act = '#fe9801' # active case - yellow

# converter
from pandas.plotting import register_matplotlib_converters
register_matplotlib_converters()   

# hide warnings
import warnings
warnings.filterwarnings('ignore')

# html embedding
from IPython.display import Javascript
from IPython.core.display import display
from IPython.core.display import HTML",0,No Code Smell
3388,30827911,5,"# list files
#!ls ../input/corona-virus-report
# https://www.kaggle.com/imdevskp/corona-virus-report",1,Code Smell
3389,30827911,6,"# importing datasets


full_table = pd.read_csv('../input/corona-virus-report/covid_19_clean_complete.csv', parse_dates=['Date'])
train = pd.read_csv('/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_train.csv')
sub = pd.read_csv('/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_submission.csv')
test = pd.read_csv('/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_test.csv')",1,Code Smell
3390,30827911,7,"full_table[full_table['Province/State']=='California']
",0,No Code Smell
3391,30827911,8,train,0,No Code Smell
3392,30827911,9,"ca_by_state = train.copy()

train.columns
ca_by_state.columns =['Id', 'Province/State', 'Country/Region', 'Lat', 'Long', 'Date',
       'Confirmed', 'Deaths']
ca_by_state = ca_by_state[ca_by_state.Date >'2020-03-09']
",0,No Code Smell
3393,30827911,10,"
ca_by_state['Active'] = ca_by_state.Confirmed - ca_by_state.Deaths
ca_by_state",0,No Code Smell
3394,30827911,11,"print ('Last update of this dataset was ' + str(train.loc[len(train)-1]['Date']))
print ('Last update of the studay dataset was ' + str(full_table.loc[len(full_table)-1]['Date']))",0,No Code Smell
3395,30827911,12,"#rates
",0,No Code Smell
3396,30827911,13,"dict = {
        'California':ca_by_state,
        #'United States': us_by_date,
}",0,No Code Smell
3397,30827911,14,"def plots_by_country (country, country_name):

    temp = country

    # adding two more columns
    temp['No. of Deaths to 100 Confirmed Cases'] = round(temp['Deaths']/temp['Confirmed'], 3)*100
    # temp['No. of Recovered to 1 Death Case'] = round(temp['Recovered']/temp['Deaths'], 3)
    #print (temp)

    
    #print (temp.iloc[13]['Date'])
    last_date = temp.iloc[len(temp)-1]['Date']
    death_rate = temp[temp.Date ==last_date]['No. of Deaths to 100 Confirmed Cases']
    temp = temp.melt(id_vars='Date', value_vars=['No. of Deaths to 100 Confirmed Cases', ], 
                     var_name='Ratio', value_name='Value')

    #str(full_table.loc[len(full_table)-1]['Date'])

    fig = px.line(temp, x=""Date"", y=""Value"", color='Ratio', log_y=True, width=1000, height=700,
                  title=country_name + ' Recovery and Mortality Rate Over Time', color_discrete_sequence=[dth, rec])
    fig.show()
    return death_rate, 0
        
rates = []
for key, value in dict.items():
    death_rate, recovered_rate  = plots_by_country (value,key)
    rates.append ([key,np.float(death_rate),np.float(recovered_rate)]) ",0,No Code Smell
3398,30827911,15,"import pylab
from scipy.optimize import curve_fit

def sigmoid(x, x0, k):
     y = 1 / (1 + np.exp(-k*(x-x0)))
     return y

def exp (x,a,b):
    y = a* np.exp(x*b)
    return y

def gaussian(x, a, x0, sigma):
    return a*np.exp(-(x-x0)**2/(2*sigma**2))

def growth_rate_over_time (f, country, attribute, title):
    ydata = country[attribute]
    

    xdata = list(range(len(ydata)))

    rates = []
    for i, x in enumerate(xdata):
        if i > 2:
#            print (xdata[:x+1])
#            print (ydata[:x+1])

            popt, pcov = curve_fit(f, xdata[:x+1], ydata[:x+1])
            rates.append (popt[1])
    rates = np.array(rates) 
    pylab.style.use('dark_background')
    pylab.figure(figsize=(12,8))
    xdata = np.array(xdata)
    #pylab.grid(True, linestyle='-', color='0.75')
    pylab.plot(xdata[3:]+1, 100*rates, 'o', linestyle='solid', label=attribute)
    #if fit_good:
    #    pylab.plot(x,y, label='fit')
    #pylab.ylim(0, ymax*1.05)
    #pylab.legend(loc='best')
    pylab.xlabel('Days Since Start')
    pylab.ylabel('Growth rate percentage ' + attribute)
    pylab.title(title + attribute, size = 15)
    pylab.show()
    
        
    

def plot_curve_fit (f, country, attribute, title, normalize = False, curve = 'Exp'):
    #print (country)
    #country = country[10:]
    fit_good = True
    ydata = country[attribute]
    #ydata = np.array(ydata)
    xdata = range(len(ydata))
    mu = np.mean(ydata)
    sigma = np.std(ydata)
    ymax = np.max(ydata)    
    if normalize:
        ydata_norm = ydata/ymax
    else:
        ydata_norm = ydata
    #f = sigmoid
    try:
        if curve == 'Gauss': # pass the mean and stddev
            popt, pcov = curve_fit(f, xdata, ydata_norm, p0 = [1, mu, sigma])
        else:    
            popt, pcov = curve_fit(f, xdata, ydata_norm,)
    except RuntimeError:
        print ('Exception - RuntimeError - could not fit curve')
        fit_good = False
    else:

        fit_good = True
        
    if fit_good:
        if curve == 'Exp':   
            print (key + ' -- Coefficients for y = a * e^(x*b)  are ' + str(popt))
            print ('Growth rate is now ' + str(round(popt[1],2)))
        elif curve == 'Gauss':
            print (key + ' -- Coefficients are ' + str(popt))
        else:   # sigmoid 
            print (key + ' -- Coefficients for y 1/(1 + e^(-k*(x-x0)))  are ' + str(popt))
            
        print ('Mean error for each coefficient: ' + str(np.sqrt(np.diag(pcov))/popt))
    else:
        print (key + ' -- Could not resolve coefficients ---')
    x = np.linspace(-1, len(ydata), 100)
    if fit_good:
        y = f(x, *popt)
        if normalize:
            y = y * ymax
    plt.style.use('dark_background')
    pylab.figure(figsize=(15,12)) 
    #pylab.grid(True, linestyle='-', color='0.75')
    pylab.plot(xdata, ydata, 'o', label=attribute)
    if fit_good:
        pylab.plot(x,y, label='fit')
    pylab.ylim(0, ymax*1.05)
    pylab.legend(loc='best')
    pylab.xlabel('Days Since Start')
    pylab.ylabel('Number of ' + attribute)
    pylab.title(title + attribute, size = 15)
    pylab.show()
",0,No Code Smell
3399,30827911,16,"for key, value in dict.items():
    if key in [""China"",'Rest of China w/o Hubei']:
        pass
    else:
        #growth_rate_over_time (exp, value, 'Confirmed', ""Growth Rate Percentage - "")
        growth_rate_over_time (exp, value, 'Confirmed', key + ' - Growth Rate Percentage for ',)
        #growth_rate_over_time (exp, value, 'Deaths', key + ' - Growth Curve for ',)
        #growth_rate_over_time (exp, value, 'Recovered', key + ' - Growth Curve for ',False)",0,No Code Smell
3400,30827911,17,"round (72/35,2)",0,No Code Smell
3401,30827911,18,"for key, value in dict.items():
    if key in [""China"",'Rest of China w/o Hubei']:
        pass
    else:
        plot_curve_fit (exp, value, 'Confirmed', key + ' - Growth Curve for ',False,'Exp')
        plot_curve_fit (exp, value, 'Deaths', key + ' - Growth Curve for ',False,'Exp')
        #plot_curve_fit (exp, value, 'Recovered', key + ' - Growth Curve for ',False,'Exp')",0,No Code Smell
3402,30827911,19,"#    plot_curve_fit (sigmoid, value, 'Recovered', key + ' - Logistic Growth Curve for ',True,'Logistic')",0,No Code Smell
3403,30827911,20,"plot_curve_fit (gaussian, ca_by_state, 'Active', 'California' + ' - Curve for Cases ',False,'Gauss')",0,No Code Smell
3404,30827911,21,"x0 = 33
k = 0.27
kd = 0.3
x0_death = 35
results = [] 
total_estimated = 500000
total_deaths = total_estimated * 0.15
for x in range(1,44):
    conf = int(total_estimated * sigmoid(x, x0, k))
    deaths = int(total_deaths * sigmoid(x, x0_death, kd))
    print ('Confirmed estimate for day ' + str(x) + ' - ' + str(conf))
    print ('Death estimate for day ' + str(x) + ' - ' + str(deaths))
    results.append([x,conf,deaths])",0,No Code Smell
3405,30827911,22,ca_by_state,0,No Code Smell
3406,30827911,23,"r = pd.DataFrame(results)
r.columns = sub.columns
sub = r.copy()
sub
",0,No Code Smell
3407,30827911,24,"sub.to_csv(""submission.csv"", index=False)",0,No Code Smell
3408,30725465,0,"#Libraried
import pandas as pd
pd.set_option('display.max_columns', None)
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings(""ignore"")

import datetime
from time import time
from scipy import stats

from sklearn.model_selection import GroupKFold
import lightgbm as lgb
import xgboost as xgb
from catboost import CatBoostRegressor, CatBoostClassifier
import plotly.express as px
import plotly.graph_objects as go
import plotly.figure_factory as ff
from plotly.subplots import make_subplots
import os
import glob
import copy

import numpy as np
from scipy.integrate import odeint",0,No Code Smell
3409,30725465,1,"ca_train = pd.read_csv('/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_train.csv')
ca_test = pd.read_csv('/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_test.csv')
ca_submission = pd.read_csv('/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_submission.csv')

train_df = ca_train
test_df =  ca_test
submission_df =  ca_submission",1,Code Smell
3410,30725465,2,train_df.head(),0,No Code Smell
3411,30725465,3,"x_1 = train_df['Date']
y_1 = train_df['ConfirmedCases']
y_2 = train_df['Fatalities']

fig = make_subplots(rows=1, cols=1)

fig.add_trace(
    go.Scatter(x=x_1, mode='lines+markers', y=y_1, marker=dict(color=""mediumaquamarine""), showlegend=False,
               name=""Original signal""),
    row=1, col=1
)

fig.add_trace(
    go.Scatter(x=x_1, mode='lines+markers', y=y_2, marker=dict(color=""darkgreen""), showlegend=False,
               name=""Original signal""),
    row=1, col=1
)

fig.update_layout(height=400, width=800, title_text=""ConfirmedCases (pale) vs. Fatalities (dark) "")
fig.show()",0,No Code Smell
3412,30725465,4,"# SI model
N = 1000000          # Total population
I = np.zeros(200)  # Infected
S = np.zeros(200)   # Susceptible

r = 10             # This value defines how quickly the disease spreads
B = 0.01            # Probability of being infected

I[0] = 1           # On day 0, there's only one infected person
S[0] = N-I[0]      # So the suspecptible people is equal = N - I[0]

for idx in range(199):
    S[idx+1] = S[idx] - r*B*I[idx]*S[idx]/N
    I[idx+1] = I[idx] + r*B*I[idx]*S[idx]/N
",0,No Code Smell
3413,30725465,5,"sns.lineplot(x=np.arange(200), y=S, label='Susceptible')
sns.lineplot(x=np.arange(200), y=I, label='Infected')",0,No Code Smell
3414,30725465,6,"N = 36000000        # Total population
days = 200          # Period
E = np.zeros(days)  # Exposed          
E[0] = 0            # Day 0 exposed
I = np.zeros(days)  # Infected
I[0] = 144          # Day 0 infected                                                                
S = np.zeros(days)  # Susceptible
S[0] = N - I[0]     # Day 0 susceptible
R = np.zeros(days)  # Recovered
R[0] = 0

r = 20              # Number of susceptible could be contactes by an infected
B = 0.03            # Probability of spread for infected
a = 0.1             # Probability of converted from exposed to infected
r2 = 20             # Number of susceptible could be contactes by an exposed
B2 = 0.03           # Probability of spread for exposed
y = 0.1             # Probability of recovered


for idx in range(days-1):
    S[idx+1] = S[idx] - r*B*S[idx]*I[idx]/N - r2*B2*S[idx]*E[idx]/N
    E[idx+1] = E[idx] + r*B*S[idx]*I[idx]/N -a*E[idx] + r2*B2*S[idx]*E[idx]/N
    I[idx+1] = I[idx] + a*E[idx] - y*I[idx]
    R[idx+1] = R[idx] + y*I[idx]

    
plt.figure(figsize=(16,9))
sns.lineplot(x=np.arange(200), y=S, label='Susceptible')
sns.lineplot(x=np.arange(200), y=I, label='Infected')
sns.lineplot(x=np.arange(200), y=E, label='Exposed')
sns.lineplot(x=np.arange(200), y=R, label='Recovered')



I_origin = copy.copy(I)",0,No Code Smell
3415,30725465,7,"N = 36000000        # Total population
days = 200          # Period
E = np.zeros(days)  # Exposed          
E[0] = 0            # Day 0 exposed
I = np.zeros(days)  # Infected
I[0] = 144            # Day 0 infected                                                                
S = np.zeros(days)  # Susceptible
S[0] = N - I[0]     # Day 0 susceptible
R = np.zeros(days)  # Recovered
R[0] = 0

r = 20              # Number of susceptible could be contactes by an infected
B = 0.03            # Probability of spread for infected
a = 0.1             # Probability of converted from exposed to infected
r2 = 20             # Number of susceptible could be contactes by an exposed
B2 = 0.03           # Probability of spread for exposed
y = 0.1             # Probability of recovered


for idx in range(days-1):
    if idx>10:
        r = 5
        r2 = 5
    S[idx+1] = S[idx] - r*B*S[idx]*I[idx]/N - r2*B2*S[idx]*E[idx]/N
    E[idx+1] = E[idx] + r*B*S[idx]*I[idx]/N -a*E[idx] + r2*B2*S[idx]*E[idx]/N
    I[idx+1] = I[idx] + a*E[idx] - y*I[idx]
    R[idx+1] = R[idx] + y*I[idx]

plt.figure(figsize=(16,9))
sns.lineplot(x=np.arange(200), y=S, label='Secestible')
sns.lineplot(x=np.arange(200), y=I, label='Infected')
sns.lineplot(x=np.arange(200), y=E, label='Exposed')
sns.lineplot(x=np.arange(200), y=R, label='Recovered')

I_sd = copy.copy(I)",0,No Code Smell
3416,30725465,8,"plt.figure(figsize=(16,9))
sns.lineplot(x=np.arange(200), y=I_origin, label='Infected w/o social distancing')
sns.lineplot(x=np.arange(200), y=I_sd, label='Infected w/ social distancing')",0,No Code Smell
3417,30725465,9,"def base_seir_model(init_vals, params, t):
    S_0, E_0, I_0, R_0 = init_vals
    S, E, I, R = [S_0], [E_0], [I_0], [R_0]
    alpha, beta, gamma = params
    dt = t[1] - t[0]
    for _ in t[1:]:
        next_S = S[-1] - (beta*S[-1]*I[-1])*dt
        next_E = E[-1] + (beta*S[-1]*I[-1] - alpha*E[-1])*dt
        next_I = I[-1] + (alpha*E[-1] - gamma*I[-1])*dt
        next_R = R[-1] + (gamma*I[-1])*dt
        S.append(next_S)
        E.append(next_E)
        I.append(next_I)
        R.append(next_R)
    return np.stack([S, E, I, R]).T",0,No Code Smell
3418,30725465,10,"# Define parameters
t_max = 100
dt = .1
t = np.linspace(0, t_max, int(t_max/dt) + 1)
N = 36000000
init_infected = 144
init_exposed = 200
init_vals = 1 - (init_infected + init_exposed)/N,  init_exposed/N, init_infected/N, 0
alpha = 0.2
beta = 1.75
gamma = 0.5
params = alpha, beta, gamma
# Run simulation
results = base_seir_model(init_vals, params, t)
results_df = pd.DataFrame(results*N, columns=['susceptible', 'exposed',
                               'infected', 'recovered'])
results_df.head()
plt.figure(figsize=(16,9))
sns.lineplot(x=results_df.index, y=results_df['infected'], label='infected')
sns.lineplot(x=results_df.index, y=results_df['susceptible'], label='susceptible')
sns.lineplot(x=results_df.index, y=results_df['exposed'], label='exposed')
sns.lineplot(x=results_df.index, y=results_df['recovered'], label='recovered')
",0,No Code Smell
3419,30725465,11,"def seir_model_with_soc_dist(init_vals, params, t):
    S_0, E_0, I_0, R_0 = init_vals
    S, E, I, R = [S_0], [E_0], [I_0], [R_0]
    alpha, beta, gamma, rho = params
    dt = t[1] - t[0]
    for _ in t[1:]:
        next_S = S[-1] - (rho*beta*S[-1]*I[-1])*dt
        next_E = E[-1] + (rho*beta*S[-1]*I[-1] - alpha*E[-1])*dt
        next_I = I[-1] + (alpha*E[-1] - gamma*I[-1])*dt
        next_R = R[-1] + (gamma*I[-1])*dt
        S.append(next_S)
        E.append(next_E)
        I.append(next_I)
        R.append(next_R)
    return np.stack([S, E, I, R]).T",0,No Code Smell
3420,30725465,12,"# Define parameters
t_max = 100
dt = .1
t = np.linspace(0, t_max, int(t_max/dt) + 1)
N = 36000000
init_infected = 144
init_exposed = 5000
init_vals = 1 - (init_infected + init_exposed)/N,  init_exposed/N, init_infected/N, 0
alpha = 0.2
beta = 1.75
gamma = 0.5
rho = 0.8
params = alpha, beta, gamma, rho
# Run simulation
results = seir_model_with_soc_dist(init_vals, params, t)
results_df = pd.DataFrame(results*N, columns=['susceptible', 'exposed',
                               'infected', 'recovered'])
results_df.head()

plt.figure(figsize=(16,9))
sns.lineplot(x=results_df.index, y=results_df['infected'], label='infected')
sns.lineplot(x=results_df.index, y=results_df['susceptible'], label='susceptible')
sns.lineplot(x=results_df.index, y=results_df['exposed'], label='exposed')
sns.lineplot(x=results_df.index, y=results_df['recovered'], label='recovered')
",0,No Code Smell
3421,30725465,13,"# # Define parameters
# t_max = 100
# dt = .1
# t = np.linspace(0, t_max, int(t_max/dt) + 1)
# N = 36000000
# init_infected = 144
# init_exposed = 300
# init_vals = 1 - (init_infected + init_exposed)/N,  init_exposed/N, init_infected/N, 0
# alpha = 0.2
# beta = 1.75
# gamma = 0.5
# rho = 1
# params = alpha, beta, gamma, rho
# # Run simulation
# results = seir_model_with_soc_dist(init_vals, params, t)
# results_df = pd.DataFrame(results*N, columns=['susceptible', 'exposed',
#                                'infected', 'recovered'])
# results_df.head()
# plt.figure(figsize=(16,9))
# sns.lineplot(x=results_df.index, y=results_df['infected'], label='infected')
# sns.lineplot(x=results_df.index, y=results_df['susceptible'], label='susceptible')
# sns.lineplot(x=results_df.index, y=results_df['exposed'], label='exposed')
# sns.lineplot(x=results_df.index, y=results_df['recovered'], label='recovered')
",0,No Code Smell
3422,30725465,14,"ca_covid_df = ca_train[(ca_train['Province/State']=='California') & (ca_train['Date']>='2020-03-10')]
ca_covid_df['pred_confirmed'] = results_df[:ca_covid_df.shape[0]]['infected'].values
ca_covid_df",0,No Code Smell
3423,30725465,15,"results_df['Date'] = pd.date_range('2020-03-10', periods=results_df.shape[0]).values
submission_df['Date'] =  pd.date_range(start=ca_test['Date'].min(), periods=len(submission_df))
death_rate = 0.012
submission_df = pd.merge(submission_df, results_df, how='left', on='Date')
submission_df['ConfirmedCases'] = submission_df['infected']
submission_df['Fatalities'] = submission_df['ConfirmedCases'] * death_rate
submission_df[['ForecastId', 'ConfirmedCases', 'Fatalities']].to_csv('submission.csv', index=False)",0,No Code Smell
3424,30725465,16,submission_df,0,No Code Smell
3425,30696294,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the ""../input/"" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# Any results you write to the current directory are saved as output.",1,Code Smell
3426,30696294,1,"train = pd.read_csv(""/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_train.csv"", parse_dates=['Date'])
",1,Code Smell
3427,30696294,2,train.head(),0,No Code Smell
3428,30696294,3,train.tail(),0,No Code Smell
3429,30696294,4,"test = pd.read_csv(""/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_test.csv"", parse_dates=['Date'])
",1,Code Smell
3430,30696294,5,test.head(),0,No Code Smell
3431,30696294,6,test.tail(),0,No Code Smell
3432,30696294,7,"#Are rows unique?
(len(train)==train.index.nunique()) & (len(test)==test.Date.nunique())",0,No Code Smell
3433,30696294,8,"from fbprophet import Prophet
",1,Code Smell
3434,30696294,9,"df_c = pd.DataFrame()
df_c['ds'] = pd.to_datetime(train.Date)
df_c['y'] = train.ConfirmedCases
df_c.set_index('ds', inplace=True)
df_c = df_c.loc[""2020-03-09"":""2020-03-11""]
df_c.reset_index(inplace=True)

df_f = pd.DataFrame()
df_f['ds'] = pd.to_datetime(train.Date)
df_f['y'] = train.Fatalities
df_f.set_index('ds', inplace=True)
df_f = df_f.loc[""2020-03-09"":""2020-03-11""]
df_f.reset_index(inplace=True)",0,No Code Smell
3435,30696294,10,"m_c, m_f = Prophet(), Prophet()
m_c.fit(df_c)
m_f.fit(df_f)

future = m_c.make_future_dataframe(periods=len(test))

forecast_c = m_c.predict(future)
forecast_f = m_f.predict(future)",0,No Code Smell
3436,30696294,11,"forecast_c = forecast_c[['ds','yhat']]
forecast_f = forecast_f[['ds','yhat']]

forecast_c= forecast_c.rename(columns={""yhat"":""ConfirmedCases""})
forecast_f = forecast_f.rename(columns={""yhat"":""Fatalities""})
",0,No Code Smell
3437,30696294,12,"conf_cases = forecast_c.ConfirmedCases.iloc[3:].reset_index(drop=True)
fatal_cases = forecast_f.Fatalities.iloc[3:].reset_index(drop=True)

",0,No Code Smell
3438,30696294,13,"test.reset_index(inplace=True,drop=True)",0,No Code Smell
3439,30696294,14,"submissions = pd.concat([test.ForecastId, conf_cases,fatal_cases], axis=1)

",0,No Code Smell
3440,30696294,15,submissions.head(),0,No Code Smell
3441,30696294,16,"submissions.ConfirmedCases = submissions.ConfirmedCases.astype(int)
submissions.Fatalities = submissions.Fatalities.astype(int)",0,No Code Smell
3442,30696294,17,submissions.head(),0,No Code Smell
3443,30696294,18,"submissions.to_csv('submission.csv', index=False)",0,No Code Smell
3444,30696294,19,,0,No Code Smell
3445,30846479,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the ""../input/"" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# Any results you write to the current directory are saved as output.",1,Code Smell
3446,30846479,1,"import datetime as dt
import matplotlib.pyplot as plt
%matplotlib inline
import matplotlib.patches as mpatches",0,No Code Smell
3447,30846479,2,"ca_test_covid = pd.read_csv('/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_test.csv')
ca_test_covid['Date']=pd.to_numeric(ca_test_covid.Date.str.replace('-',''))
ca_test_covid.head()
",1,Code Smell
3448,30846479,3,"ca_train_covid = pd.read_csv('/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_train.csv')
ca_train_covid.head()",1,Code Smell
3449,30846479,4,"#plt.close('all')
plt.figure()
ca_train_covid_confirm_cases=ca_train_covid[ca_train_covid['ConfirmedCases']>0]
ca_train_covid_confirm_cases['Date']=pd.to_numeric(ca_train_covid_confirm_cases.Date.str.replace('-',''))

",0,No Code Smell
3450,30846479,5,"def calculateDateRank(date_values):
    listRank=()
    arrayVal=np.array(date_values)
    rank=1
    for i in np.arange(len(arrayVal)):
        listRank=np.append(listRank,rank)
        rank=rank+1   
    
    return listRank",0,No Code Smell
3451,30846479,6,"ca_train_covid_confirm_cases['DateRank']=calculateDateRank(ca_train_covid_confirm_cases.Date)
ca_train_covid_confirm_cases.head()",0,No Code Smell
3452,30846479,7,"ca_test_covid['DateRank']=calculateDateRank(ca_test_covid.Date)
ca_train_covid_confirm_cases.plot.scatter(x='Date',y='ConfirmedCases')
ca_train_covid_confirm_cases.plot.scatter(x='Date',y='Fatalities')",0,No Code Smell
3453,30846479,8,ca_train_covid_confirm_cases.corr(),1,Code Smell
3454,30846479,9,"import matplotlib.pyplot as plt
import numpy as np
from sklearn import datasets, linear_model
from sklearn.metrics import mean_squared_error, r2_score,mean_squared_log_error
from sklearn.preprocessing import PolynomialFeatures",1,Code Smell
3455,30846479,10,"# Create linear regression model
#regModelConfirmedCases = linear_model.LinearRegression()
regModelConfirmedCases = linear_model.RidgeCV(alphas=np.logspace(-6, 6, 13))
trainX=np.array(ca_train_covid_confirm_cases['DateRank']).reshape(-1,1)
trainY=np.array(ca_train_covid_confirm_cases['ConfirmedCases']).reshape(-1,1)",0,No Code Smell
3456,30846479,11,"# Train the model using the training sets
poly_features = PolynomialFeatures(degree=4)

X_train_poly = poly_features.fit_transform(trainX)    
    
regModelConfirmedCases.fit(X_train_poly, trainY)",0,No Code Smell
3457,30846479,12,"# Make predictions using the testing set
ca_test_covid_date_gt_26Mar=ca_test_covid
testX_Date=np.array(ca_test_covid.DateRank).reshape(-1,1)
pred_ConfirmedCase = regModelConfirmedCases.predict(poly_features.fit_transform(testX_Date))",0,No Code Smell
3458,30846479,13,"#print(""Graph of a Train Date point vs ConfirmedCase prediction regression line"")
# Plot outputs
plt.scatter(trainX, trainY,  color='green')
plt.plot(testX_Date, pred_ConfirmedCase, color='blue', linewidth=1)
plt.scatter(testX_Date, pred_ConfirmedCase,  color='yellow')

plt.title('training / predicted values across regression line for ')
plt.xticks(())
plt.yticks(())
plt.xlabel(""Date Rank"")
plt.ylabel(""ConfirmedCase"")

green_patch = mpatches.Patch(color='green', label='Train Values')
yellow_patch = mpatches.Patch(color='yellow', label='Predicted Values')

plt.legend(handles=[green_patch,yellow_patch])

plt.show()",0,No Code Smell
3459,30846479,14,"# Train the model using the training sets
trainY_Fatalities=np.array(ca_train_covid_confirm_cases['Fatalities']).reshape(-1,1)
regrModelFatalities = linear_model.RidgeCV(alphas=np.logspace(-6, 6, 13))
regrModelFatalities.fit(X_train_poly, trainY_Fatalities)

predY_Fatalities=regrModelFatalities.predict(poly_features.fit_transform(testX_Date))
",0,No Code Smell
3460,30846479,15,"# Plot outputs

plt.scatter(trainX, trainY_Fatalities,  color='green')
plt.plot(testX_Date, predY_Fatalities, color='blue', linewidth=1)
plt.scatter(testX_Date, predY_Fatalities,  color='yellow')

plt.xticks(())
plt.yticks(())
plt.xlabel(""Date Rank"")
plt.ylabel(""Fatalities"")
plt.title('training data (DateRank)/Predicted Data(Fatalities) and regression line for ')

green_patch = mpatches.Patch(color='green', label='Train Values')
yellow_patch = mpatches.Patch(color='yellow', label='Predicted Values')

plt.legend(handles=[green_patch,yellow_patch])
plt.show()
#print(ca_test_covid.Date)",0,No Code Smell
3461,30846479,16,"ca_test_covid['ConfirmedCases']=pred_ConfirmedCase
ca_test_covid['Fatalities']=predY_Fatalities
ca_test_covid_submission=ca_test_covid[['ForecastId','ConfirmedCases','Fatalities']]
ca_test_covid_submission.to_csv('submission.csv', index=False)
",0,No Code Smell
3462,30846479,17,ca_test_covid_submission,0,No Code Smell
3463,30544538,0,"import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import integrate, optimize
from sklearn.linear_model import LinearRegression",1,Code Smell
3464,30544538,1,"ca_train = pd.read_csv('../input/covid19-local-us-ca-forecasting-week-1/ca_train.csv')
ca_test = pd.read_csv('../input/covid19-local-us-ca-forecasting-week-1/ca_test.csv')
ca_submission = pd.read_csv('../input/covid19-local-us-ca-forecasting-week-1/ca_submission.csv')

train_df = ca_train
test_df =  ca_test
submission_df =  ca_submission",1,Code Smell
3465,30544538,2,train_df.head(),0,No Code Smell
3466,30544538,3,"reported = train_df[train_df['Date']>= '2020-03-10'].reset_index()
reported['day_count'] = list(range(1,len(reported)+1))
reported.head()",0,No Code Smell
3467,30544538,4,"ydata = [i for i in reported.ConfirmedCases.values]
xdata = reported.day_count
ydata = np.array(ydata, dtype=float)
xdata = np.array(xdata, dtype=float)",0,No Code Smell
3468,30544538,5,"N = 36000000 #population of California
inf0 = ydata[0] #Infectious
sus0 = N - inf0 #Susceptible
exp0 = 0.0 #Exposed
rec0 = 0.0 #Recovered
init_state = [sus0, exp0, inf0, rec0]
#beta = 1.0 #constant.
#gamma = 1.0 / 7.0 #constant.",0,No Code Smell
3469,30544538,6,"# Define differential equation of SEIR model

'''
dS/dt = -beta * S * I / N
dE/dt = beta* S * I / N - epsilon * E
dI/dt = epsilon * E - gamma * I
dR/dt = gamma * I

[v[0], v[1], v[2], v[3]]=[S, E, I, R]

dv[0]/dt = -beta * v[0] * v[2] / N
dv[1]/dt = beta * v[0] * v[2] / N - epsilon * v[1]
dv[2]/dt = epsilon * v[1] - gamma * v[2]
dv[3]/dt = gamma * v[2]

'''

def seir_model(v, x, beta, epsilon, gamma, N):
    return [-beta * v[0] * v[2] / N ,beta * v[0] * v[2] / N - epsilon * v[1],
            epsilon * v[1] - gamma * v[2],gamma * v[2]]

def fit_odeint(x, beta, epsilon, gamma):
    return integrate.odeint(seir_model, init_state, x, args=(beta, epsilon, gamma, N))[:,2]",0,No Code Smell
3470,30544538,7,"popt, pcov = optimize.curve_fit(fit_odeint, xdata, ydata)
fitted = fit_odeint(xdata, *popt)",0,No Code Smell
3471,30544538,8,"print(""Optimal parameters: beta = "", popt[0], ""epsilon = "", popt[1], "", gamma = "", popt[2])",0,No Code Smell
3472,30544538,9,"N = 36000000 #population of California
inf0 = ydata[0] #Infectious
sus0 = N - inf0 #Susceptible
exp0 = 0.0 #Exposed
rec0 = 0.0 #Recovered
init_state = [sus0, exp0, inf0, rec0]
beta = 1.0 #constant.
#gamma = 1.0 / 7.0 #constant.",0,No Code Smell
3473,30544538,10,"# Define differential equation of SEIR model
def seir_model(v, x, beta, epsilon, gamma, N):
    return [-beta * v[0] * v[2] / N ,beta * v[0] * v[2] / N - epsilon * v[1],
            epsilon * v[1] - gamma * v[2],gamma * v[2]]

def fit_odeint(x, epsilon, gamma):
    return integrate.odeint(seir_model, init_state, x, args=(beta, epsilon, gamma, N))[:,2]",0,No Code Smell
3474,30544538,11,"popt, pcov = optimize.curve_fit(fit_odeint, xdata, ydata)
fitted = fit_odeint(xdata, *popt)",0,No Code Smell
3475,30544538,12,"print(""Optimal parameters: epsilon = "", popt[0], "", gamma = "", popt[1])",0,No Code Smell
3476,30544538,13,"N = 36000000 #population of California
inf0 = ydata[0] #Infectious
sus0 = N - inf0 #Susceptible
exp0 = 0.0 #Exposed
rec0 = 0.0 #Recovered
init_state = [sus0, exp0, inf0, rec0]
beta = 1.0 #constant.
gamma = 1.0 / 7.0 #constant.",0,No Code Smell
3477,30544538,14,"# Define differential equation of SEIR model
def seir_model(v, x, beta, epsilon, gamma, N):
    return [-beta * v[0] * v[2] / N ,beta * v[0] * v[2] / N - epsilon * v[1],
            epsilon * v[1] - gamma * v[2],gamma * v[2]]

def fit_odeint(x, epsilon):
    return integrate.odeint(seir_model, init_state, x, args=(beta, epsilon, gamma, N))[:,2]",0,No Code Smell
3478,30544538,15,"popt, pcov = optimize.curve_fit(fit_odeint, xdata, ydata)
fitted = fit_odeint(xdata, *popt)",0,No Code Smell
3479,30544538,16,"inf_period = 1.0/gamma
lat_period = 1.0/popt[0]
print(""Optimal parameters: gamma ="", gamma, "", epsilon = "", popt[0], ""\ninfectious period(day) = "", inf_period, "", latency period(day) = "", lat_period)",0,No Code Smell
3480,30544538,17,"plt.plot(xdata, ydata, 'o')
plt.plot(xdata, fitted)
plt.title(""Fit of SEIR model to global infected cases"")
plt.ylabel(""Population infected"")
plt.xlabel(""Days"")
plt.show()",0,No Code Smell
3481,30544538,18,"# parameters
t_max = 100 #days
dt = 1

N = 36000000 #population of California
inf0 = ydata[0] #Infectious
sus0 = N - inf0 #Susceptible
exp0 = 0.0 #Exposed
rec0 = 0.0 #Recovered
init_state = [sus0, exp0, inf0, rec0]
beta_const = 1.0 #Assumption: Infection rate is constant.
epsilon_const = popt[0]
gamma_const = 1.0 / 7.0 #Assumption: Recovery rate is constant.",0,No Code Smell
3482,30544538,19,"# numerical integration
times = np.arange(0, t_max, dt)
args = (beta_const, epsilon_const, gamma_const, N)

# Numerical Solution using scipy.integrate
# Solver SEIR model
result = integrate.odeint(seir_model, init_state, times, args)
# plot
plt.plot(times, result)
plt.legend(['Susceptible', 'Exposed', 'Infectious', 'Removed'])
plt.title(""SEIR model  COVID-19"")
plt.xlabel('time(days)')
plt.ylabel('population')
plt.grid()

plt.show()",0,No Code Smell
3483,30544538,20,"result_df = pd.DataFrame(data=result, columns=['Susceptible', 'Exposed', 'Infectious', 'Removed'])
result_df.shape",0,No Code Smell
3484,30544538,21,"lr = LinearRegression()
X_train = reported[['ConfirmedCases']].values
Y_train = reported[['Fatalities']].values
lr.fit(X_train, Y_train)
print('coefficient = ', lr.coef_[0], '(which means Fatality rate)')
print('intercept = ', lr.intercept_)",0,No Code Smell
3485,30544538,22,"X_pred = result_df[['Infectious']].values
Y_pred = lr.predict(X_pred)
plt.scatter(X_train, Y_train, c='blue')
plt.plot(X_pred, Y_pred, c='red')
plt.title(""Regression Line"")
plt.xlabel('ConfirmedCases')
plt.ylabel('Fatalities')
plt.grid()

plt.xlim([100,800])
plt.ylim([0,20])

plt.show()",0,No Code Smell
3486,30544538,23,"Y_pred_df = pd.DataFrame(Y_pred)
result_df['Fatalities'] = Y_pred_df
result_df.head()",0,No Code Smell
3487,30544538,24,"submission = result_df[0:len(submission_df)].reset_index()
submission_df['ConfirmedCases'] = submission['Infectious']
submission_df['Fatalities'] = submission['Fatalities']
submission_df.head()",0,No Code Smell
3488,30544538,25,"submission_df.to_csv(""submission.csv"", index=False)",0,No Code Smell
3489,30544538,26,,0,No Code Smell
3490,30636382,0,"#!/usr/bin/env python3
# -*- coding: utf-8 -*-
""""""
Created on Sat Mar 21 15:36:47 2020

@author: rahul
""""""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import datetime

df_train=pd.read_csv('../input/covid19-local-us-ca-forecasting-week-1/ca_train.csv')
df_test=pd.read_csv('../input/covid19-local-us-ca-forecasting-week-1/ca_test.csv')
df_sub=pd.read_csv('../input/covid19-local-us-ca-forecasting-week-1/ca_submission.csv')

""""""
df_train=pd.read_csv('ca_train.csv')
df_test=pd.read_csv('ca_test.csv')
df_sub=pd.read_csv('ca_submission.csv')
""""""

reported=df_train[df_train['Date']>='2020-03-10'].reset_index()
reported['day_count']=list(range(1, len(reported)+1))

X_train=reported.iloc[:, 9:10].values
y_train=reported.iloc[:, 7:8].values

X_test=list(range(1, 53))

from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
poly_reg=PolynomialFeatures(degree=2)
X_poly=poly_reg.fit_transform(X_train)
lin_reg=LinearRegression()
lin_reg.fit(X_poly, y_train)

X_grid = np.arange(min(X_train), max(X_train), 0.1)
X_grid = X_grid.reshape((len(X_grid), 1))

plt.scatter(X_train, y_train, color = 'red')
plt.scatter(X_train, y_train, color='red')
plt.plot(X_grid, lin_reg.predict(poly_reg.fit_transform(X_grid)), color='blue')
plt.title('Covid 19')
plt.xlabel('Date')
plt.ylabel('Confirmed Cases-Training Dataset')
plt.show()

y_train_pred=lin_reg.predict(poly_reg.fit_transform(X_train))

X_test=np.arange(1, 53, 1)
X_test = X_test.reshape((len(X_test), 1))

y_pred=lin_reg.predict(poly_reg.fit_transform(X_test))

plt.plot(X_test, y_pred, color='blue')
plt.title('Covid 19')
plt.xlabel('Date')
plt.ylabel('Confirmed Cases-Test Dataset')
plt.show()

regressor=LinearRegression()
X_train_2=reported[['ConfirmedCases']].values
y_train_2=reported[['Fatalities']].values
regressor.fit(X_train_2, y_train_2)

X_test_2=y_pred
y_pred_2 = regressor.predict(X_test_2)

plt.scatter(X_train_2, y_train_2, c='blue')
plt.plot(X_test_2, y_pred_2, c='red')
plt.title(""Regression Line"")
plt.xlabel('ConfirmedCases')
plt.ylabel('Fatalities')
plt.grid()
plt.xlim([100,800])
plt.ylim([0,20])
plt.show()

result=pd.DataFrame(y_pred)
result[1]=y_pred_2
submission=result[0:len(df_sub)].reset_index()
df_sub['ConfirmedCases'] = submission[0]
df_sub['Fatalities'] = submission[1]

df_sub.to_csv(""submission.csv"", index=False)",0,No Code Smell
3491,30636382,1,,0,No Code Smell
3492,30487343,0,"#Libraried
import pandas as pd
pd.set_option('display.max_columns', None)
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings(""ignore"")

import datetime
from time import time
from scipy import stats

from sklearn.model_selection import GroupKFold
import lightgbm as lgb
import xgboost as xgb
from catboost import CatBoostRegressor, CatBoostClassifier
import plotly.express as px
import plotly.graph_objects as go
import plotly.figure_factory as ff
from plotly.subplots import make_subplots
import os
import glob

import numpy as np
from scipy.integrate import odeint",0,No Code Smell
3493,30487343,1,"ca_train = pd.read_csv('/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_train.csv')
ca_test = pd.read_csv('/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_test.csv')
ca_submission = pd.read_csv('/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_submission.csv')

train_df = ca_train
test_df =  ca_test
submission_df =  ca_submission",1,Code Smell
3494,30487343,2,train_df.head(),0,No Code Smell
3495,30487343,3,"x_1 = train_df['Date']
y_1 = train_df['ConfirmedCases']
y_2 = train_df['Fatalities']

fig = make_subplots(rows=1, cols=1)

fig.add_trace(
    go.Scatter(x=x_1, mode='lines+markers', y=y_1, marker=dict(color=""mediumaquamarine""), showlegend=False,
               name=""Original signal""),
    row=1, col=1
)

fig.add_trace(
    go.Scatter(x=x_1, mode='lines+markers', y=y_2, marker=dict(color=""darkgreen""), showlegend=False,
               name=""Original signal""),
    row=1, col=1
)

fig.update_layout(height=400, width=800, title_text=""ConfirmedCases (pale) vs. Fatalities (dark) "")
fig.show()",0,No Code Smell
3496,30487343,4,"x_1 = train_df['Date']
y_1 = train_df['Lat']
y_2 = train_df['Long']

fig = make_subplots(rows=1, cols=1)

fig.add_trace(
    go.Scatter(x=x_1, mode='lines+markers', y=y_1, marker=dict(color=""mediumaquamarine""), showlegend=False,
               name=""Original signal""),
    row=1, col=1
)

fig.add_trace(
    go.Scatter(x=x_1, mode='lines+markers', y=y_2, marker=dict(color=""darkgreen""), showlegend=False,
               name=""Original signal""),
    row=1, col=1
)

fig.update_layout(height=400, width=800, title_text=""Lat (pale) vs. Long (dark) "")
fig.show()",0,No Code Smell
3497,30487343,5,"#define seir

def SEIR_EQ(v, t, beta, epsilon, gamma, N ):
    return [-beta * v[0] * v[2] / N ,beta * v[0] * v[2] / N - epsilon * v[1],
            epsilon * v[1] - gamma * v[2],gamma * v[2]]",0,No Code Smell
3498,30487343,6,"# parameters
t_max = 100 #days
dt = 1

# initial_state
S_0 = 36000000 #population of California
E_0 = 0  #number of Exposed on Mar 10: it can't be 0 actually
I_0 = 144 #number of Infectious on Mar 10
R_0 = 0
N_pop = S_0 + E_0 + I_0 + R_0
ini_state = [S_0, E_0, I_0, R_0]  # [S[0],E,[0], I[0], R[0]]


#infection rate
beta_const = 1 #infection rate

#infection rate　after expose
latency_period = 2 #days
epsilon_const = 1/latency_period


infectious_period = 7.4 #days
gamma_const = 1/infectious_period

#case fatality rate
death_rate = 0.01",0,No Code Smell
3499,30487343,7,"# numerical integration
times = np.arange(0, t_max, dt)
args = (beta_const, epsilon_const, gamma_const, N_pop)

# Numerical Solution using scipy.integrate
# Solver SEIR model
result = odeint(SEIR_EQ, ini_state, times, args)
# plot
plt.plot(times, result)
plt.legend(['Susceptible', 'Exposed', 'Infectious', 'Removed'])
plt.title(""SEIR model  COVID-19"")
plt.xlabel('time(days)')
plt.ylabel('population')
plt.grid()

plt.show()",0,No Code Smell
3500,30487343,8,"predicted = pd.DataFrame(result)
predicted.columns = ['Susceptible', 'Exposed', 'Infectious', 'Removed']
predicted['death'] = predicted['Infectious']*death_rate",0,No Code Smell
3501,30487343,9,"reported = train_df[train_df['Date']>= '2020-03-10'].reset_index()
reported.head()",0,No Code Smell
3502,30487343,10,"tmp_predicted = predicted[0:len(reported)]

reported_rate = reported['ConfirmedCases']/tmp_predicted['Infectious']
reported_rate_c = np.average(a= reported_rate, weights=reported['ConfirmedCases'])
reported_rate_c",0,No Code Smell
3503,30487343,11,"tmp_predicted = predicted[0:len(reported)]

reported_rate = reported['Fatalities']/tmp_predicted['death']
reported_rate_d = np.average(a= reported_rate, weights=reported['Fatalities'])
reported_rate_d",0,No Code Smell
3504,30487343,12,"predicted_s = predicted[0:len(submission_df)]
submission_df['ConfirmedCases'] = predicted_s['Infectious']*reported_rate_c
submission_df['Fatalities'] = predicted_s['death']*reported_rate_d
submission_df.head()",0,No Code Smell
3505,30487343,13,"x_1 = test_df['Date']
y_1 = submission_df['ConfirmedCases']
y_2 = submission_df['Fatalities']

fig = make_subplots(rows=1, cols=1)

fig.add_trace(
    go.Scatter(x=x_1, mode='lines+markers', y=y_1, marker=dict(color=""mediumaquamarine""), showlegend=False,
               name=""Original signal""),
    row=1, col=1
)

fig.add_trace(
    go.Scatter(x=x_1, mode='lines+markers', y=y_2, marker=dict(color=""darkgreen""), showlegend=False,
               name=""Original signal""),
    row=1, col=1
)

fig.update_layout(height=400, width=800, title_text=""Predicted ConfirmedCases (pale) vs. Fatalities (dark) "")
fig.show()",0,No Code Smell
3506,30487343,14,"submission_df.to_csv(""submission.csv"", index=False)",0,No Code Smell
3507,30834836,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the ""../input/"" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# Any results you write to the current directory are saved as output.",1,Code Smell
3508,30834836,1,"from scipy.integrate import odeint
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
from matplotlib.dates import DateFormatter, DayLocator, WeekdayLocator
import datetime as dt
from matplotlib.dates import MO, TU, WE, TH, FR, SA, SU
from matplotlib import gridspec
from matplotlib import dates
from matplotlib.ticker import (MultipleLocator, FormatStrFormatter,
                               AutoMinorLocator)

train_data = pd.read_csv(""/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_train.csv"")
test_data = pd.read_csv(""/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_test.csv"")",1,Code Smell
3509,30834836,2,train_data.head(),0,No Code Smell
3510,30834836,3,test_data.head(),0,No Code Smell
3511,30834836,4,"train_data.shape, test_data.shape",0,No Code Smell
3512,30834836,5,"fig, (ax1, ax2) = plt.subplots(1,2, figsize = (18,8))
fig.suptitle('Number of Confirmed Cases and Fatalities in CA')
plt .xticks(np.arange(0, 60,  step = 10)) 
#Left plot
ax1.plot(train_data['Date'], train_data['ConfirmedCases'], color = 'purple', marker = 'o',linewidth = 1)
ax1.set(xlabel = 'Date',
        ylabel = 'Number of ConfirmedCases in CA')
ax1.set_xticks(np.arange(0, 60,  step = 12))
ax1.grid()
#Right plot

ax2.plot(train_data['Date'], train_data['Fatalities'], color = 'orange', marker = 'o', linewidth = 1)
ax2.set(xlabel = 'Date',
        ylabel = 'Number of Fatalities in CA')
ax2.set_xticks(np.arange(0, 60,  step = 12))
ax2.grid()

plt.rcParams[""font.family""] = ""Times New Roman""
plt.rcParams[""font.size""] = ""12""

plt.show()",0,No Code Smell
3513,30834836,6,"train_initial = train_data[48:]
train_initial = train_initial.reset_index()
y1_train= train_initial['ConfirmedCases']
y2_train = train_initial['Fatalities']",0,No Code Smell
3514,30834836,7,"#SIR Model 
def SIR_DEQ(y, time, beta, k, N):
    DS = -beta * y[0] * y[1]/N
    DI = (beta * y[0] * y[1] - k * y[1])/N
    DR = k * y[1]/N
    return [DS, DI, DR]

#initial conditions for training data
N = 39560000  #Population of California 
I0 = 144
S0 = N  # initial population of susceptible individual
R0 = 2 # initial number of fatalities 
init_state = [S0, I0, R0]

# Parameters
t0 = 0 
tmax = 15
dt = 1
# Rate of infection
beta = 0.2
# Rate of recovery
k = 1/10

time = np.arange(t0, tmax, dt)
args = (beta, k, N)

solution = odeint(SIR_DEQ, init_state, time, args)

plt.plot(time, solution[:, 1], 'g', marker = 'x', label  = 'Infected SIR model')
plt.plot(time, y1_train, 'r', marker = 'o', label  = 'Infected Data')
plt.legend(['Infected SIR Model', 'Infected Input Data'])",0,No Code Smell
3515,30834836,8,"#initial conditions for ouput
N = 39560000  #Population of California 
I0 = 221
S0 = N  # initial population of susceptible individual
R0 = 4 # initial number of fatalities 
init_state_out = [S0, I0, R0]

time = np.arange(t0, 43, dt)
args = (beta, k, N)
solution_out = odeint(SIR_DEQ, init_state_out, time, args)
",0,No Code Smell
3516,30834836,9,"submission_file = pd.read_csv(""/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_submission.csv"")
",1,Code Smell
3517,30834836,10,"submission_file['ConfirmedCases']= solution_out[:,1]
submission_file['Fatalities'] = solution_out[:,2]",0,No Code Smell
3518,30834836,11,"submission_file.to_csv(""submission.csv"", index=False)",0,No Code Smell
3519,30835008,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the ""../input/"" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# Any results you write to the current directory are saved as output.",1,Code Smell
3520,30835008,1,"%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
sns.set()

import warnings
warnings.filterwarnings('ignore')",0,No Code Smell
3521,30835008,2,"ca_train = pd.read_csv(""/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_train.csv"",parse_dates=[5],index_col=0)
ca_test = pd.read_csv(""/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_test.csv"",parse_dates=[5],index_col=0)
ca_submission = pd.read_csv(""/kaggle/input/covid19-local-us-ca-forecasting-week-1/ca_test.csv"",parse_dates=[5])",1,Code Smell
3522,30835008,3,"ca_train.info()
#ca_test.info()",1,Code Smell
3523,30835008,4,"#any null values?
ca_train.isnull().sum()",0,No Code Smell
3524,30835008,5,"#remove unnecessary columns
ca_train.nunique()",0,No Code Smell
3525,30835008,6,"for i in range(ca_train.shape[1]):
    if (ca_train.iloc[:,i].nunique() == 1):
        print(ca_train.columns[i],'\t',ca_train.iloc[:,i].value_counts())    ",0,No Code Smell
3526,30835008,7,"#necessary data
ca_filtered_train = ca_train.copy()
ca_filtered_train.tail(10)",0,No Code Smell
3527,30835008,8,"ca_filtered_train['weekday'] = ca_filtered_train.Date.dt.day_name()
ca_filtered_train['Total_affected_people'] = ca_filtered_train['ConfirmedCases']+ca_filtered_train['Fatalities']
ca_filtered_train.tail()",0,No Code Smell
3528,30835008,9,"print('Total CA population is 39.56M as of year 2018')
print('confirmed cases population percentage',round((sum(ca_train['ConfirmedCases'])/39.56e6)*100,2),'%')
print('fatalities population percentage',round((sum(ca_train['Fatalities'])/39.56e6)*100,4),'%')",0,No Code Smell
3529,30835008,10,"affected_people = pd.DataFrame(ca_filtered_train[47:].groupby('ConfirmedCases')['Fatalities'].sum())
affected_people['Cumulative_deaths_percentage'] = round(affected_people['Fatalities']/sum(affected_people['Fatalities'])*100,2)
sns.scatterplot(x=affected_people.index,y=affected_people.Cumulative_deaths_percentage)",0,No Code Smell
3530,30835008,11,"cats = [ 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
ca_filtered_train.groupby('weekday')['ConfirmedCases','Fatalities'].sum().reindex(cats).plot(kind='bar')
plt.title('daywise analysis')",0,No Code Smell
3531,30835008,12,"plt.figure(figsize=(20,12))
ax = sns.lineplot(x='Date',y='ConfirmedCases',data=ca_filtered_train[40:],label='Confirmed_cases')
ax = sns.lineplot(x='Date',y='Fatalities',data=ca_filtered_train[40:],label='Deaths')
ax = sns.lineplot(x='Date',y='Total_affected_people',data=ca_filtered_train[47:],label='Total_affected_people')
plt.xticks(ca_filtered_train['Date'][40:],rotation='vertical')
ax.annotate('Lockdown', xy=('2020-03-20',1177), xytext=('2020-03-17', 1300),arrowprops=dict(facecolor='black',shrink=0.05),fontsize=15)
ax.annotate('Declared State of Emergency', xy=('2020-03-04',0), xytext=('2020-03-04', 500),arrowprops=dict(facecolor='black',shrink=0.05),fontsize=15)
ax.annotate('Increased health care capacity', xy=('2020-03-21',1364), xytext=('2020-03-14', 1600),arrowprops=dict(facecolor='black',shrink=0.05),fontsize=15)
ax.legend(loc='upper left',fontsize='x-large',fancybox=True,shadow=True,borderpad=1)
plt.ylabel('Confirmed_cases and Fatalities')
plt.xticks(rotation='vertical')
plt.title('Trend over a MARCH month',fontsize=20)",0,No Code Smell
3532,30835008,13,"ca_train.index = ca_train['Date']
ca_train.drop('Date',axis=1,inplace=True)",0,No Code Smell
3533,30835008,14,"ca_train = ca_train[['ConfirmedCases','Fatalities']]
train_confirmedcases = ca_train[['ConfirmedCases']]
train_confirmedcases= train_confirmedcases.iloc[47:]",0,No Code Smell
3534,30835008,15,"train_fatalities = ca_train[['Fatalities']]
train_fatalities= train_fatalities.iloc[47:]",0,No Code Smell
3535,30835008,16,"#ts_diff_1 = train_confirmedcases - train_confirmedcases.shift()
#ts_diff_1.dropna(inplace=True)",0,No Code Smell
3536,30835008,17,"from statsmodels.tsa.arima_model import ARIMA
from pandas import datetime

#fit model on confirmedcases
model = ARIMA(train_confirmedcases, order=(1,1,1)) # (ARIMA) = (1,1,0)
model_fit = model.fit(disp=0)",0,No Code Smell
3537,30835008,18,"# predict
start_index = datetime(2020, 3, 12)
end_index = datetime(2020, 4, 23)
forecast_confirmedcases = model_fit.predict(start=start_index, end=end_index)",0,No Code Smell
3538,30835008,19,"#fit model on fatalities
model_F = ARIMA(train_fatalities, order=(1,1,0)) # (ARIMA) = (1,1,0)
model_fit_F = model_F.fit(disp=0)",0,No Code Smell
3539,30835008,20,"# predict
start_index = datetime(2020, 3, 12)
end_index = datetime(2020, 4, 23)
forecast_fatalities = model_fit_F.predict(start=start_index, end=end_index)",0,No Code Smell
3540,30835008,21,"df=pd.concat([forecast_confirmedcases.astype(int),forecast_fatalities.astype(int)],axis=1)",0,No Code Smell
3541,30835008,22,ca_submission.head(),0,No Code Smell
3542,30835008,23,"ca_submission['ConfirmedCases'] = list(df[0])
ca_submission['Fatalities'] = list(df[1])
ca_submission.head()",0,No Code Smell
3543,30835008,24,"ca_submission = ca_submission[['ForecastId','ConfirmedCases','Fatalities']]
ca_submission.head()",0,No Code Smell
3544,30835008,25,"# visualization
plt.figure(figsize=(22,10))
plt.plot(train_confirmedcases.index,train_confirmedcases.ConfirmedCases,label = ""original"")
plt.plot(forecast_confirmedcases,label = ""predicted"")
plt.legend(loc='upper left',fontsize='x-large',fancybox=True,shadow=True,borderpad=1)
plt.title('For ConfirmedCases')
plt.show()",0,No Code Smell
3545,30835008,26,"# visualization
plt.figure(figsize=(22,10))
plt.plot(train_fatalities.index,train_fatalities.Fatalities,label = ""original"")
plt.plot(forecast_fatalities,label = ""predicted"")
plt.legend(loc='upper left',fontsize='x-large',fancybox=True,shadow=True,borderpad=1)
plt.title('For Fatalities')
plt.show()",0,No Code Smell
3546,30835008,27,"ca_submission.to_csv('submission.csv',index=False)
ca_submission.head()",0,No Code Smell
3547,30700486,0,"import pandas as pd
import numpy as np",1,Code Smell
3548,30700486,1,"df_train = pd.read_csv(""../input/covid19-local-us-ca-forecasting-week-1/ca_train.csv"")
df_train",1,Code Smell
3549,30700486,2,"diff_cases = [df_train['ConfirmedCases'][i+1] - df_train['ConfirmedCases'][i] for i in range(len(df_train)-1)]
diff_fatalities = [df_train['Fatalities'][i+1] - df_train['Fatalities'][i] for i in range(len(df_train)-1)]",0,No Code Smell
3550,30700486,3,diff_cases,0,No Code Smell
3551,30700486,4,diff_fatalities,0,No Code Smell
3552,30700486,5,"diff_fatal_case = [f/c if c != 0 else 0 for f, c in zip(diff_fatalities, diff_cases)]",0,No Code Smell
3553,30700486,6,diff_fatal_case,0,No Code Smell
3554,30700486,7,np.mean([x for x in diff_fatal_case if x != 0]),0,No Code Smell
3555,30700486,8,increase = [df_train['ConfirmedCases'][i+1]/diff_cases[i] if diff_cases[i] != 0 else 0 for i in range(len(diff_cases))],0,No Code Smell
3556,30700486,9,increase,0,No Code Smell
3557,30700486,10,"increase_rate = np.mean([x for x in increase if x != 0])
increase_rate",0,No Code Smell
3558,30700486,11,"fatality_rate = np.mean([x for x in diff_fatal_case if x != 0])
fatality_rate",0,No Code Smell
3559,30700486,12,"def n_step_pred(cases, n):
    new_cases = []
    new_fatalities = []
    for i in range(n):
        if i == 0:
            new_cases.append(cases + int(cases/increase_rate))
        else:
            new_cases.append(new_cases[i-1] + int(new_cases[i-1]/increase_rate))
        new_fatalities.append(int(new_cases[i] * fatality_rate))
    
    pred_df = pd.DataFrame(list(zip(forecast_id ,new_cases, new_fatalities)), columns=[""ForecastId"", ""ConfirmedCases"", ""Fatalities""])
    pred_df = pred_df.set_index(""ForecastId"")
    return pred_df",0,No Code Smell
3560,30700486,13,"test_df = pd.read_csv(""../input/covid19-local-us-ca-forecasting-week-1/ca_test.csv"")
forecast_id = list(test_df[""ForecastId""])
test_df.head()",1,Code Smell
3561,30700486,14,"predicted = n_step_pred(df_train['ConfirmedCases'].iloc[-1], len(test_df))",0,No Code Smell
3562,30700486,15,"predicted.to_csv(""submission.csv"")",0,No Code Smell
3563,30700486,16,,0,No Code Smell
3564,31316040,0,"import numpy as np
import pandas as pd
import os

from PIL import Image

import matplotlib.pyplot as plt
%matplotlib inline

import torch
import torchvision
import torchvision.transforms as transforms

import torch.nn as nn
import torch.nn.functional as F

import torch.optim as optim

# for creating validation set
from sklearn.model_selection import train_test_split

# for evaluating the model
from sklearn.metrics import accuracy_score
from tqdm import tqdm

# PyTorch libraries and modules
import torch
from torch.autograd import Variable
from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout
from torch.optim import Adam, SGD",0,No Code Smell
3565,31316040,1,!pip install -U -q kaggle --force,0,No Code Smell
3566,31316040,2,"from google.colab import files
f=files.upload()",0,No Code Smell
3567,31316040,3,!mkdir -p ~/.kaggle,0,No Code Smell
3568,31316040,4,!cp kaggle.json ~/.kaggle/,0,No Code Smell
3569,31316040,5,!chmod 600 /root/.kaggle/kaggle.json,1,Code Smell
3570,31316040,6,!kaggle competitions download -c nnfl-cnn-lab2,0,No Code Smell
3571,31316040,7,"%%bash
cd /content
unzip nnfl-cnn-lab2.zip",0,No Code Smell
3572,31316040,8,"import numpy as np
import pandas as pd 
from keras.preprocessing.image import ImageDataGenerator, load_img
from keras.utils import to_categorical
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import random
import os
print(os.listdir(""../content""))
",1,Code Smell
3573,31316040,9,"FAST_RUN = False
IMAGE_WIDTH=150
IMAGE_HEIGHT=150
IMAGE_SIZE=(IMAGE_WIDTH, IMAGE_HEIGHT)
IMAGE_CHANNELS=3",0,No Code Smell
3574,31316040,10,cd upload,0,No Code Smell
3575,31316040,11,ls,1,Code Smell
3576,31316040,12,"df = pd.read_csv(""train_set.csv"") ",1,Code Smell
3577,31316040,13,df.head(),0,No Code Smell
3578,31316040,14,df.tail(),0,No Code Smell
3579,31316040,15,df['label'].value_counts().plot.bar(),0,No Code Smell
3580,31316040,16,df['label']=df['label'].astype(str),0,No Code Smell
3581,31316040,17,"transform = transforms.Compose([transforms.Resize(150),
                                transforms.ToTensor()                               
                                ])

class TrainingDataset(torch.utils.data.Dataset):

    def __init__(self, csv_file, root_dir, transform=transform):
        """"""
        Args:
            csv_file(string): path to csv file
            root_dir(string): directory with all train images
        """"""
        self.name_frame = pd.read_csv(csv_file, usecols=range(1))
        self.label_frame = pd.read_csv(csv_file, usecols=range(1,2))
        self.root_dir = root_dir
        self.transform = transform

    def __len__(self):
        return len(self.name_frame)

    def __getitem__(self, idx):
        img_name = os.path.join(self.root_dir, self.name_frame.iloc[idx, 0])
        image = Image.open(img_name)
        image = self.transform(image)
        labels = self.label_frame.iloc[idx, 0]
        #sample = {'image': image, 'labels': labels}

        return image, labels

TrainSet = TrainingDataset(csv_file = './train_set.csv', root_dir = './train_images/train_images')

TrainLoader = torch.utils.data.DataLoader(TrainSet,batch_size=1, shuffle=True, num_workers=2)",1,Code Smell
3582,31316040,18,"def imshow(img):
    npimg = img.numpy()
    plt.imshow(np.transpose(npimg, (1, 2, 0))) # transform to size x size x #channels
    plt.show()

# get some random training images
dataiter = iter(TrainLoader)
image, label = dataiter.next()

print(image.shape)

# show images
imshow(torchvision.utils.make_grid(image))
# print labels
print(label.item())",0,No Code Smell
3583,31316040,19,"import pandas as pd
import numpy as np 
import matplotlib.pyplot as plt
import os
from urllib.request import urlopen,urlretrieve
from PIL import Image
from tqdm import tqdm_notebook
%matplotlib inline
from sklearn.utils import shuffle
import cv2


from tensorflow.keras.models import load_model
from sklearn.datasets import load_files   
from keras.utils import np_utils
from glob import glob
from tensorflow.keras import applications
from tensorflow.keras.preprocessing.image import ImageDataGenerator 
from tensorflow.keras import optimizers
from tensorflow.keras.models import Sequential,Model,load_model
from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D,GlobalMaxPooling2D
from tensorflow.keras.callbacks import TensorBoard,ReduceLROnPlateau,ModelCheckpoint

img_height,img_width = 150,150
num_classes = 6
base_model = applications.resnet_v2.ResNet50V2(weights= None, include_top=False, input_shape= (img_height,img_width,3))

x = base_model.output
x = GlobalMaxPooling2D()(x)

#x = Dense(32, activation='relu')(x)
#x = Dropout(0.7)(x)

predictions = Dense(num_classes, activation= 'softmax')(x)
model = Model(inputs = base_model.input, outputs = predictions)
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

model.summary()",0,No Code Smell
3584,31316040,20,"from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau",1,Code Smell
3585,31316040,21,earlystop = EarlyStopping(patience=10),0,No Code Smell
3586,31316040,22,"learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', 
                                            patience=2, 
                                            verbose=1, 
                                            factor=0.5, 
                                            min_lr=0.00001)",0,No Code Smell
3587,31316040,23,"from tensorflow.keras.callbacks import ModelCheckpoint

callbacks = [earlystop, learning_rate_reduction,ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]",0,No Code Smell
3588,31316040,24,df['label'].head(),0,No Code Smell
3589,31316040,25,"train_df, validate_df = train_test_split(df, test_size=0.20, random_state=42)
train_df = train_df.reset_index(drop=True)
validate_df = validate_df.reset_index(drop=True)",0,No Code Smell
3590,31316040,26,train_df['label'].value_counts().plot.bar(),0,No Code Smell
3591,31316040,27,validate_df['label'].value_counts().plot.bar(),0,No Code Smell
3592,31316040,28,"total_train = train_df.shape[0]
total_validate = validate_df.shape[0]
batch_size=15",0,No Code Smell
3593,31316040,29,"print(total_train)
print(total_validate)",0,No Code Smell
3594,31316040,30,"train_datagen = ImageDataGenerator(
    rotation_range=-10,
    rescale=1./255,
    shear_range=0.1,
    zoom_range=0.1,
    horizontal_flip=True,
    width_shift_range=0.1,
    height_shift_range=0.1
)

train_generator = train_datagen.flow_from_dataframe(
    train_df, 
    ""./train_images/train_images"", 
    x_col='image_name',
    y_col='label',
    target_size=IMAGE_SIZE,
    class_mode='categorical',
    batch_size=batch_size,
)",0,No Code Smell
3595,31316040,31,train_df['label']=train_df['label'].astype(str),0,No Code Smell
3596,31316040,32,validate_df['label']=validate_df['label'].astype(str),0,No Code Smell
3597,31316040,33,"validation_datagen = ImageDataGenerator(rescale=1./255)
validation_generator = validation_datagen.flow_from_dataframe(
    validate_df, 
    ""./train_images/train_images"", 
    x_col='image_name',
    y_col='label',
    target_size=IMAGE_SIZE,
    class_mode='categorical',
    batch_size=batch_size
)",0,No Code Smell
3598,31316040,34,train_df['label']=train_df['label'].astype(str),0,No Code Smell
3599,31316040,35,"example_df = train_df.sample(n=1).reset_index(drop=True)
example_generator = train_datagen.flow_from_dataframe(
    example_df, 
    ""./train_images/train_images"", 
    x_col='image_name',
    y_col='label',
    target_size=IMAGE_SIZE,
    class_mode='categorical'
)",0,No Code Smell
3600,31316040,36,"plt.figure(figsize=(12, 12))
for i in range(0, 15):
    plt.subplot(5, 3, i+1)
    for X_batch, Y_batch in example_generator:
        image = X_batch[0]
        plt.imshow(image)
        break
plt.tight_layout()
plt.show()",0,No Code Smell
3601,31316040,37,"epochs=3 if FAST_RUN else 40
history = model.fit(
    train_generator, 
    epochs=epochs,
    validation_data=validation_generator,
    validation_steps=total_validate//batch_size,
    steps_per_epoch=total_train//batch_size,
    callbacks=callbacks
)",0,No Code Smell
3602,31316040,38,"model.save_weights(""model.h5"")",1,Code Smell
3603,31316040,39,"fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12))
ax1.plot(history.history['loss'], color='b', label=""Training loss"")
ax1.plot(history.history['val_loss'], color='r', label=""validation loss"")
ax1.set_xticks(np.arange(1, epochs, 1))
ax1.set_yticks(np.arange(0, 1, 0.1))

ax2.plot(history.history['accuracy'], color='b', label=""Training accuracy"")
ax2.plot(history.history['val_accuracy'], color='r',label=""Validation accuracy"")
ax2.set_xticks(np.arange(1, epochs, 1))

legend = plt.legend(loc='best', shadow=True)
plt.tight_layout()
plt.show()",0,No Code Smell
3604,31316040,40,"test_filenames = os.listdir(""./test_images/test_images"")
test_df = pd.DataFrame({
    'filename': test_filenames
})
nb_samples = test_df.shape[0]",0,No Code Smell
3605,31316040,41,"test_gen = ImageDataGenerator(rescale=1./255)
test_generator = test_gen.flow_from_dataframe(
    test_df, 
    ""./test_images/test_images"", 
    x_col='filename',
    y_col=None,
    class_mode=None,
    target_size=IMAGE_SIZE,
    batch_size=batch_size,
    shuffle=False
)",0,No Code Smell
3606,31316040,42,"predict = model.predict_generator(test_generator, steps=np.ceil(nb_samples/batch_size))",0,No Code Smell
3607,31316040,43,"test_df['category'] = np.argmax(predict, axis=-1)",0,No Code Smell
3608,31316040,44,"label_map = dict((v,k) for k,v in train_generator.class_indices.items())
test_df['category'] = test_df['category'].replace(label_map)",0,No Code Smell
3609,31316040,45,test_df['category'].value_counts().plot.bar(),0,No Code Smell
3610,31316040,46,"sample_test = test_df.head(18)
sample_test.head()
plt.figure(figsize=(12, 24))
for index, row in sample_test.iterrows():
    filename = row['filename']
    category = row['category']
    img = load_img(""./test_images/test_images/""+filename, target_size=IMAGE_SIZE)
    plt.subplot(6, 3, index+1)
    plt.imshow(img)
    plt.xlabel(filename + '(' + ""{}"".format(category) + ')' )
plt.tight_layout()
plt.show()",0,No Code Smell
3611,31316040,47,"submission_df = test_df.copy()
submission_df['image_name'] = submission_df['filename']
submission_df['label'] = submission_df['category']
submission_df.drop(['filename', 'category'], axis=1, inplace=True)
submission_df.to_csv('submission.csv', index=False)",0,No Code Smell
3612,31316040,48,"from google.colab import files
files.download(""submission.csv"")",0,No Code Smell
3613,31316040,49,"files.download(""model.h5"")",0,No Code Smell
3614,31316040,50,"from google.colab import drive
drive.mount('/content/gdrive')",1,Code Smell
3615,31316040,51,"mv best_model.h5 /content/gdrive/'My Drive'
",1,Code Smell
3616,31316040,52,mv model.h5 /content/gdrive/'My Drive',1,Code Smell
3617,31316040,53,,0,No Code Smell
4615,32592428,0,"import os
import numpy as np 
import pandas as pd 
import json",1,Code Smell
4616,32592428,1,os.listdir('../input/imet-2020-fgvc7'),1,Code Smell
4617,32592428,2,"submission = pd.read_csv('../input/imet-2020-fgvc7/sample_submission.csv')
submission.head()",1,Code Smell
4618,32592428,3,"# ====================================================
# Library
# ====================================================

import sys

import gc
import os
import random
import time
from contextlib import contextmanager
from pathlib import Path
from collections import defaultdict, Counter

import cv2
from PIL import Image
import numpy as np
import pandas as pd
import scipy as sp

import sklearn.metrics
from sklearn.metrics import accuracy_score
from sklearn.model_selection import StratifiedKFold

from functools import partial
from tqdm import tqdm

import torch
import torch.nn as nn
from torch.optim import Adam, SGD
from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau
from torch.utils.data import DataLoader, Dataset
import torchvision.models as models

from albumentations import Compose, Normalize, Resize, RandomResizedCrop, RandomCrop, HorizontalFlip
from albumentations.pytorch import ToTensorV2

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device",1,Code Smell
4619,32592428,4,"# ====================================================
# Utils
# ====================================================

@contextmanager
def timer(name):
    t0 = time.time()
    LOGGER.info(f'[{name}] start')
    yield
    LOGGER.info(f'[{name}] done in {time.time() - t0:.0f} s.')

    
def init_logger(log_file='train.log'):
    from logging import getLogger, DEBUG, FileHandler,  Formatter,  StreamHandler
    
    log_format = '%(asctime)s %(levelname)s %(message)s'
    
    stream_handler = StreamHandler()
    stream_handler.setLevel(DEBUG)
    stream_handler.setFormatter(Formatter(log_format))
    
    file_handler = FileHandler(log_file)
    file_handler.setFormatter(Formatter(log_format))
    
    logger = getLogger('Herbarium')
    logger.setLevel(DEBUG)
    logger.addHandler(stream_handler)
    logger.addHandler(file_handler)
    
    return logger

LOG_FILE = 'train.log'
LOGGER = init_logger(LOG_FILE)


def seed_torch(seed=777):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True

SEED = 777
seed_torch(SEED)",0,No Code Smell
4620,32592428,5,"N_CLASSES = 3474


class TrainDataset(Dataset):
    def __init__(self, df, labels, transform=None):
        self.df = df
        self.labels = labels
        self.transform = transform
        
    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        file_name = self.df['id'].values[idx]
        file_path = f'../input/imet-2020-fgvc7/train/{file_name}.png'
        image = cv2.imread(file_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        if self.transform:
            augmented = self.transform(image=image)
            image = augmented['image']
            
        label = self.labels.values[idx]
        target = torch.zeros(N_CLASSES)
        for cls in label.split():
            target[int(cls)] = 1
        
        return image, target
    

class TestDataset(Dataset):
    def __init__(self, df, transform=None):
        self.df = df
        self.transform = transform
        
    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        file_name = self.df['id'].values[idx]
        file_path = f'../input/imet-2020-fgvc7/test/{file_name}.png'
        image = cv2.imread(file_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        if self.transform:
            augmented = self.transform(image=image)
            image = augmented['image']
        
        return image",0,No Code Smell
4621,32592428,6,"HEIGHT = 128
WIDTH = 128


def get_transforms(*, data):
    
    assert data in ('train', 'valid')
    
    if data == 'train':
        return Compose([
            #Resize(HEIGHT, WIDTH),
            RandomResizedCrop(HEIGHT, WIDTH),
            Normalize(
                mean=[0.485, 0.456, 0.406],
                std=[0.229, 0.224, 0.225],
            ),
            ToTensorV2(),
        ])
    
    elif data == 'valid':
        return Compose([
            #Resize(HEIGHT, WIDTH),
            RandomCrop(256, 256),
            HorizontalFlip(p=0.5),
            Normalize(
                mean=[0.485, 0.456, 0.406],
                std=[0.229, 0.224, 0.225],
            ),
            ToTensorV2(),
            
        ])",0,No Code Smell
4622,32592428,7,"batch_size = 128

test_dataset = TestDataset(submission, transform=get_transforms(data='valid'))
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)",0,No Code Smell
4623,32592428,8,"from functools import partial

import torch
from torch import nn
from torch.nn import functional as F
import torchvision.models as M

class AvgPool(nn.Module):
    def forward(self, x):
        return F.avg_pool2d(x, x.shape[2:])

def create_net(net_cls, pretrained: bool):
    if True and pretrained:
        net = net_cls()
        model_name = net_cls.__name__
        weights_path = f'../input/{model_name}/{model_name}.pth'
        net.load_state_dict(torch.load(weights_path))
    else:
        net = net_cls(pretrained=pretrained)
    return net


class ResNet(nn.Module):
    def __init__(self, num_classes,
                 pretrained=False, net_cls=M.resnet50, dropout=False):
        super().__init__()
        self.net = create_net(net_cls, pretrained=pretrained)
        self.net.avgpool = AvgPool()
        if dropout:
            self.net.fc = nn.Sequential(
                nn.Dropout(),
                nn.Linear(self.net.fc.in_features, num_classes),
            )
        else:
            self.net.fc = nn.Linear(self.net.fc.in_features, num_classes)

    def fresh_params(self):
        return self.net.fc.parameters()

    def forward(self, x):
        return self.net(x)


class DenseNet(nn.Module):
    def __init__(self, num_classes,
                 pretrained=False, net_cls=M.densenet121):
        super().__init__()
        self.net = create_net(net_cls, pretrained=pretrained)
        self.avg_pool = AvgPool()
        self.net.classifier = nn.Linear(
            self.net.classifier.in_features, num_classes)

    def fresh_params(self):
        return self.net.classifier.parameters()

    def forward(self, x):
        out = self.net.features(x)
        out = F.relu(out, inplace=True)
        out = self.avg_pool(out).view(out.size(0), -1)
        out = self.net.classifier(out)
        return out


resnet18 = partial(ResNet, net_cls=M.resnet18)
resnet34 = partial(ResNet, net_cls=M.resnet34)
resnet50 = partial(ResNet, net_cls=M.resnet50)
resnet101 = partial(ResNet, net_cls=M.resnet101)
resnet152 = partial(ResNet, net_cls=M.resnet152)

densenet121 = partial(DenseNet, net_cls=M.densenet121)
densenet169 = partial(DenseNet, net_cls=M.densenet169)
densenet201 = partial(DenseNet, net_cls=M.densenet201)
densenet161 = partial(DenseNet, net_cls=M.densenet161)",0,No Code Smell
4624,32592428,9,"criterion = nn.BCEWithLogitsLoss(reduction='none')
model = resnet50(num_classes=N_CLASSES, pretrained=True)",0,No Code Smell
4625,32592428,10,from typing import Dict,1,Code Smell
4626,32592428,11,"def load_model(model: nn.Module, path: Path) -> Dict:
    state = torch.load(str(path))
    model.load_state_dict(state['model'])
    print('Loaded model from epoch {epoch}, step {step:,}'.format(**state))
    return state",0,No Code Smell
4627,32592428,12,"load_model(model, '../input/imet2020/best-model.pt')",0,No Code Smell
4628,32592428,13,"with timer('inference'):
    
    model.to(device) 
    
    preds = []
    tk0 = tqdm(enumerate(test_loader), total=len(test_loader))

    for i, images in tk0:
            
        images = images.to(device)
            
        with torch.no_grad():
            y_preds = model(images)
            
        preds.append(torch.sigmoid(y_preds).to('cpu').numpy())",0,No Code Smell
4629,32592428,14,"threshold = 0.10
predictions = np.concatenate(preds) > threshold

for i, row in enumerate(predictions):
    ids = np.nonzero(row)[0]
    submission.iloc[i].attribute_ids = ' '.join([str(x) for x in ids])
    
submission.to_csv('submission.csv', index=False)
submission.head()",0,No Code Smell
4630,32592428,15,,0,No Code Smell
4872,34360968,0,"from datetime import date
from lightgbm import LGBMRegressor
from sklearn.ensemble import VotingRegressor
from sklearn.metrics import mean_squared_error, mean_squared_log_error
from sklearn.model_selection import train_test_split

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import plotly.express as px
import seaborn as sns
sns.set()

seed = 42",0,No Code Smell
4873,34360968,1,"PATH_TO_DATA = ""/kaggle/input/ieor242hw4""
train = pd.read_csv(PATH_TO_DATA + ""/train.csv"", parse_dates=[""pickup_datetime""])
test = pd.read_csv(PATH_TO_DATA + ""/test.csv"", parse_dates=[""pickup_datetime""])
sub = pd.read_csv(PATH_TO_DATA + ""/submission.csv"")

print(""Train sample:"")
display(train.sample(5))
print(""Test sample:"")
display(test.sample(5))",1,Code Smell
4874,34360968,2,"PATH_TO_LOOKUP = ""/kaggle/input/nyc-yellow-taxi-zone-lookup-table""
lookup = pd.read_csv(PATH_TO_LOOKUP + ""/taxi_zone_lookup.csv"")

loc_to_borough = dict(zip(lookup[""LocationID""], lookup[""Borough""].apply(lambda x: str(x).lower())))
loc_to_zone = dict(zip(lookup[""LocationID""], lookup[""Zone""].apply(lambda x: str(x).lower())))

zone_to_loc = {value: key for key, value in loc_to_zone.items()}
zone_to_loc[""unknown""] = 265

borough_to_label = {'Bronx': 1, 'Brooklyn': 2, 'EWR': 3, 'Manhattan': 4, 'Queens': 5, 'Staten Island': 6, 'Unknown': 7}
borough_to_label = {key.lower(): value for key, value in borough_to_label.items()}",1,Code Smell
4875,34360968,3,train.isnull().mean(),0,No Code Smell
4876,34360968,4,"train[
    train[""VendorID""].isnull() |
    train[""passenger_count""].isnull()
][[""VendorID"", ""passenger_count""]].isnull().mean()",0,No Code Smell
4877,34360968,5,test.isnull().mean(),0,No Code Smell
4878,34360968,6,"out_val = 999

def preprocess_data(data):
    # VendorID and passenger count
    data[""VendorID""] = data[""VendorID""].replace({np.nan: 0}).apply(int)
    data[""passenger_count""] = data[""passenger_count""].replace({np.nan: 0}).apply(int)

    # Pickup and dropoff boroughs and zones
    data[""pickup_borough""] = data[""pickup_borough""].apply(lambda x: borough_to_label[x.lower()])
    data[""dropoff_borough""] = data[""dropoff_borough""].apply(lambda x: borough_to_label[x.lower()])
    data[""pickup_zone""] = data[""pickup_zone""].replace({np.nan: ""Unknown""}).apply(lambda x: zone_to_loc[x.lower()])
    data[""dropoff_zone""] = data[""dropoff_zone""].replace({np.nan: ""Unknown""}).apply(lambda x: zone_to_loc[x.lower()])

    # Pickup datetime
    data[""pickup_datetime""] = data[""pickup_datetime""].replace({np.nan: out_val})
    data[""pickup_year""] = data[""pickup_datetime""].apply(lambda x: int(x.year) if x != out_val else out_val)
    data[""pickup_month""] = data[""pickup_datetime""].apply(lambda x: int(x.month) if x != out_val else out_val)
    data[""pickup_day""] = data[""pickup_datetime""].apply(lambda x: int(x.day) if x != out_val else out_val)
    data[""pickup_hour""] = data[""pickup_datetime""].apply(lambda x: x.hour if x != out_val else out_val)
    data[""pickup_minute""] = data[""pickup_datetime""].apply(lambda x: x.minute if x != out_val else out_val)
   
    # Further extraction: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Timestamp.html
    data[""pickup_dayofweek""] = data[""pickup_datetime""].apply(lambda x: int(x.dayofweek) if x != out_val else out_val)
    data[""pickup_dayofyear""] = data[""pickup_datetime""].apply(lambda x: int(x.dayofyear) if x != out_val else out_val)
    # data[""pickup_weekofyear""] = data[""pickup_datetime""].apply(lambda x: int(x.weekofyear) if x != out_val else out_val)
    
    # Drop useless columns
    data = data.drop(columns=[""row_id""])
    
    return data.reset_index(drop=True)


train = preprocess_data(train)
test = preprocess_data(test)",0,No Code Smell
4879,34360968,7,"categorical_columns = [
    ""VendorID"", ""passenger_count"", ""pickup_year"", ""pickup_month"", ""pickup_day"", ""pickup_hour"", ""pickup_minute"",
    ""pickup_dayofweek"", ""pickup_borough"", ""dropoff_borough""
]
numerical_columns = [""trip_distance"", ""pickup_dayofyear""]


def display_data(data):
    nb_rows, nb_cols = max((len(categorical_columns + numerical_columns) - 3) // 4 + 1, 2), 4
    fig, ax = plt.subplots(figsize=(14.5, 4 * nb_rows), nrows=nb_rows, ncols=nb_cols)
    for ind, column in enumerate(categorical_columns + numerical_columns):
        if column in categorical_columns:
            sns.countplot(data[data[column] != out_val][column], ax=ax[ind // nb_cols, ind % nb_cols])
        elif column in numerical_columns:
            sns.distplot(data[data[column] != out_val][column], ax=ax[ind // nb_cols, ind % nb_cols])
            ax[ind // nb_cols, ind % nb_cols].set_ylim((0, 0.05))

    plt.show()


print(""Train EDA:"")
display_data(train)
print(""Test EDA:"")
display_data(test)",0,No Code Smell
4880,34360968,8,"fig, ax = plt.subplots(figsize=(15, 4 * 2), nrows=2, ncols=2)

ax[0, 0].scatter(train[""trip_distance""], train[""duration""], s=1)
ax[0, 0].set_title(""Trip duration vs. distance"")
max_duration = 3600 * 3
# Some outlier removal performed here, be careful
train = train[(train[""duration""] <= max_duration) & (train[""trip_distance""] <= test[""trip_distance""].max())]
ax[0, 1].scatter(train[""trip_distance""], train[""duration""], s=1)
ax[0, 1].set_title(""Trip duration vs. distance (duration < 3 hrs)"")

negative_dist = train[(train[""trip_distance""] < 0)]
ax[1, 0].scatter(negative_dist[""trip_distance""], negative_dist[""duration""], s=1)
ax[1, 0].set_title(""Trip duration vs. distance (distance < 0)"")

positive_dist = train[(train[""trip_distance""] >= 0)]
ax[1, 1].scatter(positive_dist[""trip_distance""], positive_dist[""duration""], s=1)
ax[1, 1].scatter(negative_dist[""trip_distance""].apply(abs), negative_dist[""duration""], s=1)
ax[1, 1].set_xlim(0, 40)
ax[1, 1].set_ylim(0, max_duration)
ax[1, 1].set_title(""Trip duration vs. distance (absolute distance)"")

plt.show()",0,No Code Smell
4881,34360968,9,"fig = px.scatter(
    x=train[""trip_distance""], y=train[""duration""], range_x=[0, 40], range_y=[0, 3600 * 2]
)
fig.update_traces(marker=dict(size=3))
fig.show()",0,No Code Smell
4882,34360968,10,"fig, ax = plt.subplots(figsize=(15, 4), nrows=1, ncols=2)
ax[0].scatter(train[""trip_distance""], train[""duration""], s=1)
slow_distance, slow_duration = 2.61, 29828
fast_distance, fast_duration = 35.7, 2124
ax[0].plot(
    [0.5, 0.5, fast_distance, 125],
    [0, 0.5 * fast_duration / fast_distance, fast_duration, 125 * fast_duration / fast_distance],
    color=""green""
)
ax[0].plot(
    [0, slow_distance * 3000 / slow_duration, slow_distance * 2, slow_distance * 86400 / slow_duration],
    [3000, 3000, slow_duration * 2, 86400],
    color=""green""
)
ax[0].set_xlim(0, 40)
ax[0].set_ylim(0, 3600 * 2)
ax[0].set_title(""Trip duration vs. distance (with outliers)"")

outliers = (
    (train[""trip_distance""] == 0) |
    (train[""duration""] == 0) |
    (train[""trip_distance""] / train[""duration""] >= fast_distance / fast_duration) & (train[""trip_distance""] >= 0.5) |
    (train[""trip_distance""] / train[""duration""] <= slow_distance / slow_duration) & (train[""duration""] >= 3000)
)
ax[1].scatter(train[~outliers][""trip_distance""], train[~outliers][""duration""], s=1)
ax[1].set_xlim(0, 40)
ax[1].set_ylim(0, 3600 * 2)
ax[1].set_title(""Trip duration vs. distance (without outliers)"")

plt.show()",0,No Code Smell
4883,34360968,11,"def correct_outliers(data):
    data[""trip_distance""] = data[""trip_distance""].apply(abs)
    
    return data.reset_index(drop=True)


train = correct_outliers(train)
test = correct_outliers(test)",0,No Code Smell
4884,34360968,12,"def remove_outliers(data):
    data = data[(data[""duration""] <= max_duration)]  # Already removed before, be careful
    outliers = (
        (data[""trip_distance""] == 0) |
        (data[""duration""] == 0) |
        (data[""trip_distance""] / data[""duration""] >= fast_distance / fast_duration) & (data[""trip_distance""] >= 0.5) |
        (data[""trip_distance""] / data[""duration""] <= slow_distance / slow_duration) & (data[""duration""] >= 3000)
    )
    data = data[~outliers]

    return data.reset_index(drop=True)


train = remove_outliers(train)",0,No Code Smell
4885,34360968,13,"plt.figure(figsize=(15, 4))
train[""duration""].apply(np.log).hist(bins=160)
plt.xlim(4, 9)
plt.show()",0,No Code Smell
4886,34360968,14,"def feature_engineering(data):
    data[""pickup_quarterhour""] = data[""pickup_datetime""].apply(
        lambda x: (x - pd.Timestamp(int(x.year), 1, 1)).seconds // (60 * 15) if x != out_val else out_val
    )
    data = data.drop(columns=[
        ""pickup_datetime"", ""pickup_year"", ""pickup_month"", ""pickup_day"", ""pickup_hour"", ""pickup_minute""
    ])
    
    return data.reset_index(drop=True)


train = feature_engineering(train)
test = feature_engineering(test)",0,No Code Smell
4887,34360968,15,"def one_hot_encoding(data):
    for i in range(1, 8):
        data[""pickup_borough_{}"".format(i)] = data[""pickup_zone""].apply(
            lambda x: x if borough_to_label[loc_to_borough[x]] == i else 0
        )
        data[""dropoff_borough_{}"".format(i)] = data[""dropoff_zone""].apply(
            lambda x: x if borough_to_label[loc_to_borough[x]] == i else 0
        )

    data = data.drop(columns=[""pickup_zone"", ""dropoff_zone"", ""pickup_borough"", ""dropoff_borough""])
    
    return data


train = one_hot_encoding(train)
test = one_hot_encoding(test)

train.shape",0,No Code Smell
4888,34360968,16,(train >= 0).mean(),1,Code Smell
4889,34360968,17,"def downcast_data(data):
    data[""trip_distance""] = (100 * data[""trip_distance""]).astype(int)

    for column in data.columns:
        data[column] = pd.to_numeric(data[column], downcast='unsigned')
    
    return data.reset_index(drop=True)


train = downcast_data(train)
test = downcast_data(test)

train.dtypes",0,No Code Smell
4890,34360968,18,"plt.figure(figsize=(15, 15))
sns.heatmap(np.abs(np.round(train.corr(), 2)), square=True, annot=True, cmap=plt.cm.Blues)
plt.show()",0,No Code Smell
4891,34360968,19,"X, y = train.drop(columns=[""duration""]), train[""duration""]
X_test = test

validate = True

if validate:
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=seed)",0,No Code Smell
4892,34360968,20,"commargs = {""learning_rate"": 0.03, ""colsample_bytree"": 0.9, ""reg_lambda"": 0.2, ""random_state"": seed, ""n_jobs"": -1}

lgbm_63 = LGBMRegressor(n_estimators=12000, num_leaves=63, **commargs)
lgbm_127 = LGBMRegressor(n_estimators=6000, num_leaves=127, **commargs)
lgbm_255 = LGBMRegressor(n_estimators=4000, num_leaves=255, **commargs)
lgbm_511 = LGBMRegressor(n_estimators=2000, num_leaves=511, **commargs)
lgbm_1023 = LGBMRegressor(n_estimators=1000, num_leaves=1023, **commargs)

lgbm_estimators = [
    (""LGBM_63"", lgbm_63), (""LGBM_127"", lgbm_127), (""LGBM_255"", lgbm_255),
    (""LGBM_511"", lgbm_511), (""LGBM_1023"", lgbm_1023),
]

lgbm_voting = VotingRegressor(lgbm_estimators)",0,No Code Smell
4893,34360968,21,"def plot_classifiers_validation(regs, X_train, y_train, X_val, y_val):
    fitted_regs = []
    nb_rows, nb_cols = 2, 3
    fig, ax = plt.subplots(figsize=(15, 10), nrows=nb_rows, ncols=nb_cols)
    for ind, reg in enumerate(regs):
        reg.fit(X_train, np.log(y_train))
        fitted_regs.append(reg)
        y_pred = reg.predict(X_val)
        ax[ind // nb_cols, ind % nb_cols].scatter(y_val, np.exp(y_pred), s=1)
        max_plot_value = max(y_val.max(), np.exp(y_pred).max())
        ax[ind // nb_cols, ind % nb_cols].plot([0, max_plot_value], [0, max_plot_value], color=""orange"")
        ax[ind // nb_cols, ind % nb_cols].set_title(""IN-RMSE = {0:.2f}, IN-RMSLE = {1:.5f}"".format(
            np.sqrt(mean_squared_error(y_val[y_val <= 7000], np.exp(y_pred)[y_val <= 7000])),
            np.sqrt(mean_squared_log_error(y_val[y_val <= 7000], np.exp(y_pred)[y_val <= 7000]))
        ))
        ax[ind // nb_cols, ind % nb_cols].set_xlim(0, max_plot_value)
        ax[ind // nb_cols, ind % nb_cols].set_ylim(0, max_plot_value)
    plt.show()
    return fitted_regs


if validate:
    lgbm_regs = plot_classifiers_validation(
        [lgbm_63, lgbm_127, lgbm_255, lgbm_511, lgbm_1023, lgbm_voting], X_train, y_train, X_val, y_val
    )",0,No Code Smell
4894,34360968,22,"categorical_columns = [""VendorID"", ""passenger_count"", ""pickup_dayofweek"", ""pickup_borough"", ""dropoff_borough""]
numerical_columns = [""trip_distance"", ""pickup_dayofyear"", ""pickup_quarterhour""]


def plot_lgbm_feature_importance(clf, ax):
    ft_imp_dummies = dict(zip(X_train.columns, clf.feature_importances_))
    ft_imp = {
        column: sum([value for key, value in ft_imp_dummies.items() if column in key])
        if column in categorical_columns else ft_imp_dummies[column]
        for column in categorical_columns + numerical_columns
    }
    ft_imp = {key: value for key, value in sorted(ft_imp.items(), key=lambda item: item[1])}

    labels, values = list(ft_imp.keys()), list(ft_imp.values())
    ylocs = np.arange(len(values))
    ax.barh(ylocs, values, align='center', height=0.2)
    for x, y in zip(values, ylocs):
        ax.text(x + 1, y, x, va='center')
    ax.set_yticks(ylocs)
    ax.set_yticklabels(labels)
    ax.set_title(""Feature importance for LGBM"")


if validate:
    fig, ax = plt.subplots(figsize=(13, 8), nrows=3, ncols=2)
    for ind, reg in enumerate(lgbm_regs[:-1]):
        plot_lgbm_feature_importance(reg, ax=ax[ind // 2, ind % 2])
    plt.subplots_adjust(wspace=0.5, hspace=0.4)
    plt.show()",0,No Code Smell
4895,34360968,23,"fit_predict = True
predict_on_train = True

if fit_predict:
    plt.figure(figsize=(15, 4))
    lgbm_voting.fit(X, np.log(y))

    if predict_on_train:
        y_pred = np.exp(lgbm_voting.predict(X))
        print(""Train RMSE: "", np.sqrt(mean_squared_error(y, y_pred)))
        plt.hist(y, density=True, bins=[50 * i for i in range(160)])

    y_sub = np.exp(lgbm_voting.predict(X_test))
    plt.hist(y_sub, density=True, bins=[50 * i for i in range(160)], alpha=0.5)
    plt.xlim((0, 3500))
    plt.show()

    sub[""duration""] = y_sub
    display(sub)
    sub.to_csv(""lgbm-voting-final.csv"", index=False)",0,No Code Smell
5076,32617166,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the ""../input/"" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# Any results you write to the current directory are saved as output.",1,Code Smell
5077,32617166,1,"train_df = pd.read_csv(""/kaggle/input/explicit-content-detection/train.csv"")
test_df = pd.read_csv(""/kaggle/input/explicit-content-detection/test.csv"")

titles = train_df[""title""].values
urls = train_df[""url""].values
y = train_df[""target""].astype(int).values",1,Code Smell
5078,32617166,2,train_df.head(),0,No Code Smell
5079,32617166,3,"from string import punctuation
import nltk",1,Code Smell
5080,32617166,4,"# Define symbols & words we don't need
translator = {ord(c): ' ' for c in punctuation + '0123456789«»—–…✅№'}
nltk.download('stopwords')
stop_words = nltk.corpus.stopwords.words('russian') + nltk.corpus.stopwords.words('english')
print(stop_words[:10])",0,No Code Smell
5081,32617166,5,"'''
Remove stop words
'''
def remove_stops(collection):
    return [w for w in collection if w not in stop_words]


'''
Split titles array into tokens
'''
def split_into_tokens(X):
    n_samples = X.size
    tokens = []

    print('Splitting...')
    for i in range(n_samples):
        tokens.append(remove_stops(X[i].translate(translator).lower().split()))

        if i % 10000 == 0:
            print(f'Done: {i}/{n_samples}')

    print(f'Done: {n_samples}/{n_samples}\n')
    return tokens


from nltk.stem.snowball import SnowballStemmer 

'''
Do stemming with splitted word tokens
'''
def do_stemming(X):
    n_samples = len(X)
    stemmer = SnowballStemmer(""russian"")
    stemmed = []

    print('Stemming...')
    for i in range(n_samples):
        stemmed.append(list(map(stemmer.stem, X[i])))

        if i % 10000 == 0:
            print(f'Done: {i}/{n_samples}')

    print(f'Done: {n_samples}/{n_samples}\n')
    return stemmed",0,No Code Smell
5082,32617166,6,from sklearn.model_selection import train_test_split,1,Code Smell
5083,32617166,7,"# Split data into test and train sets
train_titles, test_titles, train_urls, test_urls, train_y, test_y = train_test_split(titles, urls, y, test_size=0.33, stratify=y)
n_samples = len(train_y)",0,No Code Smell
5084,32617166,8,X_stemmed_tokens = do_stemming(split_into_tokens(train_titles)),0,No Code Smell
5085,32617166,9,from collections import Counter,1,Code Smell
5086,32617166,10,"'''
Create corpus of all words
'''
def make_corpus(X, y):
    corpus_all = []
    corpus_porn = []

    for sample, is_porn in zip(X, y):
        corpus_all.extend(set(sample))

        if is_porn:
            corpus_porn.extend(set(sample))
            
    return corpus_all, corpus_porn",0,No Code Smell
5087,32617166,11,"# Count words
# We need to do this to calculate importance of each word
corpus_titles_all, corpus_titles_porn = make_corpus(X_stemmed_tokens, train_y)
count_titles_all = Counter(corpus_titles_all)
count_titles_porn = Counter(corpus_titles_porn)

# Count urls
corpus_urls_porn = [train_urls[i] for i in range(n_samples) if train_y[i]]
count_urls_all = Counter(train_urls)
count_urls_porn = Counter(corpus_urls_porn)",0,No Code Smell
5088,32617166,12,"'''
Entropy of word
We have to estimate, how valuable deviation of given porn share from overall mean 
'''
def word_porn_rate(porn_share, mean_share, penalize):
    if porn_share > mean_share:
        return (porn_share - mean_share) / (1 - mean_share)
    
    fine = mean_share / (1 - mean_share) if penalize else 1
    return (porn_share - mean_share) * fine / mean_share",0,No Code Smell
5089,32617166,13,"'''
Define word score based on it's frequency and entropy
'''
def word_score(porn_rate, frequency, f_weight = 0.3):
    return ((frequency * f_weight) + np.abs(porn_rate) * (1 - f_weight)) / 2

'''
Get n best words based on word_score
'''
def get_n_best_words(n, x_size, mean_share, counter_porn, counter_all, min_freq=0.001, f_weight=0.3, penalize=True):
    word_scores = {}
    word_weights = {}
    
    # Evaluate scores for each popular word
    for word, count in counter_all.items():
        if count/x_size >= min_freq:
            porn_count = counter_porn[word]
            porn_share = porn_count / count
            frequency = count / x_size
            porn_rate = word_porn_rate(porn_share, mean_share, penalize)
            word_weights[word] = porn_rate
            word_scores[word] = word_score(porn_rate, frequency, f_weight)
            
    # Get n best words (or all words we have, if n is too big)
    best_words, best_weights = [], []
    
    for word in sorted(word_scores.keys(), key=lambda w: word_scores[w], reverse=True)[:n]:
        best_words.append(word)
        best_weights.append(word_weights[word])
            
    return best_words, np.array(best_weights)",0,No Code Smell
5090,32617166,14,"mean = np.mean(train_y)
best_words, best_word_weights = get_n_best_words(100, n_samples, mean, count_titles_porn, count_titles_all, min_freq=0.001, f_weight=0.3)
best_urls, best_url_weights = get_n_best_words(100000, n_samples, mean, count_urls_porn, count_urls_all, min_freq=0.0001, f_weight=0.3, penalize=False)",0,No Code Smell
5091,32617166,15,"print('Best words:')
for i in range(70, 100):
    word = best_words[i]
    print(f'{word} : {best_word_weights[i]}, {count_titles_all[word]}')
    
print('\nBest urls:')
for i in range(30):
    url = best_urls[i]
    print(f'{url} : {best_url_weights[i]}, {count_urls_all[url]}')",0,No Code Smell
5092,32617166,16,"'''
Transform the tokens array into vectors
'''
def tokens_to_vecs(tokens_X, n_samples, words, n_words):
    vectors_X = np.empty((n_samples, n_words))
    
    print('Vectorising...')
    for i in range(n_samples):
        tokens = tokens_X[i]
        vectors_X[i] = [int(word in tokens) for word in words]
        
        if i % 10000 == 0:
            print(f'Done: {i}/{n_samples}')
        
    print(f'Done: {n_samples}/{n_samples}')
    return vectors_X",0,No Code Smell
5093,32617166,17,"'''
Make target prediction based on valuable words
'''
def vec_to_predict(tokens_vec, best_weights):
    return int(np.mean(tokens_vec * best_weights) > 0)",0,No Code Smell
5094,32617166,18,"from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt",1,Code Smell
5095,32617166,19,"# test_X_vecs = tokens_to_vecs(test_X, len(test_X), best_words, len(best_words))
# train_X_vecs = tokens_to_vecs(train_X, len(train_X), best_words, len(best_words))

# test_accuracies = []
# train_accuracies = []
# always_false = [0 for _ in test_X_vecs]
# xs = np.arange(10, 210, 10)
# const_accuracies = [1 - mean for i in xs]

# for i in xs:
#     test_predictions = [vec_to_predict(vec[:i], best_weights[:i]) for vec in test_X_vecs]
#     test_accuracies.append(accuracy_score(test_y, test_predictions))
    
# plt.plot(xs, test_accuracies, 'r', xs, const_accuracies, 'g')
# plt.plot(max(xs, key=lambda x: test_accuracies[int(x/10 - 1)]), max(test_accuracies), 'r^')
# plt.show()",0,No Code Smell
5096,32617166,20,"# test_predictions = [vec_to_predict(vec[:70], best_weights[:70]) for vec in test_X_vecs]
# print(f'Accuracy: {accuracy_score(test_y, test_predictions)}')",0,No Code Smell
5097,32617166,21,"X_urls = train_df[""url""].values",0,No Code Smell
5098,32617166,22,"# # Split data into test and train sets
train_X_urls, test_X_urls, train_y_urls, test_y_urls = train_test_split(X_urls, y_train, test_size=0.33, stratify=y_train)
porn_urls = [url for i, url in enumerate(train_X_urls) if train_y_urls[i]]

# Count urls
count_urls = Counter(train_X_urls)
count_urls_porn = Counter(porn_urls)",0,No Code Smell
5099,32617166,23,"# Get valuable urls
mean = np.mean(train_y_urls)
x_size = len(train_X_urls)
best_urls, best_urls_weights = get_n_best_words(25000, x_size, mean, count_urls_porn, count_urls, min_freq=0.00001, f_weight=0.3, penalize=False)",0,No Code Smell
5100,32617166,24,len(best_urls),0,No Code Smell
5101,32617166,25,"for i in range(100):
    url = best_urls[i]
    print(f'{url}  count: {count_urls[url]}  share: {count_urls_porn[url]/count_urls[url]}  weight: {best_urls_weights[i]}')",0,No Code Smell
5102,32617166,26,"def url_to_predict(url, urls, weights):
    try:
        return int(weights[urls.index(url)] > 0)
    except ValueError:
        return 0",0,No Code Smell
5103,32617166,27,"# test_accuracies_urls = []
# xs = np.arange(10000, 25500, 500)

# for x in xs:
#     urls, weights = best_urls[:x], best_urls_weights[:x]
#     predictions = [url_to_predict(url, urls, weights) for url in test_X_urls]
#     test_accuracies_urls.append(accuracy_score(test_y_urls, predictions))
    
# plt.plot(xs, test_accuracies_urls, 'r')
# plt.show()",0,No Code Smell
5104,32617166,28,"predictions = [url_to_predict(url, best_urls, best_urls_weights) for url in test_X_urls]
accuracy_score(test_y_urls, predictions)",0,No Code Smell
5105,32617166,29,"# X_test_urls = test_df[""url""].values
# n_samples = X_test_urls.size
# validate_predictions = [bool(url_to_predict(url, best_urls, best_urls_weights)) for url in X_test_urls]

# data = {
#     'id': [i for i in range(135309, 135309 + n_samples)],
#     'target': validate_predictions
# }

# validate_df = pd.DataFrame(data)
# validate_df.to_csv('simple_urls.csv', index=False)",0,No Code Smell
5944,32985411,0,"from tqdm import tqdm
import numpy as np

from collections import defaultdict
import itertools
from functools import lru_cache

import pickle

import warnings
warnings.filterwarnings('ignore')",0,No Code Smell
5945,32985411,1,np.random.seed(42),0,No Code Smell
5946,32985411,2,"def cost(photo1, photo2):
    intersect = len(photo1.intersection(photo2))
    return min(len(photo1) - intersect, len(photo2) - intersect, intersect)

def sequence_cost(sequence):
    total_cost = 0
    for i in range(len(sequence) - 1):
        if sequence[i + 1] == -1:
            break
            
        if isinstance(sequence[i], tuple):
            old_tags = photos[sequence[i][0]][1].union(photos[sequence[i][1]][1])
        else:
            old_tags = photos[sequence[i]][1]
            
        if isinstance(sequence[i + 1], tuple):
            new_tags = photos[sequence[i + 1][0]][1].union(photos[sequence[i + 1][1]][1])
        else:
            new_tags = photos[sequence[i + 1]][1]
            
        total_cost += cost(old_tags, new_tags)
    return total_cost

# Read our input
with open('../input/hashcode-photo-slideshow/d_pet_pictures.txt', 'r') as ifp:
    lines = ifp.readlines()

photos = []
all_tags = list()
photos_per_tag = defaultdict(list)
for i, line in enumerate(lines[1:]):
    orient, _, *tags = line.strip().split()
    photos.append((orient, set(tags)))
    for tag in tags:
        photos_per_tag[tag].append(i)

# Create some variables to store the solution in
sequence = [-1] * len(photos)
total_cost = 0

# Sample our first slide (must be horizontal)
sequence[0] = np.random.choice([i for i in range(len(photos)) if photos[i][0] == 'H'])
tags = photos[sequence[0]][1]
for tag in photos[sequence[0]][1]:
    photos_per_tag[tag].remove(sequence[0])
    
remaining_pics = list(set(range(len(photos))) - set(sequence))
remaining_horizontal_pics = [p for p in remaining_pics if photos[p][0] == 'H']
remaining_vertical_pics = [p for p in remaining_pics if photos[p][0] == 'V']

# Iteratively add a slide to the sequence
for i in tqdm(range(1, len(sequence))):
    # Fallback: In case we do not find any candidates, we just take 1 random horizontal or 2 random vertical pics
    if len(remaining_horizontal_pics) > 0:
        best_j = np.random.choice(remaining_horizontal_pics)
    elif len(remaining_vertical_pics) > 1:
        best_j = tuple(np.random.choice(remaining_vertical_pics, size=2, replace=False))
    else:
        break
        
    best_cost = total_cost
    
    # Get a list of K possible good candidates
    K = 2500
    k = 0.5
    vertical_candidates = set()
    horizontal_candidates = set()
    in_common_tags = defaultdict(int)
    for tag in tags:
        for p in photos_per_tag[tag]:
            in_common_tags[p] += 1
            
    if len(in_common_tags) > 0:
            
        max_tags = max(in_common_tags.values())
        for p in in_common_tags:
            if in_common_tags[p] == max_tags:
                if photos[p][0] == 'H':
                    horizontal_candidates.add(p)
                else:
                    vertical_candidates.add(p)
                    
        for p in in_common_tags:
            if len(horizontal_candidates) + len(vertical_candidates) > K:
                break

            if in_common_tags[p] >= k * max_tags:
                if photos[p][0] == 'H':
                    horizontal_candidates.add(p)
                else:
                    vertical_candidates.add(p)

        # Candidates consist of all possible horizontal candidates and all combinations of 2 vertical candidates
        candidates = list(horizontal_candidates) + list(itertools.combinations(vertical_candidates, 2))

        # Iterate over candidates and pick the one that increases the score the most.
        curr_best = 0
        old_cost = best_cost
        for j in candidates:
            if isinstance(j, tuple):
                new_tags = photos[j[0]][1].union(photos[j[1]][1])
            else:
                new_tags = photos[j][1]

            if len(new_tags) <= 2*curr_best:
                continue

            new_cost = total_cost + cost(tags, new_tags)

            if new_cost >= best_cost:
                best_cost = new_cost
                curr_best = new_cost - old_cost
                best_j = j

    # Assign a new picture to the next slide
    total_cost = best_cost
    sequence[i] = best_j
    
    if isinstance(best_j, tuple):
        tags = photos[sequence[i][0]][1].union(photos[sequence[i][1]][1])
        remaining_pics.remove(best_j[0])
        remaining_vertical_pics.remove(best_j[0])
        for tag in photos[sequence[i][0]][1]:
            photos_per_tag[tag].remove(sequence[i][0])
        remaining_pics.remove(best_j[1])
        remaining_vertical_pics.remove(best_j[1])
        for tag in photos[sequence[i][1]][1]:
            photos_per_tag[tag].remove(sequence[i][1])
    else:
        remaining_horizontal_pics.remove(best_j)
        remaining_pics.remove(best_j)
        tags = photos[sequence[i]][1]
        for tag in photos[sequence[i]][1]:
            photos_per_tag[tag].remove(sequence[i])",0,No Code Smell
5947,32985411,3,print('Score = {}'.format(sequence_cost(sequence))),0,No Code Smell
5948,32985411,4,"with open('submission.txt', 'w+') as ofp:
    ofp.write('{}\n'.format(sum(np.array(sequence) != -1)))
    for p in sequence:
        if p == -1:
            break
            
        if isinstance(p, tuple):
            ofp.write('{} {}\n'.format(p[0], p[1]))
        else:
            ofp.write('{}\n'.format(p))",0,No Code Smell
5949,32985411,5,"# CHECKS:
# 1) We dont want duplicates
# 2) We want vertical pictures to always be paired with another vertical picture
# 3) We don't want horizontal pictures to be paired
# 4) Preferably, we assign all of the pictures to slides
# 5) We cannot assign a picture to two different slides
done = set()
for i, p in enumerate(sequence):
    if p == -1:
        break
    if isinstance(p, tuple):
        assert p[0] != p[1]
        assert p[0] not in done
        assert photos[p[0]][0] == 'V'
        done.add(p[0])
        
        assert p[1] not in done
        assert photos[p[1]][0] == 'V'
        done.add(p[1])
    else:
        assert p not in done
        assert photos[p][0] == 'H'
        done.add(p)
print(i, len(done))
print(done - set(range(len(photos))))",0,No Code Smell
5950,32985411,6,!wc -l submission.txt,0,No Code Smell
5951,32985411,7,!tail submission.txt,0,No Code Smell
5952,32985411,8,!head submission.txt,0,No Code Smell
5953,32985411,9,,0,No Code Smell
6968,33450719,0,"%matplotlib inline
import pandas as pd
from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = ""all""
pd.set_option('display.max_columns', 99)
pd.set_option('display.max_rows', 99)
import os
import numpy as np
from matplotlib import pyplot as plt
from tqdm import tqdm
import datetime as dt",0,No Code Smell
6969,33450719,1,"import matplotlib.pyplot as plt
plt.rcParams['figure.figsize'] = [16, 10]
plt.rcParams['font.size'] = 14
import seaborn as sns
sns.set_palette(sns.color_palette('tab20', 20))

import plotly.express as px
import plotly.graph_objects as go",0,No Code Smell
6970,33450719,2,"COMP = '../input/covid19-global-forecasting-week-4'
DATEFORMAT = '%Y-%m-%d'


def get_comp_data(COMP):
    train = pd.read_csv(f'{COMP}/train.csv')
    test = pd.read_csv(f'{COMP}/test.csv')
    submission = pd.read_csv(f'{COMP}/submission.csv')
    print(train.shape, test.shape, submission.shape)
    train['Country_Region'] = train['Country_Region'].str.replace(',', '')
    test['Country_Region'] = test['Country_Region'].str.replace(',', '')

    train['Location'] = train['Country_Region'] + '-' + train['Province_State'].fillna('')

    test['Location'] = test['Country_Region'] + '-' + test['Province_State'].fillna('')

    train['LogConfirmed'] = to_log(train.ConfirmedCases)
    train['LogFatalities'] = to_log(train.Fatalities)
    train = train.drop(columns=['Province_State'])
    test = test.drop(columns=['Province_State'])

    country_codes = pd.read_csv('../input/covid19-metadata/country_codes.csv', keep_default_na=False)
    train = train.merge(country_codes, on='Country_Region', how='left')
    test = test.merge(country_codes, on='Country_Region', how='left')

    train['DateTime'] = pd.to_datetime(train['Date'])
    test['DateTime'] = pd.to_datetime(test['Date'])
    
    return train, test, submission


def process_each_location(df):
    dfs = []
    for loc, df in tqdm(df.groupby('Location')):
        df = df.sort_values(by='Date')
        df['Fatalities'] = df['Fatalities'].cummax()
        df['ConfirmedCases'] = df['ConfirmedCases'].cummax()
        df['LogFatalities'] = df['LogFatalities'].cummax()
        df['LogConfirmed'] = df['LogConfirmed'].cummax()
        df['LogConfirmedNextDay'] = df['LogConfirmed'].shift(-1)
        df['ConfirmedNextDay'] = df['ConfirmedCases'].shift(-1)
        df['DateNextDay'] = df['Date'].shift(-1)
        df['LogFatalitiesNextDay'] = df['LogFatalities'].shift(-1)
        df['FatalitiesNextDay'] = df['Fatalities'].shift(-1)
        df['LogConfirmedDelta'] = df['LogConfirmedNextDay'] - df['LogConfirmed']
        df['ConfirmedDelta'] = df['ConfirmedNextDay'] - df['ConfirmedCases']
        df['LogFatalitiesDelta'] = df['LogFatalitiesNextDay'] - df['LogFatalities']
        df['FatalitiesDelta'] = df['FatalitiesNextDay'] - df['Fatalities']
        dfs.append(df)
    return pd.concat(dfs)


def add_days(d, k):
    return dt.datetime.strptime(d, DATEFORMAT) + dt.timedelta(days=k)


def to_log(x):
    return np.log(x + 1)


def to_exp(x):
    return np.exp(x) - 1
",1,Code Smell
6971,33450719,3,"start = dt.datetime.now()
train, test, submission = get_comp_data(COMP)
train.shape, test.shape, submission.shape
train.head(2)
test.head(2)",0,No Code Smell
6972,33450719,4,"train[train.geo_region.isna()].Country_Region.unique()
train = train.fillna('#N/A')
test = test.fillna('#N/A')

train[train.duplicated(['Date', 'Location'])]
train.count()",0,No Code Smell
6973,33450719,5,"train.describe()
train.nunique()
train.dtypes
train.count()

TRAIN_START = train.Date.min()
TEST_START = test.Date.min()
TRAIN_END = train.Date.max()
TEST_END = test.Date.max()
TRAIN_START, TRAIN_END, TEST_START, TEST_END",1,Code Smell
6974,33450719,6,"train = train.sort_values(by='Date')
countries_latest_state = train[train['Date'] == TRAIN_END].groupby([
    'Country_Region', 'continent', 'geo_region', 'country_iso_code_3']).sum()[[
    'ConfirmedCases', 'Fatalities']].reset_index()
countries_latest_state['Log10Confirmed'] = np.log10(countries_latest_state.ConfirmedCases + 1)
countries_latest_state['Log10Fatalities'] = np.log10(countries_latest_state.Fatalities + 1)
countries_latest_state = countries_latest_state.sort_values(by='Fatalities', ascending=False)
countries_latest_state.to_csv('countries_latest_state.csv', index=False)

countries_latest_state.shape
countries_latest_state.head()",0,No Code Smell
6975,33450719,7,"fig = go.Figure(data=go.Choropleth(
    locations = countries_latest_state['country_iso_code_3'],
    z = countries_latest_state['Log10Confirmed'],
    text = countries_latest_state['Country_Region'],
    colorscale = 'viridis_r',
    autocolorscale=False,
    reversescale=False,
    marker_line_color='darkgray',
    marker_line_width=0.5,
    colorbar_tickprefix = '10^',
    colorbar_title = 'Confirmed cases <br>(log10 scale)',
))

_ = fig.update_layout(
    title_text=f'COVID-19 Global Cases [Updated: {TRAIN_END}]',
    geo=dict(
        showframe=False,
        showcoastlines=False,
        projection_type='equirectangular'
    )
)

fig.show()",0,No Code Smell
6976,33450719,8,"fig = go.Figure(data=go.Choropleth(
    locations = countries_latest_state['country_iso_code_3'],
    z = countries_latest_state['Log10Fatalities'],
    text = countries_latest_state['Country_Region'],
    colorscale = 'viridis_r',
    autocolorscale=False,
    reversescale=False,
    marker_line_color='darkgray',
    marker_line_width=0.5,
    colorbar_tickprefix = '10^',
    colorbar_title = 'Deaths <br>(log10 scale)',
))

_ = fig.update_layout(
    title_text=f'COVID-19 Global Deaths [Updated: {TRAIN_END}]',
    geo=dict(
        showframe=False,
        showcoastlines=False,
        projection_type='equirectangular'
    )
)

fig.show()",0,No Code Smell
6977,33450719,9,"countries_latest_state['DeathConfirmedRatio'] = (countries_latest_state.Fatalities + 1) / (countries_latest_state.ConfirmedCases + 1)
countries_latest_state['DeathConfirmedRatio'] = countries_latest_state['DeathConfirmedRatio'].clip(0, 0.15) 
fig = px.scatter(countries_latest_state,
                 x='ConfirmedCases',
                 y='Fatalities',
                 color='DeathConfirmedRatio',
                 size='Log10Fatalities',
                 size_max=20,
                 hover_name='Country_Region',
                 color_continuous_scale='viridis_r'
)
_ = fig.update_layout(
    title_text=f'COVID-19 Deaths vs Confirmed Cases by Country [Updated: {TRAIN_END}]',
    xaxis_type=""log"",
    yaxis_type=""log"",
    width = 1600,
    height = 900,
)
fig.show()",0,No Code Smell
6978,33450719,10,"# The source dataset is not necessary cumulative we will force it
latest_loc = train[train['Date'] == TRAIN_END][['Location', 'ConfirmedCases', 'Fatalities']]
max_loc = train.groupby(['Location'])[['ConfirmedCases', 'Fatalities']].max().reset_index()
check = pd.merge(latest_loc, max_loc, on='Location')
np.mean(check.ConfirmedCases_x == check.ConfirmedCases_y)
np.mean(check.Fatalities_x == check.Fatalities_y)
check[check.Fatalities_x != check.Fatalities_y]
check[check.ConfirmedCases_x != check.ConfirmedCases_y]",0,No Code Smell
6979,33450719,11,"train_clean = process_each_location(train)

train_clean.shape
train_clean.tail()",0,No Code Smell
6980,33450719,12,"regional_progress = train_clean.groupby(['DateTime', 'continent']).sum()[['ConfirmedCases', 'Fatalities']].reset_index()
regional_progress['Log10Confirmed'] = np.log10(regional_progress.ConfirmedCases + 1)
regional_progress['Log10Fatalities'] = np.log10(regional_progress.Fatalities + 1)
regional_progress = regional_progress[regional_progress.continent != '#N/A']
regional_progress = regional_progress.sort_values(by=['continent', 'DateTime'])

regional_progress['ConfirmedCasesDiff'] = regional_progress.groupby('continent').ConfirmedCases.diff().rolling(3).mean()
regional_progress['FatalitiesDiff'] = regional_progress.groupby('continent').Fatalities.diff().rolling(3).mean()",0,No Code Smell
6981,33450719,13,"fig = px.area(regional_progress, x=""DateTime"", y=""ConfirmedCases"", color=""continent"")
_ = fig.update_layout(
    title_text=f'COVID-19 Cumulative Confirmed Cases by Continent [Updated: {TRAIN_END}]',
    width=1600,
    height=900
)
fig.show()
fig2 = px.line(regional_progress, x='DateTime', y='ConfirmedCases', color='continent')
_ = fig2.update_layout(
    yaxis_type=""log"",
    title_text=f'COVID-19 Cumulative Confirmed Cases by Continent [Updated: {TRAIN_END}]'
)
fig2.show()

fig3 = px.line(regional_progress, x='DateTime', y='ConfirmedCasesDiff', color='continent')
_ = fig3.update_layout(
    title_text=f'COVID-19 Daily New Confirmed Cases by Continent [Updated: {TRAIN_END}]'
)
fig3.show()

",0,No Code Smell
6982,33450719,14,"fig = px.area(regional_progress, x=""DateTime"", y=""Fatalities"", color=""continent"")
_ = fig.update_layout(
    title_text=f'COVID-19 Cumulative Confirmed Deaths by Continent [Updated: {TRAIN_END}]'
)
fig.show()
fig2 = px.line(regional_progress, x='DateTime', y='Fatalities', color='continent')
_ = fig2.update_layout(
    yaxis_type=""log"",
    title_text=f'COVID-19 Cumulative Confirmed Deaths by Continent [Updated: {TRAIN_END}]'
)
fig2.show()
fig3 = px.line(regional_progress, x='DateTime', y='FatalitiesDiff', color='continent')
_ = fig3.update_layout(
    title_text=f'COVID-19 Daily New Fatalities by Continent [Updated: {TRAIN_END}]'
)
fig3.show()",0,No Code Smell
6983,33450719,15,"china = train_clean[train_clean.Location.str.startswith('China')]
top10_locations = china.groupby('Location')[['ConfirmedCases']].max().sort_values(
    by='ConfirmedCases', ascending=False).reset_index().Location.values[:10]
fig2 = px.line(china[china.Location.isin(top10_locations)], x='DateTime', y='ConfirmedCases', color='Location')
_ = fig2.update_layout(
    yaxis_type=""log"",
    title_text=f'COVID-19 Cumulative Confirmed Cases in China [Updated: {TRAIN_END}]'
)
fig2.show()",0,No Code Smell
6984,33450719,16,"europe = train_clean[train_clean.continent == 'Europe']
top10_locations = europe.groupby('Location')[['ConfirmedCases']].max().sort_values(
    by='ConfirmedCases', ascending=False).reset_index().Location.values[:10]
fig2 = px.line(europe[europe.Location.isin(top10_locations)], x='DateTime', y='ConfirmedCases', color='Location')
_ = fig2.update_layout(
    yaxis_type=""log"",
    title_text=f'COVID-19 Cumulative Confirmed Cases in Europe [Updated: {TRAIN_END}]'
)
fig2.show()",0,No Code Smell
6985,33450719,17,"us = train_clean[train_clean.Country_Region == 'US']
top10_locations = us.groupby('Location')[['ConfirmedCases']].max().sort_values(
    by='ConfirmedCases', ascending=False).reset_index().Location.values[:10]
fig2 = px.line(us[us.Location.isin(top10_locations)], x='DateTime', y='ConfirmedCases', color='Location')
_ = fig2.update_layout(
    yaxis_type=""log"",
    title_text=f'COVID-19 Cumulative Confirmed Cases in the USA [Updated: {TRAIN_END}]'
)
fig2.show()",0,No Code Smell
6986,33450719,18,"africa = train_clean[train_clean.continent == 'Africa']
top10_locations = africa.groupby('Location')[['ConfirmedCases']].max().sort_values(
    by='ConfirmedCases', ascending=False).reset_index().Location.values[:10]
fig2 = px.line(africa[africa.Location.isin(top10_locations)], x='DateTime', y='ConfirmedCases', color='Location')
_ = fig2.update_layout(
    yaxis_type=""log"",
    title_text=f'COVID-19 Cumulative Confirmed Cases in Africa [Updated: {TRAIN_END}]'
)
fig2.show()",0,No Code Smell
6987,33450719,19,"country_progress = train_clean.groupby(['Date', 'DateTime', 'Country_Region']).sum()[[
    'ConfirmedCases', 'Fatalities', 'ConfirmedDelta', 'FatalitiesDelta']].reset_index()
top10_countries = country_progress.groupby('Country_Region')[['Fatalities']].max().sort_values(
    by='Fatalities', ascending=False).reset_index().Country_Region.values[:10]

fig2 = px.line(country_progress[country_progress.Country_Region.isin(top10_countries)],
               x='DateTime', y='ConfirmedCases', color='Country_Region')
_ = fig2.update_layout(
    yaxis_type=""log"",
    title_text=f'COVID-19 Cumulative Confirmed Cases by Country [Updated: {TRAIN_END}]'
)
fig2.show()
fig3 = px.line(country_progress[country_progress.Country_Region.isin(top10_countries)],
               x='DateTime', y='Fatalities', color='Country_Region')
_ = fig3.update_layout(
    yaxis_type=""log"",
    title_text=f'COVID-19 Cumulative Deaths by Country [Updated: {TRAIN_END}]'
)
fig3.show()",0,No Code Smell
6988,33450719,20,"countries_0301 = country_progress[country_progress.Date == '2020-03-01'][[
    'Country_Region', 'ConfirmedCases', 'Fatalities']]
countries_0331 = country_progress[country_progress.Date == '2020-03-31'][[
    'Country_Region', 'ConfirmedCases', 'Fatalities']]
countries_in_march = pd.merge(countries_0301, countries_0331, on='Country_Region', suffixes=['_0301', '_0331'])
countries_in_march['IncreaseInMarch'] = countries_in_march.ConfirmedCases_0331 / (countries_in_march.ConfirmedCases_0301 + 1)
countries_in_march = countries_in_march[countries_in_march.ConfirmedCases_0331 > 200].sort_values(
    by='IncreaseInMarch', ascending=False)
countries_in_march.tail(15)",0,No Code Smell
6989,33450719,21,"selected_countries = [
    'Italy', 'Vietnam', 'Bahrain', 'Singapore', 'Taiwan*', 'Japan', 'Kuwait', 'Korea, South', 'China']
fig2 = px.line(country_progress[country_progress.Country_Region.isin(selected_countries)],
               x='DateTime', y='ConfirmedCases', color='Country_Region')
_ = fig2.update_layout(
    yaxis_type=""log"",
    title_text=f'COVID-19 Cumulative Confirmed Cases by Country [Updated: {TRAIN_END}]'
)
fig2.show()
fig3 = px.line(country_progress[country_progress.Country_Region.isin(selected_countries)],
               x='DateTime', y='Fatalities', color='Country_Region')
_ = fig3.update_layout(
    yaxis_type=""log"",
    title_text=f'COVID-19 Cumulative Deaths by Country [Updated: {TRAIN_END}]'
)
fig3.show()",0,No Code Smell
6990,33450719,22,"train_clean['Geo#Country#Contintent'] = train_clean.Location + '#' + train_clean.Country_Region + '#' + train_clean.continent
latest = train_clean[train_clean.Date == TRAIN_END][[
    'Geo#Country#Contintent', 'ConfirmedCases', 'Fatalities', 'LogConfirmed', 'LogFatalities']]
daily_confirmed_deltas = train_clean[train_clean.Date >= '2020-03-17'].pivot(
    'Geo#Country#Contintent', 'Date', 'LogConfirmedDelta').round(3).reset_index()
daily_confirmed_deltas = latest.merge(daily_confirmed_deltas, on='Geo#Country#Contintent')
daily_confirmed_deltas.shape
daily_confirmed_deltas.head()
daily_confirmed_deltas.to_csv('daily_confirmed_deltas.csv', index=False)",0,No Code Smell
6991,33450719,23,"deltas = train_clean[np.logical_and(
        train_clean.LogConfirmed > 2,
        ~train_clean.Location.str.startswith('China')
)].dropna().sort_values(by='LogConfirmedDelta', ascending=False)

deltas['start'] = deltas['LogConfirmed'].round(0)
confirmed_deltas = pd.concat([
    deltas.groupby('start')[['LogConfirmedDelta']].mean(),
    deltas.groupby('start')[['LogConfirmedDelta']].std(),
    deltas.groupby('start')[['LogConfirmedDelta']].count()
], axis=1)

deltas.mean()

confirmed_deltas.columns = ['avg', 'std', 'cnt']
confirmed_deltas
confirmed_deltas.to_csv('confirmed_deltas.csv')",0,No Code Smell
6992,33450719,24,"fig = px.box(deltas,  x=""start"", y=""LogConfirmedDelta"", range_y=[0, 0.35])
fig.show()",0,No Code Smell
6993,33450719,25,"fig = px.box(deltas[deltas.Date >= '2020-03-01'],  x=""DateTime"", y=""LogConfirmedDelta"", range_y=[0, 0.6])
fig.update_layout(
    width = 1600,
    height = 800,
)
fig.show()",0,No Code Smell
6994,33450719,26,"deltas = train_clean[np.logical_and(
        train_clean.LogConfirmed > 0,
        ~train_clean.Location.str.startswith('China')
)].dropna().sort_values(by='LogConfirmedDelta', ascending=False)
deltas = deltas[deltas['Date'] >= '2020-03-12']

confirmed_deltas = pd.concat([
    deltas.groupby('Location')[['LogConfirmedDelta']].mean(),
    deltas.groupby('Location')[['LogConfirmedDelta']].std(),
    deltas.groupby('Location')[['LogConfirmedDelta']].count(),
    deltas.groupby('Location')[['LogConfirmed']].max()
], axis=1)
confirmed_deltas.columns = ['avg', 'std', 'cnt', 'max']

confirmed_deltas.sort_values(by='avg').head(10)
confirmed_deltas.sort_values(by='avg').tail(10)
confirmed_deltas.to_csv('confirmed_deltas.csv')",0,No Code Smell
6995,33450719,27,"end = dt.datetime.now()
print('Finished', end, (end - start).seconds, 's')",0,No Code Smell
7689,38265550,0,"import os
import pandas as pd
import tensorflow as tf
import tensorflow_hub as hub
import matplotlib.pyplot as plt
from six import BytesIO
import numpy as np
import xml.etree.ElementTree as et
import ast
import tqdm
from itertools import chain
from xml.dom import minidom
from PIL import Image
from PIL import ImageColor
from PIL import ImageDraw
from PIL import ImageFont
from PIL import ImageOps
import cv2
import glob
import time",1,Code Smell
7690,38265550,1,path1='/kaggle/input/open-images-object-detection-rvc-2020/test/',0,No Code Smell
7691,38265550,2,"sample = pd.read_csv(""/kaggle/input/open-images-object-detection-rvc-2020/sample_submission.csv"")
sample.head()",1,Code Smell
7692,38265550,3,sample.shape,0,No Code Smell
7693,38265550,4,"ids = []
for i in range(len(sample)):
    ids.append(sample['ImageId'][i])",0,No Code Smell
7694,38265550,5,ids[0:5],0,No Code Smell
7695,38265550,6,"img_data=[]
for i in range(len(sample)):
    img_data.append(glob.glob('/kaggle/input/open-images-object-detection-rvc-2020/test/{0}.jpg'.format(ids[i])))",0,No Code Smell
7696,38265550,7,img_data[0:5],0,No Code Smell
7697,38265550,8,img_data=list(chain.from_iterable(img_data)),0,No Code Smell
7698,38265550,9,img_data[0:5],0,No Code Smell
7699,38265550,10,"def get_prediction_string(result):
    with tf.device('/device:GPU:0'):
        df = pd.DataFrame(columns=['Ymin','Xmin','Ymax', 'Xmax','Score','Label','Class_label','Class_name'])
        min_score=0.01
        for i in range(result['detection_boxes'].shape[0]):
           if (result[""detection_scores""][i]) >= min_score:
              df.loc[i]= tuple(result['detection_boxes'][i])+(result[""detection_scores""][i],)+(result[""detection_class_labels""][i],)+(result[""detection_class_names""][i],)+(result[""detection_class_entities""][i],)
        return df",0,No Code Smell
7700,38265550,11,"import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()",1,Code Smell
7701,38265550,12,"module_handle = ""https://tfhub.dev/google/faster_rcnn/openimages_v4/inception_resnet_v2/1""
with tf.device('/device:GPU:0'):
    with tf.Graph().as_default():
        detector = hub.Module(module_handle)
        image_string_placeholder = tf.placeholder(tf.string)
        decoded_image = tf.image.decode_jpeg(image_string_placeholder)
        decoded_image_float = tf.image.convert_image_dtype(
            image=decoded_image, dtype=tf.float32)
        module_input = tf.expand_dims(decoded_image_float, 0)
        result = detector(module_input, as_dict=True)
        init_ops = [tf.global_variables_initializer(), tf.tables_initializer()]

        session = tf.Session()
        session.run(init_ops)",0,No Code Smell
7702,38265550,13,"def nms(dets, thresh):
    x1 = dets[:, 0]
    y1 = dets[:, 1]
    x2 = dets[:, 2]
    y2 = dets[:, 3]
    scores = dets[:, 4]

    areas = (x2 - x1 + 1) * (y2 - y1 + 1)
    order = scores.argsort()[::-1]

    keep = []
    while order.size > 0:
        i = order[0]
        keep.append(i)
        xx1 = np.maximum(x1[i], x1[order[1:]])
        yy1 = np.maximum(y1[i], y1[order[1:]])
        xx2 = np.minimum(x2[i], x2[order[1:]])
        yy2 = np.minimum(y2[i], y2[order[1:]])

        w = np.maximum(0.0, xx2 - xx1 + 1)
        h = np.maximum(0.0, yy2 - yy1 + 1)
        inter = w * h
        ovr = inter / (areas[i] + areas[order[1:]] - inter)

        inds = np.where(ovr <= thresh)[0]
        order = order[inds + 1]

    return keep",0,No Code Smell
7703,38265550,14,image_paths = img_data[0:20],0,No Code Smell
7704,38265550,15,"images = []
for f in image_paths:
    images.append(np.asarray(Image.open(f)))",0,No Code Smell
7705,38265550,16,!mkdir deepak,0,No Code Smell
7706,38265550,17,"image_id = sample['ImageId']
def format_prediction_string(image_id, result):
    prediction_strings = []
    
    for i in range(len(result['Score'])):
        class_name = result['Class_label'][i].decode(""utf-8"")
        YMin,XMin,YMax,XMax = result['Ymin'][i],result['Xmin'][i],result['Ymax'][i],result['Xmax'][i]
        score = result['Score'][i]
        
        prediction_strings.append(
            f""{class_name} {score} {XMin} {YMin} {XMax} {YMax}""
        )
        
    prediction_string = "" "".join(prediction_strings)

    return {
        ""PredictionString"": prediction_string
    }",0,No Code Smell
7707,38265550,18,"k =-1
predictions = []
with tf.device('/device:GPU:0'):
    for image_path in image_paths:
        k=k+1
        img_path = img_data[k]
        img = cv2.imread(img_path)
        with tf.gfile.Open(image_path, ""rb"") as binfile:
            image_string = binfile.read()

        inference_start_time = time.time()
        result_out, image_out = session.run(
            [result, decoded_image],
            feed_dict={image_string_placeholder: image_string})
        df1=get_prediction_string(result_out)
        z1=nms(df1.values,0.68)
        z=df1.iloc[z1]
        z=z.reset_index()
        predictions.append(format_prediction_string(image_id, z))
        data1=z
        COLORS = np.random.uniform(0, 255, size=(len(z['Class_name']), 3))
        for m in range(len(data1)):
            if data1['Score'][m] >=0.01:
                img_class=data1.iloc[m].Class_name
                img_xmax, img_ymax =images[k].shape[1],images[k].shape[0]
                bbox_x_max, bbox_x_min = data1.Xmax[m] * img_xmax, data1.Xmin[m] * img_xmax
                bbox_y_max ,bbox_y_min = data1.Ymax[m] * img_ymax, data1.Ymin[m] * img_ymax
                xmin = int(bbox_x_min)
                ymin = int(bbox_y_min)
                xmax = int(bbox_x_max)
                ymax = int(bbox_y_max)
                width = xmax - xmin
                height = ymax - ymin
                label = str(data1['Class_name'][m])
                color = COLORS[m]
                cv2.rectangle(img, (xmin, ymax), (xmax, ymin), color, 2)
                path1 = '/kaggle/working/deepak/'+str(k)+'.jpg'
                img_path = path1
                cv2.imwrite(path1, img)
                cv2.putText(img, label, (xmax,ymin), cv2.FONT_HERSHEY_SIMPLEX, 0.9,color, 2)",0,No Code Smell
7708,38265550,19,"def load_images(folder):
    images = []
    for filename in os.listdir(folder):
        img = Image.open(os.path.join(folder, filename))
        if img is not None:
            images.append(img)
    return images",0,No Code Smell
7709,38265550,20,"z = load_images(""/kaggle/working/deepak"")",1,Code Smell
7710,38265550,21,z[0],0,No Code Smell
7711,38265550,22,z[3],0,No Code Smell
7712,38265550,23,z[4],0,No Code Smell
7713,38265550,24,z[6],0,No Code Smell
7714,38265550,25,z[9],0,No Code Smell
7715,38265550,26,z[10],0,No Code Smell
7716,38265550,27,z[11],0,No Code Smell
7717,38265550,28,z[15],0,No Code Smell
7718,38265550,29,z[18],0,No Code Smell
7719,38265550,30,"pred_df = pd.DataFrame(predictions)
pred_df.head()",0,No Code Smell
7720,38265550,31,sample['PredictionString']= pred_df['PredictionString'],0,No Code Smell
7721,38265550,32,sample.head(),0,No Code Smell
7776,36234681,0,"import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import numpy as np #
import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))
        if filename.find(""rain"")>0:
            print('reading train')            
            train=pd.read_csv(os.path.join(dirname, filename) )
        if filename.find(""est"")>0:
            print('reading test')
            test=pd.read_csv(os.path.join(dirname, filename) )
",1,Code Smell
7777,36234681,1,"trainr=train
train",0,No Code Smell
7778,36234681,2,"from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer
cv = TfidfVectorizer()#CountVectorizer()
train_tf=cv.fit_transform(train.ItemId.fillna(''))
count_df=pd.DataFrame(cv.transform(train['ItemId']).toarray(), columns=cv.get_feature_names())
words=count_df.sum()
words=pd.DataFrame(words,columns=['pos'])
words",0,No Code Smell
7779,36234681,3,"from sklearn.metrics.pairwise import cosine_similarity

item_item=cosine_similarity(train_tf.T,train_tf.T)
item_item",1,Code Smell
7780,36234681,4,np.asarray(train.iloc[3]['ItemId'].split(' ')),0,No Code Smell
7781,36234681,5,"recommended=pd.DataFrame(item_item.T*cv.transform(train.iloc[0:1].ItemId).T,index=cv.get_feature_names()).sort_values(0,ascending=False)[:100]
#1938 490 128 1197 2893 2983 1861 1307 2547 231...
recommended=[xi for xi in recommended.index if xi not in np.asarray(train.iloc[0]['ItemId'].split(' '))]
' '.join(recommended)",0,No Code Smell
7782,36234681,6,"for xi in range(len(train)):
    if xi/100==int(xi/100):
        print(xi)
    recommended=pd.DataFrame(item_item.T*cv.transform(train.iloc[xi:xi+1].ItemId).T,index=cv.get_feature_names()).sort_values(0,ascending=False)[:100]
    recommended=[ri for ri in recommended.index if ri not in np.asarray(train.loc[xi].ItemId.split(' '))]
    predi=' '.join(recommended)
    train.iat[xi,1]=predi
    
train",0,No Code Smell
7783,36234681,7,"train.to_csv('submit.csv',index=False)",0,No Code Smell
7784,36234681,8,"user_user=cosine_similarity(train_tf,train_tf)
user_user",0,No Code Smell
7785,36234681,9,"train_tf[:,1]",0,No Code Smell
7786,36234681,10,"    sameuser=pd.DataFrame(user_user*train_tf[:,1]).sort_values(0,ascending=False)[:3]
    sameuser",0,No Code Smell
7787,36234681,11,user_user[0][1:].max(),1,Code Smell
7788,36234681,12,"comarket=(user_user*train_tf)+(item_item.T*train_tf.T).T
",0,No Code Smell
7789,36234681,13,comarket.shape,0,No Code Smell
7790,36234681,14,"for xi in range(train_tf.shape[0]):
    if xi/100==int(xi/100):
        print(xi)
    
    recommended=pd.DataFrame(comarket[xi],index=cv.get_feature_names()).sort_values(0,ascending=False)[:100]
    recommended=[ri for ri in recommended.index if ri not in np.asarray(trainr.loc[xi]['ItemId'].split(' '))]
    predi=' '.join(recommended)
    train.iat[xi,1]=predi
    ",0,No Code Smell
7791,36234681,15,"train.to_csv('submit2.csv',index=False)",0,No Code Smell
7792,35883312,0,"# import json
# import pandas as pd

# def convert_to_df(json_filepath, filename):
#     with open(json_filepath) as json_file:
#         data = json.load(json_file)
        
    
#     train_annotations = pd.DataFrame(data['annotations'])
#     train_images = pd.DataFrame(data['images'])
#     train_categories = pd.DataFrame(data['categories'])

#     categories_dict = pd.Series(train_categories.name.values,index=train_categories.id).to_dict() # for mapping
#     train_images.columns = ['file_name', 'image_id'] # for merging
    
#     df = pd.merge(train_annotations, train_images, on='image_id', how='left')
    
#     df['file_name'] = df['file_name'].apply(lambda x: '/kaggle/input/til2020/train/train/' + x)
    
#     df['xmin'] = df['bbox'].apply(lambda x: int(x[0]))
#     df['ymin'] = df['bbox'].apply(lambda x: int(x[1]))

#     df['xmax'] = df['bbox'].apply(lambda x: int(x[0] +  x[2]))
#     df['ymax'] = df['bbox'].apply(lambda x: int(x[1] + x[3]))
#     df['class'] = df['category_id'].apply(lambda x: categories_dict[x])
#     df = df.iloc[:, 6:]
#     df.to_csv(filename, index=False)
    
#     return df",0,No Code Smell
7793,35883312,1,"# convert_to_df('/kaggle/input/til2020/train.json', 'train_df.csv').head()",0,No Code Smell
7794,35883312,2,"# def convert_to_df(json_filepath, filename):
#     with open(json_filepath) as json_file:
#         data = json.load(json_file)
        
    
#     train_annotations = pd.DataFrame(data['annotations'])
#     train_images = pd.DataFrame(data['images'])
#     train_categories = pd.DataFrame(data['categories'])

#     categories_dict = pd.Series(train_categories.name.values,index=train_categories.id).to_dict() # for mapping
#     train_images.columns = ['file_name', 'image_id'] # for merging
    
#     df = pd.merge(train_annotations, train_images, on='image_id', how='left')
    
#     df['file_name'] = df['file_name'].apply(lambda x: '/kaggle/input/til2020/val/val/' + x)
    
#     df['xmin'] = df['bbox'].apply(lambda x: int(x[0]))
#     df['ymin'] = df['bbox'].apply(lambda x: int(x[1]))

#     df['xmax'] = df['bbox'].apply(lambda x: int(x[0] +  x[2]))
#     df['ymax'] = df['bbox'].apply(lambda x: int(x[1] + x[3]))
    
#     df['class'] = df['category_id'].apply(lambda x: categories_dict[x])
#     df = df.iloc[:, 6:]
#     df.to_csv(filename, index=False)
    
#     return df",0,No Code Smell
7795,35883312,3,"# convert_to_df('/kaggle/input/til2020/val.json', 'val_df.csv').head()",0,No Code Smell
7796,35883312,4,# data['categories'],0,No Code Smell
7797,35883312,5,# !git clone https://github.com/xuannianz/EfficientDet.git,0,No Code Smell
7798,35883312,6,"# import os

# os.chdir('/kaggle/input/efficientdet-til/EfficientDet')",1,Code Smell
7799,35883312,7,!cp -r /kaggle/input/efficientdet-til/EfficientDet /kaggle/working,0,No Code Smell
7800,35883312,8,"import os
os.chdir('/kaggle/working/EfficientDet')

!ls",1,Code Smell
7801,35883312,9,!pip install numpy --user,0,No Code Smell
7802,35883312,10,!pip install . --user,0,No Code Smell
7803,35883312,11,!python setup.py build_ext --inplace,0,No Code Smell
7804,35883312,12,# !pip install -r requirements.txt,0,No Code Smell
7805,35883312,13,!pip install progressbar2 ,0,No Code Smell
7806,35883312,14,!python train.py --snapshot imagenet --phi 0 --gpu 0 --weighted-bifpn --epochs 10 --random-transform --compute-val-loss --batch-size 2 --steps 4113 csv /kaggle/input/til-df/train_df.csv /kaggle/input/til-df/class.csv --val /kaggle/input/til-df/val_df.csv,0,No Code Smell
7807,35883312,15,"import tensorflow
tensorflow.test.is_gpu_available()
print(tensorflow.__version__)",0,No Code Smell
7808,35883312,16,,0,No Code Smell
8705,41034990,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only ""../input/"" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session",1,Code Smell
8706,41034990,1,"trainData=pd.read_csv(""/kaggle/input/mobile-price-range-prediction-is2020-v2/train_data.csv"")
trainData",1,Code Smell
8707,41034990,2,"testData=pd.read_csv(""/kaggle/input/mobile-price-range-prediction-is2020-v2/test_data.csv"")
testData",1,Code Smell
8708,41034990,3,"submissionData=pd.read_csv(""/kaggle/input/mobile-price-range-prediction-is2020-v2/sample_submission.csv"")
submissionData",1,Code Smell
8709,41034990,4,"x_train=trainData.drop(columns=['price_range','id'])
y_train=trainData['price_range']
x_test=testData.drop(columns=['id'])",0,No Code Smell
8710,41034990,5,"print(x_train,y_train,x_test,sep=""\n"")",0,No Code Smell
8711,41034990,6,"from sklearn.preprocessing import StandardScaler as ss
x_trainscale =ss().fit_transform(x_train)
x_testscale =ss().fit_transform(x_test)",0,No Code Smell
8712,41034990,7,"from sklearn.linear_model import LogisticRegression as lr
from sklearn.ensemble import RandomForestClassifier as rfc
from sklearn.model_selection import cross_val_score as cvs
r = rfc().fit(x_trainscale,y_train)
y_pred=r.predict(x_testscale)
v=cvs(rfc(),x_trainscale,y_train,cv=3)
v",0,No Code Smell
8713,41034990,8,"res=pd.DataFrame({'id':testData['id'],'price_range':y_pred})",0,No Code Smell
8714,41034990,9,"res.to_csv('/kaggle/working/result_dtc.csv',index=False)",0,No Code Smell
8715,41034990,10,"reg=lr(C=1000,penalty='l2')
reg.fit(x_trainscale,y_train)
y_pred=reg.predict(x_testscale)
value=cvs(lr(),x_trainscale,y_train,cv=3)
print(value)",0,No Code Smell
8716,41034990,11,"res2=pd.DataFrame({'id':testData['id'],'price_range':y_pred})
res2.to_csv('/kaggle/working/result_lr.csv',index=False)",0,No Code Smell
8717,41034990,12,,0,No Code Smell
8718,40629572,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only ""../input/"" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session",1,Code Smell
8719,40629572,1,"trainData=pd.read_csv(""/kaggle/input/mobile-price-range-prediction-is2020-v2/train_data.csv"")
trainData",1,Code Smell
8720,40629572,2,"testData=pd.read_csv(""/kaggle/input/mobile-price-range-prediction-is2020-v2/test_data.csv"")
testData",1,Code Smell
8721,40629572,3,"submissionData=pd.read_csv(""/kaggle/input/mobile-price-range-prediction-is2020-v2/sample_submission.csv"")
submissionData",1,Code Smell
8722,40629572,4,"x_train=trainData.drop(columns=['price_range','id'])
y_train=trainData['price_range']
x_test=testData.drop(columns=['id'])",0,No Code Smell
8723,40629572,5,"print(x_train,y_train,x_test,sep=""\n"")",0,No Code Smell
8724,40629572,6,"from sklearn.preprocessing import StandardScaler as ss
x_trainscale =ss().fit_transform(x_train)
x_testscale =ss().fit_transform(x_test)",0,No Code Smell
8725,40629572,7,"from sklearn.linear_model import LogisticRegression as lr
from sklearn.ensemble import RandomForestClassifier as rfc
from sklearn.model_selection import cross_val_score as cvs
r = rfc().fit(x_trainscale,y_train)
y_pred=r.predict(x_testscale)
v=cvs(rfc(),x_trainscale,y_train,cv=3)
v",0,No Code Smell
8726,40629572,8,"res=pd.DataFrame({'id':testData['id'],'price_range':y_pred})",0,No Code Smell
8727,40629572,9,"reg=lr(C=1000,penalty='l2')
reg.fit(x_trainscale,y_train)
y_pred=reg.predict(x_testscale)
value=cvs(lr(),x_trainscale,y_train,cv=3)
print(value)",0,No Code Smell
8728,40629572,10,"res2=pd.DataFrame({'id':testData['id'],'price_range':y_pred})
res2.to_csv('/kaggle/working/result_lr.csv',index=False)",0,No Code Smell
8729,40629982,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only ""../input/"" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session",1,Code Smell
8730,40629982,1,"trainData=pd.read_csv(""/kaggle/input/mobile-price-range-prediction-is2020-v2/train_data.csv"")
trainData",1,Code Smell
8731,40629982,2,"testData=pd.read_csv(""/kaggle/input/mobile-price-range-prediction-is2020-v2/test_data.csv"")
testData",1,Code Smell
8732,40629982,3,"submissionData=pd.read_csv(""/kaggle/input/mobile-price-range-prediction-is2020-v2/sample_submission.csv"")
submissionData",1,Code Smell
8733,40629982,4,"x_train=trainData.drop(columns=['price_range','id'])
y_train=trainData['price_range']
x_test=testData.drop(columns=['id'])",0,No Code Smell
8734,40629982,5,"print(x_train,y_train,x_test,sep=""\n"")",0,No Code Smell
8735,40629982,6,"from sklearn.preprocessing import StandardScaler as ss
x_trainscale =ss().fit_transform(x_train)
x_testscale =ss().fit_transform(x_test)",0,No Code Smell
8736,40629982,7,"from sklearn.linear_model import LogisticRegression as lr
from sklearn.ensemble import RandomForestClassifier as rfc
from sklearn.model_selection import cross_val_score as cvs
r = rfc().fit(x_trainscale,y_train)
y_pred=r.predict(x_testscale)
v=cvs(rfc(),x_trainscale,y_train,cv=3)
v",0,No Code Smell
8737,40629982,8,"res=pd.DataFrame({'id':testData['id'],'price_range':y_pred})",0,No Code Smell
8738,40629982,9,"reg=lr(C=1000,penalty='l2')
reg.fit(x_trainscale,y_train)
y_pred=reg.predict(x_testscale)
value=cvs(lr(),x_trainscale,y_train,cv=3)
print(value)",0,No Code Smell
8739,40629982,10,"res2=pd.DataFrame({'id':testData['id'],'price_range':y_pred})
res2.to_csv('/kaggle/working/result_lr.csv',index=False)",0,No Code Smell
8740,40630451,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only ""../input/"" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session",1,Code Smell
8741,40630451,1,"trainData=pd.read_csv(""/kaggle/input/mobile-price-range-prediction-is2020-v2/train_data.csv"")
trainData",1,Code Smell
8742,40630451,2,"testData=pd.read_csv(""/kaggle/input/mobile-price-range-prediction-is2020-v2/test_data.csv"")
testData",1,Code Smell
8743,40630451,3,"submissionData=pd.read_csv(""/kaggle/input/mobile-price-range-prediction-is2020-v2/sample_submission.csv"")
submissionData",1,Code Smell
8744,40630451,4,"x_train=trainData.drop(columns=['price_range','id'])
y_train=trainData['price_range']
x_test=testData.drop(columns=['id'])",0,No Code Smell
8745,40630451,5,"print(x_train,y_train,x_test,sep=""\n"")",0,No Code Smell
8746,40630451,6,"from sklearn.preprocessing import StandardScaler as ss
x_trainscale =ss().fit_transform(x_train)
x_testscale =ss().fit_transform(x_test)",0,No Code Smell
8747,40630451,7,"from sklearn.linear_model import LogisticRegression as lr
from sklearn.ensemble import RandomForestClassifier as rfc
from sklearn.model_selection import cross_val_score as cvs
r = rfc().fit(x_trainscale,y_train)
y_pred=r.predict(x_testscale)
v=cvs(rfc(),x_trainscale,y_train,cv=3)
v",0,No Code Smell
8748,40630451,8,"res=pd.DataFrame({'id':testData['id'],'price_range':y_pred})",0,No Code Smell
8749,40630451,9,"reg=lr(C=1000,penalty='l2')
reg.fit(x_trainscale,y_train)
y_pred=reg.predict(x_testscale)
value=cvs(lr(),x_trainscale,y_train,cv=3)
print(value)",0,No Code Smell
8750,40630451,10,"res2=pd.DataFrame({'id':testData['id'],'price_range':y_pred})
res2.to_csv('/kaggle/working/result_lr.csv',index=False)",0,No Code Smell
8751,41060706,0,"import pandas as pd
df=pd.read_csv('/kaggle/input/mobile-price-range-prediction-is2020-v2/train_data.csv')
test_data=pd.read_csv('/kaggle/input/mobile-price-range-prediction-is2020-v2/test_data.csv')
sample_submission=pd.read_csv('../input/mobile-price-range-prediction-is2020-v2/sample_submission.csv')
#how does ram effected by price
import seaborn as sns
sns.jointplot(x='ram',y='price_range',data=df,color='red',kind='kde');

# Internal Memory vs PriceRange
sns.pointplot(y=""int_memory"", x=""price_range"", data=df)

# % of phones with support 3G
import matplotlib.pyplot as plt
labels = [""3G-supported"",'Not supported']
values=df['three_g'].value_counts().values
fig1, ax1 = plt.subplots()
ax1.pie(values, labels=labels, autopct='%1.1f%%',shadow=True,startangle=90)
plt.show()

# % of phones with support 4G
labels = [""4G-supported"",'Not supported']
values=df['four_g'].value_counts().values
fig1, ax1 = plt.subplots()
ax1.pie(values, labels=labels, autopct='%1.1f%%',shadow=True,startangle=90)
plt.show()

# Battery power vs Price_range
sns.boxplot(x=""price_range"", y=""battery_power"", data=df)

# No of Phones vs Camera megapixels of front and primary camera
plt.figure(figsize=(10,6))
df['fc'].hist(alpha=0.5,color='blue',label='Front camera')
df['pc'].hist(alpha=0.5,color='red',label='Primary camera')
plt.legend()
plt.xlabel('MegaPixels')

# MobileWeight vs PriceRange
sns.jointplot(x='mobile_wt',y='price_range',data=df,kind='kde');

# Talktime vs PriceRange
sns.pointplot(y=""talk_time"", x=""price_range"", data=df)

#Algorithms
x_train=df.drop(columns=['price_range','id'])
y_train=df['price_range']
x_test=test_data.drop(columns=['id'])
print(x_train,y_train,x_test,sep=""\n"")
from sklearn.preprocessing import StandardScaler as ss
x_trainscale=ss().fit_transform(x_train)
x_testscale=ss().fit_transform(x_test)


from sklearn.linear_model import LogisticRegression as lr
from sklearn.ensemble import RandomForestClassifier as rfc
from sklearn.tree import DecisionTreeClassifier as dtc
from sklearn.neighbors import KNeighborsClassifier as knn
from sklearn.svm import SVC as svm
from sklearn.naive_bayes import GaussianNB as nvb
from sklearn.model_selection import cross_val_score as cvs

r=rfc().fit(x_trainscale,y_train)
y_pred=r.predict(x_testscale)
v=cvs(rfc(),x_trainscale,y_train,cv=3)
v

res=pd.DataFrame({'id':test_data['id'],'price_range':y_pred})
res.to_csv('/kaggle/working/res_rfc.csv',index=False)

reg=lr(C=1000,penalty='l2')
reg.fit(x_trainscale,y_train)
y_pred=reg.predict(x_testscale)
value=cvs(lr(),x_trainscale,y_train,cv=3)
print(value)

res2=pd.DataFrame({'id':test_data['id'],'price_range':y_pred})
res2.to_csv('/kaggle/working/res_lr.csv',index=False)

#decision tree algorithm


r=dtc().fit(x_trainscale,y_train)
y_pred=r.predict(x_testscale)
v=cvs(dtc(),x_trainscale,y_train,cv=3)
v


res3=pd.DataFrame({'id':test_data['id'],'price_range':y_pred})
res3.to_csv('/kaggle/working/res_dtc.csv',index=False)

#KNeighborsClassifier algorithm


r=rfc().fit(x_trainscale,y_train)
y_pred=r.predict(x_testscale)
v=cvs(rfc(),x_trainscale,y_train,cv=3)
v

res4=pd.DataFrame({'id':test_data['id'],'price_range':y_pred})
res4.to_csv('/kaggle/working/res_knn.csv',index=False)

#support vector machine algorithm


r=rfc().fit(x_trainscale,y_train)
y_pred=r.predict(x_testscale)
v=cvs(rfc(),x_trainscale,y_train,cv=3)
v

res5=pd.DataFrame({'id':test_data['id'],'price_range':y_pred})
res5.to_csv('/kaggle/working/res_svm.csv',index=False)

#naive bayes algorithm


r=rfc().fit(x_trainscale,y_train)
y_pred=r.predict(x_testscale)
v=cvs(rfc(),x_trainscale,y_train,cv=3)
v

res6=pd.DataFrame({'id':test_data['id'],'price_range':y_pred})
res6.to_csv('/kaggle/working/res_nvb.csv',index=False)
",0,No Code Smell
8752,41062064,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only ""../input/"" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session",1,Code Smell
8753,41062064,1,"import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline",0,No Code Smell
8754,41062064,2,dataset=pd.read_csv('../input/mobile-price-range-prediction-is2020-v2/train_data.csv'),1,Code Smell
8755,41062064,3,dataset.head(),0,No Code Smell
8756,41062064,4,dataset.info(),1,Code Smell
8757,41062064,5,dataset.describe(),1,Code Smell
8758,41062064,6,"sns.jointplot(x='ram',y='price_range',data=dataset,color='red',kind='kde');",0,No Code Smell
8759,41062064,7,"sns.pointplot(y=""int_memory"", x=""price_range"", data=dataset)",0,No Code Smell
8760,41062064,8,"labels = [""3G-supported"",'Not supported']
values=dataset['three_g'].value_counts().values",0,No Code Smell
8761,41062064,9,"fig1, ax1 = plt.subplots()
ax1.pie(values, labels=labels, autopct='%1.1f%%',shadow=True,startangle=90)
plt.show()
",0,No Code Smell
8762,41062064,10,"labels4g = [""4G-supported"",'Not supported']
values4g = dataset['four_g'].value_counts().values
fig1, ax1 = plt.subplots()
ax1.pie(values4g, labels=labels4g, autopct='%1.1f%%',shadow=True,startangle=90)
plt.show()",0,No Code Smell
8763,41062064,11,"sns.boxplot(x=""price_range"", y=""battery_power"", data=dataset)",0,No Code Smell
8764,41062064,12,"plt.figure(figsize=(10,6))
dataset['fc'].hist(alpha=0.5,color='blue',label='Front camera')
dataset['pc'].hist(alpha=0.5,color='red',label='Primary camera')
plt.legend()
plt.xlabel('MegaPixels')",0,No Code Smell
8765,41062064,13,"sns.jointplot(x='mobile_wt',y='price_range',data=dataset,kind='kde');",0,No Code Smell
8766,41062064,14,"sns.pointplot(y=""talk_time"", x=""price_range"", data=dataset)",0,No Code Smell
8767,41062064,15,"X=dataset.drop('price_range',axis=1)",0,No Code Smell
8768,41062064,16,y=dataset['price_range'],1,Code Smell
8769,41062064,17,from sklearn.model_selection import train_test_split,1,Code Smell
8770,41062064,18,"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=101)",0,No Code Smell
8771,41062064,19,"from sklearn.linear_model import LinearRegression
lm = LinearRegression()",0,No Code Smell
8772,41062064,20,"lm.fit(X_train,y_train)",0,No Code Smell
8773,41062064,21,"lm.score(X_test,y_test)",0,No Code Smell
8774,41062064,22,"from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=10)
knn.fit(X_train,y_train)",0,No Code Smell
8775,41062064,23,"knn.score(X_test,y_test)",0,No Code Smell
8776,41062064,24,"error_rate = []
for i in range(1,20):
    
    knn = KNeighborsClassifier(n_neighbors=i)
    knn.fit(X_train,y_train)
    pred_i = knn.predict(X_test)
    error_rate.append(np.mean(pred_i != y_test))",0,No Code Smell
8777,41062064,25,"plt.figure(figsize=(10,6))
plt.plot(range(1,20),error_rate,color='blue', linestyle='dashed', marker='o',
         markerfacecolor='red', markersize=5)
plt.title('Error Rate vs. K Value')
plt.xlabel('K')
plt.ylabel('Error Rate')",0,No Code Smell
8778,41062064,26,"from sklearn.linear_model import LogisticRegression
logmodel = LogisticRegression()
",0,No Code Smell
8779,41062064,27,"logmodel.fit(X_train,y_train)",0,No Code Smell
8780,41062064,28,"logmodel.score(X_test,y_test)",0,No Code Smell
8781,41062064,29,"from sklearn.tree import DecisionTreeClassifier
dtree = DecisionTreeClassifier()",0,No Code Smell
8782,41062064,30,"dtree.fit(X_train,y_train)",0,No Code Smell
8783,41062064,31,"dtree.score(X_test,y_test)",0,No Code Smell
8784,41062064,32,"feature_names=['battery_power', 'blue', 'clock_speed', 'dual_sim', 'fc', 'four_g',
       'int_memory', 'm_dep', 'mobile_wt', 'n_cores', 'pc', 'px_height',
       'px_width', 'ram', 'sc_h', 'sc_w', 'talk_time', 'three_g',
       'touch_screen', 'wifi']",0,No Code Smell
8785,41062064,33,"#For tree Visualization as kaggle does't support pydotplus just install the pydotplus in your systems's conda terminal
'''
import pydotplus as pydot

from IPython.display import Image

from sklearn.externals.six import StringIO

dot_data = StringIO()

tree.export_graphviz(dtree, out_file=dot_data,feature_names=feature_names)

graph = pydot.graph_from_dot_data(dot_data.getvalue())

Image(graph.create_png())'''",0,No Code Smell
8786,41062064,34,"#Another way
'''from IPython.display import Image  
from sklearn.externals.six import StringIO  
from sklearn.tree import export_graphviz
import pydot 
import os
os.environ[""PATH""] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'
dot_data = StringIO()  
export_graphviz(dtree, out_file=dot_data,feature_names=feature_names,filled=True)

graph = pydot.graph_from_dot_data(dot_data.getvalue())  
Image(graph[0].create_png())'''  ",0,No Code Smell
8787,41062064,35,"from sklearn.ensemble import RandomForestClassifier
rfc = RandomForestClassifier(n_estimators=200)
rfc.fit(X_train, y_train)",0,No Code Smell
8788,41062064,36,"rfc.score(X_test,y_test)",0,No Code Smell
8789,41062064,37,"y_pred=lm.predict(X_test)
",0,No Code Smell
8790,41062064,38,"plt.scatter(y_test,y_pred)
",0,No Code Smell
8791,41062064,39,"plt.plot(y_test,y_pred)",0,No Code Smell
8792,41062064,40,"from sklearn.metrics import classification_report,confusion_matrix",1,Code Smell
8793,41062064,41,pred = knn.predict(X_test),0,No Code Smell
8794,41062064,42,"print(classification_report(y_test,pred))",0,No Code Smell
8795,41062064,43,"matrix=confusion_matrix(y_test,pred)
print(matrix)",0,No Code Smell
8796,41062064,44,"plt.figure(figsize = (10,7))
sns.heatmap(matrix,annot=True)
",0,No Code Smell
8797,41062064,45,data_test=pd.read_csv('../input/mobile-price-range-prediction-is2020-v2/test_data.csv'),1,Code Smell
8798,41062064,46,data_test.head(),0,No Code Smell
8799,41062064,47,"data_test=data_test.drop('id',axis=1)",0,No Code Smell
8800,41062064,48,data_test.head(),0,No Code Smell
8801,41062064,49,predicted_price=knn.predict(data_test),0,No Code Smell
8802,41062064,50,predicted_price,0,No Code Smell
8803,41062064,51,data_test['price_range']=predicted_price,0,No Code Smell
8804,41062064,52,data_test,0,No Code Smell
8805,39999420,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only ""../input/"" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session
import warnings
warnings.filterwarnings('ignore')",1,Code Smell
8806,39999420,1,"train=pd.read_csv(""/kaggle/input/mobile-price-range-prediction-is2020-v2/train_data.csv"")
test=pd.read_csv(""/kaggle/input/mobile-price-range-prediction-is2020-v2/test_data.csv"")
submissiond=pd.read_csv(""/kaggle/input/mobile-price-range-prediction-is2020-v2/sample_submission.csv"")",1,Code Smell
8807,39999420,2,train.info(),1,Code Smell
8808,39999420,3,test.info(),1,Code Smell
8809,39999420,4,"x_train=train.drop(columns=['price_range','id'])
x_test=test.drop(columns=['id'])
y_train=train['price_range']",0,No Code Smell
8810,39999420,5,"print("" x train:{} y train{}"".format(x_train,y_train))

",0,No Code Smell
8811,39999420,6,"y_train.value_counts()

",0,No Code Smell
8812,39999420,7,print(x_test),0,No Code Smell
8813,39999420,8,"import sklearn 
from sklearn.preprocessing import StandardScaler 
x_trains=StandardScaler().fit_transform(x_train)
x_tests=StandardScaler().fit_transform(x_test)
",0,No Code Smell
8814,39999420,9,"print(""X trains {}"".format(x_trains))
print(""X tests {}"".format(x_tests))
",1,Code Smell
8815,39999420,10,"pd.DataFrame(x_trains).head()
",0,No Code Smell
8816,39999420,11,pd.DataFrame(x_tests).head(),0,No Code Smell
8817,39999420,12,"from sklearn.linear_model import LogisticRegression as l
#from sklearn.ensemble import RandomForestClassifier as l
LR=l().fit(x_trains,y_train)
y_pred=LR.predict(x_tests)
",0,No Code Smell
8818,39999420,13,"from sklearn.model_selection import cross_val_score as c
#from sklearn.metrics import classification_report
val=c(l(),x_trains,y_train,cv=3,scoring='accuracy')
print(val)",0,No Code Smell
8819,39999420,14,print(val.mean()),0,No Code Smell
8820,39999420,15,"result=pd.DataFrame({'id':test['id'],'price_range':y_pred})
result.to_csv('/kaggle/working/result_rf4.csv',index=False)",0,No Code Smell
8821,41079129,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only ""../input/"" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session
import warnings
warnings.filterwarnings(""ignore"")",1,Code Smell
8822,41079129,1,"train=pd.read_csv(""/kaggle/input/mobile-price-range-prediction-is2020-v2/train_data.csv"")
test=pd.read_csv(""/kaggle/input/mobile-price-range-prediction-is2020-v2/test_data.csv"")
sample_submission=pd.read_csv(""/kaggle/input/mobile-price-range-prediction-is2020-v2/sample_submission.csv"")",1,Code Smell
8823,41079129,2,train.head(),0,No Code Smell
8824,41079129,3,test.head(),0,No Code Smell
8825,41079129,4,"print(train.shape,test.shape)",0,No Code Smell
8826,41079129,5,train.info(),1,Code Smell
8827,41079129,6,test.info(),1,Code Smell
8828,41079129,7,train.isnull().sum(),0,No Code Smell
8829,41079129,8,test.isnull().sum(),0,No Code Smell
8830,41079129,9,"train_clean =train.drop(columns=[""id""])
test_clean=test.drop(columns=[""id""])",0,No Code Smell
8831,41079129,10,train_clean,0,No Code Smell
8832,41079129,11,test_clean,0,No Code Smell
8833,41079129,12,"x_train=train_clean.drop(columns=[""price_range""])
y_train=train_clean[""price_range""]
print(y_train.shape)
print(type(y_train))",0,No Code Smell
8834,41079129,13,"import sklearn
from sklearn.preprocessing import StandardScaler
x_train_std=StandardScaler().fit_transform(x_train)
pd.DataFrame(x_train_std).head()",0,No Code Smell
8835,41079129,14,"from sklearn.linear_model import LogisticRegression
lr=LogisticRegression().fit(x_train_std,y_train)",0,No Code Smell
8836,41079129,15,y_pred=lr.predict(test_clean),0,No Code Smell
8837,41079129,16,"from sklearn.metrics import classification_report
print(classification_report(y_pred,sample_submission[""price_range""]))",0,No Code Smell
8838,41079129,17,"from sklearn.model_selection import cross_val_score
scores=cross_val_score(LogisticRegression(C=1),x_train_std,y_train,cv=5)
print(scores)
print(scores.mean())",0,No Code Smell
8839,41079129,18,"from sklearn.model_selection import GridSearchCV
grid={""C"":[0.6,0.7,0.8,0.9,1],""penalty"":[""l1"",""l2""]}
grid",0,No Code Smell
8840,41079129,19,"score=GridSearchCV(LogisticRegression(solver='liblinear'),grid).fit(x_train_std,y_train)
print(score.best_params_)
print(score.best_score_)",0,No Code Smell
8841,41079129,20,test_clean_std= StandardScaler().fit_transform(test_clean),0,No Code Smell
8842,41079129,21,"lr=LogisticRegression(solver=""liblinear"",penalty='l1',C=0.9).fit(x_train_std,y_train)
y_pred=lr.predict(test_clean_std)",0,No Code Smell
8843,41079129,22,lr.predict_proba(test_clean_std),0,No Code Smell
8844,41079129,23,"final_output = test.assign(price_range = y_pred)[['id','price_range']]",0,No Code Smell
8845,41079129,24,final_output,0,No Code Smell
8846,41079129,25,"result=pd.DataFrame(final_output)
result.to_csv(""/kaggle/working/result.lr.csv"",index=False)",0,No Code Smell
8847,41079129,26,"from sklearn.ensemble import RandomForestClassifier
RF=RandomForestClassifier(criterion='gini',n_estimators=25,random_state=0)
RF=RF.fit(x_train_std,y_train)
y_pred_ref=RF.predict(test_clean)
from sklearn.metrics import classification_report
scores=cross_val_score(RandomForestClassifier(),x_train,y_train,cv=5)
print(scores)
print(scores.mean())",0,No Code Smell
8848,41079129,27,"model=RandomForestClassifier().fit(x_train,y_train)
y_pred_RF=model.predict(test_clean)
final_output_RF = test.assign(price_range = y_pred_RF)[['id','price_range']]
final_output_RF",0,No Code Smell
8849,41079129,28,"result1=pd.DataFrame(final_output_RF)
result1.to_csv(""/kaggle/working/result.rf.csv"",index=False)",0,No Code Smell
8850,41079129,29,"from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import cross_val_score
dtc=DecisionTreeClassifier(criterion=""gini"",max_depth=4,random_state=0)
scores=cross_val_score(dtc,x_train,y_train,cv=5)
print(scores)
print(scores.mean())",0,No Code Smell
8851,41079129,30,"model1=dtc.fit(x_train,y_train)
y_pred_dtc=model1.predict(test_clean)
final_output_dtc = test.assign(price_range = y_pred_dtc)[['id','price_range']]
final_output_dtc",0,No Code Smell
8852,41079129,31,"result2=pd.DataFrame(final_output_dtc)
result2.to_csv(""/kaggle/working/result.dtc.csv"",index=False)",0,No Code Smell
8853,41079129,32,"from sklearn.svm import SVC
from sklearn.model_selection import cross_val_score
svc=SVC(kernel='linear',C=1,random_state=0)
scores=cross_val_score(svc,x_train_std,y_train,cv=5)
print(scores)
print(scores.mean())",0,No Code Smell
8854,41079129,33,"model2=svc.fit(x_train_std,y_train)
y_pred_svm=model2.predict(test_clean_std)
final_output_svm = test.assign(price_range = y_pred_svm)[['id','price_range']]
final_output_svm",0,No Code Smell
8855,41079129,34,"result3=pd.DataFrame(final_output_svm)
result3.to_csv(""/kaggle/working/result.svm.csv"",index=False)",0,No Code Smell
8856,41079129,35,"from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import cross_val_score
knn = KNeighborsClassifier(n_neighbors=5,metric='minkowski',p=2)
knn=knn.fit(x_train_std,y_train)
scores=cross_val_score(knn,x_train_std,y_train,cv=5)
print(scores)
print(scores.mean())",0,No Code Smell
8857,41079129,36,"model3=knn.fit(x_train_std,y_train)
y_pred_knn=model3.predict(test_clean_std)
final_output_knn = test.assign(price_range = y_pred_knn)[['id','price_range']]
final_output_knn",0,No Code Smell
8858,41079129,37,"result4=pd.DataFrame(final_output_knn)
result4.to_csv(""/kaggle/working/result.knn.csv"",index=False)",0,No Code Smell
8859,41079129,38,"from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
nb=GaussianNB()
nb=nb.fit(x_train_std,y_train)
scores=cross_val_score(svc,x_train_std,y_train,cv=5)
print(scores)
print(scores.mean())",0,No Code Smell
8860,41079129,39,"model4=nb.fit(x_train_std,y_train)
y_pred_nb=model4.predict(test_clean_std)
final_output_nb = test.assign(price_range = y_pred_nb)[['id','price_range']]
final_output_nb",0,No Code Smell
8861,41079129,40,"result5=pd.DataFrame(final_output_nb)
result5.to_csv(""/kaggle/working/result.nb.csv"",index=False)",0,No Code Smell
8862,41952353,0,"train = pd.read_csv('/kaggle/input/mobile-price-range-prediction-is2020-v2/train_data.csv')
test = pd.read_csv('/kaggle/input/mobile-price-range-prediction-is2020-v2/test_data.csv')
submissions = pd.read_csv('/kaggle/input/mobile-price-range-prediction-is2020-v2/sample_submission.csv')

train_clean = train.drop(columns=['id'])
test_clean = test.drop(columns=['id'])

x_train = train_clean.drop(columns=['price_range'])
y_train = train_clean[['price_range']]

x_test = test_clean

from sklearn.preprocessing import StandardScaler
x_train_scale = StandardScaler().fit_transform(x_train)

x_test_scale = StandardScaler().fit_transform(x_test)


from sklearn.model_selection import cross_val_score

y = np.array(y_train)
y = y.ravel()

from sklearn.linear_model import LogisticRegression
model = LogisticRegression()

from sklearn.model_selection import StratifiedKFold

accuracy = []
skf = StratifiedKFold(n_splits=15, random_state=None)
skf.get_n_splits(x_train_scale, y)

for train_index, test_index in skf.split(x_train_scale, y):
  x1_train, x1_test = x_train_scale[train_index], x_train_scale[test_index]
  y1_train, y1_test = y[train_index], y[test_index]

  model.fit(x1_train, y1_train)  
  score = model.score(x1_test, y1_test)
  accuracy.append(score)

print(accuracy)


y_pred = model.predict(x_test_scale)

from sklearn.metrics import classification_report
print(classification_report(y_pred, submissions['price_range']))

data = {'id':submissions['id'],
       'price_range':y_pred}
results = pd.DataFrame(data)
results.to_csv('/kaggle/working/results_lr.csv', index=False)",1,Code Smell
8863,41952353,1,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only ""../input/"" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session",1,Code Smell
8864,44285437,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only ""../input/"" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session",1,Code Smell
8865,44285437,1,"train=pd.read_csv('/kaggle/input/mobile-price-range-prediction-is2020-v2/train_data.csv')
test=pd.read_csv('/kaggle/input/mobile-price-range-prediction-is2020-v2/test_data.csv')
price_sub=pd.read_csv('/kaggle/input/mobile-price-range-prediction-is2020-v2/sample_submission.csv')",1,Code Smell
8866,44285437,2,train.head(),0,No Code Smell
8867,44285437,3,test.head(),0,No Code Smell
8868,44285437,4,train.info(),1,Code Smell
8869,44285437,5,test.info(),1,Code Smell
8870,44285437,6,train.isnull().sum(),0,No Code Smell
8871,44285437,7,test.isnull().sum(),0,No Code Smell
8872,44285437,8,train.describe(),0,No Code Smell
8873,44285437,9,"import seaborn as sns
sns.pointplot(y=""talk_time"",x=""price_range"",data=train)",0,No Code Smell
8874,44285437,10,"sns.pointplot(y=""battery_power"",x=""price_range"",data=train)",0,No Code Smell
8875,44285437,11,"sns.pointplot(y=""ram"",x=""price_range"",data=train)",0,No Code Smell
8876,44285437,12,"sns.pointplot(y=""int_memory"",x=""price_range"",data=train)",0,No Code Smell
8877,44285437,13,"sns.pointplot(y=""mobile_wt"",x=""price_range"",data=train)",0,No Code Smell
8878,44285437,14,"sns.pointplot(y=""dual_sim"",x=""price_range"",data=train)",0,No Code Smell
8879,44285437,15,"sns.pointplot(y=""four_g"",x=""price_range"",data=train)",0,No Code Smell
8880,44285437,16,"sns.pointplot(y=""three_g"",x=""price_range"",data=train)",0,No Code Smell
8881,44285437,17,train_clean=train.drop(columns=['id']),0,No Code Smell
8882,44285437,18,"x_train=train_clean.drop(columns=['price_range'])
y_train=train_clean['price_range']",0,No Code Smell
8883,44285437,19,data_test=test.drop(columns=['id']),0,No Code Smell
8884,44285437,20,data_test.head(),0,No Code Smell
8885,44285437,21,"import matplotlib.pyplot as plt
plt.figure(figsize=(10,6))
train['fc'].hist(alpha=0.5,color='blue',label='Front camera')

plt.legend()
plt.xlabel('MegaPixels')",0,No Code Smell
8886,44285437,22,"import matplotlib.pyplot as plt
plt.figure(figsize=(10,6))

train['pc'].hist(alpha=0.5,color='red',label='Primary camera')
plt.legend()
plt.xlabel('MegaPixels')",0,No Code Smell
8887,44285437,23,"plt.figure(figsize=(10,6))
train['fc'].hist(alpha=0.5,color='blue',label='Front camera')
train['pc'].hist(alpha=0.5,color='red',label='Primary camera')
plt.legend()
plt.xlabel('MegaPixels')",0,No Code Smell
8888,44285437,24,"sns.pointplot(y=""dual_sim"",x=""price_range"",data=train)",0,No Code Smell
8889,44285437,25,"sns.pointplot(y=""four_g"",x=""price_range"",data=train)",0,No Code Smell
8890,44285437,26,train_clean=train.drop(columns=['id']),0,No Code Smell
8891,44285437,27,data_test.head(),0,No Code Smell
8892,44285437,28,"from sklearn.preprocessing import StandardScaler
x_train_scales=StandardScaler().fit_transform(x_train)
pd.DataFrame(x_train_scales).head()
",0,No Code Smell
8893,44285437,29,x_train.head(),0,No Code Smell
8894,44285437,30,y_train.head(),0,No Code Smell
8895,44285437,31,"from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import cross_val_score
knn=KNeighborsClassifier(n_neighbors=10).fit(x_train,y_train)
scores=cross_val_score(knn,x_train_scales,y_train,cv=5)
print(scores)
print(scores.mean())",0,No Code Smell
8896,44285437,32,"from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import cross_val_score
dtc=DecisionTreeClassifier().fit(x_train,y_train)
scores=cross_val_score(dtc,x_train_scales,y_train,cv=5)
print(scores)
print(scores.mean())",0,No Code Smell
8897,44285437,33,"price_pred_knn=knn.predict(data_test)
print(price_pred_knn)",0,No Code Smell
8898,44285437,34,"price_pred_dtc=dtc.predict(data_test)
print(price_pred_dtc)",0,No Code Smell
8899,44285437,35,"data = {'id':price_sub['id'],'price_range':price_pred_knn}
results_knn = pd.DataFrame(data)
results_knn.to_csv('/kaggle/working/results_knn.csv', index=False)",0,No Code Smell
8900,44285437,36,"data = {'id':price_sub['id'],'price_range':price_pred_dtc}
results_dtc = pd.DataFrame(data)
results_dtc.to_csv('/kaggle/working/results_dtc.csv', index=False)",0,No Code Smell
8901,41038003,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only ""../input/"" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session",1,Code Smell
8902,41038003,1,"import pandas as pd
import numpy as np",1,Code Smell
8903,41038003,2,"train=pd.read_csv(""/kaggle/input/mobile-price-range-prediction-is2020-v2/train_data.csv"")
test=pd.read_csv(""/kaggle/input/mobile-price-range-prediction-is2020-v2/test_data.csv"")
submission=pd.read_csv(""/kaggle/input/mobile-price-range-prediction-is2020-v2/sample_submission.csv"")",1,Code Smell
8904,41038003,3,train.head(),0,No Code Smell
8905,41038003,4,train.columns,0,No Code Smell
8906,41038003,5,train.info(),1,Code Smell
8907,41038003,6,"y = train['price_range']
x = train.drop('price_range', axis = 1)",0,No Code Smell
8908,41038003,7,y.unique(),0,No Code Smell
8909,41038003,8,"from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test= train_test_split(x, y, test_size = 0.2, random_state = 101)",0,No Code Smell
8910,41038003,9,"print(x_train.shape)
print(x_test.shape)",0,No Code Smell
8911,41038003,10,"import matplotlib.pyplot as plt
import seaborn as sns",1,Code Smell
8912,41038003,11,"plt.figure(figsize=(12,12))
sns.heatmap(train.corr(),annot=True, cmap=""GnBu"")
plt.show()",0,No Code Smell
8913,41038003,12,from sklearn.linear_model import LogisticRegression,1,Code Smell
8914,41038003,13,"lr =LogisticRegression()
lr.fit(x_train,y_train)
y_pred_lr=lr.predict(x_test)
y_pred_lr",0,No Code Smell
8915,41038003,14,"from sklearn import metrics
from sklearn.metrics import confusion_matrix, classification_report 
from sklearn.metrics import accuracy_score",1,Code Smell
8916,41038003,15,"confusion_matrix = metrics.confusion_matrix(y_test, y_pred_lr)
confusion_matrix",0,No Code Smell
8917,41038003,16,"print(metrics.classification_report(y_test, y_pred_lr))",0,No Code Smell
8918,41038003,17,"acc_lr = metrics.accuracy_score(y_test, y_pred_lr)
acc_lr",0,No Code Smell
8919,41038003,18,from sklearn.tree import DecisionTreeClassifier,1,Code Smell
8920,41038003,19,"dt = DecisionTreeClassifier(random_state=101)
dt.fit(x_train, y_train)
y_pred_dt = dt.predict(x_test)",0,No Code Smell
8921,41038003,20,"confusion_matrix = metrics.confusion_matrix(y_test, y_pred_dt)
confusion_matrix",0,No Code Smell
8922,41038003,21,"print(metrics.classification_report(y_test, y_pred_dt))",0,No Code Smell
8923,41038003,22,"acc_dt = metrics.accuracy_score(y_test, y_pred_dt)
acc_dt",0,No Code Smell
8924,41038003,23,from sklearn.ensemble import RandomForestClassifier,1,Code Smell
8925,41038003,24,"rf = DecisionTreeClassifier(random_state=0)
rf.fit(x_train, y_train)
y_pred_rf = rf.predict(x_test)",0,No Code Smell
8926,41038003,25,"confusion_matrix = metrics.confusion_matrix(y_test, y_pred_rf)
confusion_matrix",0,No Code Smell
8927,41038003,26,"print(metrics.classification_report(y_test, y_pred_rf))",0,No Code Smell
8928,41038003,27,"acc_rf = metrics.accuracy_score(y_test, y_pred_rf)
acc_rf",0,No Code Smell
8929,41038003,28,from sklearn.svm import SVC,1,Code Smell
8930,41038003,29,"svc = SVC()
svc.fit(x_train, y_train)
y_pred_svc = svc.predict(x_test)",0,No Code Smell
8931,41038003,30,"confusion_matrix = metrics.confusion_matrix(y_test, y_pred_svc)
confusion_matrix",0,No Code Smell
8932,41038003,31,"print(metrics.classification_report(y_test, y_pred_svc))",0,No Code Smell
8933,41038003,32,"acc_svc = metrics.accuracy_score(y_test, y_pred_svc)
acc_svc",0,No Code Smell
8934,41038003,33,"from sklearn.neighbors import KNeighborsClassifier
model_knn = KNeighborsClassifier(n_neighbors=3)  
model_knn.fit(x_train, y_train)",0,No Code Smell
8935,41038003,34,y_pred_knn = model_knn.predict(x_test),0,No Code Smell
8936,41038003,35,"print(metrics.confusion_matrix(y_test, y_pred_knn))",0,No Code Smell
8937,41038003,36,"print(accuracy_score(y_test, y_pred_knn))",0,No Code Smell
8938,41038003,37,"from sklearn.model_selection import GridSearchCV
parameters = {'n_neighbors':np.arange(1,30)}
knn = KNeighborsClassifier()

model = GridSearchCV(knn, parameters, cv=5)
model.fit(x_train, y_train)
model.best_params_",0,No Code Smell
8939,41038003,38,"knn = KNeighborsClassifier(n_neighbors=26)
knn.fit(x_train, y_train)
y_pred_knn = knn.predict(x_test)",0,No Code Smell
8940,41038003,39,"confusion_matrix = metrics.confusion_matrix(y_test, y_pred_knn)
confusion_matrix",0,No Code Smell
8941,41038003,40,"print(metrics.classification_report(y_test, y_pred_knn))",0,No Code Smell
8942,41038003,41,"acc_knn = metrics.accuracy_score(y_test, y_pred_knn)
acc_knn",0,No Code Smell
8943,41038003,42,"models = ['logistic regression', 'decision tree', 'random forest', 'support vector machine','knn']
acc_scores = [acc_lr,acc_dt,acc_rf,acc_svc,acc_knn]
print(acc_scores)
plt.bar(models, acc_scores, color=['lightblue', 'pink', 'lightgrey', 'cyan','lightgreen'])
plt.ylabel(""accuracy scores"")
plt.title(""Which model is the most accurate?"")
plt.show()",0,No Code Smell
8944,41038003,43,"predicted_price=svc.predict(test)
predicted_price",0,No Code Smell
8945,41038003,44,test['price_range']=predicted_price,0,No Code Smell
8946,41038003,45,"data={'Id':test['id'],'price_range':predicted_price}
result=pd.DataFrame(data)
result.to_csv('/kaggle/working/prediction.csv',index=False)",0,No Code Smell
8947,44284275,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only ""../input/"" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session",1,Code Smell
8948,44284275,1,"train=pd.read_csv('/kaggle/input/mobile-price-range-prediction-is2020-v2/train_data.csv')
test=pd.read_csv('/kaggle/input/mobile-price-range-prediction-is2020-v2/test_data.csv')
price_sub=pd.read_csv('/kaggle/input/mobile-price-range-prediction-is2020-v2/sample_submission.csv')",1,Code Smell
8949,44284275,2,train.head(),0,No Code Smell
8950,44284275,3,test.head(),0,No Code Smell
8951,44284275,4,train.info(),1,Code Smell
8952,44284275,5,test.info(),1,Code Smell
8953,44284275,6,test.isnull().sum(),0,No Code Smell
8954,44284275,7,train.describe(),0,No Code Smell
8955,44284275,8,"import seaborn as sns
sns.pointplot(y=""talk_time"",x=""price_range"",data=train)",0,No Code Smell
8956,44284275,9,"sns.pointplot(y=""battery_power"",x=""price_range"",data=train)",0,No Code Smell
8957,44284275,10,"sns.pointplot(y=""ram"",x=""price_range"",data=train)",0,No Code Smell
8958,44284275,11,"sns.pointplot(y=""int_memory"",x=""price_range"",data=train)",0,No Code Smell
8959,44284275,12,"sns.pointplot(y=""mobile_wt"",x=""price_range"",data=train)",0,No Code Smell
8960,44284275,13,"sns.pointplot(y=""dual_sim"",x=""price_range"",data=train)",0,No Code Smell
8961,44284275,14,"sns.pointplot(y=""four_g"",x=""price_range"",data=train)",0,No Code Smell
8962,44284275,15,"sns.pointplot(y=""three_g"",x=""price_range"",data=train)",0,No Code Smell
8963,44284275,16,train_clean=train.drop(columns=['id']),0,No Code Smell
8964,44284275,17,"x_train=train_clean.drop(columns=['price_range'])
y_train=train_clean['price_range']",0,No Code Smell
8965,44284275,18,data_test=test.drop(columns=['id']),0,No Code Smell
8966,44284275,19,data_test.head(),0,No Code Smell
8967,44284275,20,"import matplotlib.pyplot as plt
plt.figure(figsize=(10,6))
train['fc'].hist(alpha=0.5,color='blue',label='Front camera')

plt.legend()
plt.xlabel('MegaPixels')",0,No Code Smell
8968,44284275,21,"import matplotlib.pyplot as plt
plt.figure(figsize=(10,6))

train['pc'].hist(alpha=0.5,color='red',label='Primary camera')
plt.legend()
plt.xlabel('MegaPixels')",0,No Code Smell
8969,44284275,22,"plt.figure(figsize=(10,6))
train['fc'].hist(alpha=0.5,color='blue',label='Front camera')
train['pc'].hist(alpha=0.5,color='red',label='Primary camera')
plt.legend()
plt.xlabel('MegaPixels')

",0,No Code Smell
8970,44284275,23,"sns.pointplot(y=""dual_sim"",x=""price_range"",data=train)",0,No Code Smell
8971,44284275,24,"sns.pointplot(y=""four_g"",x=""price_range"",data=train)",0,No Code Smell
8972,44284275,25,data_test.head(),0,No Code Smell
8973,44284275,26,"from sklearn.preprocessing import StandardScaler
x_train_scales=StandardScaler().fit_transform(x_train)
pd.DataFrame(x_train_scales).head()",0,No Code Smell
8974,44284275,27,x_train.head(),0,No Code Smell
8975,44284275,28,"from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import cross_val_score
knn = KNeighborsClassifier(n_neighbors=10).fit(x_train,y_train)
scores=cross_val_score(knn,x_train_scales,y_train,cv=5)
print(scores)
print(scores.mean())",0,No Code Smell
8976,44284275,29,"from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import cross_val_score
dtc = DecisionTreeClassifier().fit(x_train,y_train)
scores=cross_val_score(dtc,x_train_scales,y_train,cv=5)
print(scores)
print(scores.mean())",0,No Code Smell
8977,44284275,30,"price_pred_dtc=dtc.predict(data_test)
print(price_pred_dtc)",0,No Code Smell
8978,44284275,31,"price_pred_knn=knn.predict(data_test)
print(price_pred_knn)",0,No Code Smell
8979,44284275,32,"data={'id':price_sub['id'],'price_range':price_pred_knn}
result_knn=pd.DataFrame(data)
result_knn.to_csv('/kaggle/working/result_knn.csv',index=False)
",0,No Code Smell
8980,44284275,33,"data={'id':price_sub['id'],'price_range':price_pred_dtc}
result_dtc=pd.DataFrame(data)
result_dtc.to_csv('/kaggle/working/result_dtc.csv',index=False)
",0,No Code Smell
8981,41954265,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only ""../input/"" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session",1,Code Smell
8982,41954265,1,"import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline",0,No Code Smell
8983,41954265,2,"dataset=pd.read_csv('../input/mobile-price-range-prediction-is2020-v2/train_data.csv')
dataset=dataset.drop('id',axis=1)
dataset.info()
dataset.describe()",1,Code Smell
8984,41954265,3,"sns.jointplot(x='ram',y='price_range',data=dataset,kind='kde');",0,No Code Smell
8985,41954265,4,"sns.pointplot(y=""int_memory"", x=""price_range"", data=dataset)",0,No Code Smell
8986,41954265,5,"labels = [""3G-supported"",'Not supported']
values=dataset['three_g'].value_counts().values",0,No Code Smell
8987,41954265,6,"fig1, ax1 = plt.subplots()
ax1.pie(values, labels=labels, autopct='%1.1f%%',shadow=True,startangle=90)
plt.show()",0,No Code Smell
8988,41954265,7,"labels4g = [""4G-supported"",'Not supported']
values4g = dataset['four_g'].value_counts().values
fig1, ax1 = plt.subplots()
ax1.pie(values4g, labels=labels4g, autopct='%1.1f%%',shadow=True,startangle=90)
plt.show()",0,No Code Smell
8989,41954265,8,"sns.boxplot(x=""price_range"", y=""battery_power"", data=dataset)",0,No Code Smell
8990,41954265,9,"plt.figure(figsize=(10,6))
dataset['fc'].hist(alpha=0.5,color='blue',label='Front camera')
dataset['pc'].hist(alpha=0.5,color='red',label='Primary camera')
plt.legend()
plt.xlabel('MegaPixels')",0,No Code Smell
8991,41954265,10,"sns.jointplot(x='mobile_wt',y='price_range',data=dataset,kind='kde');",0,No Code Smell
8992,41954265,11,"sns.pointplot(y=""talk_time"", x=""price_range"", data=dataset)",0,No Code Smell
8993,41954265,12,"X=dataset.drop('price_range',axis=1)",0,No Code Smell
8994,41954265,13,y=dataset['price_range'],1,Code Smell
8995,41954265,14,"from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=101)",0,No Code Smell
8996,41954265,15,"from sklearn.linear_model import LinearRegression
lm = LinearRegression()
lm.fit(X_train,y_train)
lm.score(X_test,y_test)",0,No Code Smell
8997,41954265,16,"from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=10)
knn.fit(X_train,y_train)",0,No Code Smell
8998,41954265,17,"knn.score(X_test,y_test)",0,No Code Smell
8999,41954265,18,"error_rate = []
for i in range(1,20):
    
    knn = KNeighborsClassifier(n_neighbors=i)
    knn.fit(X_train,y_train)
    pred_i = knn.predict(X_test)
    error_rate.append(np.mean(pred_i != y_test))
",0,No Code Smell
9000,41954265,19,"plt.figure(figsize=(10,6))
plt.plot(range(1,20),error_rate,color='blue', linestyle='dashed', marker='o',
         markerfacecolor='red', markersize=5)
plt.title('Error Rate vs. K Value')
plt.xlabel('K')
plt.ylabel('Error Rate')",0,No Code Smell
9001,41954265,20,"from sklearn.linear_model import LogisticRegression
logmodel = LogisticRegression()
",0,No Code Smell
9002,41954265,21,"logmodel.fit(X_train,y_train)",0,No Code Smell
9003,41954265,22,"logmodel.score(X_test,y_test)",0,No Code Smell
9004,41954265,23,"from sklearn.tree import DecisionTreeClassifier
dtree = DecisionTreeClassifier()",0,No Code Smell
9005,41954265,24,"dtree.fit(X_train,y_train)",0,No Code Smell
9006,41954265,25,"dtree.score(X_test,y_test)",0,No Code Smell
9007,41954265,26,"feature_names=['battery_power', 'blue', 'clock_speed', 'dual_sim', 'fc', 'four_g',
       'int_memory', 'm_dep', 'mobile_wt', 'n_cores', 'pc', 'px_height',
       'px_width', 'ram', 'sc_h', 'sc_w', 'talk_time', 'three_g',
       'touch_screen', 'wifi']",0,No Code Smell
9008,41954265,27,"from sklearn.ensemble import RandomForestClassifier
rfc = RandomForestClassifier(n_estimators=200)
rfc.fit(X_train, y_train)",0,No Code Smell
9009,41954265,28,"rfc.score(X_test,y_test)",0,No Code Smell
9010,41954265,29,y_pred=lm.predict(X_test),0,No Code Smell
9011,41954265,30,"plt.scatter(y_test,y_pred)",0,No Code Smell
9012,41954265,31,"plt.plot(y_test,y_pred)",0,No Code Smell
9013,41954265,32,"from sklearn.metrics import classification_report,confusion_matrix",1,Code Smell
9014,41954265,33,pred = knn.predict(X_test),0,No Code Smell
9015,41954265,34,"matrix=confusion_matrix(y_test,pred)
plt.figure(figsize = (10,7))
sns.heatmap(matrix,annot=True)
",0,No Code Smell
9016,41954265,35,"data_test=pd.read_csv('../input/mobile-price-range-prediction-is2020-v2/test_data.csv')
data_test.head()",1,Code Smell
9017,41954265,36,"data_test=data_test.drop('id',axis=1)",0,No Code Smell
9018,41954265,37,predicted_price_range=knn.predict(data_test),0,No Code Smell
9019,41954265,38,predicted_price_range,0,No Code Smell
9020,41954265,39,data_test['price_range']=predicted_price,0,No Code Smell
9021,41954265,40,data_test,0,No Code Smell
9022,41954265,41,,0,No Code Smell
9023,41954265,42,,0,No Code Smell
9024,39858539,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only ""../input/"" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session",1,Code Smell
9025,39858539,1,"test=pd.read_csv('/kaggle/input//mobile-price-range-prediction-is2020-v2/test_data.csv')
train=pd.read_csv('/kaggle/input//mobile-price-range-prediction-is2020-v2/train_data.csv')
sample_submission=pd.read_csv('/kaggle/input//mobile-price-range-prediction-is2020-v2/sample_submission.csv')",1,Code Smell
9026,39858539,2,test.head(),0,No Code Smell
9027,39858539,3,train.head(),0,No Code Smell
9028,39858539,4,train.shape,0,No Code Smell
9029,39858539,5,train.describe(),0,No Code Smell
9030,39858539,6,"y=train['price_range']
x=train.drop('price_range',axis=1)
",0,No Code Smell
9031,39858539,7,y.unique(),0,No Code Smell
9032,39858539,8,"import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns",0,No Code Smell
9033,39858539,9,"from sklearn.model_selection import train_test_split

x_train, x_valid, y_train, y_valid = train_test_split(x, y, test_size = 0.01, random_state = 101, stratify = y)

#x_train, x_valid, y_train, y_valid = train_test_split(x, y, test_size = 0.2, random_state = 101, stratify = y)",0,No Code Smell
9034,39858539,10,"print(x_train.shape)
print(x_valid.shape)",0,No Code Smell
9035,39858539,11,"fig = plt.subplots (figsize = (12, 12))
sns.heatmap(train.corr (), square = True, cbar = True, annot = True, cmap=""GnBu"", annot_kws = {'size': 8})
plt.title('Correlations between Attributes')
plt.show ()",0,No Code Smell
9036,39858539,12,"from sklearn.linear_model import LogisticRegression

lr = LogisticRegression(multi_class = 'multinomial', solver = 'sag',  max_iter = 10000)
lr.fit(x_train, y_train)",0,No Code Smell
9037,39858539,13,y_pred_lr = lr.predict(x_valid),0,No Code Smell
9038,39858539,14,"from sklearn import metrics
from sklearn.metrics import accuracy_score
confusion_matrix = metrics.confusion_matrix(y_valid, y_pred_lr)
confusion_matrix",0,No Code Smell
9039,39858539,15,"acc_lr = metrics.accuracy_score(y_valid, y_pred_lr)
acc_lr",0,No Code Smell
9040,39858539,16,"from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import mean_squared_error
import math
dt = DecisionTreeClassifier(random_state=101)
dt_model = dt.fit(x_train, y_train)",0,No Code Smell
9041,39858539,17,y_pred_dt = dt.predict(x_valid),0,No Code Smell
9042,39858539,18,"
dt_model",0,No Code Smell
9043,39858539,19,"
print(metrics.confusion_matrix(y_valid, y_pred_dt))",0,No Code Smell
9044,39858539,20,"print(metrics.classification_report(y_valid, y_pred_dt))",0,No Code Smell
9045,39858539,21,"acc_dt = metrics.accuracy_score(y_valid, y_pred_dt)
acc_dt",0,No Code Smell
9046,39858539,22,"from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
rf = RandomForestClassifier(n_estimators = 100, random_state=101, criterion = 'entropy', oob_score = True) 
model_rf = rf.fit(x_train, y_train)",0,No Code Smell
9047,39858539,23,y_pred_rf = rf.predict(x_valid),0,No Code Smell
9048,39858539,24,"print(metrics.confusion_matrix(y_valid, y_pred_rf))",0,No Code Smell
9049,39858539,25,"pd.crosstab(y_valid, y_pred_rf, rownames=['Actual Class'], colnames=['Predicted Class'])",0,No Code Smell
9050,39858539,26,"acc_rf = metrics.accuracy_score(y_valid, y_pred_rf)
acc_rf",0,No Code Smell
9051,39858539,27,"model_knn = KNeighborsClassifier(n_neighbors=3)  
model_knn.fit(x_train, y_train)",0,No Code Smell
9052,39858539,28,y_pred_knn = model_knn.predict(x_valid),0,No Code Smell
9053,39858539,29,"print(metrics.confusion_matrix(y_valid, y_pred_knn))",0,No Code Smell
9054,39858539,30,"print(accuracy_score(y_valid, y_pred_knn))",0,No Code Smell
9055,39858539,31,"from sklearn.model_selection import GridSearchCV
parameters = {'n_neighbors':np.arange(1,30)}
knn = KNeighborsClassifier()

model = GridSearchCV(knn, parameters, cv=5)
model.fit(x_train, y_train)
model.best_params_",0,No Code Smell
9056,39858539,32,"model_knn = KNeighborsClassifier(n_neighbors=9)  
model_knn.fit(x_train, y_train)",0,No Code Smell
9057,39858539,33,y_pred_knn = model_knn.predict(x_valid),0,No Code Smell
9058,39858539,34,"print(metrics.confusion_matrix(y_valid, y_pred_knn))",0,No Code Smell
9059,39858539,35,"acc_knn = accuracy_score(y_valid, y_pred_knn)
acc_knn",0,No Code Smell
9060,39858539,36,"models = ['logistic regression', 'decision tree', 'random forest', 'knn']
acc_scores = [0.73, 0.83, 0.90, 0.95]

plt.bar(models, acc_scores, color=['lightblue', 'pink', 'lightgrey', 'cyan'])
plt.ylabel(""accuracy scores"")
plt.title(""Which model is the most accurate?"")
plt.show()",0,No Code Smell
9061,39858539,37,test.head(),0,No Code Smell
9062,39858539,38,predicted_price_range = model_knn.predict(test),0,No Code Smell
9063,39858539,39,predicted_price_range ,0,No Code Smell
9064,39858539,40,train.head(),0,No Code Smell
9065,39858539,41,"data={'id':sample_submission['id'],
     'price_range':predicted_price_range}
result=pd.DataFrame(data)
result.to_csv(""/kaggle/working/result_12.csv"",index=False)
output=pd.read_csv('/kaggle/working/result_12.csv')",0,No Code Smell
9066,43372368,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only ""../input/"" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session
import warnings
warnings.filterwarnings('ignore')",1,Code Smell
9067,43372368,1,"train_data=pd.read_csv('../input/mobile-price-range-prediction-is2020-v2/train_data.csv')
train_data.head()",1,Code Smell
9068,43372368,2,"X=train_data.drop('price_range',axis=1).values
Y=train_data['price_range']",0,No Code Smell
9069,43372368,3,"from sklearn.model_selection import train_test_split
df_test=pd.read_csv('../input/mobile-price-prediction-is2020-v2/test_data.csv')
id_test=df_test['id']
X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.5,random_state=101)",1,Code Smell
9070,43372368,4,"from sklearn.neighbors import KNeighborsClassifier
kn=KNeighborsClassifier(n_neighbors=10)",0,No Code Smell
9071,43372368,5,"import numpy as np
error_rate=[]
for i in range(1,20):
    kn=KNeighborsClassifier(n_neighbors=i)
    kn.fit(X_train,Y_train)
    pred_i=kn.predict(X_test)
    error_rate.append(np.mean(pred_i!=Y_test))",0,No Code Smell
9072,43372368,6,"from sklearn.linear_model import LogisticRegression
logmodel=LogisticRegression()
logmodel.fit(X_train,Y_train)
Y_pred=logmodel.predict(X_test)",0,No Code Smell
9073,43372368,7,"from sklearn.metrics import classification_report,confusion_matrix
pred=kn.predict(X_test)

data_test=pd.read_csv('../input/mobile-price-range-prediction-is2020-v2/test_data.csv')
predicted_price=kn.predict(data_test)
data_test['price_range']=predicted_price

data_test=pd.DataFrame({'id':id_test,'price_range':predicted_price})
data_test.to_csv('output.csv',index=False)
data_test

",0,No Code Smell
9074,43430221,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only ""../input/"" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session
import warnings
warnings.filterwarnings('ignore')
",1,Code Smell
9075,43430221,1,"train_data=pd.read_csv('../input/mobile-price-range-prediction-is2020-v2/train_data.csv')
train_data.head()
",1,Code Smell
9076,43430221,2,train_data.info(),1,Code Smell
9077,43430221,3,"X=train_data.drop('price_range',axis=1).values
Y=train_data['price_range']",0,No Code Smell
9078,43430221,4,"from sklearn.model_selection import train_test_split
df_test=pd.read_csv('../input/mobile-price-range-prediction-is2020-v2/test_data.csv')
id_test=df_test['id']
X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.5,random_state=101)",1,Code Smell
9079,43430221,5,"from sklearn.neighbors import KNeighborsClassifier
kn=KNeighborsClassifier(n_neighbors=10)",0,No Code Smell
9080,43430221,6,"import numpy as np
error_rate=[]
for i in range(1,20):
    kn=KNeighborsClassifier(n_neighbors=i)
    kn.fit(X_train,Y_train)
    pred_i=kn.predict(X_test)
    error_rate.append(np.mean(pred_i!=Y_test))",0,No Code Smell
9081,43430221,7,"from sklearn.linear_model import LogisticRegression
logmodel=LogisticRegression()
logmodel.fit(X_train,Y_train)
Y_pred=logmodel.predict(X_test)",0,No Code Smell
9082,43430221,8,"from sklearn.metrics import classification_report,confusion_matrix
pred=kn.predict(X_test)

data_test=pd.read_csv('../input/mobile-price-range-prediction-is2020-v2/test_data.csv')
predicted_price=kn.predict(data_test)
data_test['price_range']=predicted_price

data_test=pd.DataFrame({'id':id_test,'price_range':predicted_price})
data_test.to_csv('output.csv',index=False)
data_test
",0,No Code Smell
9083,43431036,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only ""../input/"" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session
import warnings
warnings.filterwarnings('ignore')",1,Code Smell
9084,43431036,1,"train_data=pd.read_csv('../input/mobile-price-range-prediction-is2020-v2/train_data.csv')
train_data.head()",1,Code Smell
9085,43431036,2,train_data.info(),1,Code Smell
9086,43431036,3,"X=train_data.drop('price_range',axis=1).values
Y=train_data['price_range']",0,No Code Smell
9087,43431036,4,"from sklearn.model_selection import train_test_split
df_test=pd.read_csv('../input/mobile-price-range-prediction-is2020-v2/test_data.csv')
id_test=df_test['id']
X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.5,random_state=101)",1,Code Smell
9088,43431036,5,"from sklearn.neighbors import KNeighborsClassifier
kn=KNeighborsClassifier(n_neighbors=10)",0,No Code Smell
9089,43431036,6,"import numpy as np
error_rate=[]
for i in range(1,20):
    kn=KNeighborsClassifier(n_neighbors=i)
    kn.fit(X_train,Y_train)
    pred_i=kn.predict(X_test)
    error_rate.append(np.mean(pred_i!=Y_test))
    ",0,No Code Smell
9090,43431036,7,"from sklearn.linear_model import LogisticRegression
logmodel=LogisticRegression()
logmodel.fit(X_train,Y_train)
Y_pred=logmodel.predict(X_test)",0,No Code Smell
9091,43431036,8,"from sklearn.metrics import classification_report,confusion_matrix
pred=kn.predict(X_test)
data_test=pd.read_csv('../input/mobile-price-range-prediction-is2020-v2/test_data.csv')
predicted_price=kn.predict(data_test)
data_test['price_range']=predicted_price

data_test=pd.DataFrame({'id':id_test,'price_range':predicted_price})
data_test.to_csv('output.csv',index=False)
data_test",0,No Code Smell
9092,43474743,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only ""../input/"" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session",1,Code Smell
9093,43474743,1,"train_data=pd.read_csv('../input/mobile-price-range-prediction-is2020-v2/train_data.csv')
train_data.head()",1,Code Smell
9094,43474743,2,"X=train_data.drop('price_range',axis=1).values
Y=train_data['price_range']",0,No Code Smell
9095,43474743,3,"from sklearn.model_selection import train_test_split
df_test=pd.read_csv('../input/mobile-price-range-prediction-is2020-v2/test_data.csv')
id_test=df_test['id']
X_train,X_test,Y_train,Y_test=train_test_split(X,Y, test_size=0.5,random_state=101)",1,Code Smell
9096,43474743,4,"from sklearn.neighbors import KNeighborsClassifier
kn=KNeighborsClassifier(n_neighbors=10)",0,No Code Smell
9097,43474743,5,"import numpy as np
error_rate=[]
for i in range(1,20):
    kn=KNeighborsClassifier(n_neighbors=i)
    kn.fit(X_train,Y_train)
    pred_i=kn.predict(X_test)
    error_rate.append(np.mean(pred_i!=Y_test))",0,No Code Smell
9098,43474743,6,"from sklearn.linear_model import LogisticRegression
logmodel=LogisticRegression()
logmodel.fit(X_train,Y_train)",0,No Code Smell
9099,43474743,7,"from sklearn.metrics import classification_report,confusion_matrix
pred=kn.predict(X_test)

data_test=pd.read_csv('../input/mobile-price-range-prediction-is2020-v2/test_data.csv')
predicted_price=kn.predict(data_test)
data_test['price_range']=predicted_price

data_test=pd.DataFrame({'id':id_test,'price_range':predicted_price})
data_test.to_csv('output.csv',index=False)
data_test",0,No Code Smell
9100,43443958,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only ""../input/"" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session",1,Code Smell
9101,43443958,1,"import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)



import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))",1,Code Smell
9102,43443958,2,"train=pd.read_csv('/kaggle/input/mobile-price-range-prediction-is2020-v2/train_data.csv')
test=pd.read_csv('/kaggle/input/mobile-price-range-prediction-is2020-v2/test_data.csv')
sample_submission=pd.read_csv('/kaggle/input/mobile-price-range-prediction-is2020-v2/sample_submission.csv')",1,Code Smell
9103,43443958,3,train.head(),0,No Code Smell
9104,43443958,4,test.head(),0,No Code Smell
9105,43443958,5,"print(train.shape, test.shape)",0,No Code Smell
9106,43443958,6,train.info(),1,Code Smell
9107,43443958,7,test.info(),1,Code Smell
9108,43443958,8,train.isnull().sum(),0,No Code Smell
9109,43443958,9,"import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns",0,No Code Smell
9110,43443958,10,"from sklearn.svm import svm as svm
regr = SVR(C=1.0, epsilon=0.2)
regr.fit(X,y)",0,No Code Smell
9111,43443958,11,SVR(epsilon=0.2),0,No Code Smell
9112,43443958,12,"X=train.drop('price_range',axis=1).values
Y=train['price_range']",0,No Code Smell
9113,43443958,13,"from sklearn.model_selection import train_test_split
df_test=pd.read_csv('../input/mobile-price-range-prediction-is2020-v2/test_data.csv')
id_test=df_test['id']
X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.5,random_state=101)
",1,Code Smell
9114,43443958,14,"from sklearn.neighbors import KNeighborsClassifier
kn=KNeighborsClassifier(n_neighbors=10)
",0,No Code Smell
9115,43443958,15,"import numpy as np
error_rate=[]
for i in range(1,20):
    kn=KNeighborsClassifier(n_neighbors=i)
    kn.fit(X_train,Y_train)
    pred_i=kn.predict(X_test)
    error_rate.append(np.mean(pred_i!=Y_test))
    ",0,No Code Smell
9116,43443958,16,"from sklearn.linear_model import LogisticRegression
logmodel=LogisticRegression()
logmodel.fit(X_train,Y_train)
Y_pred=logmodel.predict(X_test)
",0,No Code Smell
9117,43443958,17,"from sklearn.metrics import classification_report,confusion_matrix
pred=kn.predict(X_test)
data_test=pd.read_csv('../input/mobile-price-range-prediction-is2020-v2/test_data.csv')
predicted_price=kn.predict(data_test)
data_test['price_range']=predicted_price
",1,Code Smell
9118,43443958,18,"data_test=pd.DataFrame({'price_range':predicted_price})
data_test.to_csv('output.csv',index=False)
data_test",0,No Code Smell
9119,44229684,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only ""../input/"" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session",1,Code Smell
9120,44229684,1,"trainData=pd.read_csv(""/kaggle/input/mobile-price-range-prediction-is2020-v2/train_data.csv"")
trainData",1,Code Smell
9121,44229684,2,"testData=pd.read_csv(""/kaggle/input/mobile-price-range-prediction-is2020-v2/test_data.csv"")
testData",1,Code Smell
9122,44229684,3,"submissionData=pd.read_csv(""/kaggle/input/mobile-price-range-prediction-is2020-v2/sample_submission.csv"")
submissionData",1,Code Smell
9123,44229684,4,"x_train=trainData.drop(columns=['price_range','id'])
y_train=trainData['price_range']
x_test=testData.drop(columns=['id'])
print(x_train,y_train,x_test,sep=""\n"")",0,No Code Smell
9124,44229684,5,"from sklearn.preprocessing import StandardScaler as ss
x_trainscale =ss().fit_transform(x_train)
x_testscale =ss().fit_transform(x_test)",0,No Code Smell
9125,44229684,6,"from sklearn.ensemble import RandomForestClassifier as rfc
from sklearn.linear_model import LogisticRegression as lr

from sklearn.model_selection import cross_val_score as cvs
r = rfc().fit(x_trainscale,y_train)
y_pred=r.predict(x_testscale)
v=cvs(rfc(),x_trainscale,y_train,cv=3)
res=pd.DataFrame({'id':testData['id'],'price_range':y_pred})
res.to_csv('/kaggle/working/result_dtc.csv',index=False)
reg=lr(C=1000,penalty='l2')
reg.fit(x_trainscale,y_train)
y_pred=reg.predict(x_testscale)
value=cvs(lr(),x_trainscale,y_train,cv=3)
print(value)",0,No Code Smell
9126,44229684,7,,0,No Code Smell
9127,44221495,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only ""../input/"" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session",1,Code Smell
9128,44221495,1,"trainData=pd.read_csv(""/kaggle/input/mobile-price-range-prediction-is2020-v2/train_data.csv"")
trainData",1,Code Smell
9129,44221495,2,"testData=pd.read_csv(""/kaggle/input/mobile-price-range-prediction-is2020-v2/test_data.csv"")
testData",1,Code Smell
9130,44221495,3,"submissionData=pd.read_csv(""/kaggle/input/mobile-price-range-prediction-is2020-v2/sample_submission.csv"")
submissionData",1,Code Smell
9131,44221495,4,"x_train=trainData.drop(columns=['price_range','id'])
y_train=trainData['price_range']
x_test=testData.drop(columns=['id'])
print(x_train,y_train,x_test,sep=""\n"")",0,No Code Smell
9132,44221495,5,"from sklearn.preprocessing import StandardScaler as ss
x_trainscale =ss().fit_transform(x_train)
x_testscale =ss().fit_transform(x_test)",0,No Code Smell
9133,44221495,6,"from sklearn.linear_model import LogisticRegression as lr
from sklearn.ensemble import RandomForestClassifier as rfc
from sklearn.model_selection import cross_val_score as cvs
r = rfc().fit(x_trainscale,y_train)
y_pred=r.predict(x_testscale)
v=cvs(rfc(),x_trainscale,y_train,cv=3)
res=pd.DataFrame({'id':testData['id'],'price_range':y_pred})
res.to_csv('/kaggle/working/result_dtc.csv',index=False)
reg=lr(C=1000,penalty='l2')
reg.fit(x_trainscale,y_train)
y_pred=reg.predict(x_testscale)
value=cvs(lr(),x_trainscale,y_train,cv=3)
print(value)",0,No Code Smell
9134,44221495,7,,0,No Code Smell
9135,44242087,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only ""../input/"" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session",1,Code Smell
9136,44242087,1,"import pandas as pd
trainData=pd.read_csv(""/kaggle/input/mobile-price-range-prediction-is2020-v2/train_data.csv"")
trainData
",1,Code Smell
9137,44242087,2,"testData=pd.read_csv(""/kaggle/input/mobile-price-range-prediction-is2020-v2/test_data.csv"")
testData",1,Code Smell
9138,44242087,3,"submissionData=pd.read_csv(""/kaggle/input/mobile-price-range-prediction-is2020-v2/sample_submission.csv"")
submissionData",1,Code Smell
9139,44242087,4,"x_train=trainData.drop(columns=['price_range','id'])
y_train=trainData['price_range']
x_test=testData.drop(columns=['id'])
print(x_train,y_train,x_test,sep=""\n"")",0,No Code Smell
9140,44242087,5,"from sklearn.preprocessing import StandardScaler as ss
x_trainscale =ss().fit_transform(x_train)
x_testscale =ss().fit_transform(x_test)
",0,No Code Smell
9141,44242087,6,"from sklearn.linear_model import LogisticRegression as lr
from sklearn.ensemble import RandomForestClassifier as rfc
from sklearn.model_selection import cross_val_score as cvs
r = rfc().fit(x_trainscale,y_train)
y_pred=r.predict(x_testscale)
v=cvs(rfc(),x_trainscale,y_train,cv=3)
res=pd.DataFrame({'id':testData['id'],'price_range':y_pred})
res.to_csv('/kaggle/working/result_dtc.csv',index=False)
reg=lr(C=1000,penalty='l2')
reg.fit(x_trainscale,y_train)
y_pred=reg.predict(x_testscale)
value=cvs(lr(),x_trainscale,y_train,cv=3)
print(value)",0,No Code Smell
9142,44230862,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only ""../input/"" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session",1,Code Smell
9143,44230862,1,"trainData=pd.read_csv(""/kaggle/input/mobile-price-range-prediction-is2020-v2/train_data.csv"")
trainData",1,Code Smell
9144,44230862,2,"testData=pd.read_csv(""/kaggle/input/mobile-price-range-prediction-is2020-v2/test_data.csv"")
testData",1,Code Smell
9145,44230862,3,"submissionData=pd.read_csv(""/kaggle/input/mobile-price-range-prediction-is2020-v2/sample_submission.csv"")
submissionData",1,Code Smell
9146,44230862,4,"x_train=trainData.drop(columns=['price_range','id'])
y_train=trainData['price_range']
x_test=testData.drop(columns=['id'])
print(x_train,y_train,x_test,sep=""\n"")",0,No Code Smell
9147,44230862,5,"from sklearn.preprocessing import StandardScaler as ss
x_trainscale =ss().fit_transform(x_train)
x_testscale =ss().fit_transform(x_test)",0,No Code Smell
9148,44230862,6,"from sklearn.linear_model import LogisticRegression as lr
from sklearn.ensemble import RandomForestClassifier as rfc
from sklearn.model_selection import cross_val_score as cvs
r = rfc().fit(x_trainscale,y_train)
y_pred=r.predict(x_testscale)
v=cvs(rfc(),x_trainscale,y_train,cv=3)
res=pd.DataFrame({'id':testData['id'],'price_range':y_pred})
res.to_csv('/kaggle/working/result_dtc.csv',index=False)
reg=lr(C=1000,penalty='l2')
reg.fit(x_trainscale,y_train)
y_pred=reg.predict(x_testscale)
value=cvs(lr(),x_trainscale,y_train,cv=3)
print(value)",0,No Code Smell
9149,44270080,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only ""../input/"" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session",1,Code Smell
9150,44270080,1,"trainData=pd.read_csv(""/kaggle/input/mobile-price-range-prediction-is2020-v2/train_data.csv"")
trainData",1,Code Smell
9151,44270080,2,"testData=pd.read_csv(""/kaggle/input/mobile-price-range-prediction-is2020-v2/test_data.csv"")
testData",1,Code Smell
9152,44270080,3,"submissionData=pd.read_csv(""/kaggle/input/mobile-price-range-prediction-is2020-v2/sample_submission.csv"")
submissionData",1,Code Smell
9153,44270080,4,"x_train=trainData.drop(columns=['price_range','id'])
y_train=trainData['price_range']
x_test=testData.drop(columns=['id'])
print(x_train,y_train,x_test,sep=""\n"")",0,No Code Smell
9154,44270080,5,,0,No Code Smell
9155,44270080,6,"from sklearn.preprocessing import StandardScaler as ss
x_trainscale =ss().fit_transform(x_train)
x_testscale =ss().fit_transform(x_test)",0,No Code Smell
9156,44270080,7,"from sklearn.linear_model import LogisticRegression as lr
from sklearn.ensemble import RandomForestClassifier as rfc
from sklearn.model_selection import cross_val_score as cvs
r = rfc().fit(x_trainscale,y_train)
y_pred=r.predict(x_testscale)
v=cvs(rfc(),x_trainscale,y_train,cv=3)
res=pd.DataFrame({'id':testData['id'],'price_range':y_pred})
res.to_csv('/kaggle/working/result_dtc.csv',index=False)
reg=lr(C=1000,penalty='l2')
reg.fit(x_trainscale,y_train)
y_pred=reg.predict(x_testscale)
value=cvs(lr(),x_trainscale,y_train,cv=3)
print(value)",0,No Code Smell
9157,44270080,8,,0,No Code Smell
9158,44272600,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only ""../input/"" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session",1,Code Smell
9159,44272600,1,"trainData=pd.read_csv(""/kaggle/input/mobile-price-range-prediction-is2020-v2/train_data.csv"")
trainData",1,Code Smell
9160,44272600,2,"testData=pd.read_csv(""/kaggle/input/mobile-price-range-prediction-is2020-v2/test_data.csv"")
testData",1,Code Smell
9161,44272600,3,"submissionData=pd.read_csv(""/kaggle/input/mobile-price-range-prediction-is2020-v2/sample_submission.csv"")
submissionData",1,Code Smell
9162,44272600,4,"x_train=trainData.drop(columns=['price_range','id'])
y_train=trainData['price_range']
x_test=testData.drop(columns=['id'])
print(x_train,y_train,x_test,sep=""\n"")
",0,No Code Smell
9163,44272600,5,"from sklearn.preprocessing import StandardScaler as ss
x_trainscale =ss().fit_transform(x_train)
x_testscale =ss().fit_transform(x_test)",0,No Code Smell
9164,44272600,6,"from sklearn.linear_model import LogisticRegression as lr
from sklearn.ensemble import RandomForestClassifier as rfc
from sklearn.model_selection import cross_val_score as cvs
r = rfc().fit(x_trainscale,y_train)
y_pred=r.predict(x_testscale)
v=cvs(rfc(),x_trainscale,y_train,cv=3)
res=pd.DataFrame({'id':testData['id'],'price_range':y_pred})
res.to_csv('/kaggle/working/result_dtc.csv',index=False)
reg=lr(C=1000,penalty='l2')
reg.fit(x_trainscale,y_train)
y_pred=reg.predict(x_testscale)
value=cvs(lr(),x_trainscale,y_train,cv=3)
print(value)",0,No Code Smell
9165,42014625,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only ""../input/"" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session",1,Code Smell
9166,42014625,1,"import warnings
warnings.filterwarnings('ignore')",0,No Code Smell
9167,42014625,2,"train=pd.read_csv('../input/mobile-price-range-prediction-is2020-v2/train_data.csv')
test=pd.read_csv('../input/mobile-price-range-prediction-is2020-v2/test_data.csv')
sample_submission= pd.read_csv('../input/mobile-price-range-prediction-is2020-v2/sample_submission.csv')",1,Code Smell
9168,42014625,3,train.head(),0,No Code Smell
9169,42014625,4,test.head(),0,No Code Smell
9170,42014625,5,"print(train.shape,test.shape)",0,No Code Smell
9171,42014625,6,"x_train=train.drop(['price_range','id'],axis=1)
y_train=train['price_range']
x_test=test.drop(['id'],axis=1)
x_test.head()",0,No Code Smell
9172,42014625,7,"from sklearn.linear_model import LogisticRegression
logisticRegr = LogisticRegression()
logisticRegr.fit(x_train, y_train)
y_pred = logisticRegr.predict(x_test)",0,No Code Smell
9173,42014625,8,"from sklearn.model_selection import cross_val_score
scores=cross_val_score(logisticRegr,x_train,y_train,cv=5)
print(scores)
print(scores.mean())",0,No Code Smell
9174,42014625,9,"from sklearn import svm
clf = svm.SVC()
clf.fit(x_train, y_train)
y_pred = clf.predict(x_test)
from sklearn.model_selection import cross_val_score
scores=cross_val_score(logisticRegr,x_train,y_train,cv=5)
print(scores)
print(scores.mean())",0,No Code Smell
9175,42014625,10,"data={'id':sample_submission['id'],'price_range':y_pred}
result_dtr=pd.DataFrame(data)
result_dtr.to_csv('/kaggle/working/result_svm.csv',index=False)",0,No Code Smell
9176,42014625,11,"from sklearn.tree import DecisionTreeClassifier
Dec_tree=DecisionTreeClassifier()
Dec_tree=Dec_tree.fit(x_train,y_train)
y_pred=Dec_tree.predict(x_test)
from sklearn.model_selection import cross_val_score
scores=cross_val_score(logisticRegr,x_train,y_train,cv=5)
print(scores)
print(scores.mean())",0,No Code Smell
9177,42014625,12,"data={'id':sample_submission['id'],'price_range': y_pred}
result_dtr=pd.DataFrame(data)
result_dtr.to_csv('/kaggle/working/result_dtr.csv',index=False)",0,No Code Smell
9178,42014625,13,"from sklearn.ensemble import RandomForestClassifier
Ran_forest=RandomForestClassifier()
Ran_forest=Ran_forest.fit(x_train,y_train)
y_pred=Ran_forest.predict(x_test)
from sklearn.model_selection import cross_val_score
scores=cross_val_score(logisticRegr,x_train,y_train,cv=5)
print(scores)
print(scores.mean())",0,No Code Smell
9179,42014625,14,"data={'id':sample_submission['id'],'price_range':y_pred}
result_rf=pd.DataFrame(data)
result_rf.to_csv('/kaggle/working/result_rf.csv',index=False)",0,No Code Smell
9180,39816524,0,"# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only ""../input/"" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session",1,Code Smell
9181,39816524,1,"Train_data=pd.read_csv(""/kaggle/input/mobile-price-range-prediction-is2020-v2/train_data.csv"")
Train_data.info()",1,Code Smell
9182,39816524,2,"Test_data=pd.read_csv(""/kaggle/input/mobile-price-range-prediction-is2020-v2/test_data.csv"")
Test_data.info()",1,Code Smell
9183,39816524,3,"Submission_data=pd.read_csv(""/kaggle/input/mobile-price-range-prediction-is2020-v2/sample_submission.csv"")
print(Submission_data)
print(Submission_data.head(10))",1,Code Smell
9184,39816524,4,"print(Train_data.head(10))
print(Test_data.head(10))",0,No Code Smell
9185,39816524,5,"x_train=Train_data.drop(columns=['price_range','id'])
y_train=Train_data['price_range']",0,No Code Smell
9186,39816524,6,"print(x_train,y_train,sep=""\n"")",0,No Code Smell
9187,39816524,7,"x_test=Test_data.drop(columns=['id'])
print(x_test)",0,No Code Smell
9188,39816524,8,y_train.value_counts(),0,No Code Smell
9189,39816524,9,"from sklearn.preprocessing import StandardScaler as ss
x_trainscale=ss().fit_transform(x_train)
x_testscale=ss().fit_transform(x_test)",0,No Code Smell
9190,39816524,10,pd.DataFrame(x_trainscale).head(),0,No Code Smell
9191,39816524,11,pd.DataFrame(x_testscale).head(),0,No Code Smell
9192,39816524,12,"from sklearn.linear_model import LogisticRegression as lr
lr?",0,No Code Smell
9193,39816524,13,"from sklearn.ensemble import RandomForestClassifier as rfc
from sklearn.model_selection import cross_val_score as cvs
ranfor=rfc().fit(x_trainscale,y_train)
y_prediction=ranfor.predict(x_testscale)
value=cvs(rfc(),x_trainscale,y_train,cv=3)
print(value)",0,No Code Smell
9194,39816524,14,print(value.mean()),1,Code Smell
9195,39816524,15,"res=pd.DataFrame({'id':Test_data['id'],'price_range':y_prediction})",0,No Code Smell
9196,39816524,16,"res.to_csv('/kaggle/working/result_rf.csv',index=False)",0,No Code Smell
9197,39816524,17,"from sklearn.svm import SVC
svc=SVC(kernel='linear',C=1)
y_prediction_svc=svc.fit(x_trainscale,y_train).predict(x_testscale)
value=cvs(svc,x_trainscale,y_train,cv=3)
print(value)",0,No Code Smell
9198,39816524,18,print(value.mean()),1,Code Smell
9199,39816524,19,"res1=pd.DataFrame({'id':Test_data['id'],'price_range':y_prediction_svc})
res1.to_csv('/kaggle/working/result_dtc.csv',index=False)",0,No Code Smell
9200,39816524,20,"from sklearn.model_selection import GridSearchCV
gsc={'C':np.logspace(-3,3,7),'penalty':['l1','l2']}
value=GridSearchCV(lr(),gsc).fit(x_trainscale,y_train)
import warnings
warnings.filterwarnings(""ignore"")
print(value.best_params_)
print(value.best_score_)",0,No Code Smell
9201,39816524,21,"reg=lr(C=1000,penalty='l2')
reg.fit(x_trainscale,y_train)
y_pred=reg.predict(x_testscale)
value=cvs(lr(),x_trainscale,y_train,cv=3)
print(value)",0,No Code Smell
9202,39816524,22,"res2=pd.DataFrame({'id':Test_data['id'],'price_range':y_pred})
res2.to_csv('/kaggle/working/result_lr.csv',index=False)",0,No Code Smell
9445,43694573,0,!pip install pytorch-tabnet,0,No Code Smell
9446,43694573,1,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only ""../input/"" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session",1,Code Smell
9447,43694573,2,"import pandas as pd
import numpy as np
from sklearn.metrics import f1_score,classification_report
from catboost import CatBoostClassifier,CatBoostRegressor
from sklearn.model_selection import train_test_split

from sklearn.base import BaseEstimator, ClassifierMixin

from sklearn.model_selection import cross_val_predict

import itertools
from tqdm import tqdm_notebook
import gc

from sklearn.naive_bayes import BernoulliNB
from sklearn.neighbors import KNeighborsClassifier
import lightgbm as lgb

from sklearn.linear_model import Lasso

from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler


from pytorch_tabnet.tab_model import TabNetRegressor
import torch
import warnings
from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import RandomForestRegressor",1,Code Smell
9448,43694573,3,"data = pd.read_csv('/kaggle/input/mf-accelerator/contest_train.csv')
target = data.TARGET
data = data.fillna(0)",1,Code Smell
9449,43694573,4,"features = data.drop(columns=[""TARGET"",""ID""])
features.head()",0,No Code Smell
9450,43694573,5,"data_test = pd.read_csv('/kaggle/input/mf-accelerator/contest_test.csv')
features_test = data_test.copy().drop(columns=[""ID""])
features_test.head()",1,Code Smell
9451,43694573,6,"features_train,features_val,labels_train,labels_val = train_test_split(features,target, test_size = 0.3,\
                                                                   shuffle=True,random_state=1,\
                                                                   stratify = target)
features_train.head()",0,No Code Smell
9452,43694573,7,"class Stacking(BaseEstimator, ClassifierMixin):  
    """"""Стекинг моделей 
    на основе материалов А. Дьяконова
    """"""
    

    def __init__(self, models, metamodel,merge=False):
        """"""
        Инициализация
        models - базовые модели для стекинга
        metamodel - метамодель
        """"""
        self.models = models
        self.metamodel = metamodel
        self.n = len(models)
        self.meta = None
        self.merge = merge


    def fit(self, X, y=None, p=0.25, random_state=0):
        """"""
        Обучение стекинга

        p - в каком отношении делить на выборку 
        на подвыборки для базовых и метаалгоритма
        random_state - для воспроизводимости
        merge - слить полученные признаки и исходные при работе метаалгоритма    
        """"""
        # разбиение на обучение моделей и метамодели
        base, meta, y_base, y_meta = train_test_split(X, y, test_size=p, random_state=random_state,stratify = y)
            
        # заполнение матрицы для обучения метамодели
        self.meta = np.zeros((meta.shape[0], self.n))
        for t, base_model in enumerate(self.models):
            base_model.fit(np.array(base), np.array(y_base))
                
            self.meta[:, t] = base_model.predict(meta).reshape((1,-1))#reshape для работы нейросетей и катбуста
            print(f""Ok {t}"")

        # обучение метамодели
        if self.merge:#если обучаем метамодель на объединенной выборке с исходными признаками и новыми
            data_meta_ext = np.concatenate((meta,self.meta),axis=1)
            self.metamodel.fit(data_meta_ext, y_meta)
        else:
            self.metamodel.fit(self.meta, y_meta)
        print(""------"")
        print(""Ok"")


        return self
    


    def predict(self, X, y=None):
        """"""
        Предсказание стекингом
        """"""
        # заполение матрицы для мета-классификатора
        X_meta = np.zeros((X.shape[0], self.n))
        
        print(""------"")
        print(""Prediction"")  



        for t, base_model in enumerate(self.models):
            
            X_meta[:, t] = base_model.predict(X).reshape((1,-1))
          
            print(f""Ok{t}"")  
          

        if self.merge:#если объединенная выборка для обучения метамодели
            data_meta_test = np.concatenate((X,X_meta),axis=1)
            res = self.metamodel.predict(data_meta_test)

        else:
            res = self.metamodel.predict(X_meta)
        
        return (res)",0,No Code Smell
9453,43694573,8,"class NNWrapper(BaseEstimator, ClassifierMixin):  
    """"""Обертка для нейросетей для совместимости со стекингом""""""

    def __init__(self, model,scaler,X_valid=None,
                 y_valid=None,max_epochs=10, patience=150):
        
        self.model = model
        self.X_valid = X_valid
        self.y_valid = y_valid
        self.max_epochs = max_epochs
        self.patience = patience
        self.scaler = scaler

    def fit(self, X, y=None):
        X_sc = self.scaler.fit_transform(X)
        self.model.fit(X_train=np.array(X_sc), y_train=np.array(y), X_valid=self.X_valid,y_valid=self.y_valid,
                  max_epochs=self.max_epochs, patience=self.patience) 

        return self

    def predict(self, X_test, y=None):
        X_test_sc = self.scaler.transform(np.array(X_test))
        prediction = self.model.predict(np.array(X_test_sc)).reshape((1,-1))#Ключевая строка
        
        
        return prediction

class LMWrapper(BaseEstimator, ClassifierMixin):  
    """"""Обертка для линейных и иных простых моделей 
    для использования масштабирования""""""

    def __init__(self, model,scaler):
        
        self.model = model
        self.scaler = scaler
        
    def fit(self, X, y=None):
        
        X_sc = self.scaler.fit_transform(X)
        self.model.fit(X_sc, y) 

        return self

    def predict(self, X_test, y=None):
        X_test_sc = self.scaler.transform(X_test)
        prediction = self.model.predict(X_test_sc)
        
        
        return prediction",0,No Code Smell
9454,43694573,9,"ls0 = Lasso(alpha=0.01,random_state=0)
knn1 = LMWrapper(KNeighborsRegressor(n_neighbors=3,),StandardScaler())
knn2 = LMWrapper(KNeighborsRegressor(n_neighbors=10),StandardScaler())
rf2 = RandomForestRegressor(n_estimators=100, max_depth=10,random_state=100)
gbm1 = lgb.LGBMRegressor(boosting_type='gbdt', learning_rate=0.05, max_depth=7, n_estimators=200, nthread=-1,
                        objective='regression',random_state=0) 
cb_reg1 = CatBoostRegressor(task_type='GPU',random_state=0,
                             iterations=1000,verbose=False)
reg_tabnet = NNWrapper(TabNetRegressor(verbose=0,seed=0),StandardScaler(),max_epochs=100, patience=150,
                       X_valid=np.array(features_val), y_valid = np.array(labels_val).reshape(-1, 1))
clf_cb_1 = CatBoostClassifier(task_type='GPU',random_state=0, loss_function='MultiClass',
                                auto_class_weights=""Balanced"",iterations=1000,verbose=False)
clf_lr = LMWrapper(LogisticRegression(multi_class=""multinomial"",class_weight=""balanced"",
                                        C=1e-1,max_iter=300,random_state=0),StandardScaler())
clf_nb1 =  BernoulliNB(alpha=1,binarize=0.3)",0,No Code Smell
9455,43694573,10,"%%time
warnings.filterwarnings(""ignore"")
models = [ls0,knn1, knn2,rf2,gbm1,cb_reg1,
           reg_tabnet,clf_cb_1,clf_nb1,clf_lr]

meta_model = CatBoostClassifier(task_type='GPU',random_state=0, loss_function='MultiClassOneVsAll',
                                auto_class_weights=""Balanced"",iterations=1000,verbose=False)

stack = Stacking(models, meta_model,merge=True)
stack.fit(features_train,np.array(labels_train).reshape(-1, 1),p=0.2,random_state=0)# сложно из-за нейросети
preds = stack.predict(features_val)
print(classification_report(labels_val,preds))
print(""------"")
print(f""Macro f1 score: {f1_score(labels_val,preds,average='macro')}"")",0,No Code Smell
9456,43694573,11,,0,No Code Smell
9553,47064716,0,"import pandas as pd
import numpy as np
import os
import requests
import json
import datetime
import time
",1,Code Smell
9554,47064716,1,"MIN_FINAL_RATING = 1500 # top submission in a match must have reached this score
num_api_calls_today = 0
",0,No Code Smell
9555,47064716,2,"all_files = []
for root, dirs, files in os.walk('../input/', topdown=False):
    all_files.extend(files)
seen_episodes = [int(f.split('.')[0]) for f in all_files 
                      if '.' in f and f.split('.')[0].isdigit() and f.split('.')[1] == 'json']
print('{} games in existing library'.format(len(seen_episodes)))
",0,No Code Smell
9556,47064716,3,"NUM_TEAMS = 1
EPISODES = 600 

BUFFER = 1

base_url = ""https://www.kaggle.com/requests/EpisodeService/""
get_url = base_url + ""GetEpisodeReplay""
list_url = base_url + ""ListEpisodes""",0,No Code Smell
9557,47064716,4,"# inital team list

r = requests.post(list_url, json = {""teamId"":  18039598}) # arbitrary ID, change to leading ID during challenge

rj = r.json()

teams_df = pd.DataFrame(rj['result']['teams'])",0,No Code Smell
9558,47064716,5,"teams_df.sort_values('publicLeaderboardRank', inplace = True)
teams_df.head(6)",0,No Code Smell
9559,47064716,6,"def getTeamEpisodes(team_id):
    # request
    r = requests.post(list_url, json = {""teamId"":  int(team_id)})
    rj = r.json()

    # update teams list
    global teams_df
    teams_df_new = pd.DataFrame(rj['result']['teams'])
    
    if len(teams_df.columns) == len(teams_df_new.columns) and (teams_df.columns == teams_df_new.columns).all():
        teams_df = pd.concat( (teams_df, teams_df_new.loc[[c for c in teams_df_new.index if c not in teams_df.index]] ) )
        teams_df.sort_values('publicLeaderboardRank', inplace = True)
    else:
        print('teams dataframe did not match')
    
    # make df
    team_episodes = pd.DataFrame(rj['result']['episodes'])
    team_episodes['avg_score'] = -1;
    
    for i in range(len(team_episodes)):
        agents = team_episodes['agents'].loc[i]
        agent_scores = [a['updatedScore'] for a in agents if a['updatedScore'] is not None]
        team_episodes.loc[i, 'submissionId'] = [a['submissionId'] for a in agents if a['submission']['teamId'] == team_id][0]
        team_episodes.loc[i, 'updatedScore'] = [a['updatedScore'] for a in agents if a['submission']['teamId'] == team_id][0]
        
        if len(agent_scores) > 0:
            team_episodes.loc[i, 'avg_score'] = np.mean(agent_scores)

    for sub_id in team_episodes['submissionId'].unique():
        sub_rows = team_episodes[ team_episodes['submissionId'] == sub_id ]
        max_time = max( [r['seconds'] for r in sub_rows['endTime']] )
        final_score = max( [r['updatedScore'] for r_idx, (r_index, r) in enumerate(sub_rows.iterrows())
                                if r['endTime']['seconds'] == max_time] )

        team_episodes.loc[sub_rows.index, 'final_score'] = final_score
        
    team_episodes.sort_values('avg_score', ascending = False, inplace=True)
    return rj, team_episodes",0,No Code Smell
9560,47064716,7,"def saveEpisode(epid, rj):
    # request
    re = requests.post(get_url, json = {""EpisodeId"": int(epid)})
        
    # save replay
    with open('{}.json'.format(epid), 'w') as f:
        f.write(re.json()['result']['replay'])

    # save episode info
    with open('{}_info.json'.format(epid), 'w') as f:
        json.dump([r for r in rj['result']['episodes'] if r['id']==epid][0], f)",0,No Code Smell
9561,47064716,8,"global num_api_calls_today

pulled_teams = {}
pulled_episodes = []
start_time = datetime.datetime.now()
r = BUFFER;

while num_api_calls_today < EPISODES:
    # pull team
    top_teams = [i for i in teams_df.id if i not in pulled_teams]
    if len(top_teams) > 0:
        team_id = top_teams[0]
    else:
        break;
        
    # get team data
    team_json, team_df = getTeamEpisodes(team_id); r+=1;
    num_api_calls_today+=1
    print('{} games for {}'.format(len(team_df), teams_df.loc[teams_df.id == team_id].iloc[0].teamName))

    
    team_df = team_df[  (MIN_FINAL_RATING is None or (team_df.final_score > MIN_FINAL_RATING))]
    
    print('   {} in score range from {} submissions'.format(len(team_df), len(team_df.submissionId.unique() ) ) )
    
    team_df = team_df[~team_df.id.isin(pulled_episodes + seen_episodes)]        
    print('      {} remain to be downloaded\n'.format(len(team_df)))
        
    # pull games
    target_team_games = int(np.ceil(EPISODES / NUM_TEAMS))
    if target_team_games + len(pulled_episodes) > EPISODES:
        target_team_games = EPISODES - len(pulled_episodes)
     
    pulled_teams[team_id] = 0
    
    i = 0
    while i < len(team_df) and pulled_teams[team_id] < target_team_games:
        epid = team_df.id.iloc[i]
        if not (epid in pulled_episodes or epid in seen_episodes):
            try:
                saveEpisode(epid, team_json); r+=1;
                num_api_calls_today+=1
            except:
                time.sleep(20)
                i+=1;
                continue;
                
            pulled_episodes.append(epid)
            pulled_teams[team_id] += 1
            try:
                size = os.path.getsize('{}.json'.format(epid)) / 1e6
                print(str(num_api_calls_today) + ': Saved Episode #{} @ {:.1f}MB'.format(epid, size))
            except:
                print('  file {}.json did not seem to save'.format(epid))    
            if r > (datetime.datetime.now() - start_time).seconds:
                time.sleep( r - (datetime.datetime.now() - start_time).seconds)
                

        i+=1;
    print(); print()",0,No Code Smell
9562,47064716,9,,0,No Code Smell
9563,47064716,10,,0,No Code Smell
9564,48346260,0,"import numpy as np
from numpy import linalg as LA

import matplotlib.pyplot as plt
import pandas as pd
import json
import glob
import seaborn as sns

from tqdm import tqdm

import math

from collections import defaultdict
import collections",1,Code Smell
9565,48346260,1,"TEAMNAME = ""WeKick""
replay_dir = ""../input/wekick-small/wekick_small/""",0,No Code Smell
9566,48346260,2,"action_set_v1=[
""idle"",""left"",""top_left"",""top"",""top_right"",""right"",""bottom_right"",""bottom"",""bottom_left"",""long_pass"",""high_pass"",""short_pass"",""shot"",""sprint"",""release_direction"",""release_sprint"",""sliding"",""dribble"",""release_dribble""
]",0,No Code Smell
9567,48346260,3,"json_paths=[]
for path in glob.glob(replay_dir+""*""): 
    json_paths.append(path)
        
print(""replay num: {}"".format(len(json_paths)))",0,No Code Smell
9568,48346260,4,"# return action list
def create_episode_dict(json_path):
    act_lis =[]
    
    json_open = open(json_path, 'r')
    json_load = json.load(json_open)
    
    sub_id = int(json_path.split(""/"")[-1].split(""_"")[0])
    
    for frame in range(len(json_load[""steps""])-1):
        if TEAMNAME in json_load[""info""][""TeamNames""][0]:
            team=0
        elif TEAMNAME in json_load[""info""][""TeamNames""][1]:
            team=1
        else:
            raise BaseException(""teamname{} not found!"".format(TEAMNAME))

        raw = json_load[""steps""][frame][team][""observation""][""players_raw""][0]
        action = json_load[""steps""][frame+1][team][""action""][0]
        
        act_lis.append(action)

    
    return act_lis,sub_id",0,No Code Smell
9569,48346260,5,"from joblib import Parallel, delayed
sub = Parallel(n_jobs=-1, verbose=10)( [delayed(create_episode_dict)(j) for j in json_paths] )",0,No Code Smell
9570,48346260,6,"subs = defaultdict(list)

for act_lis,sub_id in sub:
    subs[sub_id].extend(act_lis)",0,No Code Smell
9571,48346260,7,"sub_action_prob= defaultdict(list)

for subid, actions in subs.items():
    c = collections.Counter(actions)
    
    for act in range(19):
        sub_action_prob[subid].append(c[act]/len(actions))",0,No Code Smell
9572,48346260,8,"act_df = pd.DataFrame(sub_action_prob)
act_df.index = action_set_v1
act_df = act_df.reindex(sorted(act_df.columns), axis=1)
act_df",0,No Code Smell
9573,48346260,9,"plt.figure(figsize = (15,5))
plt.plot(act_df)
plt.xticks(rotation=45)
plt.grid()",0,No Code Smell
9574,48346260,10,"df_corr = act_df.corr()
plt.figure(figsize = (8,8))
sns.heatmap(df_corr, square=True,vmax=1, vmin=-1, center=0)",0,No Code Smell
9575,48346260,11,"from scipy.spatial.distance import squareform
from scipy.cluster.hierarchy import dendrogram, linkage

corr_condensed =squareform(1-df_corr)

z = linkage(corr_condensed, method='average')
dn = dendrogram(z,  leaf_rotation=70)",1,Code Smell
9576,43704621,0,"import pandas as pd
import numpy as np
import os
import requests
import json
import datetime
import time
",1,Code Smell
9577,43704621,1,"MIN_FINAL_RATING = 500 # top submission in a match must have reached this score
num_api_calls_today = 0
",0,No Code Smell
9578,43704621,2,"all_files = []
for root, dirs, files in os.walk('../input/', topdown=False):
    all_files.extend(files)
seen_episodes = [int(f.split('.')[0]) for f in all_files 
                      if '.' in f and f.split('.')[0].isdigit() and f.split('.')[1] == 'json']
print('{} games in existing library'.format(len(seen_episodes)))
",0,No Code Smell
9579,43704621,3,"NUM_TEAMS = 1
EPISODES = 600 

BUFFER = 1

base_url = ""https://www.kaggle.com/requests/EpisodeService/""
get_url = base_url + ""GetEpisodeReplay""
list_url = base_url + ""ListEpisodes""",0,No Code Smell
9580,43704621,4,"# inital team list

r = requests.post(list_url, json = {""teamId"":  5586412}) # arbitrary ID, change to leading ID during challenge

rj = r.json()

teams_df = pd.DataFrame(rj['result']['teams'])",0,No Code Smell
9581,43704621,5,"teams_df.sort_values('publicLeaderboardRank', inplace = True)
teams_df.head(6)",0,No Code Smell
9582,43704621,6,"def getTeamEpisodes(team_id):
    # request
    r = requests.post(list_url, json = {""teamId"":  int(team_id)})
    rj = r.json()

    # update teams list
    global teams_df
    teams_df_new = pd.DataFrame(rj['result']['teams'])
    
    if len(teams_df.columns) == len(teams_df_new.columns) and (teams_df.columns == teams_df_new.columns).all():
        teams_df = pd.concat( (teams_df, teams_df_new.loc[[c for c in teams_df_new.index if c not in teams_df.index]] ) )
        teams_df.sort_values('publicLeaderboardRank', inplace = True)
    else:
        print('teams dataframe did not match')
    
    # make df
    team_episodes = pd.DataFrame(rj['result']['episodes'])
    team_episodes['avg_score'] = -1;
    
    for i in range(len(team_episodes)):
        agents = team_episodes['agents'].loc[i]
        agent_scores = [a['updatedScore'] for a in agents if a['updatedScore'] is not None]
        team_episodes.loc[i, 'submissionId'] = [a['submissionId'] for a in agents if a['submission']['teamId'] == team_id][0]
        team_episodes.loc[i, 'updatedScore'] = [a['updatedScore'] for a in agents if a['submission']['teamId'] == team_id][0]
        
        if len(agent_scores) > 0:
            team_episodes.loc[i, 'avg_score'] = np.mean(agent_scores)

    for sub_id in team_episodes['submissionId'].unique():
        sub_rows = team_episodes[ team_episodes['submissionId'] == sub_id ]
        max_time = max( [r['seconds'] for r in sub_rows['endTime']] )
        final_score = max( [r['updatedScore'] for r_idx, (r_index, r) in enumerate(sub_rows.iterrows())
                                if r['endTime']['seconds'] == max_time] )

        team_episodes.loc[sub_rows.index, 'final_score'] = final_score
        
    team_episodes.sort_values('avg_score', ascending = False, inplace=True)
    return rj, team_episodes",0,No Code Smell
9583,43704621,7,"def saveEpisode(epid, rj):
    # request
    re = requests.post(get_url, json = {""EpisodeId"": int(epid)})
        
    # save replay
    with open('{}.json'.format(epid), 'w') as f:
        f.write(re.json()['result']['replay'])

    # save episode info
    with open('{}_info.json'.format(epid), 'w') as f:
        json.dump([r for r in rj['result']['episodes'] if r['id']==epid][0], f)",0,No Code Smell
9584,43704621,8,"global num_api_calls_today

pulled_teams = {}
pulled_episodes = []
start_time = datetime.datetime.now()
r = BUFFER;

while num_api_calls_today < EPISODES:
    # pull team
    top_teams = [i for i in teams_df.id if i not in pulled_teams]
    if len(top_teams) > 0:
        team_id = top_teams[0]
    else:
        break;
        
    # get team data
    team_json, team_df = getTeamEpisodes(team_id); r+=1;
    num_api_calls_today+=1
    print('{} games for {}'.format(len(team_df), teams_df.loc[teams_df.id == team_id].iloc[0].teamName))

    
    team_df = team_df[  (MIN_FINAL_RATING is None or (team_df.final_score > MIN_FINAL_RATING))]
    
    print('   {} in score range from {} submissions'.format(len(team_df), len(team_df.submissionId.unique() ) ) )
    
    team_df = team_df[~team_df.id.isin(pulled_episodes + seen_episodes)]        
    print('      {} remain to be downloaded\n'.format(len(team_df)))
        
    # pull games
    target_team_games = int(np.ceil(EPISODES / NUM_TEAMS))
    if target_team_games + len(pulled_episodes) > EPISODES:
        target_team_games = EPISODES - len(pulled_episodes)
     
    pulled_teams[team_id] = 0
    
    i = 0
    while i < len(team_df) and pulled_teams[team_id] < target_team_games:
        epid = team_df.id.iloc[i]
        if not (epid in pulled_episodes or epid in seen_episodes):
            try:
                saveEpisode(epid, team_json); r+=1;
                num_api_calls_today+=1
            except:
                time.sleep(20)
                i+=1;
                continue;
                
            pulled_episodes.append(epid)
            pulled_teams[team_id] += 1
            try:
                size = os.path.getsize('{}.json'.format(epid)) / 1e6
                print(str(num_api_calls_today) + ': Saved Episode #{} @ {:.1f}MB'.format(epid, size))
            except:
                print('  file {}.json did not seem to save'.format(epid))    
            if r > (datetime.datetime.now() - start_time).seconds:
                time.sleep( r - (datetime.datetime.now() - start_time).seconds)
                

        i+=1;
    print(); print()",0,No Code Smell
9585,43704621,9,,0,No Code Smell
9586,43704621,10,,0,No Code Smell
9587,44678142,0,"# Install:
# GFootball environment (https://github.com/google-research/football/)

!apt-get update
!apt-get install -y libsdl2-gfx-dev libsdl2-ttf-dev

# Update kaggle-environments to the newest version.
!pip3 install kaggle-environments -U

# Make sure that the Branch in git clone and in wget call matches !!
!git clone -b v2.3 https://github.com/google-research/football.git
!mkdir -p football/third_party/gfootball_engine/lib

!wget https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_v2.3.so -O football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so
!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install .",0,No Code Smell
9588,44678142,1,"from gfootball.env.wrappers import Simple115StateWrapper
from kaggle_environments import make
env = make(""football"", 
           configuration={""save_video"": False, 
                          ""scenario_name"": ""11_vs_11_kaggle"", 
                          ""running_in_notebook"": True,
                         })
obs = env.reset()",0,No Code Smell
9589,44678142,2,"# all game information
obs",0,No Code Smell
9590,44678142,3,"# get raw obs for the first player we control.
obs[0]['observation']['players_raw']",0,No Code Smell
9591,44678142,4,"from gfootball.env import observation_preprocessing
raw_obs = obs[0]['observation']['players_raw']
obs_smm = observation_preprocessing.generate_smm(raw_obs)[0]
print(obs_smm)
print(obs_smm.shape)",0,No Code Smell
9592,44678142,5,"from gfootball.env.wrappers import Simple115StateWrapper
raw_obs = obs[0]['observation']['players_raw']
# Note: simple115v2 enables fixed_positions option.
# Source code in https://github.com/google-research/football/blob/3603de77d2bf25e53a1fbd52bc439f1377397b3b/gfootball/env/wrappers.py#L119
obs_115 = Simple115StateWrapper.convert_observation(raw_obs, fixed_positions=True)[0]
print(obs_115)
print(obs_115.shape)",0,No Code Smell
9593,44678142,6,"%%writefile ./test.py

from gfootball.env import observation_preprocessing
from gfootball.env.wrappers import Simple115StateWrapper
import random

def agent(obs):
    
    # error:
    # raw_obs = obs[0]['observation']['players_raw']
    # obs115 = Simple115StateWrapper.convert_observation(raw_obs, True)[0]
    # obs_smm = observation_preprocessing.generate_smm([raw_obs])[0]
    
    # correct:
    raw_obs = obs['players_raw'][0]
    obs_115 = Simple115StateWrapper.convert_observation([raw_obs], True)[0]
    obs_smm = observation_preprocessing.generate_smm([raw_obs])[0]
    
    agent_output = random.randint(1, 18)
    
    # you need return a list contains your single action(a int type number from [1, 18])
    # be ware of your model output might be a float number, so make sure return a int type number.
    return [int(agent_output)]",0,No Code Smell
9594,44678142,7,"from kaggle_environments import make

log = []

# you can set debug=True or/and logs to get more information for debug.
env = make(""football"", 
           configuration={""save_video"": True, 
                          ""scenario_name"": ""11_vs_11_kaggle"", 
                          ""running_in_notebook"": True,
                         }, debug=True, logs=log)
output = env.run([""./test.py"", ""./test.py""])[-1]
print('Left player: reward = %s, status = %s, info = %s' % (output[0]['reward'], output[0]['status'], output[0]['info']))
print('Right player: reward = %s, status = %s, info = %s' % (output[1]['reward'], output[1]['status'], output[1]['info']))

# you can print detailed log
# print(log)

env.render(mode=""human"", width=800, height=600)",0,No Code Smell
9595,44678142,8,,0,No Code Smell
9596,47965519,0,"import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))
import requests
import json
import time",1,Code Smell
9597,47965519,1,"base_url = ""https://www.kaggle.com/requests/EpisodeService/""
get_url = base_url + ""GetEpisodeReplay""
list_url = base_url + ""ListEpisodes""
# inital team list

r = requests.post(list_url, json = {""teamId"":  5696217}) # arbitrary ID, change to leading ID during challenge

rj = r.json()

teams_df = pd.DataFrame(rj['result']['teams'])
teams_df.sort_values('publicLeaderboardRank', inplace = True)
teams_df.head(6)",0,No Code Smell
9598,47965519,2,"# target_sub_id = int(teams_df[teams_df['teamName'] == 'SaltyFish']['publicLeaderboardSubmissionId'])
target_sub_id = 18239675",0,No Code Smell
9599,47965519,3,,0,No Code Smell
9600,47965519,4,"# rr = requests.post(list_url, json = {""teamId"":  int(5696217)})
# rrj = rr.json()
# tteam_episodes = pd.DataFrame(rrj['result']['episodes'])",0,No Code Smell
9601,47965519,5,,0,No Code Smell
9602,47965519,6,"rs = requests.post(list_url, json = {""SubmissionId"":int(target_sub_id)})
rsj = rs.json()
steam_episodes = pd.DataFrame(rsj['result']['episodes'])",0,No Code Smell
9603,47965519,7,steam_episodes.head(),0,No Code Smell
9604,47965519,8,steam_episodes['agents'][0],0,No Code Smell
9605,47965519,9,!mkdir episodes_log,0,No Code Smell
9606,47965519,10,"def saveEpisode(epid, rj):
    # request
    re = requests.post(get_url, json = {""EpisodeId"": int(epid)})
        
    # save replay
    with open('./episodes_log/{}.json'.format(epid), 'w') as f:
        f.write(re.json()['result']['replay'])

    # save episode info
    with open('./episodes_log/{}_info.json'.format(epid), 'w') as f:
        json.dump([r for r in rj['result']['episodes'] if r['id']==epid][0], f)",0,No Code Smell
9607,47965519,11,len(steam_episodes),0,No Code Smell
9608,47965519,12,"max_episodes = 150

if max_episodes > len(steam_episodes):
    max_episodes = len(steam_episodes)",0,No Code Smell
9609,47965519,13,max_episodes,0,No Code Smell
9610,47965519,14,"i = 0
# while i < 5:
while i < max_episodes:
    epid = steam_episodes['id'].iloc[i]
    try:
        saveEpisode(epid, rsj); r+=1;
        num_api_calls_today+=1
    except:
        time.sleep(20)
        i+=1;
        continue;

    i+=1;",0,No Code Smell
9611,47965519,15,"# rre = requests.post(get_url, json = {""EpisodeId"": int(4790921)})",0,No Code Smell
9612,47965519,16,# data = json.load(open('./episodes_log/4790921.json')),0,No Code Smell
9613,47965519,17,# d_info = json.load(open('./episodes_log/4790921_info.json')),0,No Code Smell
9614,47965519,18,# d_info,0,No Code Smell
9615,47965519,19,"# if d_info['agents'][0]['submissionId'] == '18010220':
#     team_index = 0
# else:
#     team_index = 1
",0,No Code Smell
9616,47965519,20,# data['steps'][5][team_index]['observation']['players_raw'][0],0,No Code Smell
9617,47965519,21,,0,No Code Smell
9618,45619348,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only ""../input/"" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session",1,Code Smell
9619,45619348,1,"# Install:
# Kaggle environments.
!git clone https://github.com/Kaggle/kaggle-environments.git
!cd kaggle-environments && pip install .

# GFootball environment.
!apt-get update -y
!apt-get install -y libsdl2-gfx-dev libsdl2-ttf-dev

# Make sure that the Branch in git clone and in wget call matches !!
!git clone -b v2.7 https://github.com/google-research/football.git
!mkdir -p football/third_party/gfootball_engine/lib

!wget https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_v2.7.so -O football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so
!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install .",0,No Code Smell
9620,45619348,2,"import pandas as pd
import numpy as np
import os
import requests
import json
import datetime
import time",1,Code Smell
9621,45619348,3,"MIN_FINAL_RATING = 1200 # top submission in a match must have reached this score
num_api_calls_today = 0",0,No Code Smell
9622,45619348,4,"all_files = []
for root, dirs, files in os.walk('./', topdown=False):
    all_files.extend(files)
seen_episodes = [int(f.split('.')[0]) for f in all_files 
                      if '.' in f and f.split('.')[0].isdigit() and f.split('.')[1] == 'json']
print('{} games in existing library'.format(len(seen_episodes)))",0,No Code Smell
9623,45619348,5,"NUM_TEAMS = 3
EPISODES = 90 

BUFFER = 1

base_url = ""https://www.kaggle.com/requests/EpisodeService/""
get_url = base_url + ""GetEpisodeReplay""
list_url = base_url + ""ListEpisodes""
# inital team list

r = requests.post(list_url, json = {""teamId"":  5696217}) # arbitrary ID, change to leading ID during challenge

rj = r.json()

teams_df = pd.DataFrame(rj['result']['teams'])
teams_df.sort_values('publicLeaderboardRank', inplace = True)
teams_df.head(6)",0,No Code Smell
9624,45619348,6,"def getTeamEpisodes(team_id):
    r = requests.post(list_url, json = {""teamId"":  int(team_id)})
    rj = r.json()

    # update teams list
    global teams_df
    teams_df_new = pd.DataFrame(rj['result']['teams'])
    
    if len(teams_df.columns) == len(teams_df_new.columns) and (teams_df.columns == teams_df_new.columns).all():
        teams_df = pd.concat( (teams_df, teams_df_new.loc[[c for c in teams_df_new.index if c not in teams_df.index]] ) )
        teams_df.sort_values('publicLeaderboardRank', inplace = True)
    else:
        print('teams dataframe did not match')
    
    # make df
    team_episodes = pd.DataFrame(rj['result']['episodes'])
    team_episodes['avg_score'] = -1;
    
    for i in range(len(team_episodes)):
        agents = team_episodes['agents'].loc[i]
        agent_scores = [a['updatedScore'] for a in agents if a['updatedScore'] is not None]
        team_episodes.loc[i, 'submissionId'] = [a['submissionId'] for a in agents if a['submission']['teamId'] == team_id][0]
        team_episodes.loc[i, 'updatedScore'] = [a['updatedScore'] for a in agents if a['submission']['teamId'] == team_id][0]
        
        if len(agent_scores) > 0:
            team_episodes.loc[i, 'avg_score'] = np.mean(agent_scores)

    for sub_id in team_episodes['submissionId'].unique():
        sub_rows = team_episodes[ team_episodes['submissionId'] == sub_id ]
        max_time = max( [r['seconds'] for r in sub_rows['endTime']] )
        final_score = max( [r['updatedScore'] for r_idx, (r_index, r) in enumerate(sub_rows.iterrows())
                                if r['endTime']['seconds'] == max_time] )

        team_episodes.loc[sub_rows.index, 'final_score'] = final_score
        
    team_episodes.sort_values('avg_score', ascending = False, inplace=True)
    return rj, team_episodes
def saveEpisode(epid, rj):
    # request
    re = requests.post(get_url, json = {""EpisodeId"": int(epid)})
        
    # save replay
    with open('{}.json'.format(epid), 'w') as f:
        f.write(re.json()['result']['replay'])

    # save episode info
    with open('{}_info.json'.format(epid), 'w') as f:
        json.dump([r for r in rj['result']['episodes'] if r['id']==epid][0], f)
global num_api_calls_today

pulled_teams = {}
pulled_episodes = []
start_time = datetime.datetime.now()
r = BUFFER;

while num_api_calls_today < EPISODES:
    # pull team
    top_teams = [i for i in teams_df.id if i not in pulled_teams]
    if len(top_teams) > 0:
        team_id = top_teams[0]
    else:
        break;
        
    # get team data
    team_json, team_df = getTeamEpisodes(team_id); r+=1;
    num_api_calls_today+=1
    print('{} games for {}'.format(len(team_df), teams_df.loc[teams_df.id == team_id].iloc[0].teamName))

    
    team_df = team_df[  (MIN_FINAL_RATING is None or (team_df.final_score > MIN_FINAL_RATING))]
    
    print('   {} in score range from {} submissions'.format(len(team_df), len(team_df.submissionId.unique() ) ) )
    
    team_df = team_df[~team_df.id.isin(pulled_episodes + seen_episodes)]        
    print('      {} remain to be downloaded\n'.format(len(team_df)))
        
    # pull games
    target_team_games = int(np.ceil(EPISODES / NUM_TEAMS))
    if target_team_games + len(pulled_episodes) > EPISODES:
        target_team_games = EPISODES - len(pulled_episodes)
     
    pulled_teams[team_id] = 0
    
    i = 0
    while i < len(team_df) and pulled_teams[team_id] < target_team_games:
        epid = team_df.id.iloc[i]
        if not (epid in pulled_episodes or epid in seen_episodes):
            try:
                saveEpisode(epid, team_json); r+=1;
                num_api_calls_today+=1
            except:
                time.sleep(20)
                i+=1;
                continue;
                
            pulled_episodes.append(epid)
            pulled_teams[team_id] += 1
            try:
                size = os.path.getsize('{}.json'.format(epid)) / 1e6
                print(str(num_api_calls_today) + ': Saved Episode #{} @ {:.1f}MB'.format(epid, size))
            except:
                print('  file {}.json did not seem to save'.format(epid))    
            if r > (datetime.datetime.now() - start_time).seconds:
                time.sleep( r - (datetime.datetime.now() - start_time).seconds)
                

        i+=1;
    print(); print()",0,No Code Smell
9625,45619348,7,"
import json
import os
from tqdm import tqdm, notebook",1,Code Smell
9626,45619348,8,"def convert_observation(observation, fixed_positions=False):

    def do_flatten(obj):
        if type(obj) == list:
            return np.array(obj).flatten()
        return obj.flatten()

    final_obs = []
    
    for obs in observation:
        

        o = []
        if fixed_positions:
            for i, name in enumerate(['left_team', 'left_team_direction',
                                    'right_team', 'right_team_direction']):
                o.extend(do_flatten(obs[name]))
            # If there were less than 11vs11 players we backfill missing values
            # with -1.
            if len(o) < (i + 1) * 22:
                o.extend([-1] * ((i + 1) * 22 - len(o)))
        else:
            o.extend(do_flatten(obs['left_team']))
            o.extend(do_flatten(obs['left_team_direction']))
            o.extend(do_flatten(obs['right_team']))
            o.extend(do_flatten(obs['right_team_direction']))

        # If there were less than 11vs11 players we backfill missing values with
        # -1.
        # 88 = 11 (players) * 2 (teams) * 2 (positions & directions) * 2 (x & y)
        if len(o) < 88:
            o.extend([-1] * (88 - len(o)))

        # ball position
        o.extend(obs['ball'])
        # ball direction
        o.extend(obs['ball_direction'])
        # one hot encoding of which team owns the ball
        if obs['ball_owned_team'] == -1:
            o.extend([1, 0, 0])
        if obs['ball_owned_team'] == 0:
            o.extend([0, 1, 0])
        if obs['ball_owned_team'] == 1:
            o.extend([0, 0, 1])

        active = [0] * 11
        if obs['active'] != -1:
            active[obs['active']] = 1
        o.extend(active)

        game_mode = [0] * 7
        game_mode[obs['game_mode']] = 1
        o.extend(game_mode)
        final_obs.append(o)

        return np.array(final_obs, dtype=np.float32).flatten()
",0,No Code Smell
9627,45619348,9,"y=[]
x=[]
filenames = [p for p in os.listdir('.') if 'info' not in p and 'json' in p]

for f in notebook.tqdm(filenames):
    filename = ""./"" + f

    try:
        with open(filename) as json_file:
            data = json.load(json_file)
    except:
        continue
        
    counter=0


    for team in [0,1]:
        final_score = data['steps'][-2][team]['observation']['players_raw'][0]['score'][0]

        goal=False

        for i in range(2,len(data['steps'])-2,):

            action=data['steps'][i][team]['action']
            y.append(action[0])
            
            obs=data['steps'][i][team]['observation']['players_raw'][0]
            x.append(convert_observation([obs]))
                ",0,No Code Smell
9628,45619348,10,"import lightgbm as lgb
from sklearn.metrics import accuracy_score
def evaluate_model(model,X_test,Y_test):
    preds = model.predict(X_test)
    best_preds = np.asarray([np.argmax(line) for line in preds])

    print(""Accuracy = {}"".format(accuracy_score(Y_test, best_preds)))
    ",0,No Code Smell
9629,45619348,11,x[0].shape,0,No Code Smell
9630,45619348,12,"d_train = lgb.Dataset(np.array(x).reshape((-1,115)), label=y)
params = {}
params['objective'] = 'multiclass'
params['num_classes'] = 19


mod = lgb.train(params, d_train, 100,)

mod.save_model('model.txt')    
evaluate_model(mod,x[:1000],y[:1000])",0,No Code Smell
9631,45619348,13,"def tree_agent(obs):
    try:
        obs1=obs['observation']['players_raw'][0]
    except:
        obs1=obs['players_raw'][0]

    obs1=convert_observation([obs1])

#     action=np.random.choice(np.arange(19),p=(mod.predict([obs1])).flatten())
    action=np.argmax(mod.predict([obs1]).flatten())
    return [int(action)]",0,No Code Smell
9632,45619348,14,"from kaggle_environments import make

env = make(""football"", configuration={""save_video"": True, ""scenario_name"": ""11_vs_11_kaggle"", ""running_in_notebook"": True})
output = env.run([tree_agent,tree_agent])
print('Left player: reward = %s, status = %s, info = %s' % (output[-1][0]['reward'],
                                                            output[-1][0]['status'], output[-1][0]['info']))
env.render(mode='human',width=800, height=600)",0,No Code Smell
9633,45619348,15,,0,No Code Smell
9634,44215692,0,"# Install:
# Kaggle environments.
!git clone https://github.com/Kaggle/kaggle-environments.git
!cd kaggle-environments && pip install .

# GFootball environment.
!apt-get update -y
!apt-get install -y libsdl2-gfx-dev libsdl2-ttf-dev

# Make sure that the Branch in git clone and in wget call matches !!
!git clone -b v2.3 https://github.com/google-research/football.git
!mkdir -p football/third_party/gfootball_engine/lib

!wget https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_v2.3.so -O football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so
!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install .
",0,No Code Smell
9635,44215692,1,"%%writefile submission.py
from kaggle_environments.envs.football.helpers import *
from math import sqrt

directions = [
[Action.TopLeft, Action.Top, Action.TopRight],
[Action.Left, Action.Idle, Action.Right],
[Action.BottomLeft, Action.Bottom, Action.BottomRight]]

dirsign = lambda x: 1 if abs(x) < 0.01 else (0 if x < 0 else 2)

enemyGoal = [1, 0]
perfectRange = [[0.6, 1], [-0.2, 0.2]]

def inside(pos, area):
    return area[0][0] <= pos[0] <= area[0][1] and area[1][0] <= pos[1] <= area[1][1]

@human_readable_agent
def agent(obs):
    controlled_player_pos = obs['left_team'][obs['active']]

    if obs[""game_mode""] == GameMode.Penalty:
        return Action.Shot
    if obs[""game_mode""] == GameMode.Corner:
        if controlled_player_pos[0] > 0:
            return Action.Shot
    if obs[""game_mode""] == GameMode.FreeKick:
        return Action.Shot
    # Make sure player is running.
    if  0 < controlled_player_pos[0] < 0.6 and Action.Sprint not in obs['sticky_actions']:
        return Action.Sprint
    elif 0.6 < controlled_player_pos[0] and Action.Sprint in obs['sticky_actions']:
        return Action.ReleaseSprint

    # Does the player we control have the ball?
    if obs['ball_owned_player'] == obs['active'] and obs['ball_owned_team'] == 0:
        goalkeeper = 0
        if inside(controlled_player_pos, perfectRange) and controlled_player_pos[0] < obs['ball'][0]:
            return Action.Shot
        elif abs(obs['right_team'][goalkeeper][0] - 1) > 0.2 and controlled_player_pos[0] > 0.4 and abs(controlled_player_pos[1]) < 0.2:
            return Action.Shot
        else:
            xdir = dirsign(enemyGoal[0] - controlled_player_pos[0])
            ydir = dirsign(enemyGoal[1] - controlled_player_pos[1])
            return directions[ydir][xdir]
    else:
        # Run towards the ball.
        xdir = dirsign(obs['ball'][0] - controlled_player_pos[0])
        ydir = dirsign(obs['ball'][1] - controlled_player_pos[1])
        return directions[ydir][xdir]
",0,No Code Smell
9636,44215692,2,"# Set up the Environment.
from kaggle_environments import make
env = make(""football"", configuration={""save_video"": True, ""scenario_name"": ""11_vs_11_kaggle"", ""running_in_notebook"": True}, debug=True)
output = env.run([""/kaggle/working/submission.py"", ""do_nothing""])[-1]
print('Left player: reward = %s, status = %s, info = %s' % (output[0]['reward'], output[0]['status'], output[0]['info']))
print('Right player: reward = %s, status = %s, info = %s' % (output[1]['reward'], output[1]['status'], output[1]['info']))
env.render(mode=""human"", width=800, height=600)",0,No Code Smell
9637,44665747,0,"# Install:
# Kaggle environments.
!git clone https://github.com/Kaggle/kaggle-environments.git
!cd kaggle-environments && pip install .

# GFootball environment.
!apt-get update -y
!apt-get install -y libsdl2-gfx-dev libsdl2-ttf-dev

# Make sure that the Branch in git clone and in wget call matches !!
!git clone -b v2.3 https://github.com/google-research/football.git
!mkdir -p football/third_party/gfootball_engine/lib

!wget https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_v2.3.so -O football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so
!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install .",0,No Code Smell
9638,44665747,1,!pip install pfrl==0.1.0,0,No Code Smell
9639,44665747,2,"import os
import cv2
import sys
import glob 
import random
import imageio
import pathlib
import collections
from collections import deque
import numpy as np
import argparse
import matplotlib.pyplot as plt
import seaborn as sns
sns.set()
%matplotlib inline

from gym import spaces
from tqdm import tqdm
from logging import getLogger, StreamHandler, FileHandler, DEBUG, INFO
from typing import Union, Callable, List, Tuple, Iterable, Any, Dict
from dataclasses import dataclass
from IPython.display import Image, display
sns.set()


# PyTorch
import pfrl
from pfrl.agents import CategoricalDoubleDQN
from pfrl import experiments
from pfrl import explorers
from pfrl import nn as pnn
from pfrl import utils
from pfrl import replay_buffers
from pfrl.wrappers import atari_wrappers
from pfrl.q_functions import DistributionalDuelingDQN

import torch
from torch import nn

# Env
import gym
import gfootball
import gfootball.env as football_env
from gfootball.env import observation_preprocessing",0,No Code Smell
9640,44665747,3,"# Check we can use GPU
print(torch.cuda.is_available())

# set gpu id
if torch.cuda.is_available(): 
    # NOTE: it is not number of gpu but id which start from 0
    gpu = 0
else:
    # cpu=>-1
    gpu = -1",0,No Code Smell
9641,44665747,4,"# set logger
def logger_config():
    logger = getLogger(__name__)
    handler = StreamHandler()
    handler.setLevel(""DEBUG"")
    logger.setLevel(""DEBUG"")
    logger.addHandler(handler)
    logger.propagate = False

    filepath = './result.log'
    file_handler = FileHandler(filepath)
    logger.addHandler(file_handler)
    return logger

logger = logger_config()",0,No Code Smell
9642,44665747,5,"# fixed random seed
# but this is NOT enough to fix the result of rewards.Please tell me the reason.
def seed_everything(seed=1234):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True
    utils.set_random_seed(seed)  # for PFRL
    
# Set a random seed used in PFRL.
seed = 5046
seed_everything(seed)

# Set different random seeds for train and test envs.
train_seed = seed
test_seed = 2 ** 31 - 1 - seed",0,No Code Smell
9643,44665747,6,"# wrapper for env(resize and transpose channel order)
class TransEnv(gym.ObservationWrapper):
    def __init__(self, env, channel_order=""hwc""):

        gym.ObservationWrapper.__init__(self, env)
        self.height = 84
        self.width = 84
        self.ch = env.observation_space.shape[2]
        shape = {
            ""hwc"": (self.height, self.width, self.ch),
            ""chw"": (self.ch, self.height, self.width),
        }
        self.observation_space = spaces.Box(
            low=0, high=255, shape=shape[channel_order], dtype=np.uint8
        )
        

    def observation(self, frame):
        frame = cv2.resize(frame, (self.width, self.height), interpolation=cv2.INTER_AREA)
        return frame.reshape(self.observation_space.low.shape)",0,No Code Smell
9644,44665747,7,"def make_env(test):
    # Use different random seeds for train and test envs
    env_seed = test_seed if test else train_seed
    
    # env = gym.make('GFootball-11_vs_11_kaggle-SMM-v0')
    env = football_env.create_environment(
      env_name='11_vs_11_easy_stochastic',  # easy mode
      stacked=False,
      representation='extracted',  # SMM
      rewards='scoring, checkpoints',
      write_goal_dumps=False,
      write_full_episode_dumps=False,
      render=False,
      write_video=False,
      dump_frequency=1,
      logdir='./',
      extra_players=None,
      number_of_left_players_agent_controls=1,
      number_of_right_players_agent_controls=0
    )
    env = TransEnv(env, channel_order=""chw"")

    env.seed(int(env_seed))
    if test:
        # Randomize actions like epsilon-greedy in evaluation as well
        env = pfrl.wrappers.RandomizeAction(env, random_fraction=0.0)
    return env

env = make_env(test=False)
eval_env = make_env(test=True)",0,No Code Smell
9645,44665747,8,"print('observation space:', env.observation_space.low.shape)
print('action space:', env.action_space)",0,No Code Smell
9646,44665747,9,"env.reset()
action = env.action_space.sample()
obs, r, done, info = env.step(action)
print('next observation:', obs.shape)
print('reward:', r)
print('done:', done)
print('info:', info)",1,Code Smell
9647,44665747,10,"obs_n_channels = env.observation_space.low.shape[0]
n_actions = env.action_space.n
print(""obs_n_channels: "", obs_n_channels)
print(""n_actions: "", n_actions)

# params based the original paper
n_atoms = 51
v_max = 10
v_min = -10
q_func = DistributionalDuelingDQN(n_actions, n_atoms, v_min, v_max, obs_n_channels)
print(q_func)",0,No Code Smell
9648,44665747,11,"# Noisy nets
pnn.to_factorized_noisy(q_func, sigma_scale=0.5)

# Turn off explorer
explorer = explorers.Greedy()

# Use the same hyper parameters as https://arxiv.org/abs/1710.02298
opt = torch.optim.Adam(q_func.parameters(), 6.25e-5, eps=1.5 * 10 ** -4)

# Prioritized Replay
# Anneal beta from beta0 to 1 throughout training
update_interval = 4
betasteps = 5 * 10 ** 7 / update_interval
rbuf = replay_buffers.PrioritizedReplayBuffer(
        10 ** 5,  # Default value is 10 ** 6 but it is too large in this notebook. I chose 10 ** 5.
        alpha=0.5,
        beta0=0.4,
        betasteps=betasteps,
        num_steps=3,
        normalize_by_max=""memory"",
    )


def phi(x):
    # Feature extractor
    return np.asarray(x, dtype=np.float32) / 255",0,No Code Smell
9649,44665747,12,"agent = CategoricalDoubleDQN(
        q_func,
        opt,
        rbuf,
        gpu=gpu,  
        gamma=0.99,
        explorer=explorer,
        minibatch_size=32,
        replay_start_size=2 * 10 ** 4,
        target_update_interval=32000,
        update_interval=update_interval,
        batch_accumulator=""mean"",
        phi=phi,
    )",0,No Code Smell
9650,44665747,13,"# if you have a pretrained model, agent can load pretrained weight. 
use_pretrained = False
pretrained_path = None
if use_pretrained:
    agent.load(pretrained_path)",1,Code Smell
9651,44665747,14,"%%time
experiments.train_agent_with_evaluation(
    agent=agent,
    env=env,
    steps=100000,
    eval_n_steps=None,
    eval_n_episodes=1,
    eval_interval=3000,
    outdir=""./kaggle_simulations/agent"",
    save_best_so_far_agent=True,
    eval_env=eval_env,
    logger=logger
)",0,No Code Smell
9652,44665747,15,"import csv

def text_csv_converter(datas):
    file_csv = datas.replace(""txt"", ""csv"")
    with open(datas) as rf:
        with open(file_csv, ""w"") as wf:
            readfile = rf.readlines()
            for read_text in readfile:
                read_text = read_text.split()
                writer = csv.writer(wf, delimiter=',')
                writer.writerow(read_text)

filename = ""./kaggle_simulations/agent/scores.txt""
text_csv_converter(filename)",0,No Code Smell
9653,44665747,16,!ls -la ./kaggle_simulations/agent,1,Code Smell
9654,44665747,17,"import pandas as pd
scores = pd.read_csv(""./kaggle_simulations/agent/scores.csv"")
scores.head()",1,Code Smell
9655,44665747,18,"# visualize reward each episodes
fig = plt.figure(figsize=(15, 5))
ax1 = fig.add_subplot(121)
ax2 = fig.add_subplot(122)
ax1.set_title(""median reward"")
ax2.set_title(""average loss"")
sns.lineplot(x=""episodes"", y=""median"", data=scores, ax=ax1)
sns.lineplot(x=""episodes"", y=""average_loss"", data=scores,ax=ax2)
plt.show()",0,No Code Smell
9656,44665747,19,"%%writefile ./kaggle_simulations/agent/main.py
import cv2
import collections
import gym
import numpy as np
import os
import sys
import torch

from gfootball.env import observation_preprocessing
from gfootball.env import wrappers

# PFRL
import pfrl
from pfrl.agents import CategoricalDoubleDQN
from pfrl import experiments
from pfrl import explorers
from pfrl import nn as pnn
from pfrl import utils
from pfrl import replay_buffers
from pfrl.q_functions import DistributionalDuelingDQN


def phi(x):
    # Feature extractor
    return np.asarray(x, dtype=np.float32) / 255

def make_model():
    global device
    # Q_function
    n_atoms = 51
    v_max = 10
    v_min = -10
    obs_n_channels = 4
    n_actions = 19
    q_func = DistributionalDuelingDQN(n_actions, n_atoms, v_min, v_max, obs_n_channels)

    # Noisy nets
    pnn.to_factorized_noisy(q_func, sigma_scale=0.5)

    # Turn off explorer
    explorer = explorers.Greedy()

    # Use the same hyper parameters as https://arxiv.org/abs/1710.02298
    opt = torch.optim.Adam(q_func.parameters(), 6.25e-5, eps=1.5 * 10 ** -4)

    # Prioritized Replay
    # Anneal beta from beta0 to 1 throughout training
    update_interval = 4
    betasteps = 5 * 10 ** 7 / update_interval
    rbuf = replay_buffers.PrioritizedReplayBuffer(
            10 ** 6,
            alpha=0.5,
            beta0=0.4,
            betasteps=betasteps,
            num_steps=3,
            normalize_by_max=""memory"",
        )


    # prepare agent
    model = CategoricalDoubleDQN(
            q_func,
            opt,
            rbuf,
            gpu=-1,  
            gamma=0.99,
            explorer=explorer,
            minibatch_size=32,
            replay_start_size=2 * 10 ** 4,
            target_update_interval=32000,
            update_interval=update_interval,
            batch_accumulator=""mean"",
            phi=phi,
        )
    
    model.load(""./kaggle_simulations/agent/100000_finish"")
    # model.load(""./kaggle_simulations/agent/best"")
    return model.model.to(device)


device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")
model = make_model()

def agent(obs):
    global device
    global model
    
    # Get observations for the first (and only one) player we control.
    obs = obs['players_raw'][0]
    # Agent we trained uses Super Mini Map (SMM) representation.
    # See https://github.com/google-research/seed_rl/blob/master/football/env.py for details.
    obs = observation_preprocessing.generate_smm([obs])[0]
    # preprocess for obs
    obs = cv2.resize(obs, (84,84))           # resize
    obs = np.transpose(obs, [2,0,1])         # transpose to chw
    obs = torch.tensor(obs).float()          # to tensor
    obs = torch.unsqueeze(obs,0).to(device)  # add batch

    action = model(obs)
    action = action.greedy_actions.cpu().numpy()
    return list(action)",0,No Code Smell
9657,44665747,20,"from kaggle_environments import make
from kaggle_simulations.agent import main
env = make(""football"", configuration={""save_video"": True, ""scenario_name"": ""11_vs_11_kaggle"", ""running_in_notebook"": True})
obs = env.state[0][""observation""]
action = main.agent(obs)
print(action)",0,No Code Smell
9658,44665747,21,"from kaggle_environments import make
env = make(""football"", configuration={""save_video"": True, ""scenario_name"": ""11_vs_11_kaggle"", ""running_in_notebook"": True}, debug=True)
output = env.run([""./kaggle_simulations/agent/main.py"", ""run_right""])[-1]
print('Left player: action = %s, reward = %s, status = %s, info = %s' % (output[0][""action""], output[0]['reward'], output[0]['status'], output[0]['info']))
print('Right player: action = %s, reward = %s, status = %s, info = %s' % (output[1][""action""], output[1]['reward'], output[1]['status'], output[1]['info']))
env.render(mode=""human"", width=800, height=600)",1,Code Smell
9659,44665747,22,"# Prepare a submision package containing trained model and the main execution logic.
!cd ./kaggle_simulations/agent && tar -czvf /kaggle/working/submit.tar.gz main.py best",0,No Code Smell
9660,46523501,0,"%%capture
# Install:
# Kaggle environments.
!git clone https://github.com/Kaggle/kaggle-environments.git
!cd kaggle-environments && pip install .

# GFootball environment.
!apt-get update -y
!apt-get install -y libsdl2-gfx-dev libsdl2-ttf-dev

# Make sure that the Branch in git clone and in wget call matches !!
!git clone -b v2.8 https://github.com/google-research/football.git
!mkdir -p football/third_party/gfootball_engine/lib

!wget https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_v2.8.so -O football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so
!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install .",0,No Code Smell
9661,46523501,1,"%%writefile submission.py
from kaggle_environments.envs.football.helpers import *
from random import randint


# Function to calculate distance 
def get_distance(pos1,pos2):
    return (((pos1[0]-pos2[0])**2)+((pos1[1]-pos2[1])**2))**0.5

# Function to cross ball from wing
def cross_ball():
    pass

# Movement directions
directions = [
[Action.TopLeft, Action.Top, Action.TopRight],
[Action.Left, Action.Idle, Action.Right],
[Action.BottomLeft, Action.Bottom, Action.BottomRight]]

dirsign = lambda x: 1 if abs(x) < 0.01 else (0 if x < 0 else 2)

# Set game plan parameters
goalRange = 0.65
wingRange = 0.21

@human_readable_agent
def agent(obs):
    
    # Add direction to action
    def sticky_check(action, direction):
        if direction in obs['sticky_actions']:
            return action
        else:
            return direction
    
    controlled_player_pos = obs['left_team'][obs['active']]
    
    
    # Pass when KickOff or ThrowIn
    if obs['game_mode'] == GameMode.KickOff or obs['game_mode'] == GameMode.ThrowIn:
        return sticky_check(Action.ShortPass, Action.Right) 
    
    # Shoot when freekick in goal range; If on wing then cross; Otherwise just pass
    if obs['game_mode'] == GameMode.FreeKick:
        # Shoot if in range
        if controlled_player_pos[0] > goalRange and controlled_player_pos[1] < wingRange and controlled_player_pos[1] > -(wingRange):
            ydir = randint(0,2)
            return sticky_check(Action.Shot, directions[ydir][2]) 
        # Cross from right
        if controlled_player_pos[0] > goalRange and controlled_player_pos[1] > wingRange:
            return sticky_check(Action.HighPass, Action.TopRight)
        
        # Cross from left
        if controlled_player_pos[0] > goalRange and controlled_player_pos[1] < -(wingRange):
            return sticky_check(Action.HighPass, Action.BottomRight)
    
    # Cross in for corner
    if obs['game_mode'] == GameMode.Corner and obs['ball'][1] < 0:
        return sticky_check(Action.HighPass, Action.Bottom)
    elif obs['game_mode'] == GameMode.Corner and obs['ball'][1] > 0:
        return sticky_check(Action.HighPass, Action.Top)
        
    # High pass when GoalKick 
    if obs['game_mode'] == GameMode.GoalKick:
        ydir = randint(0,2)
        return sticky_check(Action.HighPass, directions[ydir][2])
    
    # Shoot when Penalty
    if obs['game_mode'] == GameMode.Penalty:
        xdir = randint(0,2)
        ydir = randint(0,2)
        return sticky_check(Action.Shot, directions[ydir][xdir])
    
    # Make sure player is running.
    if Action.Sprint not in obs['sticky_actions']:
        return Action.Sprint
    # We always control left team (observations and actions
    # are mirrored appropriately by the environment).
    controlled_player_pos = obs['left_team'][obs['active']]
    
    # Check if we are in possession
    if obs['ball_owned_player'] == obs['active'] and obs['ball_owned_team'] == 0:
        
        # Clear if we are near our goal
        if controlled_player_pos[0] < -(goalRange):
            return sticky_check(Action.HighPass, Action.Right)
        
        # Shoot if we are in the final third and not at an acute angle
        if controlled_player_pos[0] > goalRange and controlled_player_pos[1] < wingRange and controlled_player_pos[1] > -(wingRange):
            ydir = randint(0,2)
            return sticky_check(Action.Shot, directions[ydir][2])
        #if the goalie is coming out on player near goal shoot
        elif obs['right_team'][0][0] < 0.8 or abs(obs['right_team'][0][1]) > 0.05:
            return Action.Shot
        
        # Cross from right
        if controlled_player_pos[0] > goalRange and controlled_player_pos[1] > wingRange:
            return sticky_check(Action.HighPass, Action.TopRight)
        
        # Cross from left
        if controlled_player_pos[0] > goalRange and controlled_player_pos[1] < -(wingRange):
            return sticky_check(Action.HighPass, Action.BottomRight)
        
        # Run towards the goal otherwise.
        return Action.Right
    else:
        #where ball is going we add the direction xy to ball current location
        ball_targetx=obs['ball'][0]+(1.5 * obs['ball_direction'][0])
        ball_targety=obs['ball'][1]+(1.5 * obs['ball_direction'][1])

        # Euclidian distance to ball
        e_dist=get_distance(obs['left_team'][obs['active']],obs['ball'])

        if e_dist >.005:
            # Run where ball will be
            xdir = dirsign(ball_targetx - controlled_player_pos[0])
            ydir = dirsign(ball_targety - controlled_player_pos[1])
            return directions[ydir][xdir]
        else:
            prob = randint(0,100)
            if prob > 70 and controlled_player_pos[0] < obs['right_team'][obs['active']][0]:
                return Action.Slide
            # Run towards the ball.
            xdir = dirsign(obs['ball'][0] - controlled_player_pos[0])
            ydir = dirsign(obs['ball'][1] - controlled_player_pos[1])
            return directions[ydir][xdir]
",0,No Code Smell
9662,46523501,2,"# Set up the Environment.
from kaggle_environments import make
env = make(""football"", debug=True, configuration={""save_video"": True, ""scenario_name"": ""11_vs_11_kaggle"", ""running_in_notebook"": True})
output = env.run([""/kaggle/working/submission.py"", ""do_nothing""])[-1]
print('Left player: reward = %s, status = %s, info = %s' % (output[0]['reward'], output[0]['status'], output[0]['info']))
print('Right player: reward = %s, status = %s, info = %s' % (output[1]['reward'], output[1]['status'], output[1]['info']))
env.render(mode=""human"", width=800, height=600)",0,No Code Smell
9663,46523501,3,"# Dont slide from behind
# Find player density on pitch to pass to open space / run into open space
# Dribbling
# Shoot if goalie off the line",0,No Code Smell
9664,48292226,0,"%%bash
# dependencies
apt-get -y update > /dev/null
apt-get -y install libsdl2-gfx-dev libsdl2-ttf-dev > /dev/null

# cloudpickle, pytorch, gym
pip3 install ""cloudpickle==1.3.0""
pip3 install ""torch==1.5.1""
pip3 install ""gym==0.17.2""

# gfootball
GRF_VER=v2.8
GRF_PATH=football/third_party/gfootball_engine/lib
GRF_URL=https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_${GRF_VER}.so
git clone -b ${GRF_VER} https://github.com/google-research/football.git
mkdir -p ${GRF_PATH}
wget -q ${GRF_URL} -O ${GRF_PATH}/prebuilt_gameplayfootball.so
cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install . && cd ..

# kaggle-environments
git clone https://github.com/Kaggle/kaggle-environments.git
cd kaggle-environments && pip3 install . && cd ..

# stable-baselines3
git clone https://github.com/DLR-RM/stable-baselines3.git
cd stable-baselines3 && pip3 install . && cd ..

# housekeeping
rm -rf football kaggle-environments stable-baselines3",0,No Code Smell
9665,48292226,1,"!cp ""../input/gfootball-academy/visualizer.py"" .",1,Code Smell
9666,48292226,2,"import os
import base64
import pickle
import zlib
import gym
import numpy as np
import pandas as pd
import torch as th
from torch import nn, tensor
from collections import deque
from gym.spaces import Box, Discrete
from kaggle_environments import make
from kaggle_environments.envs.football.helpers import *
from gfootball.env import create_environment, observation_preprocessing
from stable_baselines3 import PPO
from stable_baselines3.ppo import CnnPolicy
from stable_baselines3.common import results_plotter
from stable_baselines3.common.callbacks import BaseCallback
from stable_baselines3.common.env_checker import check_env
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.torch_layers import BaseFeaturesExtractor
from stable_baselines3.common.vec_env.dummy_vec_env import DummyVecEnv
from stable_baselines3.common.vec_env.subproc_vec_env import SubprocVecEnv
from IPython.display import HTML
from visualizer import visualize
from matplotlib import pyplot as plt
%matplotlib inline",0,No Code Smell
9667,48292226,3,"class FootballGym(gym.Env):
    spec = None
    metadata = None
    
    def __init__(self, config=None):
        super(FootballGym, self).__init__()
        env_name = ""academy_empty_goal_close""
        rewards = ""scoring,checkpoints""
        if config is not None:
            env_name = config.get(""env_name"", env_name)
            rewards = config.get(""rewards"", rewards)
        self.env = create_environment(
            env_name=env_name,
            stacked=False,
            representation=""raw"",
            rewards = rewards,
            write_goal_dumps=False,
            write_full_episode_dumps=False,
            render=False,
            write_video=False,
            dump_frequency=1,
            logdir=""."",
            extra_players=None,
            number_of_left_players_agent_controls=1,
            number_of_right_players_agent_controls=0)  
        self.action_space = Discrete(19)
        self.observation_space = Box(low=0, high=255, shape=(72, 96, 16), dtype=np.uint8)
        self.reward_range = (-1, 1)
        self.obs_stack = deque([], maxlen=4)
        
    def transform_obs(self, raw_obs):
        obs = raw_obs[0]
        obs = observation_preprocessing.generate_smm([obs])
        if not self.obs_stack:
            self.obs_stack.extend([obs] * 4)
        else:
            self.obs_stack.append(obs)
        obs = np.concatenate(list(self.obs_stack), axis=-1)
        obs = np.squeeze(obs)
        return obs

    def reset(self):
        self.obs_stack.clear()
        obs = self.env.reset()
        obs = self.transform_obs(obs)
        return obs
    
    def step(self, action):
        obs, reward, done, info = self.env.step([action])
        obs = self.transform_obs(obs)
        return obs, float(reward), done, info
    
check_env(env=FootballGym(), warn=True)",0,No Code Smell
9668,48292226,4,"def conv3x3(in_channels, out_channels, stride=1):
    return nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=True)

class ResidualBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1):
        super().__init__()
        self.relu = nn.ReLU()
        self.conv1 = conv3x3(in_channels, out_channels, stride)
        self.conv2 = conv3x3(out_channels, out_channels, stride)
        
    def forward(self, x):
        residual = x
        out = self.relu(x)
        out = self.conv1(out)
        out = self.relu(out)
        out = self.conv2(out)
        out += residual
        return out
    
class FootballCNN(BaseFeaturesExtractor):
    def __init__(self, observation_space, features_dim=256):
        super().__init__(observation_space, features_dim)
        in_channels = observation_space.shape[0]  # channels x height x width
        self.cnn = nn.Sequential(
            conv3x3(in_channels=in_channels, out_channels=32),
            nn.MaxPool2d(kernel_size=3, stride=2, dilation=1, ceil_mode=False),
            ResidualBlock(in_channels=32, out_channels=32),
            ResidualBlock(in_channels=32, out_channels=32),
            nn.ReLU(),
            nn.Flatten(),
        )
        self.linear = nn.Sequential(
          nn.Linear(in_features=52640, out_features=features_dim, bias=True),
          nn.ReLU(),
        )

    def forward(self, obs):
        return self.linear(self.cnn(obs))",0,No Code Smell
9669,48292226,5,"scenarios = {0: ""academy_empty_goal_close"",
             1: ""academy_empty_goal"",
             2: ""academy_run_to_score"",
             3: ""academy_run_to_score_with_keeper"",
             4: ""academy_pass_and_shoot_with_keeper"",
             5: ""academy_run_pass_and_shoot_with_keeper"",
             6: ""academy_3_vs_1_with_keeper"",
             7: ""academy_corner"",
             8: ""academy_counterattack_easy"",
             9: ""academy_counterattack_hard"",
             10: ""academy_single_goal_versus_lazy"",
             11: ""11_vs_11_kaggle""}
scenario_name = scenarios[7]",0,No Code Smell
9670,48292226,6,"def make_env(config=None, rank=0):
    def _init():
        env = FootballGym(config)
        log_file = os.path.join(""."", str(rank))
        env = Monitor(env, log_file, allow_early_resets=True)
        return env
    return _init",0,No Code Smell
9671,48292226,7,"n_envs = 4
config={""env_name"":scenario_name}
train_env = DummyVecEnv([make_env(config, rank=i) for i in range(n_envs)])
# train_env = SubprocVecEnv([make_env(config, rank=i) for i in range(n_envs)])

n_steps = 512
policy_kwargs = dict(features_extractor_class=FootballCNN,
                     features_extractor_kwargs=dict(features_dim=256))
# model = PPO(CnnPolicy, train_env, 
#             policy_kwargs=policy_kwargs, 
#             learning_rate=0.000343, 
#             n_steps=n_steps, 
#             batch_size=8, 
#             n_epochs=2, 
#             gamma=0.993,
#             gae_lambda=0.95,
#             clip_range=0.08, 
#             ent_coef=0.003, 
#             vf_coef=0.5, 
#             max_grad_norm=0.64, 
#             verbose=0)
model = PPO.load(""../input/gfootball-stable-baselines3/ppo_gfootball.zip"", train_env)",0,No Code Smell
9672,48292226,8,"from tqdm.notebook import tqdm
class ProgressBar(BaseCallback):
    def __init__(self, verbose=0):
        super(ProgressBar, self).__init__(verbose)
        self.pbar = None

    def _on_training_start(self):
        factor = np.ceil(self.locals['total_timesteps'] / self.model.n_steps)
        n = 1
        try:
            n = len(self.training_env.envs)
        except AttributeError:
            try:
                n = len(self.training_env.remotes)
            except AttributeError:
                n = 1
        total = int(self.model.n_steps * factor / n)
        self.pbar = tqdm(total=total)

    def _on_rollout_start(self):
        self.pbar.refresh()

    def _on_step(self):
        self.pbar.update(1)
        return True

    def _on_rollout_end(self):
        self.pbar.refresh()

    def _on_training_end(self):
        self.pbar.close()
        self.pbar = None

progressbar = ProgressBar()",0,No Code Smell
9673,48292226,9,"total_epochs = 200
total_timesteps = n_steps * n_envs * total_epochs
model.learn(total_timesteps=total_timesteps, callback=progressbar)
model.save(""ppo_gfootball"")",0,No Code Smell
9674,48292226,10,"plt.style.use(['seaborn-whitegrid'])
results_plotter.plot_results(["".""], total_timesteps, results_plotter.X_TIMESTEPS, ""GFootball Timesteps"")
results_plotter.plot_results(["".""], total_timesteps, results_plotter.X_EPISODES, ""GFootball Episodes"")",0,No Code Smell
9675,48292226,11,"plt.style.use(['seaborn-whitegrid'])
log_files = [os.path.join(""."", f""{i}.monitor.csv"") for i in range(n_envs)]

nrows = np.ceil(n_envs/2)
fig = plt.figure(figsize=(8, 2 * nrows))
for i, log_file in enumerate(log_files):
    if os.path.isfile(log_file):
        df = pd.read_csv(log_file, skiprows=1)
        plt.subplot(nrows, 2, i+1, label=log_file)
        df['r'].rolling(window=100).mean().plot(title=f""Rewards: Env {i}"")
        plt.tight_layout()
plt.show()",0,No Code Smell
9676,48292226,12,"model = PPO.load(""ppo_gfootball"")
test_env = FootballGym({""env_name"":scenario_name})
obs = test_env.reset()
done = False
while not done:
    action, state = model.predict(obs, deterministic=True)
    obs, reward, done, info = test_env.step(action)
    print(f""{Action(action).name.ljust(16,' ')}\t{round(reward,2)}\t{info}"")",0,No Code Smell
9677,48292226,13,"%%writefile submission.py
import base64
import pickle
import zlib
import numpy as np
import torch as th
from torch import nn, tensor
from collections import deque
from gfootball.env import observation_preprocessing

state_dict = _STATE_DICT_

state_dict = pickle.loads(zlib.decompress(base64.b64decode(state_dict)))

def conv3x3(in_channels, out_channels, stride=1):
    return nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=True)

class ResidualBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1):
        super().__init__()
        self.relu = nn.ReLU()
        self.conv1 = conv3x3(in_channels, out_channels, stride)
        self.conv2 = conv3x3(out_channels, out_channels, stride)
        
    def forward(self, x):
        residual = x
        out = self.relu(x)
        out = self.conv1(out)
        out = self.relu(out)
        out = self.conv2(out)
        out += residual
        return out
    
class PyTorchCnnPolicy(nn.Module):
    global state_dict
    def __init__(self):
        super().__init__()
        self.cnn = nn.Sequential(
            conv3x3(in_channels=16, out_channels=32),
            nn.MaxPool2d(kernel_size=3, stride=2, dilation=1, ceil_mode=False),
            ResidualBlock(in_channels=32, out_channels=32),
            ResidualBlock(in_channels=32, out_channels=32),
            nn.ReLU(),
            nn.Flatten(),
        )
        self.linear = nn.Sequential(
          nn.Linear(in_features=52640, out_features=256, bias=True),
          nn.ReLU(),
        )
        self.action_net = nn.Sequential(
          nn.Linear(in_features=256, out_features=19, bias=True),
          nn.ReLU(),
        )
        self.out_activ = nn.Softmax(dim=1)
        self.load_state_dict(state_dict)

    def forward(self, x):
        x = tensor(x).float() / 255.0  # normalize
        x = x.permute(0, 3, 1, 2).contiguous()  # 1 x channels x height x width
        x = self.cnn(x)
        x = self.linear(x)
        x = self.action_net(x)
        x = self.out_activ(x)
        return int(x.argmax())
    
obs_stack = deque([], maxlen=4)
def transform_obs(raw_obs):
    global obs_stack
    obs = raw_obs['players_raw'][0]
    obs = observation_preprocessing.generate_smm([obs])
    if not obs_stack:
        obs_stack.extend([obs] * 4)
    else:
        obs_stack.append(obs)
    obs = np.concatenate(list(obs_stack), axis=-1)
    return obs

policy = PyTorchCnnPolicy()
policy = policy.float().to('cpu').eval()
def agent(raw_obs):
    obs = transform_obs(raw_obs)
    action = policy(obs)
    return [action]",0,No Code Smell
9678,48292226,14,"model = PPO.load(""ppo_gfootball"")
_state_dict = model.policy.to('cpu').state_dict()
state_dict = {
    ""cnn.0.weight"":_state_dict['features_extractor.cnn.0.weight'], 
    ""cnn.0.bias"":_state_dict['features_extractor.cnn.0.bias'], 
    ""cnn.2.conv1.weight"":_state_dict['features_extractor.cnn.2.conv1.weight'], 
    ""cnn.2.conv1.bias"":_state_dict['features_extractor.cnn.2.conv1.bias'],
    ""cnn.2.conv2.weight"":_state_dict['features_extractor.cnn.2.conv2.weight'], 
    ""cnn.2.conv2.bias"":_state_dict['features_extractor.cnn.2.conv2.bias'], 
    ""cnn.3.conv1.weight"":_state_dict['features_extractor.cnn.3.conv1.weight'], 
    ""cnn.3.conv1.bias"":_state_dict['features_extractor.cnn.3.conv1.bias'], 
    ""cnn.3.conv2.weight"":_state_dict['features_extractor.cnn.3.conv2.weight'], 
    ""cnn.3.conv2.bias"":_state_dict['features_extractor.cnn.3.conv2.bias'], 
    ""linear.0.weight"":_state_dict['features_extractor.linear.0.weight'], 
    ""linear.0.bias"":_state_dict['features_extractor.linear.0.bias'], 
    ""action_net.0.weight"":_state_dict['action_net.weight'],
    ""action_net.0.bias"":_state_dict['action_net.bias'],
}
state_dict = base64.b64encode(zlib.compress(pickle.dumps(state_dict)))
with open('submission.py', 'r') as file:
    src = file.read()
src = src.replace(""_STATE_DICT_"", f""{state_dict}"")
with open('submission.py', 'w') as file:
    file.write(src)",0,No Code Smell
9679,48292226,15,"kaggle_env = make(""football"", debug = False,
                  configuration={""scenario_name"": scenario_name, 
                                 ""running_in_notebook"": True,
                                 ""save_video"": False})",0,No Code Smell
9680,48292226,16,"output = kaggle_env.run([""submission.py"", ""do_nothing""])",0,No Code Smell
9681,48292226,17,"scores = output[-1][0][""observation""][""players_raw""][0][""score""]
print(""Scores  {0} : {1}"".format(*scores))
print(""Rewards {0} : {1}"".format(output[-1][0][""reward""], output[-1][1][""reward""]))",0,No Code Smell
9682,48292226,18,viz = visualize(output),0,No Code Smell
9683,48292226,19,HTML(viz.to_html5_video()),0,No Code Smell
9684,47881384,0,"# Install:
# Kaggle environments.
!git clone https://github.com/Kaggle/kaggle-environments.git
!cd kaggle-environments && pip install .

# GFootball environment.
!apt-get update -y
!apt-get install -y libsdl2-gfx-dev libsdl2-ttf-dev

# Make sure that the Branch in git clone and in wget call matches !!
!git clone -b v2.7 https://github.com/google-research/football.git
!mkdir -p football/third_party/gfootball_engine/lib

!wget https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_v2.7.so -O football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so
!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install .",0,No Code Smell
9685,47881384,1,"%%writefile agent.py

# Importing Important Imports
from kaggle_environments.envs.football.helpers import *
import math

class Vector:

	'''
	Vector Object
	Parameters: Iterable of length 2 or 3
	'''

	def __init__(self, positions = [0, 0, 0]):
		if len(positions) < 3: positions.append(0)
		self.x, self.y, self.z = positions
		self.vel = None

	def dist(self, other):
		''' Euclidean distance '''
		return math.hypot(other.x - self.x, other.y - self.y)
	
	def add(self, vel):
		''' Adds one vector to the other '''
		return Vector([self.x + vel.x, self.y + vel.y, self.z + vel.z])

	def mult(self, x):
		''' Scales the vector by x '''
		return Vector([self.x * x, self.y * x, self.z * x])

@human_readable_agent
def agent(obs):

	''' Main Agent '''

	# Loading Variables

	N = len(obs['left_team'])

	# Teams
	team = list(map(Vector, obs['left_team']))
	opponents = list(map(Vector, obs['right_team']))

	# Indexes of Active Players
	baller = obs['ball_owned_player']
	active = obs['active']

	# Key Players
	player = team[active]
	goalkeeper = opponents[0]

	# Ball Variables
	ballOwned = (obs['ball_owned_team'] == 0 and active == baller)
	ball = Vector(obs['ball'])
	ball.vel = Vector(obs['ball_direction'])

	# Special Helpers
	sticky = obs['sticky_actions']
	mode = obs['game_mode']

	# Enemy Goal
	target = Vector([1, 0])

	# Directions for movement
	directions = [
		[Action.TopLeft, Action.Top, Action.TopRight],
		[Action.Left, Action.Idle, Action.Right],
		[Action.BottomLeft, Action.Bottom, Action.BottomRight]
	]

	def stickyCheck(action, direction):
		''' Checking for direction and actions '''
		if direction not in sticky:
			return direction
		return action
	
	def dirsign(value):
		''' Getting index for directions '''
		if abs(value) < 0.01: return 1
		elif value < 0: return 0
		return 2

	def getDirection(target, position = player):
		''' Getting direction to move from position to target '''
		xdir = dirsign(target.x - position.x)
		ydir = dirsign(target.y - position.y)
		return directions[ydir][xdir]

	# Always Sprint
	if Action.Sprint not in sticky:
		return Action.Sprint

	# Offense Patterns
	if ballOwned:

		# Special Situations
		if mode in [GameMode.Penalty, GameMode.Corner, GameMode.FreeKick]:
			if player.x > 0: return Action.Shot
			return Action.LongPass

		# Goalkeeper Check
		if baller == 0: 
			return Action.LongPass
		
		# Bad Angle Pass
		if abs(player.y) > 0.2 and player.x > 0.7: 
			return Action.HighPass
			
		# Close to Goalkeeper Shot
		if player.dist(goalkeeper) < 0.4:
			return Action.Shot

		# Goalkeeper is Out
		if goalkeeper.dist(target) > 0.2:
			if player.x > 0:
				return Action.Shot

		#####################
		## Your Ideas Here ##
		#####################

		# Run to Goal
		return getDirection(target)

	# Defensive Patterns
	else:

		# Find the Ball's Next Positions
		nextBall = ball.add(ball.vel.mult(3))

		# Running to the next Ball Position
		if ball.dist(player) > 0.005:
			return getDirection(nextBall)
		
		# Sliding
		elif ball.dist(player) <= 0.005:
			return Action.Slide
		
		# Running Directly at the Ball
		return getDirection(ball)",0,No Code Smell
9686,47881384,2,"%%writefile visualizer.py
from matplotlib import animation, patches, rcParams
from matplotlib import pyplot as plt
from kaggle_environments.envs.football.helpers import *

WIDTH = 110
HEIGHT = 46.2
PADDING = 10


def initFigure(figwidth=12):
    figheight = figwidth * (HEIGHT + 2 * PADDING) / (WIDTH + 2 * PADDING)

    fig = plt.figure(figsize=(figwidth, figheight))
    ax = plt.axes(xlim=(-PADDING, WIDTH + PADDING), ylim=(-PADDING, HEIGHT + PADDING))
    plt.axis(""off"")
    return fig, ax


def drawPitch(ax):
    paint = ""white""

    # Grass around pitch
    rect = patches.Rectangle((-PADDING / 2, -PADDING / 2), WIDTH + PADDING, HEIGHT + PADDING,
                             lw=1, ec=""black"", fc=""#3f995b"", capstyle=""round"")
    ax.add_patch(rect)

    # Pitch boundaries
    rect = plt.Rectangle((0, 0), WIDTH, HEIGHT, ec=paint, fc=""None"", lw=2)
    ax.add_patch(rect)

    # Middle line
    plt.plot([WIDTH / 2, WIDTH / 2], [0, HEIGHT], color=paint, lw=2)

    # Dots
    dots_x = [11, WIDTH / 2, WIDTH - 11]
    for x in dots_x:
        plt.plot(x, HEIGHT / 2, ""o"", color=paint, lw=2)

    # Penalty box
    penalty_box_dim = [16.5, 40.3]
    penalty_box_pos_y = (HEIGHT - penalty_box_dim[1]) / 2

    rect = plt.Rectangle((0, penalty_box_pos_y),
                         penalty_box_dim[0], penalty_box_dim[1], ec=paint, fc=""None"", lw=2)
    ax.add_patch(rect)
    rect = plt.Rectangle((WIDTH, penalty_box_pos_y), -
                         penalty_box_dim[0], penalty_box_dim[1], ec=paint, fc=""None"", lw=2)
    ax.add_patch(rect)

    # Goal box
    goal_box_dim = [5.5, penalty_box_dim[1] - 11 * 2]
    goal_box_pos_y = (penalty_box_pos_y + 11)

    rect = plt.Rectangle((0, goal_box_pos_y),
                         goal_box_dim[0], goal_box_dim[1], ec=paint, fc=""None"", lw=2)
    ax.add_patch(rect)
    rect = plt.Rectangle((WIDTH, goal_box_pos_y),
                         -goal_box_dim[0], goal_box_dim[1], ec=paint, fc=""None"", lw=2)
    ax.add_patch(rect)

    # Goals
    goal_width = 0.044 / 0.42 * HEIGHT
    goal_pos_y = (HEIGHT / 2 - goal_width / 2)
    rect = plt.Rectangle((0, goal_pos_y), -2, goal_width,
                         ec=paint, fc=paint, lw=2, alpha=0.3)
    ax.add_patch(rect)
    rect = plt.Rectangle((WIDTH, goal_pos_y), 2, goal_width,
                         ec=paint, fc=paint, lw=2, alpha=0.3)
    ax.add_patch(rect)

    # Middle circle
    mid_circle = plt.Circle([WIDTH / 2, HEIGHT / 2], 9.15, color=paint, fc=""None"", lw=2)
    ax.add_artist(mid_circle)

    # Penalty box arcs
    left = patches.Arc([11, HEIGHT / 2], 2 * 9.15, 2 * 9.15,
                       color=paint, fc=""None"", lw=2, angle=0, theta1=308, theta2=52)
    ax.add_patch(left)
    right = patches.Arc([WIDTH - 11, HEIGHT / 2], 2 * 9.15, 2 * 9.15,
                        color=paint, fc=""None"", lw=2, angle=180, theta1=308, theta2=52)
    ax.add_patch(right)

    # Arcs on corners
    corners = [[0, 0], [WIDTH, 0], [WIDTH, HEIGHT], [0, HEIGHT]]
    angle = 0
    for x, y in corners:
        c = patches.Arc([x, y], 2, 2,
                        color=paint, fc=""None"", lw=2, angle=angle, theta1=0, theta2=90)
        ax.add_patch(c)
        angle += 90


def scale_x(x):
    return (x + 1) * (WIDTH / 2)


def scale_y(y):
    return (y + 0.42) * (HEIGHT / 0.42 / 2)


def extract_data(raw_obs):
    obs = raw_obs[0][""observation""][""players_raw""][0]
    res = dict()
    res[""left_team""] = [(scale_x(x), scale_y(y)) for x, y in obs[""left_team""]]
    res[""right_team""] = [(scale_x(x), scale_y(y)) for x, y in obs[""right_team""]]

    ball_x, ball_y, ball_z = obs[""ball""]
    res[""ball""] = [scale_x(ball_x), scale_y(ball_y), ball_z]
    res[""score""] = obs[""score""]
    res[""steps_left""] = obs[""steps_left""]
    res[""ball_owned_team""] = obs[""ball_owned_team""]

    left_active = raw_obs[0][""observation""][""players_raw""][0][""active""]
    res[""left_player""] = res[""left_team""][left_active]

    right_active = raw_obs[1][""observation""][""players_raw""][0][""active""]
    res[""right_player""] = res[""right_team""][right_active]

    res[""right_team_roles""] = obs[""right_team_roles""]
    res[""left_team_roles""] = obs[""left_team_roles""]
    res[""left_team_direction""] = obs[""left_team_direction""]
    res[""right_team_direction""] = obs[""right_team_direction""]
    res[""game_mode""] = GameMode(obs[""game_mode""]).name
    return res


def draw_team(obs, team, side):
    x_coords, y_coords = zip(*obs[side])
    team.set_data(x_coords, y_coords)


def draw_ball(obs, ball):
    ball.set_markersize(8 + obs[""ball""][2])  # Scale size of ball based on height
    ball.set_data(obs[""ball""][:2])


def draw_active_players(obs, left_player, right_player):
    x1, y1 = obs[""left_player""]
    left_player.set_data(x1, y1)

    x2, y2 = obs[""right_player""]
    right_player.set_data(x2, y2)

    if obs[""ball_owned_team""] == 0:
        left_player.set_markerfacecolor(""yellow"")
        left_player.set_markersize(20)
        right_player.set_markerfacecolor(""blue"")
        right_player.set_markersize(18)
    elif obs[""ball_owned_team""] == 1:
        left_player.set_markerfacecolor(""firebrick"")
        left_player.set_markersize(18)
        right_player.set_markerfacecolor(""yellow"")
        right_player.set_markersize(20)
    else:
        left_player.set_markerfacecolor(""firebrick"")
        left_player.set_markersize(18)
        right_player.set_markerfacecolor(""blue"")
        right_player.set_markersize(18)


def draw_team_active(obs, team_left_active, team_right_active):
    team_left_active.set_data(WIDTH / 2 - 7, -7)
    team_right_active.set_data(WIDTH / 2 + 7, -7)

    if obs[""ball_owned_team""] == 0:
        team_left_active.set_markerfacecolor(""indianred"")
    else:
        team_left_active.set_markerfacecolor(""mistyrose"")

    if obs[""ball_owned_team""] == 1:
        team_right_active.set_markerfacecolor(""royalblue"")
    else:
        team_right_active.set_markerfacecolor(""lightcyan"")


def draw_players_directions(obs, directions, side):
    index = 0
    if ""right"" in side:
        index = 11
    for i, player_dir in enumerate(obs[f""{side}_direction""]):
        x_dir, y_dir = player_dir
        dist = (x_dir ** 2 + y_dir ** 2)**0.5 + 0.00001  # to prevent division by 0
        x = obs[side][i][0]
        y = obs[side][i][1]
        directions[i + index].set_data([x, x + x_dir / dist], [y, y + y_dir / dist])


def player_actions(step, side):
    if side == 0:
        actions = {0: ""idle"", 1: ""←"", 2: ""↖"", 3: ""↑"", 4: ""↗"", 5: ""→"", 6: ""↘"", 7: ""↓"", 8: ""↙"",
                   9: ""l_pass"", 10: ""h_pass"", 11: ""s_pass"", 12: ""shot"",
                   13: ""sprint"", 14: ""rel_dir"", 15: ""rel_spr"",
                   16: ""slide"", 17: ""dribble"", 18: ""stp_drb""}
    else:
        actions = {0: ""idle"", 1: ""→"", 2: ""↘"", 3: ""↓"", 4: ""↙"", 5: ""←"", 6: ""↖"", 7: ""↑"", 8: ""↗"",
                   9: ""l_pass"", 10: ""h_pass"", 11: ""s_pass"", 12: ""shot"",
                   13: ""sprint"", 14: ""rel_dir"", 15: ""rel_spr"",
                   16: ""slide"", 17: ""dribble"", 18: ""stp_drb""}

    obs = step[side][""observation""][""players_raw""][0]

    if obs[""sticky_actions""][8]:
        spr = ""+spr""
    else:
        spr = ""-spr""

    if obs[""sticky_actions""][9]:
        drb = ""+drb""
    else:
        drb = ""-drb""

    if 1 in obs[""sticky_actions""][0:8]:
        i = obs[""sticky_actions""][0:8].index(1) + 1
        drn = actions[i]
    else:
        drn = ""|""

    if step[side][""action""]:
        act = actions[step[side][""action""][0]]
    else:
        act = ""idle""

    return f""{spr} {drb} {drn} [{act}]"".ljust(24, "" "")


steps = None
drawings = None
directions = None
ball = left_player = right_player = None
team_left = team_right = None
team_left_active = team_right_active = None
team_left_actions = team_right_actions = None
team_left_number = team_right_number = None
team_left_direction = team_right_direction = None
text_frame = game_mode = match_info = None


def init():
    ball.set_data([], [])
    left_player.set_data([], [])
    right_player.set_data([], [])
    team_left.set_data([], [])
    team_right.set_data([], [])
    team_left_active.set_data([], [])
    team_right_active.set_data([], [])
    return drawings


def animate(i):
    obs = extract_data(steps[i])

    # Draw info about ball possesion
    draw_active_players(obs, left_player, right_player)
    draw_team_active(obs, team_left_active, team_right_active)

    # Draw players
    draw_team(obs, team_left, ""left_team"")
    draw_team(obs, team_right, ""right_team"")

    draw_players_directions(obs, directions, ""left_team"")
    draw_players_directions(obs, directions, ""right_team"")

    draw_ball(obs, ball)

    # Draw textual informations
    text_frame.set_text(f""Step {i}/{obs['steps_left'] + i - 1}"")
    game_mode.set_text(f""{obs['game_mode']} Mode"")

    score_a, score_b = obs[""score""]
    match_info.set_text(f""{score_a} : {score_b}"")

    team_left_actions.set_text(player_actions(steps[i], 0))
    team_right_actions.set_text(player_actions(steps[i], 1))

    team_left_number.set_text(str(steps[i][0][""observation""][""players_raw""][0][""active""]))
    team_right_number.set_text(str(steps[i][1][""observation""][""players_raw""][0][""active""]))

    return drawings


def visualize(trace):
    global steps
    global drawings
    global directions
    global ball, left_player, right_player
    global team_left, team_right
    global team_left_active, team_right_active
    global text_frame, game_mode, match_info
    global team_left_actions, team_right_actions
    global team_left_number, team_right_number
    global team_left_direction, team_right_direction

    rcParams['font.family'] = 'monospace'
    rcParams['font.size'] = 12

    steps = trace

    fig, ax = initFigure()
    drawPitch(ax)
    ax.invert_yaxis()

    left_player, = ax.plot([], [], ""o"", ms=18, mfc=""firebrick"", mew=0, alpha=0.5)
    right_player, = ax.plot([], [], ""o"", ms=18, mfc=""blue"", mew=0, alpha=0.5)
    team_left, = ax.plot([], [], ""o"", ms=12, mfc=""firebrick"", mew=1, mec=""white"")
    team_right, = ax.plot([], [], ""o"", ms=12, mfc=""blue"", mew=1, mec=""white"")
    ball, = ax.plot([], [], ""o"", ms=8, mfc=""wheat"", mew=1, mec=""black"")

    team_left_active, = ax.plot([], [], ""o"", ms=16, mfc=""mistyrose"", mec=""None"")
    team_right_active, = ax.plot([], [], ""o"", ms=16, mfc=""lightcyan"", mec=""None"")

    textheight = -6
    text_frame = ax.text(-5, textheight, """", ha=""left"")
    match_info = ax.text(WIDTH / 2, textheight, """", ha=""center"", fontweight=""bold"")
    game_mode = ax.text(WIDTH + 5, textheight, """", ha=""right"")

    team_left_actions = ax.text(WIDTH / 4 + 2, textheight, """", ha=""center"")
    team_right_actions = ax.text(3 * WIDTH / 4 + 2, textheight, """", ha=""center"")

    team_left_number = ax.text(WIDTH / 2 - 7, -6.3, """", ha=""center"", fontsize=10)
    team_right_number = ax.text(WIDTH / 2 + 7, -6.3, """", ha=""center"", fontsize=10)

    # Drawing of directions definitely can be done in a better way
    directions = []
    for _ in range(22):
        direction, = ax.plot([], [], color=""yellow"", lw=1.5)
        directions.append(direction)

    drawings = [team_left_active, team_right_active, left_player, right_player,
                team_left, team_right, ball, text_frame, match_info,
                game_mode, team_left_actions, team_right_actions, team_left_number, team_right_number]

    fig.subplots_adjust(left=0, bottom=0, right=1, top=1, wspace=None, hspace=None)
    anim = animation.FuncAnimation(fig, animate, init_func=init, blit=True,
                                   interval=100, frames=len(steps), repeat=True)
    return anim",0,No Code Smell
9687,47881384,3,"from kaggle_environments import make
from kaggle_environments import agent

from visualizer import visualize
from IPython.display import HTML

env = make(""football"", configuration={""save_video"": True, ""scenario_name"": ""11_vs_11_kaggle"", ""running_in_notebook"": True}, debug = True)
output = env.run(['agent.py', 'do_nothing'])
scores = output[-1][0][""observation""][""players_raw""][0][""score""]

print(""Scores  {0} : {1}"".format(*scores))

viz = visualize(output)
HTML(viz.to_html5_video())",0,No Code Smell
9688,46920963,0,"import cv2
import numpy as np
import matplotlib.pyplot as plt",1,Code Smell
9689,46920963,1,"# check if episode is rendered in 3D or 2D
# returns:
# 1: 3D
# 0: 2D
# None: could not open episode

def is3d(episode, verbose=False):

    green_channel = 1
    threshold = 100
    
    # open video
    url = f'https://www.kaggleusercontent.com/episodes/{episode}.webm'
    vcap = cv2.VideoCapture(url)
    # get first frame
    ret, frame = vcap.read()
    
    #close video
    vcap.release()

    if ret == False:
        if verbose:
            print(f'cannot read episode: {episode}!')
        return False
    else:
        mean = np.mean(frame[:,:,green_channel])
        if mean > threshold:
            # 3D
            result = 1
        else:
            result = 0
        if verbose:
            print(f'Green Channel mean: {mean}')
            if result == 1:
                print(""3D"")
            else:
                print(""2D"")
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            plt.imshow(frame)
            plt.show()
        return result",0,No Code Smell
9690,46920963,2,"print(is3d(episode='4682501', verbose=True))",0,No Code Smell
9691,46920963,3,"print(is3d(episode='4576290', verbose=True))",0,No Code Smell
9692,46920963,4,"print(is3d(episode='asdsa', verbose=True)) ",0,No Code Smell
9693,46920963,5,,0,No Code Smell
9694,45963277,0,"# Install:
# Kaggle environments.
!git clone https://github.com/Kaggle/kaggle-environments.git
!cd kaggle-environments && pip install .

# GFootball environment.
!apt-get update -y
!apt-get install -y libsdl2-gfx-dev libsdl2-ttf-dev

# Make sure that the Branch in git clone and in wget call matches !!
!git clone -b v2.6 https://github.com/google-research/football.git
!mkdir -p football/third_party/gfootball_engine/lib

!wget https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_v2.6.so -O football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so
!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install .",0,No Code Smell
9695,45963277,1,"%%writefile submission.py
from kaggle_environments.envs.football.helpers import *
from math import sqrt

directions = [
[Action.TopLeft, Action.Top, Action.TopRight],
[Action.Left, Action.Idle, Action.Right],
[Action.BottomLeft, Action.Bottom, Action.BottomRight]]

dirsign = lambda x: 1 if abs(x) < 0.01 else (0 if x < 0 else 2)

enemyGoal = [1, 0]  


def inside(pos, area):
    return area[0][0] <= pos[0] <= area[0][1] and area[1][0] <= pos[1] <= area[1][1]


def get_distance(pos1,pos2):
    return ((pos1[0]-pos2[0])**2+(pos1[1]-pos2[1])**2)**0.5


def player_direction(obs):
    controlled_player_pos = obs['left_team'][obs['active']]
    controlled_player_dir = obs['left_team_direction'][obs['active']]
    x = controlled_player_pos[0]
    y = controlled_player_pos[1]
    dx = controlled_player_dir[0]
    dy = controlled_player_dir[1]
#      if x <= dx:
#         return 0
#     if x > dx:
#         return 1
    X_dir, Y_dir = dirsign(dx), dirsign(dy) # goes straight
    return directions[Y_dir][X_dir]


def do_toward(obs, action, CUR_DIR, x, y, tx, ty, just_x=False):
    try:
        xdir = dirsign(tx - x)
        ydir = dirsign(ty - y)
        SEL_DIR = directions[ydir][xdir]
        if just_x:
            # for quick action
            # todo: select closer dir by finding which ydir is currently
            Accepted_dirs = [directions[tmp][xdir] for tmp in [0,1,2]]
            if CUR_DIR not in Accepted_dirs:
                return SEL_DIR
        else:
            if SEL_DIR != CUR_DIR:
                return SEL_DIR
    except:
        pass
    
    if SEL_DIR not in obs['sticky_actions']:
        return SEL_DIR
    
    return action


def run_pass(left_team,right_team,x,y):
    # Go and Pass
    teammateL=0
    teammateR=0
    for i in range(len(left_team)):
        #is there a teamate close to left
        if left_team[i][0] >= x:
            if left_team[i][1] < y:
                if abs(left_team[i][1] - x) <.05:
                    teammateL=teammateL+1
        
        #is there a teamate to right
        if left_team[i][0] >= x:
            if left_team[i][1] > y:
                if abs(left_team[i][1] - x) <.05:
                    teammateR=teammateR+1
    #pass only close to goal
    if x >.75:
        if teammateL > 0 or teammateR > 0:
            return Action.ShortPass
    return Action.Right
    

@human_readable_agent
def agent(obs):
    
    CUR_DIR = player_direction(obs)
    left_team,right_team = obs['left_team'],obs['right_team']
    # our selected player
    controlled_player_pos = left_team[obs['active']]
    # player position
    x,y = controlled_player_pos[0],controlled_player_pos[1]
    #vector where ball is going
    ball_targetx=obs['ball'][0]+obs['ball_direction'][0]
    ball_targety=obs['ball'][1]+obs['ball_direction'][1]
    
    
    # special plays. new start
    if obs[""game_mode""] == GameMode.Penalty:
        return Action.Shot
    if obs[""game_mode""] == GameMode.Corner:
        if controlled_player_pos[0] > 0:
            return Action.Shot
    if obs[""game_mode""] == GameMode.FreeKick:
        return Action.Shot
    if obs[""game_mode""] == GameMode.GoalKick:
        return Action.HighPass
    
    if obs[""game_mode""] in [GameMode.KickOff]:
        # start of game / after goal
        pass
    
    # SPRINT
    if  Action.Sprint not in obs['sticky_actions']:
        return Action.Sprint


    # Does the player we control have the ball?
    if obs['ball_owned_player'] == obs['active'] and obs['ball_owned_team'] == 0:
        
        if obs['right_team'][0][0] < 0.8:
            return Action.Shot
        
        if inside(controlled_player_pos, [[0.6, 1], [-0.2, 0.2]]):
            if x < obs['ball'][0]:
                return Action.Shot
        
        if inside(controlled_player_pos, [[0.2, 1], [-0.25, 0.25]]):
            if y>0:
                return do_toward(obs, Action.Shot, CUR_DIR, x, y, 1.02, 0.03)
            else:
                return do_toward(obs, Action.Shot, CUR_DIR, x, y, 1.02, -0.03)
        
        # if close to goal and too wide for shot pass the ball
        if x >.75:
            if abs(y) >.3:
                return Action.HighPass
            elif abs(y) >.20:
                return Action.LongPass
        
        if -0.2 < x < 0.2:
            return do_toward(obs, Action.ShortPass, CUR_DIR, x, y, 1.02, 0.03, True)
        
        # defense
        if x < -0.2:
            return Action.HighPass
            
        # from sides
        if abs(y) > 0.2:
            if x > 0:
                return Action.LongPass
            else:
                return Action.HighPass
        
        # which way should we run or pass
        return run_pass(left_team,right_team,x,y)
    else:
        #euclidian distance to the ball so we head off movement until very close
        e_dist=get_distance(obs['left_team'][obs['active']],obs['ball'])
        
        #if not close to ball move to where it is going
        if e_dist >.005:
            # Run where ball will be
            xdir = dirsign(ball_targetx - x)
            ydir = dirsign(ball_targety - y)
            return directions[ydir][xdir]
        else:
            #if close to ball go to ball
            return Action.Slide
",0,No Code Smell
9696,45963277,2,"# A sample game: Our agent vs. do-nothing BOT
from kaggle_environments import make
env = make(""football"", configuration={""save_video"": True, ""scenario_name"": ""11_vs_11_kaggle"", ""running_in_notebook"": True})
output = env.run([""/kaggle/working/submission.py"",""do_nothing""])[-1]
print('vs do nothing')
print('(THIS)Left player: reward = %s, status = %s, info = %s' % (output[0]['reward'], output[0]['status'], output[0]['info']))
print('(BOT)Right player: reward = %s, status = %s, info = %s' % (output[1]['reward'], output[1]['status'], output[1]['info']))
env.render(mode=""human"", width=600, height=400)",0,No Code Smell
9697,45963277,3,"!find . ! -name 'submission.py' -type d -exec rm -r -f {} +
!ls",1,Code Smell
9698,43660797,0,"!apt-get update
!apt-get install -y libsdl2-gfx-dev libsdl2-ttf-dev
!pip3 install tensorflow==2.2
!pip3 install tensorflow_probability==0.9.0
!pip3 install kaggle-environments -U

from IPython.display import Image, clear_output

clear_output()",0,No Code Smell
9699,43660797,1,"!cp -r /kaggle/input/gfootball-baseline/* .

!mkdir -p football/third_party/gfootball_engine/lib
!wget https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_v2.3.so -O football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so
!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install .

!mkdir -p football/third_party/gfootball_engine
!cd seed_rl && git checkout 34fb2874d41241eb4d5a03344619fb4e34dd9be6

!mkdir /kaggle_simulations/agent

clear_output()",0,No Code Smell
9700,43660797,2,"%%writefile train.sh
# Training launcher script.

# Make SEED RL visible to Python.
export PYTHONPATH=$PYTHONPATH:$(pwd)
#export PYTHONPATH=$PYTHONPATH:

ENVIRONMENT=$1
AGENT=$2
NUM_ACTORS=$3
shift 3

# Start actor tasks which run environment loop.
actor=0
while [ ""$actor"" -lt ${NUM_ACTORS} ]; do
  python3 seed_rl/${ENVIRONMENT}/${AGENT}_main.py --run_mode=actor --logtostderr $@ --num_actors=${NUM_ACTORS} --task=${actor} 2>/dev/null >/dev/null &
  actor=$(( actor + 1 ))
done
# Start learner task which performs training of the agent.
python3 seed_rl/${ENVIRONMENT}/${AGENT}_main.py --run_mode=learner --logtostderr $@ --num_actors=""${NUM_ACTORS}""",0,No Code Smell
9701,43660797,3,"!bash train.sh football vtrace 8 '--total_environment_frames=1000000 --game=11_vs_11_kaggle --reward_experiment=scoring,checkpoints --logdir=/kaggle_simulations/agent/'

#clear_output()",0,No Code Smell
9702,43660797,4,!ls -la /kaggle_simulations/agent/saved_model,1,Code Smell
9703,43660797,5,"%%writefile /kaggle_simulations/agent/main.py

import collections
import gym
import numpy as np
import os
import sys
import tensorflow as tf

from gfootball.env import observation_preprocessing
from gfootball.env import wrappers

EnvOutput = collections.namedtuple(
    'EnvOutput', 'reward done observation abandoned episode_step')

def prepare_agent_input(observation, prev_action, state):
    # SEED RL agent accepts input in a form of EnvOutput. When not training
    # only observation is used for generating action, so we use a dummy values
    # for the rest.
    env_output = EnvOutput(reward=tf.zeros(shape=[], dtype=tf.float32),
        done=tf.zeros(shape=[], dtype=tf.bool),
        observation=observation, abandoned=False,
        episode_step=tf.zeros(shape=[], dtype=tf.int32))
    # add batch dimension
    prev_action, env_output = tf.nest.map_structure(
        lambda t: tf.expand_dims(t, 0), (prev_action, env_output))

    return (prev_action, env_output, state)

# Previously executed action
previous_action = tf.constant(0, dtype=tf.int64)
# Queue of recent observations (SEED agent we trained uses frame stacking).
observations = collections.deque([], maxlen=4)
# Current state of the agent (used by recurrent agents).
state = ()

# Load previously trained Tensorflow model.
policy = tf.compat.v2.saved_model.load('/kaggle_simulations/agent/saved_model')

def agent(obs):
    global step
    global previous_action
    global observations
    global state
    global policy
    # Get observations for the first (and only one) player we control.
    obs = obs['players_raw'][0]
    # Agent we trained uses Super Mini Map (SMM) representation.
    # See https://github.com/google-research/seed_rl/blob/master/football/env.py for details.
    obs = observation_preprocessing.generate_smm([obs])[0]
    if not observations:
        observations.extend([obs] * 4)
    else:
        observations.append(obs)
    
    # SEED packs observations to reduce transfer times.
    # See PackedBitsObservation in https://github.com/google-research/seed_rl/blob/master/football/observation.py
    obs = np.packbits(obs, axis=-1)
    if obs.shape[-1] % 2 == 1:
        obs = np.pad(obs, [(0, 0)] * (obs.ndim - 1) + [(0, 1)], 'constant')
    obs = obs.view(np.uint16)

    # Execute our agent to obtain action to take.
    agent_output, state = policy.get_action(*prepare_agent_input(obs, previous_action, state))
    previous_action = agent_output.action[0]
    return [int(previous_action)]",0,No Code Smell
9704,43660797,6,"from kaggle_environments import make
env = make(""football"", configuration={""save_video"": True, ""scenario_name"": ""11_vs_11_kaggle"", ""running_in_notebook"": True})
env.run([""/kaggle_simulations/agent/main.py"", ""run_right""])
env.render(mode=""human"", width=800, height=600)",0,No Code Smell
9705,43660797,7,!cd /kaggle_simulations/agent && tar -czvf /kaggle/working/submit.tar.gz main.py saved_model,0,No Code Smell
9706,48788547,0,"# Install:
# Kaggle environments.
#!git clone https://github.com/Kaggle/kaggle-environments.git
#!cd kaggle-environments && pip install .

# GFootball environment.
!apt-get update -y
!apt-get install -y libsdl2-gfx-dev libsdl2-ttf-dev

# Make sure that the Branch in git clone and in wget call matches !!
!git clone -b v2.6 https://github.com/google-research/football.git
!mkdir -p football/third_party/gfootball_engine/lib

!wget https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_v2.6.so -O football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so
!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install .",0,No Code Smell
9707,48788547,1,"!rm -r /kaggle_simulations

!mkdir /kaggle_simulations
!mkdir /kaggle_simulations/agent
!mkdir /kaggle_simulations/agent/saved_model

!cp /kaggle/input/defensive-movementh5/defensive_movement.h5 /kaggle_simulations/agent/saved_model/defensive-movement.h5
!cp /kaggle/input/modelsav/model.sav /kaggle_simulations/agent/saved_model/model.sav
!cp /kaggle/input/model1sav/model1.sav /kaggle_simulations/agent/saved_model/model1.sav",0,No Code Smell
9708,48788547,2,"%%writefile /kaggle_simulations/agent/main.py
from kaggle_environments.envs.football.helpers import *
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
import pickle
import math

import tensorflow as tf

# load the model from disk
filename = '/kaggle_simulations/agent/saved_model/model1.sav'
loaded_model = pickle.load(open(filename, 'rb'))
filename1 = '/kaggle_simulations/agent/saved_model/model.sav'
loaded_model1 = pickle.load(open(filename1, 'rb'))
#result = loaded_model.score(X_test, y_test)
#print(result)

filename = '/kaggle_simulations/agent/saved_model/defensive-movement.h5'
defensive_movement = tf.keras.models.load_model(filename)
defensive_movement.compile(optimizer='adam',
        loss=['mse'],
        metrics=['mae'])

directions = [[Action.TopLeft, Action.Top, Action.TopRight],
[Action.Left, Action.Idle, Action.Right],
[Action.BottomLeft, Action.Bottom, Action.BottomRight]]

#track raw data to troubleshoot...
track_raw_data=[]

perfectRange = [[0.7, 0.95], [-0.12, 0.12]]

def inside(pos, area):
    return area[0][0] <= pos[0] <= area[0][1] and area[1][0] <= pos[1] <= area[1][1]

def get_distance(pos1,pos2):
    return ((pos1[0]-pos2[0])**2+(pos1[1]-pos2[1])**2)**0.5

def get_heading(pos1,pos2):
    raw_head=math.atan2(pos1[0]-pos2[0],pos1[1]-pos2[1])/math.pi*180

    if raw_head<0:
        head=360+raw_head
    else:
        head=raw_head
    return head

def get_action(action_num):
    if action_num==0:
        return Action.Idle
    if action_num==1:
        return Action.Left
    if action_num==2:
        return Action.TopLeft
    if action_num==3:
        return Action.Top
    if action_num==4:
        return Action.TopRight
    if action_num==5:
        return Action.Right
    if action_num==6:
        return Action.BottomRight
    if action_num==7:
        return Action.Bottom
    if action_num==8:
        return Action.BottomLeft
    if action_num==9:
        return Action.LongPass
    if action_num==10:
        return Action.HighPass
    if action_num==11:
        return Action.ShortPass
    if action_num==12:
        return Action.Shot
    if action_num==13:
        return Action.Sprint
    if action_num==14:
        return Action.ReleaseDirection
    if action_num==15:
        return Action.ReleaseSprint
    if action_num==16:
        #return Action.Sliding
        return Action.Idle
    if action_num==17:
        return Action.Dribble
    if action_num==18:
        #return Action.ReleaseDribble
        return Action.Idle
    return Action.Right

@human_readable_agent
def agent(obs):

    
    controlled_player_pos = obs['left_team'][obs['active']]
    x = controlled_player_pos[0]
    y = controlled_player_pos[1]
    pactive=obs['active']
    
    if obs[""game_mode""] == GameMode.Penalty:
        return Action.Shot
    if obs[""game_mode""] == GameMode.Corner:
        if controlled_player_pos[0] > 0:
            return Action.Shot
    if obs[""game_mode""] == GameMode.FreeKick:
        return Action.Shot
    
    # Make sure player is running.
    if  0 < controlled_player_pos[0] < 0.6 and Action.Sprint not in obs['sticky_actions']:
        return Action.Sprint
    elif 0.6 < controlled_player_pos[0] and Action.Sprint in obs['sticky_actions']:
        return Action.ReleaseSprint
    
    #if we have the ball
    if obs['ball_owned_player'] == obs['active'] and obs['ball_owned_team'] == 0:
        dat=[]
        
        to_append=[]
        to_append1= []
        #return Action.Right
        #get controller player pos
        controlled_player_pos = obs['left_team'][obs['active']]

        if inside(controlled_player_pos, perfectRange) and controlled_player_pos[0] < obs['ball'][0]:
            return Action.Shot
        
        goalx=0.0
        goaly=0.0

        sidelinex=0.0
        sideliney=0.42

        to_append.append(x)
        to_append.append(y)
        
        goal_dist=get_distance((x,y),(goalx,goaly))
        sideline_dist=get_distance((x,y),(sidelinex,sideliney))
        to_append.append(goal_dist)
        to_append.append(sideline_dist)
        to_append1.append(goal_dist)
        to_append1.append(sideline_dist)
        
        for i in range(len(obs['left_team'])):
            dist=get_distance((x,y),(obs['left_team'][i][0],obs['left_team'][i][1]))
            head=get_heading((x,y),(obs['left_team'][i][0],obs['left_team'][i][1]))
            to_append.append(dist)
            to_append.append(head)
            to_append1.append(dist)
            to_append1.append(head)
        
        for i in range(len(obs['right_team'])):
            dist=get_distance((x,y),(obs['right_team'][i][0],obs['right_team'][i][1]))
            head=get_heading((x,y),(obs['right_team'][i][0],obs['right_team'][i][1]))
            to_append.append(dist)
            to_append.append(head)
            to_append1.append(dist)
            to_append1.append(head)
        
        
        if (len(obs['sticky_actions']) != 10):
            dat1 = []
            dat1.append(to_append1)
            predicted1=loaded_model1.predict(dat1)
            do=get_action(predicted1)
            if do == None:
                return Action.Right
            else:
                return do


        for i in range(10):
            to_append.append(obs['sticky_actions'][i])
        
        dat.append(to_append)
        dat1 = []
        dat1.append(to_append1)
        
        predicted=loaded_model.predict(dat)
        predicted1=loaded_model1.predict(dat1)

        if (predicted >= 9 and predicted <= 12):
            predicted1 = predicted
        
        do=get_action(predicted1)
        
        if do == None:
            return Action.Right
        else:
            return do
    
    # if we don't have ball run to ball
    else:

        to_append = []
        dat = []

        controlled_player_pos = obs['left_team'][obs['active']]
        x = controlled_player_pos[0]
        y = controlled_player_pos[1]
        # controlled_player_dir = obs['left_team_direction'][obs['active']]

        to_append.append(x)
        to_append.append(y)
        # to_append.append(controlled_player_dir[0])
        # to_append.append(controlled_player_dir[1])

        ballpos = obs['ball']

        to_append.append(ballpos[0])
        to_append.append(ballpos[1])
        # to_append.append(ballpos[2])

        balldir = obs['ball_direction']

        to_append.append(balldir[0])
        to_append.append(balldir[1])
        # to_append.append(balldir[2])


        if (obs['ball_owned_team'] == 1):
            to_append.append(1)
        else:
            to_append.append(0)


        if Action.Sprint in obs['sticky_actions']:
            to_append.append(1)
        else:
            to_append.append(0)

        if Action.Dribble in obs['sticky_actions']:
            return Action.ReleaseDribble
        
        
        dirsign = lambda x: 1 if abs(x) < 0.01 else (0 if x < 0 else 2)
        #where ball is going
        ball_targetx=obs['ball'][0]+(obs['ball_direction'][0]*5)
        ball_targety=obs['ball'][1]+(obs['ball_direction'][1]*5)
        e_dist=get_distance(obs['left_team'][obs['active']],obs['ball'])
        if e_dist > 1:
            if e_dist >.01:
                # Run where ball will be
                xdir = dirsign(ball_targetx - controlled_player_pos[0])
                ydir = dirsign(ball_targety - controlled_player_pos[1])
                return directions[ydir][xdir]
            else:
                # Run towards the ball.
                xdir = dirsign(obs['ball'][0] - controlled_player_pos[0])
                ydir = dirsign(obs['ball'][1] - controlled_player_pos[1])
                return directions[ydir][xdir]

        dat.append(to_append)
        predicted = defensive_movement.predict(dat)

        ydir = int(round(predicted[0][0]))
        xdir = int(round(predicted[0][1]))

        return directions[ydir][xdir]",0,No Code Smell
9709,48788547,3,"# Set up the Environment.
from kaggle_environments import make
env = make(""football"", debug=False, configuration={""save_video"": True, ""scenario_name"": ""11_vs_11_kaggle"", ""running_in_notebook"": True})
output = env.run([""/kaggle_simulations/agent/main.py"", ""do_nothing""])[-1]
print('Left player: reward = %s, status = %s, info = %s' % (output[0]['reward'], output[0]['status'], output[0]['info']))
print('Right player: reward = %s, status = %s, info = %s' % (output[1]['reward'], output[1]['status'], output[1]['info']))
env.render(mode=""human"", width=800, height=600)",0,No Code Smell
9710,48788547,4,!cd /kaggle_simulations/agent && tar -czvf /kaggle/working/submit.tar.gz main.py saved_model,0,No Code Smell
9711,47213571,0,"### This is a fork of https://www.kaggle.com/david1013/tunable-baseline-bot


##  Any upvotes below to his original notebook.",0,No Code Smell
9712,47213571,1,"%%writefile submission.py

## TUNABLE BASELINE BOT

# Tune Here:
SPRINT_RANGE = 0.6

SHOT_RANGE_X = 0.7  
SHOT_RANGE_Y = 0.2

GOALIE_OUT = 0.2
LONG_SHOT_X = 0.4
LONG_SHOT_Y = 0.2
",1,Code Smell
9713,47213571,2,,0,No Code Smell
9714,47213571,3,"# Install:
# Kaggle environments.
!git clone --quiet https://github.com/Kaggle/kaggle-environments.git
!cd kaggle-environments && pip install -q .

# GFootball environment.
!apt-get -qq update -y
!apt-get -qq install -y libsdl2-gfx-dev libsdl2-ttf-dev

# Make sure that the Branch in git clone and in wget call matches !!
!git clone --quiet -b v2.3 https://github.com/google-research/football.git
!mkdir -p football/third_party/gfootball_engine/lib

!wget -q https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_v2.3.so -O football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so
!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install -q .
",0,No Code Smell
9715,47213571,4,,0,No Code Smell
9716,47213571,5,"%%writefile -a submission.py

from kaggle_environments.envs.football.helpers import *
from math import sqrt

directions = [
[Action.TopLeft, Action.Top, Action.TopRight],
[Action.Left, Action.Idle, Action.Right],
[Action.BottomLeft, Action.Bottom, Action.BottomRight]]

dirsign = lambda x: 1 if abs(x) < 0.01 else (0 if x < 0 else 2)

enemyGoal = [1, 0]
GOALKEEPER = 0
",0,No Code Smell
9717,47213571,6,"%%writefile -a submission.py

shot_range = [[SHOT_RANGE_X, 1], [-SHOT_RANGE_Y, SHOT_RANGE_Y]]
",1,Code Smell
9718,47213571,7,"%%writefile -a submission.py

def inside(pos, area):
    return area[0][0] <= pos[0] <= area[0][1] and area[1][0] <= pos[1] <= area[1][1]",0,No Code Smell
9719,47213571,8,"%%writefile -a submission.py

@human_readable_agent
def agent(obs):
    controlled_player_pos = obs['left_team'][obs['active']]
    
    if obs[""game_mode""] == GameMode.Penalty:
        return Action.Shot
    if obs[""game_mode""] == GameMode.Corner:
        if controlled_player_pos[0] > 0:
            return Action.Shot
    if obs[""game_mode""] == GameMode.FreeKick:
        return Action.Shot
    
    # Make sure player is running down the field.
    if  0 < controlled_player_pos[0] < SPRINT_RANGE and Action.Sprint not in obs['sticky_actions']:
        return Action.Sprint
    elif SPRINT_RANGE < controlled_player_pos[0] and Action.Sprint in obs['sticky_actions']:
        return Action.ReleaseSprint

    # If our player controls the ball:
    if obs['ball_owned_player'] == obs['active'] and obs['ball_owned_team'] == 0:
        
        if inside(controlled_player_pos, shot_range) and controlled_player_pos[0] < obs['ball'][0]:
            return Action.Shot
        
        elif ( abs(obs['right_team'][GOALKEEPER][0] - 1) > GOALIE_OUT   
                and controlled_player_pos[0] > LONG_SHOT_X and abs(controlled_player_pos[1]) < LONG_SHOT_Y ):
            return Action.Shot
        
        else:
            xdir = dirsign(enemyGoal[0] - controlled_player_pos[0])
            ydir = dirsign(enemyGoal[1] - controlled_player_pos[1])
            return directions[ydir][xdir]
        
    # if we we do not have the ball:
    else:
        # Run towards the ball.
        xdir = dirsign(obs['ball'][0] - controlled_player_pos[0])
        ydir = dirsign(obs['ball'][1] - controlled_player_pos[1])
        return directions[ydir][xdir]",0,No Code Smell
9720,47213571,9,#Test the agent,0,No Code Smell
9721,47213571,10,"import os
import sys 
sys.path.append(os.path.abspath(""/kaggle_simulations/agent/""))
import submission",1,Code Smell
9722,47213571,11,"from kaggle_environments import make
env = make(""football"", 
           debug=True,
           configuration={""save_video"": True, 
                          ""scenario_name"": ""11_vs_11_kaggle"", 
                          ""running_in_notebook"": True,
                          #""actTimeout"": 30,
                         })  
output = env.run([submission.agent, ""/kaggle_simulations/agent/submission.py""])[-1]
print('Left player: reward = %s, status = %s, info = %s' % (output[0]['reward'], output[0]['status'], output[0]['info']))
print('Right player: reward = %s, status = %s, info = %s' % (output[1]['reward'], output[1]['status'], output[1]['info']))
env.render(mode=""human"", width=800, height=600)",1,Code Smell
9723,47213571,12,,0,No Code Smell
9724,46889474,0,"# Install:
# Kaggle environments.
!git clone https://github.com/Kaggle/kaggle-environments.git
!cd kaggle-environments && pip install .

# GFootball environment.
!apt-get update -y
!apt-get install -y libsdl2-gfx-dev libsdl2-ttf-dev

# Make sure that the Branch in git clone and in wget call matches !!
!git clone -b v2.7 https://github.com/google-research/football.git
!mkdir -p football/third_party/gfootball_engine/lib

!wget https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_v2.7.so -O football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so
!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install . 

# Install Gym
!pip install gym

from IPython.display import clear_output
clear_output()",0,No Code Smell
9725,46889474,1,"import gfootball
import gym
import numpy as np
import tensorflow as tf
import random
from collections import deque",1,Code Smell
9726,46889474,2,"class ReplayBuffer:
    def __init__(self, size=1000000):
        self.memory = deque(maxlen=size)
        
    def remember(self, s_t, a_t, r_t, s_t_next, d_t):
        self.memory.append((s_t, a_t, r_t, s_t_next, d_t))
        
    def sample(self,batch_size):
        batch_size = min(batch_size, len(self.memory))
        return random.sample(self.memory,batch_size)",0,No Code Smell
9727,46889474,3,"class SplitLayer(tf.keras.layers.Layer):
    def __init__(self):
        super().__init__()

    def call(self, inputs):
        split0,split1,split2,split3 = tf.split(inputs,4,3)
        return [split0,split1,split2,split3]",0,No Code Smell
9728,46889474,4,"class ConvNN:

    def __init__(self,state_shape,num_actions):
        self.state_shape = state_shape
        self.num_actions = num_actions

    def _model_architecture(self):
    
        frames_input = tf.keras.layers.Input(shape=self.state_shape,name='input',batch_size=1)
        frames_split = SplitLayer()(frames_input)
        conv_branches = []
        for f,frame in enumerate(frames_split):
            conv_branches.append(self._build_conv_branch(frame,f))

        concat = tf.keras.layers.concatenate(conv_branches)
        
        fc0 = tf.keras.layers.Dense(units=8192, name='fc0', 
                                     activation='relu')(concat)
        fc1 = tf.keras.layers.Dense(units=2048, name='fc1', 
                                     activation='relu')(fc0)
        fc2 = tf.keras.layers.Dense(units=512, name='fc2', 
                                     activation='relu')(fc1)
        fc3 = tf.keras.layers.Dense(units=256, name='fc3', 
                                     activation='relu')(fc2)
        fc4 = tf.keras.layers.Dense(units=64, name='fc4', 
                                     activation='relu')(fc3)
        
        action_output = tf.keras.layers.Dense(units=self.num_actions, name='output',
                                               activation='relu')(fc4)
        return frames_input,action_output
        
    @staticmethod
    def _build_conv_branch(frame,number):
        conv1 = tf.keras.layers.Conv2D(16, kernel_size=(8, 8), strides=(4, 4),
                                        name='conv1_frame'+str(number), padding='same',
                                        activation='relu')(frame)
        mp1 = tf.keras.layers.MaxPooling2D(pool_size=2, name=""mp1_frame""+str(number))(conv1)
        conv2 = tf.keras.layers.Conv2D(24, kernel_size=(4, 4), strides=(2, 2),
                                        name='conv2_frame'+str(number),padding='same',
                                        activation='relu')(mp1)
        mp2 = tf.keras.layers.MaxPooling2D(pool_size=2, name=""mp2_frame""+str(number))(conv2)
        conv3 = tf.keras.layers.Conv2D(32, kernel_size=(3, 3), strides=(1, 1),
                                        name='conv3_frame'+str(number),padding='same',
                                        activation='relu')(mp2)
        mp3 = tf.keras.layers.MaxPooling2D(pool_size=2, name=""mp3_frame""+str(number))(conv3)
        conv4 = tf.keras.layers.Conv2D(64, kernel_size=(2, 2), strides=(1, 1),
                                        name='conv4_frame'+str(number), padding='same',
                                        activation='relu')(mp3)
        mp4 = tf.keras.layers.MaxPooling2D(pool_size=1, name=""mp4_frame""+str(number))(conv4)

        flatten = tf.keras.layers.Flatten(name='flatten'+str(number))(mp4)

        return flatten
        
    def build(self):
        frames_input,action_output = self._model_architecture()
        model = tf.keras.Model(inputs=[frames_input], outputs=[action_output])
        return model",0,No Code Smell
9729,46889474,5,"class Agent:
    def __init__(self, state_shape, num_actions,alpha, gamma, epsilon_i=1.0, epsilon_f=0.01, n_epsilon=0.1):
        self.epsilon_i = epsilon_i
        self.epsilon_f = epsilon_f
        self.n_epsilon = n_epsilon
        self.epsilon = epsilon_i
        self.gamma = gamma
        self.state_shape = state_shape
        self.num_actions = num_actions
        self.optimizer = tf.keras.optimizers.Adam(alpha) 

        self.Q = ConvNN(state_shape,num_actions).build()
        self.Q_ = ConvNN(state_shape,num_actions).build()
        
    def synchronize(self):
        self.Q_.set_weights(self.Q.get_weights())

    def act(self, s_t):
        if np.random.random() < self.epsilon:
            return np.random.randint(self.num_actions,size=3)
        return np.argmax(self.Q(s_t), axis=1)
    
    def decay_epsilon(self, n):
        self.epsilon = max(
            self.epsilon_f, 
            self.epsilon_i - (n/self.n_epsilon)*(self.epsilon_i - self.epsilon_f))

    def update(self, s_t, a_t, r_t, s_t_next, d_t):
        with tf.GradientTape() as tape:
            Q_next = tf.stop_gradient(tf.reduce_max(self.Q_(s_t_next), axis=1))
            Q_pred = tf.reduce_sum(self.Q(s_t)*tf.one_hot(a_t, self.num_actions, dtype=tf.float32), axis=1)
            loss = tf.reduce_mean(0.5*(r_t + (1-d_t)*self.gamma*Q_next - Q_pred)**2)
        grads = tape.gradient(loss, self.Q.trainable_variables)
        self.optimizer.apply_gradients(zip(grads, self.Q.trainable_variables))",0,No Code Smell
9730,46889474,6,"class VectorizedEnvWrapper(gym.Wrapper):
    def __init__(self, make_env, num_envs=1):
        super().__init__(make_env())
        self.num_envs = num_envs
        self.envs = [make_env() for env_index in range(num_envs)]
    
    def reset(self):
        return np.asarray([env.reset() for env in self.envs])
    
    def reset_at(self, env_index):
        return self.envs[env_index].reset()
    
    def step(self, actions):
        next_states, rewards, dones, infos = [], [], [], []
        for env, action in zip(self.envs, actions):
            next_state, reward, done, info = env.step(action)
            next_states.append(next_state)
            rewards.append(reward)
            dones.append(done)
            infos.append(info)
        return np.asarray(next_states), np.asarray(rewards), \
            np.asarray(dones), np.asarray(infos)",0,No Code Smell
9731,46889474,7,"class NormalizationWrapper(gym.ObservationWrapper):
    def __init__(self, env):
        super().__init__(env)
    
    def observation(self, obs):
        return obs/255",1,Code Smell
9732,46889474,8,"train_agent = Agent((72,96,4),19,alpha=0.1,gamma=0.95)
train_agent.Q.save_weights(""QWeights"")
train_agent.Q_.save_weights(""Q_Weights"")
smm_env = VectorizedEnvWrapper(lambda: NormalizationWrapper(gym.make(""GFootball-11_vs_11_kaggle-SMM-v0"")),1)
train_buffer = ReplayBuffer()",0,No Code Smell
9733,46889474,9,"def train(env=smm_env,T=3001,batch_size=3,sync_every=10,agent=train_agent,buffer=train_buffer):
    
    agent.Q.load_weights(""QWeights"")
    agent.Q_.load_weights(""Q_Weights"")
    rewards = []
    episode_rewards = 0
    s_t = env.reset()
    
    for t in range(T):
        if t%sync_every == 0:
            agent.synchronize()
        a_t = agent.act(s_t)
        s_t_next, r_t, d_t, info = env.step(a_t)
        buffer.remember(s_t, a_t, r_t, s_t_next, d_t)
        for batch in buffer.sample(batch_size):
            agent.update(*batch)
        agent.decay_epsilon(t/T)
        episode_rewards += r_t
           
    for i in range(env.num_envs):
        rewards.append(episode_rewards[i])
        episode_rewards[i] = 0
        s_t[i] = env.reset_at(i)
        
    agent.epsilon_i = 1.0
    agent.epsilon_f = 0.01
    agent.n_epsilon = 0.1
    agent.epsilon = agent.epsilon_i
    
    return rewards",0,No Code Smell
9734,46889474,10,"for i in range(2):
    print(train())
    train_agent.Q.save_weights(""QWeights"")
    train_agent.Q_.save_weights(""Q_Weights"")",0,No Code Smell
9735,46889474,11,"%%writefile submission.py

from gfootball.env import observation_preprocessing
import numpy as np
import tensorflow as tf

class SplitLayer(tf.keras.layers.Layer):
    def __init__(self):
        super().__init__()

    def call(self, inputs):
        split0,split1,split2,split3 = tf.split(inputs,4,3)
        return [split0,split1,split2,split3]
    
class ConvNN:

    def __init__(self,state_shape,num_actions):
        self.state_shape = state_shape
        self.num_actions = num_actions

    def _model_architecture(self):
    
        frames_input = tf.keras.layers.Input(shape=self.state_shape,name='input',batch_size=1)
        frames_split = SplitLayer()(frames_input)
        conv_branches = []
        for f,frame in enumerate(frames_split):
            conv_branches.append(self._build_conv_branch(frame,f))

        concat = tf.keras.layers.concatenate(conv_branches)
        
        fc0 = tf.keras.layers.Dense(units=8192, name='fc0', 
                                     activation='relu')(concat)
        fc1 = tf.keras.layers.Dense(units=2048, name='fc1', 
                                     activation='relu')(fc0)
        fc2 = tf.keras.layers.Dense(units=512, name='fc2', 
                                     activation='relu')(fc1)
        fc3 = tf.keras.layers.Dense(units=256, name='fc3', 
                                     activation='relu')(fc2)
        fc4 = tf.keras.layers.Dense(units=64, name='fc4', 
                                     activation='relu')(fc3)
        
        action_output = tf.keras.layers.Dense(units=self.num_actions, name='output',
                                               activation='relu')(fc4)
        return frames_input,action_output
        
    @staticmethod
    def _build_conv_branch(frame,number):
        conv1 = tf.keras.layers.Conv2D(16, kernel_size=(8, 8), strides=(4, 4),
                                        name='conv1_frame'+str(number), padding='same',
                                        activation='relu')(frame)
        mp1 = tf.keras.layers.MaxPooling2D(pool_size=2, name=""mp1_frame""+str(number))(conv1)
        conv2 = tf.keras.layers.Conv2D(24, kernel_size=(4, 4), strides=(2, 2),
                                        name='conv2_frame'+str(number),padding='same',
                                        activation='relu')(mp1)
        mp2 = tf.keras.layers.MaxPooling2D(pool_size=2, name=""mp2_frame""+str(number))(conv2)
        conv3 = tf.keras.layers.Conv2D(32, kernel_size=(3, 3), strides=(1, 1),
                                        name='conv3_frame'+str(number),padding='same',
                                        activation='relu')(mp2)
        mp3 = tf.keras.layers.MaxPooling2D(pool_size=2, name=""mp3_frame""+str(number))(conv3)
        conv4 = tf.keras.layers.Conv2D(64, kernel_size=(2, 2), strides=(1, 1),
                                        name='conv4_frame'+str(number), padding='same',
                                        activation='relu')(mp3)
        mp4 = tf.keras.layers.MaxPooling2D(pool_size=1, name=""mp4_frame""+str(number))(conv4)

        flatten = tf.keras.layers.Flatten(name='flatten'+str(number))(mp4)

        return flatten
        
    def build(self):
        frames_input,action_output = self._model_architecture()
        model = tf.keras.Model(inputs=[frames_input], outputs=[action_output])
        return model

class Agent:
    def __init__(self, state_shape, num_actions):
        
        self.state_shape = state_shape
        self.num_actions = num_actions

        self.Q = ConvNN(state_shape,num_actions).build()
        self.Q_ = ConvNN(state_shape,num_actions).build()
        
    def act(self, s_t):
        return np.argmax(self.Q(s_t), axis=1)
    
DQN_Agent = Agent((72,96,4),19)

DQN_Agent.Q.load_weights(""QWeights"")
DQN_Agent.Q_.load_weights(""Q_Weights"")

def agent(obs):
    obs = obs['players_raw'][0]
    obs = observation_preprocessing.generate_smm([obs])
    obs = obs/255 #Normalization
    action = DQN_Agent.act(obs)
    return [action[0]]",0,No Code Smell
9736,46889474,12,"# Set up the Environment.
from kaggle_environments import make
env = make(""football"", configuration={""save_video"": True, ""scenario_name"": ""11_vs_11_kaggle"", ""running_in_notebook"": True})
output = env.run([""/kaggle/working/submission.py"", ""do_nothing""])[-1]
print('Left player: reward = %s, status = %s, info = %s' % (output[0]['reward'], output[0]['status'], output[0]['info']))
print('Right player: reward = %s, status = %s, info = %s' % (output[1]['reward'], output[1]['status'], output[1]['info']))
env.render(mode=""human"", width=800, height=600)",0,No Code Smell
9737,47152034,0,"# Install:
# Kaggle environments.
!git clone -q https://github.com/Kaggle/kaggle-environments.git
!cd kaggle-environments && pip install .

# GFootball environment.
!apt-get update -y -qq > /dev/null
!apt-get install -y -qq libsdl2-gfx-dev libsdl2-ttf-dev

# Make sure that the Branch in git clone and in wget call matches !!
!git clone -q -b v2.8 https://github.com/google-research/football.git
!mkdir -p football/third_party/gfootball_engine/lib

!wget -q https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_v2.8.so -O football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so
!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip install .",0,No Code Smell
9738,47152034,1,!pip install rl-replicas,0,No Code Smell
9739,47152034,2,"import os

import gfootball
import gym
import torch
import torch.nn as nn

from rl_replicas.algorithms import TRPO
from rl_replicas.common.policy import Policy
from rl_replicas.common.value_function import ValueFunction
from rl_replicas.common.optimizers import ConjugateGradientOptimizer
from rl_replicas.common.torch_net import mlp

algorithm_name = 'trpo'
environment_name = 'GFootball-11_vs_11_kaggle-simple115v2-v0'
epochs = 5
steps_per_epoch = 4000
policy_network_architecture = [64, 64]
value_function_network_architecture = [64, 64]
value_function_learning_rate = 1e-3
output_dir = './trpo'

env: gym.Env = gym.make(environment_name)

policy_network = mlp(
    sizes = [env.observation_space.shape[0]]+policy_network_architecture+[env.action_space.n]
)

policy: Policy = Policy(
    network = policy_network,
    optimizer = ConjugateGradientOptimizer(params=policy_network.parameters())
)

value_function_network = mlp(
    sizes = [env.observation_space.shape[0]]+value_function_network_architecture+[1]
)
value_function: ValueFunction = ValueFunction(
    network = value_function_network,
    optimizer = torch.optim.Adam(value_function_network.parameters(), lr=value_function_learning_rate)
)

model: TRPO = TRPO(policy, value_function, env, seed=0)

print('an experiment to: {}'.format(output_dir))

print('algorithm:           {}'.format(algorithm_name))
print('epochs:              {}'.format(epochs))
print('steps_per_epoch:     {}'.format(steps_per_epoch))
print('environment:         {}'.format(environment_name))

print('value_function_learning_rate: {}'.format(value_function_learning_rate))
print('policy network:')
print(policy.network)",0,No Code Smell
9740,47152034,3,"model.learn(
    epochs=epochs,
    steps_per_epoch=steps_per_epoch,
    output_dir=output_dir,
    model_saving=True
)",0,No Code Smell
9741,47152034,4,"%%writefile ./agent.py
import time

import torch
import gfootball
import gym
from gfootball.env.wrappers import Simple115StateWrapper

from rl_replicas.common.policy import Policy
from rl_replicas.common.torch_net import mlp

start_setup_time: float = time.time()

num_observation = 115
num_action = 19
policy_network_architecture = [64, 64]
model_location = './trpo/model.pt'
model = torch.load(model_location)

policy_network = mlp(
    sizes = [num_observation]+policy_network_architecture+[num_action]
)

policy_network.load_state_dict(model['policy_state_dict'])

policy: Policy = Policy(
    network = policy_network,
    optimizer = None
)

current_step: int = 0

print('Set up Time: {:<8.3g}'.format(time.time() - start_setup_time))

def agent(observation):
    global policy
    global current_step

    start_time: float = time.time()
    current_step += 1

    raw_observation = observation['players_raw']
    simple_115_observation = Simple115StateWrapper.convert_observation(raw_observation, fixed_positions=False)
    observation_tensor: torch.Tensor = torch.from_numpy(simple_115_observation).float()

    action = policy.predict(observation_tensor)
    
    if (current_step%100) == 0:
        print('Current Step: {}'.format(current_step))

    one_step_time = time.time() - start_time
    if one_step_time >= 0.2:
        print('One Step Time exceeded 0.2 seconds: {:<8.3g}'.format(one_step_time))

    return [action.item()]
",0,No Code Smell
9742,47152034,5,"from kaggle_environments import make

env = make(""football"", 
           configuration={
             ""save_video"": True, 
             ""scenario_name"": ""11_vs_11_kaggle"",
             ""running_in_notebook"": True,
           })

output = env.run([""./agent.py"", ""do_nothing""])[-1]

print('Left player: reward = {}, status = {}, info = {}'.format(output[0]['reward'], output[0]['status'], output[0]['info']))
print('Right player: reward = {}, status = {}, info = {}'.format(output[1]['reward'], output[1]['status'], output[1]['info']))

env.render(mode=""human"", width=800, height=600)",0,No Code Smell
9743,47152034,6,,0,No Code Smell
9744,47496157,0,"%%capture
# Update kaggle-environments to the newest version.
!pip3 install kaggle-environments -U

# GFootball environment.
!apt-get update -y
!apt-get install -y libsdl2-gfx-dev libsdl2-ttf-dev

# Make sure that the Branch in git clone and in wget call matches !!
!git clone -b v2.7 https://github.com/google-research/football.git
!mkdir -p football/third_party/gfootball_engine/lib

!wget https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_v2.7.so -O football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so
!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install .",0,No Code Smell
9745,47496157,1,"%%capture
!pip install pfrl==0.1.0",0,No Code Smell
9746,47496157,2,"import os
import cv2
import sys
import glob 
import random
import imageio
import pathlib
import collections
from collections import deque
import numpy as np
import argparse
import matplotlib.pyplot as plt
import seaborn as sns
sns.set()
%matplotlib inline

from gym import spaces
from tqdm import tqdm
from logging import getLogger, StreamHandler, FileHandler, DEBUG, INFO
from typing import Union, Callable, List, Tuple, Iterable, Any, Dict
from dataclasses import dataclass
from IPython.display import Image, display
sns.set()


# PyTorch
import pfrl
from pfrl.agents import CategoricalDoubleDQN
from pfrl import experiments
from pfrl import explorers
from pfrl import nn as pnn
from pfrl import utils
from pfrl import replay_buffers
from pfrl.wrappers import atari_wrappers
from pfrl.q_functions import DistributionalDuelingDQN

import torch
from torch import nn

# Env
import gym
import gfootball
import gfootball.env as football_env
from gfootball.env import observation_preprocessing",0,No Code Smell
9747,47496157,3,"# Check we can use GPU
print(torch.cuda.is_available())

# set gpu id
if torch.cuda.is_available(): 
    # NOTE: it is not number of gpu but id which start from 0
    gpu = 0
else:
    # cpu=>-1
    gpu = -1",0,No Code Smell
9748,47496157,4,"# set logger
def logger_config():
    logger = getLogger(__name__)
    handler = StreamHandler()
    handler.setLevel(""DEBUG"")
    logger.setLevel(""DEBUG"")
    logger.addHandler(handler)
    logger.propagate = False

    filepath = './result.log'
    file_handler = FileHandler(filepath)
    logger.addHandler(file_handler)
    return logger

logger = logger_config()",0,No Code Smell
9749,47496157,5,"# fixed random seed
# but this is NOT enough to fix the result of rewards.Please tell me the reason.
def seed_everything(seed=1234):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True
    utils.set_random_seed(seed)  # for PFRL
    
# Set a random seed used in PFRL.
seed = 5046
seed_everything(seed)

# Set different random seeds for train and test envs.
train_seed = seed
test_seed = 2 ** 31 - 1 - seed",0,No Code Smell
9750,47496157,6,"# wrapper for env(resize and transpose channel order)
class TransEnv(gym.ObservationWrapper):
    def __init__(self, env, channel_order=""hwc""):

        gym.ObservationWrapper.__init__(self, env)
        self.height = 84
        self.width = 84
        self.ch = env.observation_space.shape[2]
        shape = {
            ""hwc"": (self.height, self.width, self.ch),
            ""chw"": (self.ch, self.height, self.width),
        }
        self.observation_space = spaces.Box(
            low=0, high=255, shape=shape[channel_order], dtype=np.uint8
        )
        

    def observation(self, frame):
        frame = cv2.resize(frame, (self.width, self.height), interpolation=cv2.INTER_AREA)
        return frame.reshape(self.observation_space.low.shape)",0,No Code Smell
9751,47496157,7,"def make_env(test):
    # Use different random seeds for train and test envs
    env_seed = test_seed if test else train_seed
    
    # env = gym.make('GFootball-11_vs_11_kaggle-SMM-v0')
    env = football_env.create_environment(
      env_name='11_vs_11_easy_stochastic',  # easy mode
      stacked=False,
      representation='extracted',  # SMM
      rewards='scoring,checkpoints',
      write_goal_dumps=False,
      write_full_episode_dumps=False,
      render=False,
      write_video=False,
      dump_frequency=1,
      logdir='./',
      extra_players=None,
      number_of_left_players_agent_controls=1,
      number_of_right_players_agent_controls=0
    )
    env = TransEnv(env, channel_order=""chw"")

    env.seed(int(env_seed))
    if test:
        # Randomize actions like epsilon-greedy in evaluation as well
        env = pfrl.wrappers.RandomizeAction(env, random_fraction=0.0)
    return env

env = make_env(test=False)
eval_env = make_env(test=True)",0,No Code Smell
9752,47496157,8,"print('observation space:', env.observation_space.low.shape)
print('action space:', env.action_space)",0,No Code Smell
9753,47496157,9,"env.reset()
action = env.action_space.sample()
obs, r, done, info = env.step(action)
print('next observation:', obs.shape)
print('reward:', r)
print('done:', done)
print('info:', info)",1,Code Smell
9754,47496157,10,"obs_n_channels = env.observation_space.low.shape[0]
n_actions = env.action_space.n
print(""obs_n_channels: "", obs_n_channels)
print(""n_actions: "", n_actions)

# params based the original paper
n_atoms = 51
v_max = 10
v_min = -10
q_func = DistributionalDuelingDQN(n_actions, n_atoms, v_min, v_max, obs_n_channels)
print(q_func)",0,No Code Smell
9755,47496157,11,"# Noisy nets
pnn.to_factorized_noisy(q_func, sigma_scale=0.5)

# Turn off explorer
explorer = explorers.Greedy()

# Use the same hyper parameters as https://arxiv.org/abs/1710.02298
opt = torch.optim.Adam(q_func.parameters(), 6.25e-5, eps=1.5 * 10 ** -4)

# Prioritized Replay
# Anneal beta from beta0 to 1 throughout training
update_interval = 4
betasteps = 5 * 10 ** 7 / update_interval
rbuf = replay_buffers.PrioritizedReplayBuffer(
        10 ** 5 * 2,  # Default value is 10 ** 6 but it is too large in this notebook. I chose 10 ** 5.
        alpha=0.5,
        beta0=0.4,
        betasteps=betasteps,
        num_steps=3,
        normalize_by_max=""memory"",
    )


def phi(x):
    # Feature extractor
    return np.asarray(x, dtype=np.float32) / 255",0,No Code Smell
9756,47496157,12,"agent = CategoricalDoubleDQN(
        q_func,
        opt,
        rbuf,
        gpu=gpu,  
        gamma=0.99,
        explorer=explorer,
        minibatch_size=32,
        replay_start_size=2 * 10 ** 4,
        target_update_interval=32000,
        update_interval=update_interval,
        batch_accumulator=""mean"",
        phi=phi,
    )",0,No Code Smell
9757,47496157,13,"# if you have a pretrained model, agent can load pretrained weight. 
use_pretrained = False
pretrained_path = None
if use_pretrained:
    agent.load(pretrained_path)",1,Code Smell
9758,47496157,14,"#num_steps = 1000000
num_steps = 800000",0,No Code Smell
9759,47496157,15,"%%time
experiments.train_agent_with_evaluation(
    agent=agent,
    env=env,
    steps=num_steps,
    eval_n_steps=None,
    eval_n_episodes=1,
    eval_interval=3000,
    outdir=""./"",
    checkpoint_freq=100000,
    save_best_so_far_agent=True,
    eval_env=eval_env,
    logger=logger
)",0,No Code Smell
9760,47496157,16,"import csv

def text_csv_converter(datas):
    file_csv = datas.replace(""txt"", ""csv"")
    with open(datas) as rf:
        with open(file_csv, ""w"") as wf:
            readfile = rf.readlines()
            for read_text in readfile:
                read_text = read_text.split()
                writer = csv.writer(wf, delimiter=',')
                writer.writerow(read_text)

filename = ""scores.txt""
text_csv_converter(filename)",0,No Code Smell
9761,47496157,17,"import pandas as pd
scores = pd.read_csv(""scores.csv"")
scores.tail()",1,Code Smell
9762,47496157,18,"# visualize reward each episodes
fig = plt.figure(figsize=(15, 5))
ax1 = fig.add_subplot(121)
ax2 = fig.add_subplot(122)
ax1.set_title(""median reward"")
ax2.set_title(""average loss"")
sns.lineplot(x=""episodes"", y=""median"", data=scores, ax=ax1)
sns.lineplot(x=""episodes"", y=""average_loss"", data=scores,ax=ax2)
plt.show()",0,No Code Smell
9763,47496157,19,"# clone pfrl repo and move to the directory with main.py 
!git clone https://github.com/pfnet/pfrl.git
!mkdir sub
!mv ./pfrl/pfrl sub
!rm -r ./pfrl",0,No Code Smell
9764,47496157,20,"import base64

with open(f'./{num_steps}_finish/model.pt', 'rb') as f:
    encoded_string = base64.b64encode(f.read())

with open('./sub/model_weights.py', 'w') as f:
    f.write(f'model_string={encoded_string}')",0,No Code Smell
9765,47496157,21,"%cd sub
!ls -la ",1,Code Smell
9766,47496157,22,"%%writefile main.py
import os
import sys
import cv2
import collections
import numpy as np
from gfootball.env import observation_preprocessing

# PFRL
import torch
import pfrl

import base64
from model_weights import model_string

def make_model():
    # Q_function
    model = pfrl.q_functions.DistributionalDuelingDQN(n_actions=19, n_atoms=51, v_min=-10, v_max=10, n_input_channels=4)
    
    # Noisy nets
    pfrl.nn.to_factorized_noisy(model, sigma_scale=0.5)
    
    # load weights
    with open(""model.dat"", ""wb"") as f:
        f.write(base64.b64decode(model_string))
    weights = torch.load(""model.dat"", map_location=torch.device('cpu'))
    model.load_state_dict(weights)
    return model


model = make_model()

def agent(obs):
    global model
    
    # Get observations for the first (and only one) player we control.
    obs = obs['players_raw'][0]
    # Agent we trained uses Super Mini Map (SMM) representation.
    # See https://github.com/google-research/seed_rl/blob/master/football/env.py for details.
    obs = observation_preprocessing.generate_smm([obs])[0]
    # preprocess for obs
    obs = cv2.resize(obs, (84,84))    # resize
    obs = np.transpose(obs, [2,0,1])  # transpose to chw
    obs = torch.tensor(obs).float()   # to tensor
    obs = torch.unsqueeze(obs,0)      # add batch

    actions = model(obs)
    action = int(actions.greedy_actions.numpy()[0])  # modified
    return [action]",0,No Code Smell
9767,47496157,23,!pip install stickytape,0,No Code Smell
9768,47496157,24,"!stickytape main.py --add-python-path pfrl --add-python-path . > /kaggle/working/submission.py
!rm -r pfrl
%cd /kaggle/working
!ls -la",0,No Code Smell
9769,47496157,25,"from kaggle_environments import make
from submission import agent
env = make(""football"", configuration={""save_video"": True, ""scenario_name"": ""11_vs_11_kaggle"", ""running_in_notebook"": True})
obs = env.state[0][""observation""]
action = agent(obs)
print(action)",0,No Code Smell
9770,47496157,26,"from kaggle_environments import make
env = make(""football"", configuration={""save_video"": True, ""scenario_name"": ""11_vs_11_kaggle"", ""running_in_notebook"": True}, debug=True)
agent = ""submission.py""
output = env.run([agent, agent])[-1]
print('Left player: action = %s, reward = %s, status = %s, info = %s' % (output[0][""action""], output[0]['reward'], output[0]['status'], output[0]['info']))
print('Right player: action = %s, reward = %s, status = %s, info = %s' % (output[1][""action""], output[1]['reward'], output[1]['status'], output[1]['info']))
env.render(mode=""human"", width=800, height=600)",1,Code Smell
9771,47496157,27,"# to clean output folder
!rm -r /kaggle/working/football
!rm -r /kaggle/working/kaggle-environments",0,No Code Smell
9772,46268552,0,"def get_closest_opponent(obs):
    posX = obs['left_team'][obs['active']][0]
    posY = obs['left_team'][obs['active']][1]
    shortest_distance = None
    opponent = None
    
    for i in range(1, len(obs[""right_team""])):
        distance_to_opponent = calc_distance(posX, posY, obs[""right_team""][i][0], obs[""right_team""][i][1])
             if shortest_distance == None or distance_to_opponent < shortest_distance:
                shortest_distance = distance_to_opponent
                opponent = obs[""right_team""][i]
    
    return shortest_distance
    

def calc_distance(x1, y1, x2, y2):
    return math.sqrt((x1 - x2) ** 2 + (y1 - y2) ** 2) 
",0,No Code Smell
9773,46268552,1,,0,No Code Smell
9774,43675529,0,"# Install:
# GFootball environment (https://github.com/google-research/football/),
# SEED RL for training an agent (https://github.com/google-research/seed_rl/),
# Tensorflow 2.2, which is needed by SEED RL.

!apt-get update
!apt-get install -y libsdl2-gfx-dev libsdl2-ttf-dev
!pip3 install tensorflow==2.2
!pip3 install tensorflow_probability==0.9.0

# Update kaggle-environments to the newest version.
!pip3 install kaggle-environments -U

# Make sure that the Branch in git clone and in wget call matches !!
!git clone -b v2.3 https://github.com/google-research/football.git
!mkdir -p football/third_party/gfootball_engine/lib

!wget https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_v2.3.so -O football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so
!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install .

!git clone https://github.com/google-research/seed_rl.git
!cd seed_rl && git checkout 34fb2874d41241eb4d5a03344619fb4e34dd9be6
!mkdir /kaggle_simulations/agent",0,No Code Smell
9775,43675529,1,"%%writefile train.sh
# Training launcher script.

# Make SEED RL visible to Python.
export PYTHONPATH=$PYTHONPATH:$(pwd)
ENVIRONMENT=$1
AGENT=$2
NUM_ACTORS=$3
shift 3

# Start actor tasks which run environment loop.
actor=0
while [ ""$actor"" -lt ${NUM_ACTORS} ]; do
  python3 seed_rl/${ENVIRONMENT}/${AGENT}_main.py --run_mode=actor --logtostderr $@ --num_actors=${NUM_ACTORS} --task=${actor} 2>/dev/null >/dev/null &
  actor=$(( actor + 1 ))
done
# Start learner task which performs training of the agent.
python3 seed_rl/${ENVIRONMENT}/${AGENT}_main.py --run_mode=learner --logtostderr $@ --num_actors=""${NUM_ACTORS}""
",0,No Code Smell
9776,43675529,2,"!bash train.sh football vtrace 4 '--total_environment_frames=10000 --game=11_vs_11_kaggle --reward_experiment=scoring,checkpoints --logdir=/kaggle_simulations/agent/'",0,No Code Smell
9777,43675529,3,!ls -la /kaggle_simulations/agent/saved_model,1,Code Smell
9778,43675529,4,"%%writefile /kaggle_simulations/agent/main.py

import collections
import gym
import numpy as np
import os
import sys
import tensorflow as tf

from gfootball.env import observation_preprocessing
from gfootball.env import wrappers

EnvOutput = collections.namedtuple(
    'EnvOutput', 'reward done observation abandoned episode_step')

def prepare_agent_input(observation, prev_action, state):
    # SEED RL agent accepts input in a form of EnvOutput. When not training
    # only observation is used for generating action, so we use a dummy values
    # for the rest.
    env_output = EnvOutput(reward=tf.zeros(shape=[], dtype=tf.float32),
        done=tf.zeros(shape=[], dtype=tf.bool),
        observation=observation, abandoned=False,
        episode_step=tf.zeros(shape=[], dtype=tf.int32))
    # add batch dimension
    prev_action, env_output = tf.nest.map_structure(
        lambda t: tf.expand_dims(t, 0), (prev_action, env_output))

    return (prev_action, env_output, state)

# Previously executed action
previous_action = tf.constant(0, dtype=tf.int64)
# Queue of recent observations (SEED agent we trained uses frame stacking).
observations = collections.deque([], maxlen=4)
# Current state of the agent (used by recurrent agents).
state = ()

# Load previously trained Tensorflow model.
policy = tf.compat.v2.saved_model.load('/kaggle_simulations/agent/saved_model')

def agent(obs):
    global step
    global previous_action
    global observations
    global state
    global policy
    # Get observations for the first (and only one) player we control.
    obs = obs['players_raw'][0]
    # Agent we trained uses Super Mini Map (SMM) representation.
    # See https://github.com/google-research/seed_rl/blob/master/football/env.py for details.
    obs = observation_preprocessing.generate_smm([obs])[0]
    if not observations:
        observations.extend([obs] * 4)
    else:
        observations.append(obs)
    
    # SEED packs observations to reduce transfer times.
    # See PackedBitsObservation in https://github.com/google-research/seed_rl/blob/master/football/observation.py
    obs = np.packbits(obs, axis=-1)
    if obs.shape[-1] % 2 == 1:
        obs = np.pad(obs, [(0, 0)] * (obs.ndim - 1) + [(0, 1)], 'constant')
    obs = obs.view(np.uint16)

    # Execute our agent to obtain action to take.
    agent_output, state = policy.get_action(*prepare_agent_input(obs, previous_action, state))
    previous_action = agent_output.action[0]
    return [int(previous_action)]",0,No Code Smell
9779,43675529,5,"from kaggle_environments import make
env = make(""football"", configuration={""save_video"": True, ""scenario_name"": ""11_vs_11_kaggle"", ""running_in_notebook"": True})
env.run([""/kaggle_simulations/agent/main.py"", ""run_right""])
env.render(mode=""human"", width=800, height=600)",0,No Code Smell
9780,43675529,6,"# Prepare a submision package containing trained model and the main execution logic.
!cd /kaggle_simulations/agent && tar -czvf /kaggle/working/submit.tar.gz main.py saved_model",0,No Code Smell
9781,44335473,0,"# Install:
# Kaggle environments.
!git clone https://github.com/Kaggle/kaggle-environments.git
!cd kaggle-environments && pip install .

# GFootball environment.
!apt-get update -y
!apt-get install -y libsdl2-gfx-dev libsdl2-ttf-dev

# Make sure that the Branch in git clone and in wget call matches !!
!git clone -b v2.6 https://github.com/google-research/football.git
!mkdir -p football/third_party/gfootball_engine/lib

!wget https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_v2.6.so -O football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so
!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install .",0,No Code Smell
9782,44335473,1,"%%writefile submission.py
from kaggle_environments.envs.football.helpers import *

# @human_readable_agent wrapper modifies raw observations 
# provided by the environment:
# https://github.com/google-research/football/blob/master/gfootball/doc/observation.md#raw-observations
# into a form easier to work with by humans.
# Following modifications are applied:
# - Action, PlayerRole and GameMode enums are introduced.
# - 'sticky_actions' are turned into a set of active actions (Action enum)
#    see usage example below.
# - 'game_mode' is turned into GameMode enum.
# - 'designated' field is removed, as it always equals to 'active'
#    when a single player is controlled on the team.
# - 'left_team_roles'/'right_team_roles' are turned into PlayerRole enums.
# - Action enum is to be returned by the agent function.
@human_readable_agent
def agent(obs):
    # Make sure player is running.
    if Action.Sprint not in obs['sticky_actions']:
        return Action.Sprint
    # We always control left team (observations and actions
    # are mirrored appropriately by the environment).
    controlled_player_pos = obs['left_team'][obs['active']]
    # Does the player we control have the ball?
    if obs['ball_owned_player'] == obs['active'] and obs['ball_owned_team'] == 0:
        # Shot if we are 'close' to the goal (based on 'x' coordinate).
        if controlled_player_pos[0] > 0.5:
            return Action.Shot
        # Run towards the goal otherwise.
        return Action.Right
    else:
        # Run towards the ball.
        if obs['ball'][0] > controlled_player_pos[0] + 0.05:
            return Action.Right
        if obs['ball'][0] < controlled_player_pos[0] - 0.05:
            return Action.Left
        if obs['ball'][1] > controlled_player_pos[1] + 0.05:
            return Action.Bottom
        if obs['ball'][1] < controlled_player_pos[1] - 0.05:
            return Action.Top
        # Try to take over the ball if close to the ball.
        return Action.Slide",0,No Code Smell
9783,44335473,2,"# Set up the Environment.
from kaggle_environments import make
env = make(""football"", configuration={""save_video"": True, ""scenario_name"": ""11_vs_11_kaggle"", ""running_in_notebook"": True})
output = env.run([""/kaggle/working/submission.py"", ""do_nothing""])[-1]
print('Left player: reward = %s, status = %s, info = %s' % (output[0]['reward'], output[0]['status'], output[0]['info']))
print('Right player: reward = %s, status = %s, info = %s' % (output[1]['reward'], output[1]['status'], output[1]['info']))
env.render(mode=""human"", width=800, height=600)",0,No Code Smell
9784,44925730,0,"import matplotlib.pyplot as plt
import pprint
import glob 
import imageio
import pathlib
import numpy as np
from typing import Tuple
from tqdm import tqdm
import os
import sys
from IPython.display import Image",1,Code Smell
9785,44925730,1,"# GFootball environment.
!pip install kaggle_environments
!apt-get update -y
!apt-get install -y libsdl2-gfx-dev libsdl2-ttf-dev
!git clone -b v2.3 https://github.com/google-research/football.git
!mkdir -p football/third_party/gfootball_engine/lib
!wget https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_v2.3.so -O football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so
!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install .

# Some helper code
!git clone https://github.com/garethjns/kaggle-football.git
sys.path.append(""/kaggle/working/kaggle-football/"")",0,No Code Smell
9786,44925730,2,"import gym
import gfootball  # Required as envs registered on import

simple_env = gym.make(""GFootball-11_vs_11_kaggle-simple115v2-v0"")
pixels_env = gym.make(""GFootball-11_vs_11_kaggle-Pixels-v0"")
smm_env = gym.make(""GFootball-11_vs_11_kaggle-SMM-v0"")

print(f""simple115v2:\n {simple_env.__str__()}\n"")
print(f""Pixels:\n {pixels_env.__str__()}\n"")
print(f""SMM:\n {smm_env.__str__()}\n"")",0,No Code Smell
9787,44925730,3,"from gfootball.env.football_env import FootballEnv

env_name = ""GFootballBase-v0""
gym.envs.register(id=env_name,
                  entry_point=""gfootball.env.football_env:FootballEnv"",
                  max_episode_steps=10000)",0,No Code Smell
9788,44925730,4,"from gfootball.env.config import Config

base_env = gym.make(env_name, config=Config())",0,No Code Smell
9789,44925730,5,"obs = base_env.reset()

pprint.pprint(obs[0])",0,No Code Smell
9790,44925730,6,"obs = simple_env.reset()

print(obs.shape)

pprint.pprint(obs)",0,No Code Smell
9791,44925730,7,"from kaggle_football.viz import generate_gif, plot_smm_obs

smm_env = gym.make(""GFootball-11_vs_11_kaggle-SMM-v0"")
print(smm_env.reset().shape)

generate_gif(smm_env, n_steps=200)
Image(filename='smm_env_replay.gif', format='png')",0,No Code Smell
9792,44925730,8,"from gfootball.env import create_environment

# (These are the args set by the kaggle_environments package)
COMMON_KWARGS = {""stacked"": False, ""representation"": 'raw', ""write_goal_dumps"": False,
                 ""write_full_episode_dumps"": False, ""write_video"": False, ""render"": False,
                 ""number_of_left_players_agent_controls"": 1, ""number_of_right_players_agent_controls"": 0}

create_environment(env_name='11_vs_11_kaggle')",0,No Code Smell
9793,44925730,9,"chk_reward_env = create_environment(env_name='11_vs_11_kaggle', rewards='scoring,checkpoints')

_ = chk_reward_env.reset()
for s in range(100):
    _, r, _, _ = chk_reward_env.step(5)
    if r > 0:
        print(f""Step {s} checkpoint reward recieved: {r}"")",0,No Code Smell
9794,44925730,10,run_to_score_env = create_environment(env_name='academy_run_to_score'),0,No Code Smell
9795,44925730,11,"%%writefile random_agent.py
  
from typing import Any
from typing import List

import numpy as np


class RandomAgent:
    def get_action(self, obs: Any) -> int:
        return np.random.randint(19)


AGENT = RandomAgent()


def agent(obs) -> List[int]:
    return [AGENT.get_action(obs)]",0,No Code Smell
9796,44925730,12,"from kaggle_environments import make  
env = make(""football"", configuration={""save_video"": True,
                                      ""scenario_name"": ""11_vs_11_kaggle""})

# Define players
left_player = ""random_agent.py""  # A custom agent, eg. random_agent.py or example_agent.py
right_player = ""run_right""  # eg. A built in 'AI' agent

# Run the whole sim
# Output returned is a list of length n_steps. Each step is a list containing the output for each player as a dict.
# steps
output = env.run([left_player, right_player])

for s, (left, right) in enumerate(output):
    
    # Just print the last few steps of the output
    if s > 2990:
        print(f""\nStep {s}"")

        print(f""Left player ({left_player}): \n""
              f""actions taken: {left['action']}, ""
              f""reward: {left['reward']}, ""
              f""status: {left['status']}, ""
              f""info: {left['info']}"")

        print(f""Right player ({right_player}): \n""
              f""actions taken: {right['action']}, ""
              f""reward: {right['reward']}, ""
              f""status: {right['status']}, ""
              f""info: {right['info']}\n"")

print(f""Final score: {sum([r['reward'] for r in output[0]])} : {sum([r['reward'] for r in output[1]])}"")

env.render(mode=""human"", width=800, height=600)",0,No Code Smell
9797,44925730,13,"print(output[-1][0].keys())
print(f""Left player: {output[-1][0]['status']}: {output[-1][0]['info']}"")
print(f""Right player: {output[-1][0]['status']}: {output[-1][1]['info']}"")",1,Code Smell
9798,44925730,14,"%%writefile broken_agent.py
  
from typing import Any
from typing import List

class DeliberateException(Exception):
    pass


class BrokenAgent:
    def get_action(self, obs: Any) -> int:
        raise DeliberateException(f""I am broken."")


AGENT = BrokenAgent()


def agent(obs) -> List[int]:
    return [AGENT.get_action(obs)]",0,No Code Smell
9799,44925730,15,"env = make(""football"", configuration={""save_video"": True,
                                      ""scenario_name"": ""11_vs_11_kaggle""})

output = env.run([""random_agent.py"", ""broken_agent.py""])

print(len(output))
print(f""Left player: {output[-1][0]['status']}: {output[-1][0]['info']}"")
print(f""Right player: {output[-1][0]['status']}: {output[-1][1]['info']}"")",0,No Code Smell
9800,44925730,16,"env = make(""football"", debug=True,
           configuration={""save_video"": True,
                          ""scenario_name"": ""11_vs_11_kaggle""})

try:
    output = env.run([""random_agent.py"", ""broken_agent.py""])
except DeliberateException as e:
    print(e)",0,No Code Smell
9801,44925730,17,"from random_agent import agent  


env = make(""football"", configuration={""save_video"": True, ""scenario_name"": ""11_vs_11_kaggle""})
env.reset()

# This is the observation that is passed to agent function
obs_kag_env = env.state[0]['observation']

for _ in range(3000):
    action = agent(obs_kag_env)

    # Environment step is list of agent actions, ie [[agent_1], [agent_2]], 
    # here there is 1 action per agent.
    other_agent_action = [0]
    full_obs = env.step([action, other_agent_action])
    obs_kag_env = full_obs[0]['observation']",0,No Code Smell
9802,44925730,18,!pip install reinforcement_learning_keras,0,No Code Smell
9803,44925730,19,"import gym
from reinforcement_learning_keras.agents.components.history.training_history import TrainingHistory
from reinforcement_learning_keras.agents.q_learning.exploration.epsilon_greedy import EpsilonGreedy
from reinforcement_learning_keras.agents.q_learning.linear_q_agent import LinearQAgent
from sklearn.exceptions import DataConversionWarning

import warnings


agent = LinearQAgent(name=""linear_q"",
                     env_spec=""GFootball-11_vs_11_kaggle-simple115v2-v0"",
                     eps=EpsilonGreedy(eps_initial=0.9, decay=0.001, eps_min=0.01, 
                                       decay_schedule='linear'),
                     training_history=TrainingHistory(agent_name='linear_q', 
                                                      plotting_on=True, plot_every=25, 
                                                      rolling_average=1))

with warnings.catch_warnings():
    warnings.simplefilter('ignore', DataConversionWarning)
    agent.train(verbose=True, render=False,
                n_episodes=25, max_episode_steps=2000)",0,No Code Smell
9804,46716012,0,"# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only ""../input/"" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session",1,Code Smell
9805,46716012,1,"!apt-get update -y
!apt-get install -y libsdl2-gfx-dev libsdl2-ttf-dev

# Make sure that the Branch in git clone and in wget call matches !!
!git clone -b v2.8 https://github.com/google-research/football.git
!mkdir -p football/third_party/gfootball_engine/lib

!wget https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_v2.8.so -O football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so
!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install .",0,No Code Smell
9806,46716012,2,"%%writefile submission.py
from kaggle_environments.envs.football.helpers import *
from random import randint


# Function to calculate distance 
def get_distance(pos1,pos2):
    return (((pos1[0]-pos2[0])**2)+((pos1[1]-pos2[1])**2))**0.5

# Function to cross ball from wing
def cross_ball():
    pass

# Movement directions
directions = [
[Action.TopLeft, Action.Top, Action.TopRight],
[Action.Left, Action.Idle, Action.Right],
[Action.BottomLeft, Action.Bottom, Action.BottomRight]]

dirsign = lambda x: 1 if abs(x) < 0.01 else (0 if x < 0 else 2)

# Set game plan parameters
goalRange = 0.65
wingRange = 0.21

@human_readable_agent
def agent(obs):
    
    # Add direction to action
    def sticky_check(action, direction):
        if direction in obs['sticky_actions']:
            return action
        else:
            return direction
    
    controlled_player_pos = obs['left_team'][obs['active']]
    
    
    # Pass when KickOff or ThrowIn
    if obs['game_mode'] == GameMode.KickOff or obs['game_mode'] == GameMode.ThrowIn:
        return sticky_check(Action.ShortPass, Action.Right) 
    
    # Shoot when freekick in goal range; If on wing then cross; Otherwise just pass
    if obs['game_mode'] == GameMode.FreeKick:
        # Shoot if in range
        if controlled_player_pos[0] > goalRange and controlled_player_pos[1] < wingRange and controlled_player_pos[1] > -(wingRange):
            ydir = randint(0,2)
            return sticky_check(Action.Shot, directions[ydir][2]) 
        # Cross from right
        if controlled_player_pos[0] > goalRange and controlled_player_pos[1] > wingRange:
            return sticky_check(Action.HighPass, Action.TopRight)
        
        # Cross from left
        if controlled_player_pos[0] > goalRange and controlled_player_pos[1] < -(wingRange):
            return sticky_check(Action.HighPass, Action.BottomRight)
    
    # Cross in for corner
    if obs['game_mode'] == GameMode.Corner and obs['ball'][1] < 0:
        return sticky_check(Action.HighPass, Action.Bottom)
    elif obs['game_mode'] == GameMode.Corner and obs['ball'][1] > 0:
        return sticky_check(Action.HighPass, Action.Top)
        
    # High pass when GoalKick 
    if obs['game_mode'] == GameMode.GoalKick:
        ydir = randint(0,2)
        return sticky_check(Action.HighPass, directions[ydir][2])
    
    # Shoot when Penalty
    if obs['game_mode'] == GameMode.Penalty:
        xdir = randint(0,2)
        ydir = randint(0,2)
        return sticky_check(Action.Shot, directions[ydir][xdir])
    
    # Make sure player is running.
    if Action.Sprint not in obs['sticky_actions']:
        return Action.Sprint
    # We always control left team (observations and actions
    # are mirrored appropriately by the environment).
    controlled_player_pos = obs['left_team'][obs['active']]
    
    # Check if we are in possession
    if obs['ball_owned_player'] == obs['active'] and obs['ball_owned_team'] == 0:
        
        # Clear if we are near our goal
        if controlled_player_pos[0] < -(goalRange):
            return sticky_check(Action.HighPass, Action.Right)
        
        # Shoot if we are in the final third and not at an acute angle
        if controlled_player_pos[0] > goalRange and controlled_player_pos[1] < wingRange and controlled_player_pos[1] > -(wingRange):
            ydir = randint(0,2)
            return sticky_check(Action.Shot, directions[ydir][2])
        #if the goalie is coming out on player near goal shoot
        elif obs['right_team'][0][0] < 0.8 or abs(obs['right_team'][0][1]) > 0.05:
            return Action.Shot
        
        # Cross from right
        if controlled_player_pos[0] > goalRange and controlled_player_pos[1] > wingRange:
            return sticky_check(Action.HighPass, Action.TopRight)
        
        # Cross from left
        if controlled_player_pos[0] > goalRange and controlled_player_pos[1] < -(wingRange):
            return sticky_check(Action.HighPass, Action.BottomRight)
        
        # Run towards the goal otherwise.
        return Action.Right
    else:
        #where ball is going we add the direction xy to ball current location
        ball_targetx=obs['ball'][0]+(1.5 * obs['ball_direction'][0])
        ball_targety=obs['ball'][1]+(1.5 * obs['ball_direction'][1])

        # Euclidian distance to ball
        e_dist=get_distance(obs['left_team'][obs['active']],obs['ball'])

        if e_dist >.005:
            # Run where ball will be
            xdir = dirsign(ball_targetx - controlled_player_pos[0])
            ydir = dirsign(ball_targety - controlled_player_pos[1])
            return directions[ydir][xdir]
        else:
            prob = randint(0,100)
            if prob > 70 and controlled_player_pos[0] < obs['right_team'][obs['active']][0]:
                return Action.Slide
            # Run towards the ball.
            xdir = dirsign(obs['ball'][0] - controlled_player_pos[0])
            ydir = dirsign(obs['ball'][1] - controlled_player_pos[1])
            return directions[ydir][xdir]",0,No Code Smell
9807,46716012,3,"# Set up the Environment.
from kaggle_environments import make
env = make(""football"", debug=True, configuration={""save_video"": True, ""scenario_name"": ""11_vs_11_kaggle"", ""running_in_notebook"": True})
output = env.run([""/kaggle/working/submission.py"", ""do_nothing""])[-1]
print('Left player: reward = %s, status = %s, info = %s' % (output[0]['reward'], output[0]['status'], output[0]['info']))
print('Right player: reward = %s, status = %s, info = %s' % (output[1]['reward'], output[1]['status'], output[1]['info']))
env.render(mode=""human"", width=800, height=600)",0,No Code Smell
9808,47227031,0,"import tensorflow as tf

class Net(tf.keras.Model):
  """"""A simple linear model.""""""

  def __init__(self):
    super(Net, self).__init__()
    self.l1 = tf.keras.layers.Dense(5)

  def call(self, x):
    return self.l1(x)",0,No Code Smell
9809,47227031,1,"def toy_dataset():
  inputs = tf.range(10.)[:, None]
  labels = inputs * 5. + tf.range(5.)[None, :]
  return tf.data.Dataset.from_tensor_slices(
    dict(x=inputs, y=labels)).repeat().batch(2)

def train_step(net, example, optimizer):
  """"""Trains `net` on `example` using `optimizer`.""""""
  with tf.GradientTape() as tape:
    output = net(example['x'])
    loss = tf.reduce_mean(tf.abs(output - example['y']))
  variables = net.trainable_variables
  gradients = tape.gradient(loss, variables)
  optimizer.apply_gradients(zip(gradients, variables))
  return loss",0,No Code Smell
9810,47227031,2,"net = Net()

opt = tf.keras.optimizers.Adam(0.1)
dataset = toy_dataset()
iterator = iter(dataset)

for i in range(50):
  example = next(iterator)
  loss = train_step(net, example, opt)
  if i % 10 == 0:
    print(""loss {:1.2f}"".format(loss.numpy()))

net.save_weights('model_weights.ckp')",0,No Code Smell
9811,47227031,3,"%%writefile model.py
import tensorflow as tf

class Net(tf.keras.Model):
  """"""A simple linear model.""""""

  def __init__(self):
    super(Net, self).__init__()
    self.l1 = tf.keras.layers.Dense(5)

  def call(self, x):
    return self.l1(x)",0,No Code Smell
9812,47227031,4,"%%writefile main.py
import sys
import os

ext_folder = '/kaggle_simulations/agent/'
sys.path.append(ext_folder) 

import tensorflow as tf
from model import Net

model_name = 'model_weights.ckp'

net = Net()
net.load_weights(os.path.join(ext_folder, model_name))

def agent(obs):
    _ = net.predict(tf.range(10.)[:, None])
    return [1]",0,No Code Smell
9813,47227031,5,!tar -czvf submission.tar.gz main.py model_weights.* model.py,0,No Code Smell
9814,47227031,6,,0,No Code Smell
9815,43778259,0,"# Install:
# Kaggle environments.
!git clone https://github.com/Kaggle/kaggle-environments.git
!cd kaggle-environments && pip install .

# GFootball environment.
!apt-get update -y
!apt-get install -y libsdl2-gfx-dev libsdl2-ttf-dev

# Make sure that the Branch in git clone and in wget call matches !!
!git clone -b v2.3 https://github.com/google-research/football.git
!mkdir -p football/third_party/gfootball_engine/lib

!wget https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_v2.3.so -O football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so
!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install .",0,No Code Smell
9816,43778259,1,"%%writefile submission.py
from kaggle_environments.envs.football.helpers import *

@human_readable_agent
def agent(obs):
    import numpy as np
    import pandas as pd
    from pandas import Series, DataFrame
    
    ###### 0. CONSTANTS ######
    
    ENEMY_TARGET = [ 1, 0]
    OWN_TARGET   = [-1, 0]

    STEP_HARD_DIST = 0.1
    STEP_EASY_DIST = 0.5 * STEP_HARD_DIST
    SAFE_DIST = 0.1

    ###### 1. SMART CONTROL: FUNCTIONS ######

    def get_action_steps(step_dist):
        import numpy as np

        return {
            Action.Idle:          [                                0,                                 0],
            Action.Left:          [step_dist * np.cos(        np.pi), step_dist * np.sin(        np.pi)],
            Action.TopLeft:       [step_dist * np.cos( 0.75 * np.pi), step_dist * np.sin( 0.75 * np.pi)],
            Action.Top:           [step_dist * np.cos( 0.5  * np.pi), step_dist * np.sin( 0.5  * np.pi)],
            Action.TopRight:      [step_dist * np.cos( 0.25 * np.pi), step_dist * np.sin( 0.25 * np.pi)],
            Action.Right:         [step_dist * np.cos(            0), step_dist * np.sin(            0)],
            Action.BottomRight:   [step_dist * np.cos(-0.25 * np.pi), step_dist * np.sin(-0.25 * np.pi)],
            Action.Bottom:        [step_dist * np.cos(-0.5  * np.pi), step_dist * np.sin(-0.5  * np.pi)],
            Action.BottomLeft:    [step_dist * np.cos(-0.75 * np.pi), step_dist * np.sin(-0.75 * np.pi)]
        }


    def get_point_point_dist(point0, point):
        import numpy as np

        x0, y0 = point0[0], point0[1]
        x, y = point[0], point[1]
        return np.sqrt((x0 - x) ** 2 + (y0 - y) ** 2)


    def correct_point(point0):
        x, y = point0[0], point0[1]
        return (-1 <= x <= 1) and (-1 <= y <= 1)


    def get_move_action_info(point0, enemy_points, step_dist=STEP_EASY_DIST):
        import numpy as np

        target_dists = {}
        for action, step in get_action_steps(step_dist).items():
            step_point = [point0[0] + step[0], point0[1] + step[1]]
            if correct_point(step_point):
                ## 1. Enemy min distance
                enemy_distances = [
                    get_point_point_dist(step_point, enemy_point)
                    for enemy_point in enemy_points
                ]
                enemy_dist = np.array(enemy_distances).min()

                ## 2. Target distance
                target_dist = get_point_point_dist(step_point, ENEMY_TARGET)

                target_dists[action] = {
                    ""target_dist"" : round(target_dist, 3),
                    ""enemy_dist"" : round(enemy_dist, 3)
                }

        return target_dists


    def get_best_move_action(get_move_action_info, safe_dist=SAFE_DIST):
        from pandas import Series

        safe_actions = {}
        for action, info in get_move_action_info.items():
            target_dist, enemy_dist = info[""target_dist""], info[""enemy_dist""]
            if enemy_dist >= SAFE_DIST:
                safe_actions[action] = target_dist

        if len(safe_actions) == 0:
            return Action.Right ### fix in the future

        target_action = Series(safe_actions).idxmin()

        return target_action


    def make_decision(point0, move_action_info):
        x0, y0 = point0[0], point0[1]

        ## Shot decision
        if x0 >= 0.5:
            return Action.Shot

        ## Move decision
        best_move_action = get_best_move_action(move_action_info)
        return best_move_action
    
    
    ###### 2. GAME START ######
    
    own_points = obs['left_team']
    enemy_points = obs['right_team']
    point0 = obs['left_team'][obs['active']]
    
    if Action.Sprint not in obs['sticky_actions']:
        return Action.Sprint

    if obs['ball_owned_player'] == obs['active'] and obs['ball_owned_team'] == 0:
        ###### OFFENSE: SMART CONTROL ######
        move_action_info = get_move_action_info(point0, enemy_points)
        return make_decision(point0, move_action_info)
    else:
        ###### DEFENCE: OLD STRATEGY ######
        if obs['ball'][0] > point0[0] + 0.05:
            return Action.Right
        if obs['ball'][0] < point0[0] - 0.05:
            return Action.Left
        if obs['ball'][1] > point0[1] + 0.05:
            return Action.Bottom
        if obs['ball'][1] < point0[1] - 0.05:
            return Action.Top
        return Action.Slide",0,No Code Smell
9817,43778259,2,"# Set up the Environment.
from kaggle_environments import make

env = make(
    ""football"",
    configuration={
        ""save_video"": True,
        ""scenario_name"": ""11_vs_11_kaggle"",
        ""running_in_notebook"": True
    }
)

output = env.run([""/kaggle/working/submission.py"", ""/kaggle/working/submission.py""])[-1]

print('Left player: reward = %s, status = %s, info = %s' % (output[0]['reward'], output[0]['status'], output[0]['info']))
print('Right player: reward = %s, status = %s, info = %s' % (output[1]['reward'], output[1]['status'], output[1]['info']))
env.render(mode=""human"", width=800, height=600)",0,No Code Smell
9818,46400835,0,"# Install:
# GFootball environment (https://github.com/google-research/football/),
# SEED RL for training an agent (https://github.com/google-research/seed_rl/),
# Tensorflow 2.2, which is needed by SEED RL.

!apt-get update
!apt-get install -y libsdl2-gfx-dev libsdl2-ttf-dev
!pip3 install tensorflow==2.3.1
!pip3 install keras==2.4.3
!pip3 install tensorflow_probability==0.9.0

# Update kaggle-environments to the newest version.
!pip3 install kaggle-environments -U

# Make sure that the Branch in git clone and in wget call matches !!
!git clone -b v2.6 https://github.com/google-research/football.git
!mkdir -p football/third_party/gfootball_engine/lib

!wget https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_v2.6.so -O football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so
!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install .

!git clone https://github.com/google-research/seed_rl.git
!cd seed_rl && git checkout 34fb2874d41241eb4d5a03344619fb4e34dd9be6
!mkdir /kaggle_simulations/agent",0,No Code Smell
9819,46400835,1,,0,No Code Smell
9820,46400835,2,"!mkdir /kaggle_simulations
!mkdir /kaggle_simulations/agent
!cp -r /kaggle/input//* /kaggle_simulations/agent/",0,No Code Smell
9821,46400835,3,"!ls /kaggle_simulations/agent/trainlocal/train_320.hdf5
",1,Code Smell
9822,46400835,4,"%%writefile train.sh
# Training launcher script.

# Make SEED RL visible to Python.
export PYTHONPATH=$PYTHONPATH:$(pwd)
ENVIRONMENT=$1
AGENT=$2
NUM_ACTORS=$3
shift 3

# Start actor tasks which run environment loop.
actor=0
while [ ""$actor"" -lt ${NUM_ACTORS} ]; do
  python3 seed_rl/${ENVIRONMENT}/${AGENT}_main.py --run_mode=actor --logtostderr $@ --num_actors=${NUM_ACTORS} --task=${actor} 2>/dev/null >/dev/null &
  actor=$(( actor + 1 ))
done
# Start learner task which performs training of the agent.
python3 seed_rl/${ENVIRONMENT}/${AGENT}_main.py --run_mode=learner --logtostderr $@ --num_actors=""${NUM_ACTORS}""
",0,No Code Smell
9823,46400835,5,"!bash train.sh football vtrace 4 '--total_environment_frames=10 --game=11_vs_11_kaggle --reward_experiment=scoring,checkpoints --logdir=/kaggle_simulations/agent/'",0,No Code Smell
9824,46400835,6,#!ls -la /kaggle_simulations/agent/trained,1,Code Smell
9825,46400835,7,#!cp -r /kaggle/input//* /kaggle_simulations/agent/,0,No Code Smell
9826,46400835,8,"
import numpy as np
import tensorflow as tf
from keras.callbacks import TensorBoard
from keras.layers import Input, Dense, Conv2D, MaxPooling2D, Flatten
from keras.models import Model
from keras.optimizers import Adam
from keras import backend as K
from keras.applications.mobilenet_v2 import MobileNetV2
from keras.models import load_model
import tensorflow as tf
model_actor = load_model(""../input/trainlocal/train_320.hdf5"", compile=False)",1,Code Smell
9827,46400835,9,"%%writefile /kaggle_simulations/agent/trainlocal/main.py
import collections
import gym
import numpy as np
import os
import sys
import numpy as np
import tensorflow as tf
from keras.callbacks import TensorBoard
from keras.layers import Input, Dense, Conv2D, MaxPooling2D, Flatten
from keras.models import Model
from keras.optimizers import Adam
from keras import backend as K
from keras.applications.mobilenet_v2 import MobileNetV2
from keras.models import load_model
import tensorflow as tf

from gfootball.env import observation_preprocessing
from gfootball.env import wrappers

EnvOutput = collections.namedtuple(
    'EnvOutput', 'reward done observation abandoned episode_step')


sys.path.append(""/kaggle_simulations/agent"")
# Load previously trained Tensorflow model.
model_actor = load_model(""/kaggle_simulations/agent/trainlocal/train_320.hdf5"", compile=False)

dummy_n = np.zeros((1, 1, 19))
dummy_1 = np.zeros((1, 1, 1))


def agent(obs):
    global step_nr
    global previous_action
    global observations
    global state
    global policy
    # Get observations for the first (and only one) player we control.
    obs = obs['players_raw'][0]
    # Agent we trained uses Super Mini Map (SMM) representation.
    # See https://github.com/google-research/seed_rl/blob/master/football/env.py for details.
    obs = observation_preprocessing.generate_smm([obs])[0]

    state_input=obs
    state_input = K.expand_dims(state, 0)
    action_dist = model_actor.predict([state_input, dummy_n, dummy_1, dummy_1, dummy_1], steps=1)
    action = np.random.choice(n_actions, p=action_dist[0, :])
    return [action]
",0,No Code Smell
9828,46400835,10,,0,No Code Smell
9829,46400835,11,"from kaggle_environments import make
env = make(""football"", configuration={""save_video"": True, ""scenario_name"": ""11_vs_11_kaggle"", ""running_in_notebook"": True})
env.run([""/kaggle_simulations/agent/trainlocal/main.py"", ""run_right""])
env.render(mode=""human"", width=800, height=600)",0,No Code Smell
9830,46400835,12,!mkdir /kaggle_simulations/agent/trainlocal/saved_model,0,No Code Smell
9831,46400835,13,mv /kaggle_simulations/agent/trainlocal/train_320.hdf5 /kaggle_simulations/agent/trainlocal/saved_model/,0,No Code Smell
9832,46400835,14,"# Prepare a submision package containing trained model and the main execution logic.
!cd /kaggle_simulations/agent/trainlocal && tar -czvf /kaggle/working/submit.tar.gz main.py saved_model",0,No Code Smell
9833,46400835,15,,0,No Code Smell
9834,46400835,16,,0,No Code Smell
9835,45043550,0,"# Install:
# Kaggle environments.
!git clone --quiet https://github.com/Kaggle/kaggle-environments.git
!cd kaggle-environments && pip install -q .

# GFootball environment.
!apt-get -qq update -y
!apt-get -qq install -y libsdl2-gfx-dev libsdl2-ttf-dev

# Make sure that the Branch in git clone and in wget call matches !!
!git clone --quiet -b v2.3 https://github.com/google-research/football.git
!mkdir -p football/third_party/gfootball_engine/lib

!wget -q https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_v2.3.so -O football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so
!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install -q .
",0,No Code Smell
9836,45043550,1,"%%writefile submission.py

from math import sqrt
import os

from kaggle_environments.envs.football.helpers import *

SPRINT_RANGE = float(os.environ[""SPRINT_RANGE""])
SHOT_RANGE_X = float(os.environ[""SHOT_RANGE_X""])
SHOT_RANGE_Y = float(os.environ[""SHOT_RANGE_Y""])
GOALIE_OUT = float(os.environ[""GOALIE_OUT""])
LONG_SHOT_X = float(os.environ[""LONG_SHOT_X""])
LONG_SHOT_Y = float(os.environ[""LONG_SHOT_Y""])

directions = [
    [Action.TopLeft, Action.Top, Action.TopRight],
    [Action.Left, Action.Idle, Action.Right],
    [Action.BottomLeft, Action.Bottom, Action.BottomRight]]

dirsign = lambda x: 1 if abs(x) < 0.01 else (0 if x < 0 else 2)

enemyGoal = [1, 0]
GOALKEEPER = 0

shot_range = [[SHOT_RANGE_X, 1], 
              [-SHOT_RANGE_Y, SHOT_RANGE_Y]]

def inside(pos, area):
    return area[0][0] <= pos[0] <= area[0][1] and area[1][0] <= pos[1] <= area[1][1]

@human_readable_agent
def agent(obs):
    controlled_player_pos = obs['left_team'][obs['active']]
    
    if obs[""game_mode""] == GameMode.Penalty:
        return Action.Shot
    if obs[""game_mode""] == GameMode.Corner:
        if controlled_player_pos[0] > 0:
            return Action.Shot
    if obs[""game_mode""] == GameMode.FreeKick:
        return Action.Shot
    
    # Make sure player is running down the field.
    if  0 < controlled_player_pos[0] < SPRINT_RANGE and Action.Sprint not in obs['sticky_actions']:
        return Action.Sprint
    elif SPRINT_RANGE < controlled_player_pos[0] and Action.Sprint in obs['sticky_actions']:
        return Action.ReleaseSprint

    # If our player controls the ball:
    if obs['ball_owned_player'] == obs['active'] and obs['ball_owned_team'] == 0:
        
        if inside(controlled_player_pos, shot_range) and controlled_player_pos[0] < obs['ball'][0]:
            return Action.Shot
        
        elif ( abs(obs['right_team'][GOALKEEPER][0] - 1) > GOALIE_OUT   
                and controlled_player_pos[0] > LONG_SHOT_X and abs(controlled_player_pos[1]) < LONG_SHOT_Y ):
            return Action.Shot
        
        else:
            xdir = dirsign(enemyGoal[0] - controlled_player_pos[0])
            ydir = dirsign(enemyGoal[1] - controlled_player_pos[1])
            return directions[ydir][xdir]
        
    # if we we do not have the ball:
    else:
        # Run towards the ball.
        xdir = dirsign(obs['ball'][0] - controlled_player_pos[0])
        ydir = dirsign(obs['ball'][1] - controlled_player_pos[1])
        return directions[ydir][xdir]",0,No Code Smell
9837,45043550,2,"import os

from kaggle_environments import make
import numpy as np
import optuna

# Optuna searches parameters that maximized the returned value from this objective function.
def objective(trial):
    # You can get Optuna's parameter suggestion with the `suggest_float` method.
    os.environ[""SPRINT_RANGE""] = str(trial.suggest_float(""SPRINT_RANGE"", 0.0, 1.0))
    os.environ[""SHOT_RANGE_X""] = str(trial.suggest_float(""SHOT_RANGE_X"", 0.0, 1.0))  
    os.environ[""SHOT_RANGE_Y""] = str(trial.suggest_float(""SHOT_RANGE_Y"", 0.0, 1.0))
    os.environ[""GOALIE_OUT""] = str(trial.suggest_float(""GOALIE_OUT"", 0.0, 1.0))
    os.environ[""LONG_SHOT_X""] = str(trial.suggest_float(""LONG_SHOT_X"", 0.0, 1.0))
    os.environ[""LONG_SHOT_Y""] = str(trial.suggest_float(""LONG_SHOT_Y"", 0.0, 1.0))

    # To reduce the noise in reward, let's run the game 5 times for each trial.
    rewards = []
    for _ in range(5):
        env = make(""football"", configuration={""scenario_name"": ""11_vs_11_kaggle""})
        result = env.run([""submission.py"", ""do_nothing""])
        rewards.append(result[-1][0][""reward""])

    return np.mean(rewards)

# You can run the optimization just passing the objective function and the number of trials to Optuna.
# Here, Optuna repeats to run the objective function for 35 times.
study = optuna.create_study(direction=""maximize"")
study.optimize(objective, n_trials=35, show_progress_bar=True)",0,No Code Smell
9838,45043550,3,study.best_value,0,No Code Smell
9839,45043550,4,study.best_params,0,No Code Smell
9840,45043550,5,optuna.visualization.plot_optimization_history(study),0,No Code Smell
9841,45043550,6,study.trials_dataframe().head(),0,No Code Smell
9842,45043550,7,"# The importance plot visualizes which parameters has been dominant in the optimization.
optuna.visualization.plot_param_importances(study)",0,No Code Smell
9843,45043550,8,"# The slice plot shows the objective values along each parameter.
# Here, let's focus on the most dominant parameter.
optuna.visualization.plot_slice(study, params=[""SHOT_RANGE_X""])",0,No Code Smell
9844,45043550,9,"# The parallel coordinate plot shows the relationship among multiple parameters and the objective function.
optuna.visualization.plot_parallel_coordinate(study)",0,No Code Smell
9845,45043550,10,"def objective(trial):
    os.environ[""SPRINT_RANGE""] = str(trial.suggest_float(""SPRINT_RANGE"", 0.25, 0.9))
    os.environ[""SHOT_RANGE_X""] = str(trial.suggest_float(""SHOT_RANGE_X"", 0.5, 1.0))  
    os.environ[""SHOT_RANGE_Y""] = str(trial.suggest_float(""SHOT_RANGE_Y"", 0.0, 1.0))
    os.environ[""GOALIE_OUT""] = str(trial.suggest_float(""GOALIE_OUT"", 0.0, 0.4))
    os.environ[""LONG_SHOT_X""] = str(trial.suggest_float(""LONG_SHOT_X"", 0.25, 0.75))
    os.environ[""LONG_SHOT_Y""] = str(trial.suggest_float(""LONG_SHOT_Y"", 0.5, 1.0))
    
    rewards = []
    for _ in range(5):
        env = make(""football"", configuration={""scenario_name"": ""11_vs_11_kaggle""})
        result = env.run([""submission.py"", ""do_nothing""])
        rewards.append(result[-1][0][""reward""])

    return np.mean(rewards)

# You can reuse the study object to run additional 15 trials.
study.optimize(objective, n_trials=15, show_progress_bar=True)",0,No Code Smell
9846,45043550,11,optuna.visualization.plot_optimization_history(study),0,No Code Smell
9847,45043550,12,study.best_value,0,No Code Smell
9848,45043550,13,study.best_params,0,No Code Smell
9849,45043550,14,"import pickle
with open(""study.pkl"", ""wb"") as fw: 
    pickle.dump(study, fw)

# You can load as:
# study = pickle.load(open(""study.pkl"", ""rb""))",1,Code Smell
9850,45043550,15,,0,No Code Smell
9851,43842728,0,"# Install:
# Kaggle environments.
!git clone -q https://github.com/Kaggle/kaggle-environments.git
!cd kaggle-environments && pip install -q .
# GFootball environment.
!apt-get update -qy 
!apt-get install -qy libsdl2-gfx-dev libsdl2-ttf-dev
# Make sure that the Branch in git clone and in wget call matches !!
!git clone -b v2.3 https://github.com/google-research/football.git
!mkdir -p football/third_party/gfootball_engine/lib
!wget -q --show-progress https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_v2.3.so -O football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so
!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install -q .",0,No Code Smell
9852,43842728,1,"%%writefile submission.py
import numpy as np
from kaggle_environments.envs.football.helpers import *

@human_readable_agent
def agent(obs):
    
    # Global param
    goal_threshold = 0.5
    gravity = 0.098
    pick_height = 0.5
    step_length = 0.015 # As we always sprint
    body_radius = 0.012
    slide_threshold = step_length + body_radius
    
    # Ignore drag to estimate the landing point
    def ball_landing(ball, ball_direction):
        start_height = ball[2]
        end_height = pick_height
        start_speed = ball_direction[2]
        time = np.sqrt(start_speed**2/gravity**2 - 2/gravity*(end_height-start_height)) + start_speed/gravity
        return [ball[0]+ball_direction[0]*time, ball[1]+ball_direction[1]*time]
    
    # Check whether pressing on direction buttons and take action if so
    # Else press on direction first
    def sticky_check(action, direction):
        if direction in obs['sticky_actions']:
            return action
        else:
            return direction
    
    # Find right team positions
    def_team_pos = obs['right_team']
    # Fix goalkeeper index here as PlayerRole has issues
    # Default PlayerRole [0, 7, 9, 2, 1, 1, 3, 5, 5, 5, 6]
    def_keeper_pos = obs['right_team'][0]
    
    # We always control left team (observations and actions
    # are mirrored appropriately by the environment).
    controlled_player_pos = obs['left_team'][obs['active']]
    # Get team size
    N = len(obs['left_team'])
    
    # Does the player we control have the ball?
    if obs['ball_owned_player'] == obs['active'] and obs['ball_owned_team'] == 0:
        # Kickoff strategy: short pass to teammate
        if obs['game_mode'] == GameMode.KickOff:
            return sticky_check(Action.ShortPass, Action.Top) if controlled_player_pos[1] > 0 else sticky_check(Action.ShortPass, Action.Bottom)
        # Goalkick strategy: high pass to front
        if obs['game_mode'] == GameMode.GoalKick:
            return sticky_check(Action.LongPass, Action.Right)
        # Freekick strategy: make shot when close to goal, high pass when in back field, and short pass in mid field
        if obs['game_mode'] == GameMode.FreeKick:
            if controlled_player_pos[0] > goal_threshold:
                if abs(controlled_player_pos[1]) < 0.1:
                    return sticky_check(Action.Shot, Action.Right)
                if abs(controlled_player_pos[1]) < 0.3:
                    return sticky_check(Action.Shot, Action.TopRight) if controlled_player_pos[1]>0 else sticky_check(Action.Shot, Action.BottomRight)
                return sticky_check(Action.HighPass, Action.Top) if controlled_player_pos[1]>0 else sticky_check(Action.HighPass, Action.Bottom)
            
            if controlled_player_pos[0] < -goal_threshold:
                if abs(controlled_player_pos[1]) < 0.3:
                    return sticky_check(Action.HighPass, Action.Right)
                return sticky_check(Action.HighPass, Action.Top) if controlled_player_pos[1]>0 else sticky_check(Action.HighPass, Action.Bottom)
            
            if abs(controlled_player_pos[1]) < 0.3:
                return sticky_check(Action.ShortPass, Action.Right)
            return sticky_check(Action.ShortPass, Action.Top) if controlled_player_pos[1]>0 else sticky_check(Action.ShortPass, Action.Bottom)
        # Corner strategy: high pass to goal area
        if obs['game_mode'] == GameMode.Corner:
            return sticky_check(Action.HighPass, Action.Top) if controlled_player_pos[1]>0 else sticky_check(Action.HighPass, Action.Bottom)
        # Throwin strategy: short pass into field
        if obs['game_mode'] == GameMode.ThrowIn:
            return sticky_check(Action.ShortPass, Action.Top) if controlled_player_pos[1]>0 else sticky_check(Action.ShortPass, Action.Bottom)
        # Penalty strategy: make a shot
        if obs['game_mode'] == GameMode.Penalty:
            right_actions = [Action.TopRight, Action.BottomRight, Action.Right]
            for action in right_actions:
                if action in obs['sticky_actions']:
                    return Action.Shot
            return np.random.choice(right_actions)
            
        # Defending strategy
        if controlled_player_pos[0] < -goal_threshold:
            if abs(controlled_player_pos[1]) < 0.3:
                return sticky_check(Action.HighPass, Action.Right)
            return sticky_check(Action.HighPass, Action.Top) if controlled_player_pos[1]>0 else sticky_check(Action.HighPass, Action.Bottom)
            
        # Make sure player is running.
        if Action.Sprint not in obs['sticky_actions']:
            return Action.Sprint
        
        # Shot if we are 'close' to the goal (based on 'x' coordinate).
        if controlled_player_pos[0] > goal_threshold:
            if abs(controlled_player_pos[1]) < 0.1:
                return sticky_check(Action.Shot, Action.Right)
            if abs(controlled_player_pos[1]) < 0.3:
                return sticky_check(Action.Shot, Action.TopRight) if controlled_player_pos[1]>0 else sticky_check(Action.Shot, Action.BottomRight)
            elif controlled_player_pos[0] < 0.85:
                return Action.Right
            else:
                return sticky_check(Action.HighPass, Action.Top) if controlled_player_pos[1]>0 else sticky_check(Action.HighPass, Action.Bottom)
        
        # Run towards the goal otherwise.
        return Action.Right
    else:
        # when the ball is generally on the ground not flying
        if obs['ball'][2] <= pick_height:
            # Run towards the ball's left position.
            if obs['ball'][0] > controlled_player_pos[0] + slide_threshold:
                if obs['ball'][1] > controlled_player_pos[1] + slide_threshold:
                    return Action.BottomRight
                elif obs['ball'][1] < controlled_player_pos[1] - slide_threshold:
                    return Action.TopRight
                else:
                    return Action.Right
            elif obs['ball'][0] < controlled_player_pos[0] + slide_threshold:
                if obs['ball'][1] > controlled_player_pos[1] + slide_threshold:
                    return Action.BottomLeft
                elif obs['ball'][1] < controlled_player_pos[1] - slide_threshold:
                    return Action.TopLeft
                else:
                    return Action.Left
            # When close to the ball, try to take over.
            else:
                return Action.Slide
        # when the ball is flying
        else:
            landing_point = ball_landing(obs['ball'], obs['ball_direction'])
            # Run towards the landing point's left position.
            if landing_point[0] - body_radius > controlled_player_pos[0] + slide_threshold:
                if landing_point[1] > controlled_player_pos[1] + slide_threshold:
                    return Action.BottomRight
                elif landing_point[1] < controlled_player_pos[1] - slide_threshold:
                    return Action.TopRight
                else:
                    return Action.Right
            elif landing_point[0] - body_radius < controlled_player_pos[0] + slide_threshold:
                if landing_point[1] > controlled_player_pos[1] + slide_threshold:
                    return Action.BottomLeft
                elif landing_point[1] < controlled_player_pos[1] - slide_threshold:
                    return Action.TopLeft
                else:
                    return Action.Left
            # Try to take over the ball if close to the ball.
            elif controlled_player_pos[0] > goal_threshold:
                # Keep making shot when around landing point
                return sticky_check(Action.Shot, Action.Right) if ['ball'][2] <= pick_height else Action.Idle
            else:
                return sticky_check(Action.Slide, Action.Right) if ['ball'][2] <= pick_height else Action.Idle",0,No Code Smell
9853,43842728,2,"# Set up the Environment.
from kaggle_environments import make
env = make(""football"", configuration={""save_video"": True, ""scenario_name"": ""11_vs_11_kaggle"", ""running_in_notebook"": True})
output = env.run([""/kaggle/working/submission.py"", ""do_nothing""])[-1]
print('Left player: reward = %s, status = %s, info = %s' % (output[0]['reward'], output[0]['status'], output[0]['info']))
print('Right player: reward = %s, status = %s, info = %s' % (output[1]['reward'], output[1]['status'], output[1]['info']))
env.render(mode=""human"", width=800, height=600)",0,No Code Smell
9854,43842728,3,"# Validation
from datetime import datetime
from kaggle_environments import make
start = datetime.now()
env = make(""football"", configuration={""save_video"": True, ""scenario_name"": ""11_vs_11_kaggle"", ""running_in_notebook"": True})
output = env.run([""/kaggle/working/submission.py"", ""/kaggle/working/submission.py""])[-1]
print('Left player: reward = %s, status = %s, info = %s' % (output[0]['reward'], output[0]['status'], output[0]['info']))
print('Right player: reward = %s, status = %s, info = %s' % (output[1]['reward'], output[1]['status'], output[1]['info']))
print(datetime.now()-start)
env.render(mode=""human"", width=800, height=600)",0,No Code Smell
9855,43842728,4,"import pandas as pd
log = pd.DataFrame(env.steps)",0,No Code Smell
9856,43842728,5,log[0].head(),0,No Code Smell
9857,43842728,6,"log.iloc[0,0]",0,No Code Smell
9858,43842728,7,"ball_log = pd.DataFrame()
ball_log['ball'] = log[0].apply(lambda x: x['observation']['players_raw'][0]['ball'])
ball_log['ball_direction'] = log[0].apply(lambda x: x['observation']['players_raw'][0]['ball_direction'])
ball_log.head(20)",0,No Code Smell
9859,43842728,8,"print('Ball position at step 9 is ', ball_log.iloc[9,0])
print('Ball position at step 10 is ', ball_log.iloc[10,0])
print('Ball speed at step 9 is ', ball_log.iloc[9,1])
print('Ball speed at step 10 is ', ball_log.iloc[10,1])
print('Ball position change between step 9 and 10 is ',[b - a for a, b in zip(ball_log.iloc[9,0], ball_log.iloc[10,0])])
print('Ball speed change between step 9 and 10 is ',[b - a for a, b in zip(ball_log.iloc[9,1], ball_log.iloc[10,1])])",0,No Code Smell
9860,43842728,9,"print('Ball position at step 9 is ', ball_log.iloc[8,0])
print('Ball position at step 10 is ', ball_log.iloc[9,0])
print('Ball speed at step 9 is ', ball_log.iloc[8,1])
print('Ball speed at step 10 is ', ball_log.iloc[9,1])
print('Ball position change between step 9 and 10 is ',[b - a for a, b in zip(ball_log.iloc[8,0], ball_log.iloc[9,0])])
print('Ball speed change between step 9 and 10 is ',[b - a for a, b in zip(ball_log.iloc[8,1], ball_log.iloc[9,1])])",0,No Code Smell
9861,43842728,10,"right1 = pd.DataFrame()
right1['position'] = log[0].apply(lambda x: x['observation']['players_raw'][0]['right_team'][1])
right1['speed'] = log[0].apply(lambda x: x['observation']['players_raw'][0]['right_team_direction'][1])",0,No Code Smell
9862,43842728,11,"print('Right team player 1 position at step 35 is ',right1['position'][35])
print('Right team player 1 position at step 36 is ',right1['position'][36])
print('Right team player 1 speed at step 35 is ',right1['speed'][35])
print('Right team player 1 speed at step 36 is ',right1['speed'][36])
print('Right team player 1 position change at step 35 is ',[b - a for a, b in zip(right1.iloc[35,0], right1.iloc[36,0])])
print('Right team player 1 speed change at step 35 is ',[b - a for a, b in zip(right1.iloc[35,1], right1.iloc[36,1])])",0,No Code Smell
9863,43842728,12,"step = 70
player = pd.DataFrame()
player['position'] = log[0].apply(lambda x: x['observation']['players_raw'][0]['left_team'][8])
player['speed'] = log[0].apply(lambda x: x['observation']['players_raw'][0]['left_team_direction'][8])
ball = pd.DataFrame()
ball['position'] = log[0].apply(lambda x: x['observation']['players_raw'][0]['ball'])
ball['speed'] = log[0].apply(lambda x: x['observation']['players_raw'][0]['ball_direction'])
print('Player position at step ',step,' is ',player['position'][step])
print('Ball position at step ',step,' is ',ball['position'][step])
print('Player position at step ',step+1,' is ',player['position'][step+1])
print('Ball position at step ',step+1,' is ',ball['position'][step+1])
print('Player position at step ',step+2,' is ',player['position'][step+2])
print('Ball position at step ',step+2,' is ',ball['position'][step+2])
print('Player position at step ',step+3,' is ',player['position'][step+3])
print('Ball position at step ',step+3,' is ',ball['position'][step+3])",0,No Code Smell
9864,43842728,13,"step = 150
player = pd.DataFrame()
player['position'] = log[0].apply(lambda x: x['observation']['players_raw'][0]['left_team'][9])
player['speed'] = log[0].apply(lambda x: x['observation']['players_raw'][0]['left_team_direction'][9])
ball = pd.DataFrame()
ball['position'] = log[0].apply(lambda x: x['observation']['players_raw'][0]['ball'])
ball['speed'] = log[0].apply(lambda x: x['observation']['players_raw'][0]['ball_direction'])
for i in range(5):
    print('Player position at step ',step+i,' is ',player['position'][step+i])
    print('Ball position at step ',step+i,' is ',ball['position'][step+i])
    print('Player speed at step ',step+i,' is ',player['speed'][step+i])
    print('Ball speed at step ',step+i,' is ',ball['speed'][step+i])",0,No Code Smell
9865,43842728,14,,0,No Code Smell
9866,43684243,0,"# Install:
# Kaggle environments.
!git clone https://github.com/Kaggle/kaggle-environments.git
!cd kaggle-environments && pip install .

# GFootball environment.
!apt-get update -y
!apt-get install -y libsdl2-gfx-dev libsdl2-ttf-dev

# Make sure that the Branch in git clone and in wget call matches !!
!git clone -b v2.3 https://github.com/google-research/football.git
!mkdir -p football/third_party/gfootball_engine/lib

!wget https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_v2.3.so -O football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so
!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install .",0,No Code Smell
9867,43684243,1,"%%writefile submission.py
from kaggle_environments.envs.football.helpers import *

# @human_readable_agent wrapper modifies raw observations 
# provided by the environment:
# https://github.com/google-research/football/blob/master/gfootball/doc/observation.md#raw-observations
# into a form easier to work with by humans.
# Following modifications are applied:
# - Action, PlayerRole and GameMode enums are introduced.
# - 'sticky_actions' are turned into a set of active actions (Action enum)
#    see usage example below.
# - 'game_mode' is turned into GameMode enum.
# - 'designated' field is removed, as it always equals to 'active'
#    when a single player is controlled on the team.
# - 'left_team_roles'/'right_team_roles' are turned into PlayerRole enums.
# - Action enum is to be returned by the agent function.
@human_readable_agent
def agent(obs):
    # Make sure player is running.
    if Action.Sprint not in obs['sticky_actions']:
        return Action.Sprint
    # We always control left team (observations and actions
    # are mirrored appropriately by the environment).
    controlled_player_pos = obs['left_team'][obs['active']]
    # Does the player we control have the ball?
    if obs['ball_owned_player'] == obs['active'] and obs['ball_owned_team'] == 0:
        # Shot if we are 'close' to the goal (based on 'x' coordinate).
        if controlled_player_pos[0] > 0.5:
            return Action.Shot
        # pass if not above halfline
        if controlled_player_pos[0] < -0.1:
            return Action.ShortPass
        # dribble just above halfline
        if controlled_player_pos[0] > 0 and controlled_player_pos[0] < 0.3:
            return Action.Dribble
        # Run towards the goal otherwise.
        return Action.Right
    else:
        # Run towards the ball.
        if obs['ball'][0] > controlled_player_pos[0] + 0.05:
            return Action.Right
        if obs['ball'][0] < controlled_player_pos[0] - 0.05:
            return Action.Left
        if obs['ball'][1] > controlled_player_pos[1] + 0.05:
            return Action.Bottom
        if obs['ball'][1] < controlled_player_pos[1] - 0.05:
            return Action.Top
        # Try to take over the ball if close to the ball.
        return Action.Slide",0,No Code Smell
9868,43684243,2,"# Set up the Environment.
from kaggle_environments import make
env = make(""football"", configuration={""save_video"": True, ""scenario_name"": ""11_vs_11_kaggle"", ""running_in_notebook"": True})
output = env.run([""/kaggle/working/submission.py"", ""../input/only-shoot-and-sprint/submission_onlyshootandsprint.py""])[-1]
print('Left player: reward = %s, status = %s, info = %s' % (output[0]['reward'], output[0]['status'], output[0]['info']))
print('Right player: reward = %s, status = %s, info = %s' % (output[1]['reward'], output[1]['status'], output[1]['info']))
env.render(mode=""human"", width=800, height=600)",0,No Code Smell
9869,43684243,3,,0,No Code Smell
9870,44351245,0,"# Kaggle environments.
!git clone https://github.com/Kaggle/kaggle-environments.git
!cd kaggle-environments && pip install .

# GFootball environment.
!apt-get update -y
!apt-get install -y libsdl2-gfx-dev libsdl2-ttf-dev

# Make sure that the Branch in git clone and in wget call matches !!
!git clone -b v2.6 https://github.com/google-research/football.git
!mkdir -p football/third_party/gfootball_engine/lib

!wget https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_v2.6.so -O football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so
!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install .",0,No Code Smell
9871,44351245,1,"from kaggle_environments.envs.football.helpers import *

@human_readable_agent
def agent(obs):
    # Make sure player is running.
    if Action.Sprint not in obs['sticky_actions']:
        return Action.Sprint
    # We always control left team (observations and actions
    # are mirrored appropriately by the environment).
    controlled_player_pos = obs['left_team'][obs['active']]
    # Does the player we control have the ball?
    if obs['ball_owned_player'] == obs['active'] and obs['ball_owned_team'] == 0:
        # Shot if we are 'close' to the goal (based on 'x' coordinate).
        if controlled_player_pos[0] > 0.5:
            return Action.Shot
        # Run towards the goal otherwise.
        return Action.Right
    else:
        # Run towards the ball.
        if obs['ball'][0] > controlled_player_pos[0] + 0.05:
            return Action.Right
        if obs['ball'][0] < controlled_player_pos[0] - 0.05:
            return Action.Left
        if obs['ball'][1] > controlled_player_pos[1] + 0.05:
            return Action.Bottom
        if obs['ball'][1] < controlled_player_pos[1] - 0.05:
            return Action.Top
        # Try to take over the ball if close to the ball.
        return Action.Slide",0,No Code Smell
9872,44351245,2,"# Set up the Environment.
from kaggle_environments import make
import pprint

# 1000 steps are only generated - to speed up process of generation
env = make(""football"", configuration={""save_video"": True, ""scenario_name"": ""11_vs_11_kaggle"", ""running_in_notebook"": True, ""episodeSteps"": 1000}, debug=True)
output = env.run([agent, agent])
print('Left player: reward = %s, status = %s, info = %s' % (output[-1][0]['reward'], output[-1][0]['status'], output[-1][0]['info']))
print('Right player: reward = %s, status = %s, info = %s' % (output[-1][1]['reward'], output[-1][1]['status'], output[-1][1]['info']))
#env.render(mode=""human"", width=800, height=600) - used to output video",1,Code Smell
9873,44351245,3,"import matplotlib.patches as patches
from  matplotlib.patches import Arc
from matplotlib import pyplot as plt
from matplotlib import animation
import matplotlib.patches as mpatches

# Change size of figure
plt.rcParams['figure.figsize'] = [20, 16]
def drawPitch(width, height, color=""w""):

  fig = plt.figure()
  ax = plt.axes(xlim=(-10, width + 10), ylim=(-15, height + 5))
  plt.axis('off')

  # Grass around pitch
  rect = patches.Rectangle((-5,-5), width + 10, height + 10, linewidth=1, edgecolor='gray',facecolor='#3f995b', capstyle='round')
  ax.add_patch(rect)

  # Pitch boundaries
  rect = plt.Rectangle((0, 0), width, height, ec=color, fc=""None"", lw=2)
  ax.add_patch(rect)

  # Middle line
  plt.plot([width/2, width/2], [0, height], color=color, linewidth=2)
  
  # Dots
  dots_x = [11, width/2, width-11]
  for x in dots_x:
    plt.plot(x, height/2, 'o', color=color, linewidth=2)

  # Penalty box  
  penalty_box_dim = [16.5, 40.3]
  penalty_box_pos_y = (height - penalty_box_dim[1]) / 2

  rect = plt.Rectangle((0, penalty_box_pos_y), penalty_box_dim[0], penalty_box_dim[1], ec=color, fc=""None"", lw=2)
  ax.add_patch(rect)
  rect = plt.Rectangle((width, penalty_box_pos_y), -penalty_box_dim[0], penalty_box_dim[1], ec=color, fc=""None"", lw=2)
  ax.add_patch(rect)

  #Goal box
  goal_box_dim = [5.5, penalty_box_dim[1] - 11 * 2]
  goal_box_pos_y = (penalty_box_pos_y + 11)

  rect = plt.Rectangle((0, goal_box_pos_y), goal_box_dim[0], goal_box_dim[1], ec=color, fc=""None"", lw=2)
  ax.add_patch(rect)
  rect = plt.Rectangle((width, goal_box_pos_y), -goal_box_dim[0], goal_box_dim[1], ec=color, fc=""None"", lw=2)
  ax.add_patch(rect)

  #Goals
  rect = plt.Rectangle((0, penalty_box_pos_y + 16.5), -3, 7.5, ec=color, fc=color, lw=2, alpha=0.3)
  ax.add_patch(rect)
  rect = plt.Rectangle((width, penalty_box_pos_y + 16.5), 3, 7.5, ec=color, fc=color, lw=2, alpha=0.3)
  ax.add_patch(rect)
    
  # Middle circle
  mid_circle = plt.Circle([width/2, height/2], 9.15, color=color, fc=""None"", lw=2)
  ax.add_artist(mid_circle)


  # Penalty box arcs
  left  = patches.Arc([11, height/2], 2*9.15, 2*9.15, color=color, fc=""None"", lw=2, angle=0, theta1=308, theta2=52)
  ax.add_patch(left)
  right = patches.Arc([width - 11, height/2], 2*9.15, 2*9.15, color=color, fc=""None"", lw=2, angle=180, theta1=308, theta2=52)
  ax.add_patch(right)

  # Arcs on corners
  corners = [[0, 0], [width, 0], [width, height], [0, height]]
  angle = 0
  for x,y in corners:
    c = patches.Arc([x, y], 2, 2, color=color, fc=""None"", lw=2, angle=angle,theta1=0, theta2=90)
    ax.add_patch(c)
    angle += 90
  return fig, ax",0,No Code Smell
9874,44351245,4,"WIDTH = 105
HEIGHT = 68

drawPitch(WIDTH, HEIGHT)",0,No Code Smell
9875,44351245,5,"X_RESIZE = WIDTH
Y_RESIZE = HEIGHT / 0.42

class GameMode(Enum):
    Normal = 0
    KickOff = 1
    GoalKick = 2
    FreeKick = 3
    Corner = 4
    ThrowIn = 5
    Penalty = 6

def scale_x(x):
  return (x + 1) * (X_RESIZE/2)

def scale_y(y):
  return (y + 0.42) * (Y_RESIZE/2)


def extract_data(frame):
  res = {}
  obs = frame[0]['observation']['players_raw'][0]
  res[""left_team""] = [(scale_x(x), scale_y(y)) for x, y in obs[""left_team""]]
  res[""right_team""] = [(scale_x(x), scale_y(y)) for x, y in obs[""right_team""]]

  ball_x, ball_y, ball_z = obs[""ball""]
  res[""ball""] = [scale_x(ball_x),  scale_y(ball_y), ball_z]
  res[""score""] = obs[""score""]
  res[""steps_left""] = obs[""steps_left""]
  res[""ball_owned_team""] = obs[""ball_owned_team""]
  res[""ball_owned_player""] = obs[""ball_owned_player""]
  res[""right_team_roles""] = obs[""right_team_roles""]
  res[""left_team_roles""] = obs[""left_team_roles""]
  res[""left_team_direction""] = obs[""left_team_direction""]
  res[""right_team_direction""] = obs[""right_team_direction""]
  res[""game_mode""] = GameMode(obs[""game_mode""]).name
  return res",0,No Code Smell
9876,44351245,6,"import math


def draw_team(obs, team, side):
  X = []
  Y = []
  for x, y in obs[side]:
    X.append(x)
    Y.append(y)
  team.set_data(X, Y)

def draw_ball(obs, ball):
  ball.set_markersize(10 + 5 * obs[""ball""][2]) # Scale size of ball based on height
  ball.set_data(obs[""ball""][:2])

def draw_ball_owner(obs, ball_owner, team_active):
  if obs[""ball_owned_team""] == 0:
    x, y = obs[""left_team""][obs[""ball_owned_player""]]
    ball_owner.set_data(x, y)
    team_active.set_data(WIDTH / 4 + 7, -7)
    team_active.set_markerfacecolor(""red"")
  elif obs[""ball_owned_team""] == 1:
    x, y = obs[""right_team""][obs[""ball_owned_player""]]
    ball_owner.set_data(x, y)
    team_active.set_data(WIDTH / 4 + 50, -7)
    team_active.set_markerfacecolor(""blue"")
  else:
    ball_owner.set_data([], [])
    team_active.set_data([], [])
    
def draw_players_directions(obs, directions, side):
  index = 0
  if ""right"" in side:
    index = 11
  for i, player_dir in enumerate(obs[f""{side}_direction""]):
    x_dir, y_dir = player_dir
    dist = math.sqrt(x_dir ** 2 + y_dir ** 2) + 0.00001 # to prevent division by 0
    x = obs[side][i][0]
    y = obs[side][i][1] 
    directions[i + index].set_data([x, x + x_dir / dist ], [y, y + y_dir / dist])",0,No Code Smell
9877,44351245,7,"import numpy as np
from IPython.display import HTML

fig, ax = drawPitch(WIDTH, HEIGHT)
ax.invert_yaxis()

ball_owner, = ax.plot([], [], 'o', markersize=30,  markerfacecolor=""yellow"", alpha=0.5)
team_active, = ax.plot([], [], 'o', markersize=30,  markerfacecolor=""blue"", markeredgecolor=""None"")

team_left, = ax.plot([], [], 'o', markersize=20, markerfacecolor=""r"", markeredgewidth=2, markeredgecolor=""white"")
team_right, = ax.plot([], [], 'o', markersize=20,  markerfacecolor=""b"", markeredgewidth=2, markeredgecolor=""white"")

ball, = ax.plot([], [], 'o', markersize=10,  markerfacecolor=""black"", markeredgewidth=2, markeredgecolor=""white"")
text_frame = ax.text(-5, -5, '', fontsize=25)
match_info = ax.text(105 / 4 + 10, -5, '', fontsize=25)
game_mode = ax.text(105 - 25, -5, '', fontsize=25)
goal_notification = ax.text(105 / 4 + 10, 0, '', fontsize=25)

# Drawing of directions definitely can be done in a better way
directions = []
for i in range(22):
  direction, = ax.plot([], [], color='yellow', lw=3)
  directions.append(direction)

  
drawings = [team_active, ball_owner, team_left, team_right, ball, text_frame, match_info, game_mode, goal_notification]

def init():
    team_left.set_data([], [])
    team_right.set_data([], [])
    ball_owner.set_data([], [])
    team_active.set_data([], [])
    ball.set_data([], [])
    return drawings 

def animate(i):
  global prev_score_a, prev_score_b
  obs = extract_data(output[i])

  # Draw info about ball possesion
  draw_ball_owner(obs, ball_owner, team_active)

  # Draw players
  draw_team(obs, team_left, ""left_team"")
  draw_team(obs, team_right, ""right_team"")

  draw_players_directions(obs, directions, ""left_team"")
  draw_players_directions(obs, directions, ""right_team"")
    
  draw_ball(obs, ball)

  # Draw textual informations
  text_frame.set_text(f""Frame: {i}/{obs['steps_left'] + i - 1}"")
  game_mode.set_text(f""Game mode: {obs['game_mode']}"")
  
  score_a, score_b = obs[""score""]
  match_info.set_text(f""Left team {score_a} : {score_b} Right Team"")

  return drawings  

# May take a while
anim = animation.FuncAnimation(fig, animate, init_func=init,
                               frames=1000, interval=100, blit=True)

HTML(anim.to_html5_video())",0,No Code Smell
9878,52748519,0,"import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import os",1,Code Smell
9879,52748519,1,"# Kaggle environments.
!git clone https://github.com/Kaggle/kaggle-environments.git
!cd kaggle-environments && pip install .

# GFootball environment.
!apt-get update -y
!apt-get install -y libsdl2-gfx-dev libsdl2-ttf-dev

# Make sure that the Branch in git clone and in wget call matches !!
!git clone -b v2.6 https://github.com/google-research/football.git
!mkdir -p football/third_party/gfootball_engine/lib

!wget https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_v2.6.so -O football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so
!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install .

# keras reinforcement learning
!pip install reinforcement_learning_keras",0,No Code Smell
9880,52748519,2,"import keras
import math
from keras.models import Sequential
from keras.layers import Dense, Dropout

class RLNAgent:
    def __init__(self):
        self.memory = []
        self.rewards = []
        self.gamma = 0.3
        self.epsilon = 0.7
        self.epsilon_decay = 0.85
        self.epsilon_min = 0.05
        self.learning_rate = 0.0002
        self._build_model()
    
    # setting mechanic learning model
    def _build_model(self):
        model = Sequential()
        model.add(Dense(157, input_dim = 103, activation = 'linear'))
        #model.add(Dropout(0.1))
        model.add(Dense(101, activation = 'tanh'))
        model.add(Dense(83, activation = 'tanh'))
        model.add(Dense(29, activation = 'tanh'))
        model.add(Dense(18, activation = 'tanh'))
        model.compile(loss='mean_squared_error', optimizer = keras.optimizers.Adam(lr=self.learning_rate))
        self.model = model
    
    # transfor json type to (1,103) array
    def state_as_sample103(self, obs):
        sample103 = np.concatenate((
            np.array(obs['left_team'][obs['active']]).flatten(),
            np.array(obs['left_team_direction'][obs['active']]).flatten(),
            np.array(obs['ball']).flatten(),
            np.array(obs['ball_direction']).flatten(),
            np.array(obs['left_team'][obs['active']]).flatten() - np.array(obs['ball']).flatten()[0:1],
            np.array(obs['left_team']).flatten(),
            np.array(obs['left_team_direction']).flatten(),
            np.array(obs['right_team']).flatten(),
            np.array(obs['right_team_direction']).flatten(),
            (lambda x: [1,0,0] if x == -1 else ([0,1,0] if x == 0 else [0,0,1]))(obs['ball_owned_team'])
        ))
        return sample103.reshape((1,103))
    
    # remember the play state
    def remember(self, state, predict, action, reward, next_state):
        self.memory.append((state, predict, action, reward, next_state))
        self.rewards.append(reward)

    # get the model predict for the player's best action
    def act_predict(self, state):
        return self.model.predict(state)
    
    # using reward to train the model
    def replay(self, batch_size, maxreward):
        batches = np.arange(1, len(self.memory), 1)
        memory_max_length = len(self.memory)
        
        for i in batches:
            state, predict, action, reward, next_state = self.memory[i]

            # add noise
            target_f = (predict * (1-self.epsilon)) + (np.random.rand(1,18) * self.epsilon)
            
            # the training target value is the addition rewards with follow-up (1.5s) effect            
            followup_rewards = 0
            for j in range(i, min(i + 30, memory_max_length), 1):
                followup_rewards = followup_rewards + (self.rewards[j] * math.exp( (i - j) * 2 ))
            
            #if followup_rewards < 1 and followup_rewards > 0 and (self.rewards[i] - self.rewards[i-1]) < 0:
            #    continue
            #print(""{}, followup_rewards:{}"".format(i,followup_rewards))
            
            target = target_f[0][action] + (followup_rewards * 0.01) + (self.rewards[i] - self.rewards[i-1]) * 0.5
            
            # limit the max target
            if target > 0.8:
                target = 0.8

            target_f[0][action] = target
            
            # training
            self.model.fit(state, target_f, epochs = 1, verbose = 0)
        
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
                        
    def save(self):
        self.model.save('rl_model')
        
    def load(self):
        self.model = keras.models.load_model('rl_model')",0,No Code Smell
9881,52748519,3,"from kaggle_environments import make
import copy
import random

env = make(""football"",
           configuration={""save_video"": False, 
                          ""scenario_name"": ""11_vs_11_kaggle"", 
                          ""running_in_notebook"": True})",0,No Code Smell
9882,52748519,4,"def closed_contestant_num(obs, controlled_player_pos):
    closed_num = 0
    for i in range(1, len(obs[""right_team""])):
        if abs(controlled_player_pos[0] - obs[""right_team""][i][0]) < 0.08 and abs(controlled_player_pos[1] - obs[""right_team""][i][1]) < 0.04 :
            closed_num = closed_num + 1
    return closed_num

def rule_based_agent(obs):
    controlled_player_pos = obs['left_team'][obs['active']]
    if obs['ball_owned_player'] == obs['active'] and obs['ball_owned_team'] == 0:
        closed_people = closed_contestant_num(obs, controlled_player_pos)
        if controlled_player_pos[0] > 0.5 and closed_people < 4:
            return 12
        if controlled_player_pos[0] > 0.5 and controlled_player_pos[1] > 0.3 and closed_people > 5:
            return 3
        if controlled_player_pos[0] > 0.5 and controlled_player_pos[1] < -0.3 and closed_people > 5:
            return 7
        return 5
    else:        
        if obs['ball'][0] > controlled_player_pos[0] + 0.05:
            return 5
        if obs['ball'][0] < controlled_player_pos[0] - 0.05:
            return 1
        if obs['ball'][1] > controlled_player_pos[1] + 0.05:
            return 7
        if obs['ball'][1] < controlled_player_pos[1] - 0.05:
            return 3
        return 16",0,No Code Smell
9883,52748519,5,"agent = RLNAgent()

episodes = 150  # playing the game up to {episodes} times
steps = 2000  # each game with {steps} steps

for e in range(episodes):
    
    env.reset()
    agent.memory = []
    agent.rewards = []
    
    trainer = env.train([None, ""run_right""])
    trainer.reset()
    
    obs = env.state[0]['observation']['players_raw'][0]
    maxreward = -10
    
    for time_t in range(steps):    # simulating the game step by step
        
        action = 0
        
        state = agent.state_as_sample103(obs)
        predict = agent.act_predict(state)
        action = np.argmax(predict)
        
        # learning from rule based agent coach
        if e < 10:
            action = rule_based_agent(obs)
            predict = np.reshape([(lambda x, y: 1 if x==y else 0)(x, action) for x in range(18)], (1,18))
        
        next_obs, reward, done, info = trainer.step([action])
        
        reward = - 10 if reward == None else reward
            
        next_obs = next_obs['players_raw'][0]
        next_state = agent.state_as_sample103(next_obs)
        
        # if we steal the ball, the reward will gain 0.05 point
        if obs['ball_owned_team'] == 1 and next_obs['ball_owned_team'] == 0:
            reward += 0.05

        # if we get the ball, the reward will gain with the right moving distince
        if obs['ball_owned_team'] == 0:
            reward += ( next_obs['ball'][0] - obs['ball'][0] ) * 0.1 - abs( next_obs['ball'][1] ) * 0.01

        # if active player far from the ball, the reward will lose by the moving distance
        distance = abs(obs['left_team'][obs['active']][0] - obs['ball'][0]) + abs(obs['left_team'][obs['active']][1] - obs['ball'][1])
        next_distance = abs(next_obs['left_team'][next_obs['active']][0] - next_obs['ball'][0]) + abs(next_obs['left_team'][next_obs['active']][1] - next_obs['ball'][1])
        
        if abs(next_obs['left_team'][next_obs['active']][0] - obs['left_team'][obs['active']][0]) < 0.01 and abs(next_obs['left_team'][next_obs['active']][1] - obs['left_team'][obs['active']][1]) < 0.01 :
            reward -= 0.5
        
        if next_obs['ball_owned_team'] != 0:
            reward += - distance - (next_distance - distance) * 2

        # if the next action is shooting the ball, the reward will gain 0.2 point
        if obs['ball_owned_team'] == 0 and obs['left_team'][obs['active']][0] > 0.5 and action == 12:
            reward += 0.2
            
        if obs['left_team'][obs['active']][0] < 0 and action == 12:
            reward -= 0.05
            
        # if we go out the square, the reward will lose 2 point
        if obs['left_team'][obs['active']][0] > 0.98 or obs['left_team'][obs['active']][0] < - 0.98 or obs['left_team'][obs['active']][1] < - 0.39 or obs['left_team'][obs['active']][1] > 0.39:
            reward -= 2
        
        agent.remember(state, predict, action, reward, next_state)
        
        obs = copy.deepcopy(next_obs)
        
        if maxreward < reward:
            maxreward = reward

    print(""episode: {}/{}, score: {}, action: {}, probability: {}"".format(e + 1, episodes, maxreward, action, np.max(predict,axis = 1)))
    agent.replay(steps, maxreward)",0,No Code Smell
9884,52748519,6,agent.save(),0,No Code Smell
9885,52748519,7,"%%writefile main.py
# for making a video

import numpy as np # linear algebra
from kaggle_environments.envs.football.helpers import *
import keras
from keras.models import Sequential
from keras.layers import Dense


class QNAgent:
    def __init__(self):
        self.load()
        
    def act(self, state):
        act_values = self.model.predict(state)
        return np.argmax(act_values[0])
        
    def load(self):
        self.model = keras.models.load_model('/kaggle/working/rl_model')

    def state_as_sample103(self, obs):
        sample103 = np.concatenate((
            np.array(obs['left_team'][obs['active']]).flatten(),
            np.array(obs['left_team_direction'][obs['active']]).flatten(),
            np.array(obs['ball']).flatten(),
            np.array(obs['ball_direction']).flatten(),
            np.array(obs['left_team'][obs['active']]).flatten() - np.array(obs['ball']).flatten()[0:1],
            np.array(obs['left_team']).flatten(),
            np.array(obs['left_team_direction']).flatten(),
            np.array(obs['right_team']).flatten(),
            np.array(obs['right_team_direction']).flatten(),
            (lambda x: [1,0,0] if x == -1 else ([0,1,0] if x == 0 else [0,0,1]))(obs['ball_owned_team'])
        ))
        return sample103.reshape((1,103))


qagent = QNAgent()

ActionDic = [Action.Idle,
             Action.Left,
             Action.TopLeft,
             Action.Top,
             Action.TopRight,
             Action.Right,
             Action.BottomRight,
             Action.Bottom,
             Action.BottomLeft,
             Action.LongPass,
             Action.HighPass,
             Action.ShortPass,
             Action.Shot,
             Action.Sprint,
             Action.ReleaseDirection,
             Action.ReleaseSprint,
             Action.Slide,
             Action.Dribble,
             Action.ReleaseDribble
            ]

@human_readable_agent
def agent(obs):
    state = qagent.state_as_sample103(obs)
    action = qagent.act(state)    
    return ActionDic[action]",0,No Code Smell
9886,52748519,8,"from kaggle_environments import make

env = make(""football"", configuration={""save_video"": True, ""scenario_name"": ""11_vs_11_kaggle"", ""running_in_notebook"": True})
output = env.run([""/kaggle/working/main.py"", ""run_right""])[-1]

print('Left player: reward = %s, status = %s, info = %s' % (output[0]['reward'], output[0]['status'], output[0]['info']))
print('Right player: reward = %s, status = %s, info = %s' % (output[1]['reward'], output[1]['status'], output[1]['info']))

env.render(mode=""human"", width=800, height=600)",0,No Code Smell
9887,52748519,9,"%%writefile main.py
# for submition

import numpy as np # linear algebra
from kaggle_environments.envs.football.helpers import *
import keras
from keras.models import Sequential
from keras.layers import Dense


class QNAgent:
    def __init__(self):
        self.load()
        
    def act(self, state):
        act_values = self.model.predict(state)
        return np.argmax(act_values[0])
        
    def load(self):
        self.model = keras.models.load_model('/kaggle_simulations/agent/rl_model')

    def state_as_sample103(self, obs):
        sample103 = np.concatenate((
            np.array(obs['left_team'][obs['active']]).flatten(),
            np.array(obs['left_team_direction'][obs['active']]).flatten(),
            np.array(obs['ball']).flatten(),
            np.array(obs['ball_direction']).flatten(),
            np.array(obs['left_team'][obs['active']]).flatten() - np.array(obs['ball']).flatten()[0:1],
            np.array(obs['left_team']).flatten(),
            np.array(obs['left_team_direction']).flatten(),
            np.array(obs['right_team']).flatten(),
            np.array(obs['right_team_direction']).flatten(),
            (lambda x: [1,0,0] if x == -1 else ([0,1,0] if x == 0 else [0,0,1]))(obs['ball_owned_team'])
        ))
        return sample103.reshape((1,103))


qagent = QNAgent()

ActionDic = [Action.Idle,
             Action.Left,
             Action.TopLeft,
             Action.Top,
             Action.TopRight,
             Action.Right,
             Action.BottomRight,
             Action.Bottom,
             Action.BottomLeft,
             Action.LongPass,
             Action.HighPass,
             Action.ShortPass,
             Action.Shot,
             Action.Sprint,
             Action.ReleaseDirection,
             Action.ReleaseSprint,
             Action.Slide,
             Action.Dribble,
             Action.ReleaseDribble
            ]

@human_readable_agent
def agent(obs):
    state = qagent.state_as_sample103(obs)
    action = qagent.act(state)    
    return ActionDic[action]",0,No Code Smell
9888,52748519,10,!tar -czvf submission.tar.gz main.py rl_model,1,Code Smell
9889,52748519,11,,0,No Code Smell
9890,44809691,0,"# Install:
# Kaggle environments.
!git clone https://github.com/Kaggle/kaggle-environments.git
!cd kaggle-environments && pip install .

# GFootball environment.
!apt-get update -y
!apt-get install -y libsdl2-gfx-dev libsdl2-ttf-dev

# Make sure that the Branch in git clone and in wget call matches !!
!git clone -b v2.6 https://github.com/google-research/football.git
!mkdir -p football/third_party/gfootball_engine/lib

!wget https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_v2.6.so -O football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so
!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install .",0,No Code Smell
9891,44809691,1,"%%writefile AlbertEinsteinAcademic.py
from kaggle_environments.envs.football.helpers import *
import numpy

# @human_readable_agent wrapper modifies raw observations 
# provided by the environment:
# https://github.com/google-research/football/blob/master/gfootball/doc/observation.md#raw-observations
# into a form easier to work with by humans.
# Following modifications are applied:
# - Action, PlayerRole and GameMode enums are introduced.
# - 'sticky_actions' are turned into a set of active actions (Action enum)
#    see usage example below.
# - 'game_mode' is turned into GameMode enum.
# - 'designated' field is removed, as it always equals to 'active'
#    when a single player is controlled on the team.
# - 'left_team_roles'/'right_team_roles' are turned into PlayerRole enums.
# - Action enum is to be returned by the agent function.        

@human_readable_agent
def agent(obs):
        
    # Make sure player is running.
    if Action.Sprint not in obs['sticky_actions']:
        return Action.Sprint
    # We always control left team (observations and actions
    # are mirrored appropriately by the environment).
    controlled_player_pos = obs['left_team'][obs['active']]
    # Does the player we control have the ball?
    
    
    if obs['ball_owned_team'] == 0 and obs['ball_owned_player'] == obs['active']:
        
        # Shot if we are 'close' to the goal (based on 'x-y' coordinate).
        if controlled_player_pos[0] >= 0.7 and controlled_player_pos[1] > 0.3:
            return numpy.random.choice([Action.Shot, Action.TopRight, Action.Right])
        
        if controlled_player_pos[0] >= 0.7 and controlled_player_pos[1] < -0.3:
            return  numpy.random.choice([Action.Shot, Action.BottomRight, Action.Right])
        
        if controlled_player_pos[0] > 0.5 and (controlled_player_pos[1] >= -0.5 and controlled_player_pos[1] <= 0.5):
            return numpy.random.choice([Action.Shot, Action.TopRight, Action.BottomRight]) 
        
        if controlled_player_pos[0] >= 0.7:
            if controlled_player_pos[1] >= -0.3 and controlled_player_pos[1] <= 0.3:
                return Action.Shot 
            
        
        if controlled_player_pos[0] < 0.0:
            return numpy.random.choice([Action.ShortPass, Action.LongPass, Action.HighPass])
        
        #go forward and remove ball in dangerous place.
        if controlled_player_pos[0] < -0.5:
            return Action.Right
        
        if controlled_player_pos[0] >= 0.0:
            return Action.Right

        
        # Run towards the goal otherwise.
        return Action.Right 
        
    else:
        
        right_player_pos = obs['right_team'][obs['active']]
        
        # Run towards the ball.
        if obs['ball'][0] > controlled_player_pos[0] + 0.05:
            return numpy.random.choice([Action.Right, Action.BottomRight, Action.TopRight])

        if obs['ball'][0] < controlled_player_pos[0] - 0.05:
            return numpy.random.choice([Action.Left, Action.TopLeft, Action.BottomLeft])

        if obs['ball'][1] > controlled_player_pos[1] + 0.05:
            return numpy.random.choice([Action.Bottom, Action.BottomLeft, Action.BottomRight])

        if obs['ball'][1] < controlled_player_pos[1] - 0.05:
            return numpy.random.choice([Action.Top, Action.TopLeft, Action.TopRight])
        
        #run toward the opposite player
        if right_player_pos[0] > controlled_player_pos[0] + 0.05:
            return numpy.random.choice([Action.Right, Action.BottomRight, Action.TopRight])
        
        if right_player_pos[0] < controlled_player_pos[0] - 0.05:
            return numpy.random.choice([Action.Left, Action.BottomLeft, Action.TopLeft])
        
        if right_player_pos[1] > controlled_player_pos[1] + 0.05:
            return numpy.random.choice([Action.Bottom, Action.BottomRight, Action.BottomLeft])
        
        if right_player_pos[1] < controlled_player_pos[1] - 0.05:
            return numpy.random.choice([Action.Top, Action.TopLeft, Action.TopRight])

        # Try to take over the ball if close to the ball.
        return Action.Slide",0,No Code Smell
9892,44809691,2,"%%writefile NeilsBohrAcademic.py
from kaggle_environments.envs.football.helpers import *
import numpy

# @human_readable_agent wrapper modifies raw observations 
# provided by the environment:
# https://github.com/google-research/football/blob/master/gfootball/doc/observation.md#raw-observations
# into a form easier to work with by humans.
# Following modifications are applied:
# - Action, PlayerRole and GameMode enums are introduced.
# - 'sticky_actions' are turned into a set of active actions (Action enum)
#    see usage example below.
# - 'game_mode' is turned into GameMode enum.
# - 'designated' field is removed, as it always equals to 'active'
#    when a single player is controlled on the team.
# - 'left_team_roles'/'right_team_roles' are turned into PlayerRole enums.
# - Action enum is to be returned by the agent function.        

@human_readable_agent
def agent(obs):
        
    # Make sure player is running.
    if Action.Sprint not in obs['sticky_actions']:
        return Action.Sprint
    # We always control left team (observations and actions
    # are mirrored appropriately by the environment).
    controlled_player_pos = obs['left_team'][obs['active'] - 5]
    
    # Does the player we control have the ball?
    right_player_pos = obs['right_team'][obs['active']]
    goalKeeperR = obs['right_team'][0] #opponent goal keeper
    
################################### Team Strategy ###############################################################   

    if obs['ball_owned_team'] == 0 and obs['ball_owned_player'] == obs['active']:
        
        # single player have a ball.
        player_pos = obs['left_team'][obs['active']]
        u_vector = numpy.zeros((2, 1)) # 
        w_vector = u_vector.copy()
        
        u_vector[0] = goalKeeperR[0] - player_pos[0]
        u_vector[1] = goalKeeperR[1] - player_pos[1]
        distanceG_P = numpy.linalg.norm(u_vector, 2) # distance between active players and goal keeper oppenent
        
        w_vector[0] = player_pos[0] - right_player_pos[0]
        w_vector[1] = player_pos[1] - right_player_pos[0]
        distanceP_P = numpy.linalg.norm(w_vector, 2) # distance between active player and opponent
        
        #left goal keeper kicks a ball in dangerous place
        if obs['active'] == 0: 
            x = numpy.random.choice([Action.Shot, Action.LongPass])
            print('GoalKeeper: ', x, ' Active players: ', obs['active'])
            return x
        
        
        #*************************** controlling only one players *******************************#
        
        # player's prepares  to score.
        if player_pos[0] > 0.5 and (player_pos[1] >= -0.5 and player_pos[1] <= 0.5):
            x = numpy.random.choice([Action.TopRight, Action.BottomRight, Action.Right, 
                                     Action.Shot, Action.Top, Action.Bottom]) 
            return x
        
        #player echap or dribble
        if distanceP_P < 0.025:
            return numpy.random.choice([Action.ReleaseSprint, Action.BottomRight, Action.TopRight, 
                                        Action.ReleaseDribble])
        
        # Shot if we are 'close' to the goal (based on 'x-y' coordinate).
        if player_pos[0] >= 0.7:
            
            #player tends to adjust a ball to go to score
            if player_pos[1] > 0.5:
                x = numpy.random.choice([Action.TopRight, Action.Top, Action.Shot])
                return x
            
            if player_pos[1] < -0.5:
                x = numpy.random.choice([Action.BottomRight, Action.Bottom, Action.Shot])
                return  x
            
            if distanceG_P < 0.05:
                return numpy.random.choice([Action.BottomRight, Action.TopRight, Action.Shot])
            
            if player_pos[1] >= -0.5 and player_pos[1] <= 0.5:
                x = Action.Shot
                return x
        #******************************************************************************************#
            
        # make some passing ball forward
        if (controlled_player_pos[0] - player_pos[0]) > 0: 
            x = numpy.random.choice([Action.ShortPass, Action.LongPass, Action.HighPass])
            return x
        
        # Run towards the goal otherwise.
        return Action.Right 
        
    else:
        
        # Run towards the ball.
        if (obs['ball'][0] > controlled_player_pos[0] + 0.05) and (right_player_pos[0] > controlled_player_pos[0] + 0.05):
            return numpy.random.choice([Action.Right, Action.BottomRight, Action.TopRight])

        if (obs['ball'][0] < controlled_player_pos[0] - 0.05) and (right_player_pos[0] < controlled_player_pos[0] - 0.05):
            return numpy.random.choice([Action.Left, Action.TopLeft, Action.BottomLeft])

        if (obs['ball'][1] > controlled_player_pos[1] + 0.05) and (right_player_pos[1] > controlled_player_pos[1] + 0.05):
            return numpy.random.choice([Action.Bottom, Action.BottomLeft, Action.BottomRight])

        if (obs['ball'][1] < controlled_player_pos[1] - 0.05) and (right_player_pos[1] < controlled_player_pos[1] - 0.05):
            return numpy.random.choice([Action.Top, Action.TopLeft, Action.TopRight])
        
        # Run towards the ball.
        if obs['ball'][0] > controlled_player_pos[0] + 0.05:
            return numpy.random.choice([Action.Right, Action.BottomRight, Action.TopRight])

        if obs['ball'][0] < controlled_player_pos[0] - 0.05:
            return numpy.random.choice([Action.Left, Action.TopLeft, Action.BottomLeft])

        if obs['ball'][1] > controlled_player_pos[1] + 0.05:
            return numpy.random.choice([Action.Bottom, Action.BottomLeft, Action.BottomRight])

        if obs['ball'][1] < controlled_player_pos[1] - 0.05:
            return numpy.random.choice([Action.Top, Action.TopLeft, Action.TopRight])

        # Try to take over the ball if close to the ball.
        return Action.Slide",0,No Code Smell
9893,44809691,3,print('Welcome to this beautiful Kaggle stadium. We are going to watch a big derby between Einstein Academic Physics football and Bohr Academic Physics football. Goodluck!'),1,Code Smell
9894,44809691,4,"from kaggle_environments import make
env = make(""football"", configuration={""save_video"": True, ""scenario_name"": ""11_vs_11_kaggle"", ""running_in_notebook"": True})
output = env.run([""/kaggle/working/AlbertEinsteinAcademic.py"", ""/kaggle/working/NeilsBohrAcademic.py""])[-1]
print('Left player: reward = %s, status = %s, info = %s' % (output[0]['reward'], output[0]['status'], output[0]['info']))
print('Right player: reward = %s, status = %s, info = %s' % (output[1]['reward'], output[1]['status'], output[1]['info']))
env.render(mode=""human"", width=300, height=400)",0,No Code Smell
9895,44809691,5,,0,No Code Smell
9896,48325623,0,"!git clone https://github.com/Kaggle/kaggle-environments.git
!cd kaggle-environments && pip install .
!apt-get update -y
!apt-get install -y libsdl2-gfx-dev libsdl2-ttf-dev
!git clone -b v2.8 https://github.com/google-research/football.git
!mkdir -p football/third_party/gfootball_engine/lib
!wget https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_v2.8.so -O football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so
!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install .",0,No Code Smell
9897,48325623,1,"%%writefile submission.py
from kaggle_environments.envs.football.helpers import *
import random
import math

@human_readable_agent
def agent(obs):
    if Action.Dribble in obs['sticky_actions']:
        return Action.ReleaseDribble
    #Action.ReleaseDirection
    #Action.ReleaseSprint
    
    if GameMode.KickOff == obs['game_mode']:
        return Action.ShortPass
    if GameMode.Penalty == obs['game_mode']:
        return Action.Shot
    if GameMode.GoalKick == obs['game_mode']:
        return Action.LongPass
    if GameMode.Corner == obs['game_mode']:
        return Action.LongPass
    if GameMode.FreeKick == obs['game_mode']:
        return Action.Shot
    if GameMode.ThrowIn == obs['game_mode']:
        return Action.ShortPass
    if GameMode.Normal == obs['game_mode']:
        if Action.Sprint not in obs['sticky_actions']:
            return Action.Sprint
        controlled_player_pos = obs['left_team'][obs['active']]
        if obs['ball_owned_player'] == obs['active'] and obs['ball_owned_team'] == 0:
            player_x, player_y = controlled_player_pos[0], controlled_player_pos[1]
            if controlled_player_pos[0] > 0.5:
                goalkeeper_x = obs[""right_team""][0][0] + obs[""right_team_direction""][0][0] * 13
                goalkeeper_y = obs[""right_team""][0][1] + obs[""right_team_direction""][0][1] * 13
                x1, x2, y1, y2 = player_x, player_y, goalkeeper_x, goalkeeper_y
                if math.sqrt((x1 - x2) ** 2 + (y1 * 2.38 - y2 * 2.38) ** 2) < 0.3:
                    return Action.Shot
                else:
                    dribbled = False
                    for i in range(1, len(obs[""right_team""])):
                        # if opponent is ahead of player
                        if obs[""right_team""][i][0] > (player_x - 0.02): 
                            #distance_to_opponent
                            x1, x2, y1, y2 = player_x, player_y, obs[""right_team""][i][0], obs[""right_team""][i][1]
                            if math.sqrt((x1 - x2) ** 2 + (y1 * 2.38 - y2 * 2.38) ** 2) < 0.03:
                                dribbled = True
                                return Action.Dribble
                                break
                    if dribbled == False:
                        return random.choice([Action.Right, Action.Right, Action.Shot])
            return random.choice([Action.BottomRight, Action.TopRight, Action.Right, Action.HighPass, Action.LongPass, Action.HighPass, Action.LongPass, Action.ShortPass])
        else:
            if obs['ball'][0] > controlled_player_pos[0] + 0.05:
                return Action.Right
            if obs['ball'][0] < controlled_player_pos[0] - 0.05:
                return Action.Left
            if obs['ball'][1] > controlled_player_pos[1] + 0.05:
                return Action.Bottom
            if obs['ball'][1] < controlled_player_pos[1] - 0.05:
                return Action.Top
            return Action.Slide",0,No Code Smell
9898,48325623,2,"from kaggle_environments import make
env = make(""football"", configuration={""save_video"": True, ""scenario_name"": ""11_vs_11_kaggle"", ""running_in_notebook"": True})
output = env.run([""/kaggle/working/submission.py"", ""run_right""])[-1]
print('Left player: reward = %s, status = %s, info = %s' % (output[0]['reward'], output[0]['status'], output[0]['info']))
print('Right player: reward = %s, status = %s, info = %s' % (output[1]['reward'], output[1]['status'], output[1]['info']))
env.render(mode=""human"", width=800, height=600)",0,No Code Smell
9899,43983810,0,"# Install:
# Kaggle environments.
!git clone https://github.com/Kaggle/kaggle-environments.git
!cd kaggle-environments && pip install .

# GFootball environment.
!apt-get update -y
!apt-get install -y libsdl2-gfx-dev libsdl2-ttf-dev

# Make sure that the Branch in git clone and in wget call matches !!
!git clone -b v2.3 https://github.com/google-research/football.git
!mkdir -p football/third_party/gfootball_engine/lib

!wget https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_v2.3.so -O football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so
!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install .",0,No Code Smell
9900,43983810,1,"%%writefile submission.py
from kaggle_environments.envs.football.helpers import *

# @human_readable_agent wrapper modifies raw observations 
# provided by the environment:
# https://github.com/google-research/football/blob/master/gfootball/doc/observation.md#raw-observations
# into a form easier to work with by humans.
# Following modifications are applied:
# - Action, PlayerRole and GameMode enums are introduced.
# - 'sticky_actions' are turned into a set of active actions (Action enum)
#    see usage example below.
# - 'game_mode' is turned into GameMode enum.
# - 'designated' field is removed, as it always equals to 'active'
#    when a single player is controlled on the team.
# - 'left_team_roles'/'right_team_roles' are turned into PlayerRole enums.
# - Action enum is to be returned by the agent function.
@human_readable_agent
def agent(obs):
    # Make sure player is running.
    if Action.Sprint not in obs['sticky_actions']:
        return Action.Sprint
    # We always control left team (observations and actions
    # are mirrored appropriately by the environment).
    controlled_player_pos = obs['left_team'][obs['active']]
    # Does the player we control have the ball?
    if obs['ball_owned_player'] == obs['active'] and obs['ball_owned_team'] == 0:
        # Shot if we are 'close' to the goal (based on 'x' coordinate).
        if controlled_player_pos[0] > 0.5:
            return Action.Shot
        # Run towards the goal otherwise.
        return Action.Right
    else:
        # Run towards the ball.
        if obs['ball'][0] > controlled_player_pos[0] + 0.05:
            return Action.Right
        if obs['ball'][0] < controlled_player_pos[0] - 0.05:
            return Action.Left
        if obs['ball'][1] > controlled_player_pos[1] + 0.05:
            return Action.Bottom
        if obs['ball'][1] < controlled_player_pos[1] - 0.05:
            return Action.Top
        # Try to take over the ball if close to the ball.
        return Action.Slide
    
",0,No Code Smell
9901,43983810,2,"# Set up the Environment.
# This may take a few minutes.
from kaggle_environments import make
env = make(""football"", configuration={""save_video"": True, ""scenario_name"": ""11_vs_11_kaggle"", ""running_in_notebook"": True})
output = env.run([""/kaggle/working/submission.py"", ""do_nothing""])[-1]
print('Left player: reward = %s, status = %s, info = %s' % (output[0]['reward'], output[0]['status'], output[0]['info']))
print('Right player: reward = %s, status = %s, info = %s' % (output[1]['reward'], output[1]['status'], output[1]['info']))
env.render(mode=""human"", width=800, height=600)",0,No Code Smell
9902,45436490,0,"# Install:
# Kaggle environments.
!git clone https://github.com/Kaggle/kaggle-environments.git
!cd kaggle-environments && pip install .

# GFootball environment.
!apt-get update -y
!apt-get install -y libsdl2-gfx-dev libsdl2-ttf-dev

# Make sure that the Branch in git clone and in wget call matches !!
!git clone -b v2.7 https://github.com/google-research/football.git
!mkdir -p football/third_party/gfootball_engine/lib

!wget https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_v2.7.so -O football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so
!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install .",0,No Code Smell
9903,45436490,1,"%%writefile submission.py
from kaggle_environments.envs.football.helpers import *

# @human_readable_agent wrapper modifies raw observations 
# provided by the environment:
# https://github.com/google-research/football/blob/master/gfootball/doc/observation.md#raw-observations
# into a form easier to work with by humans.
# Following modifications are applied:
# - Action, PlayerRole and GameMode enums are introduced.
# - 'sticky_actions' are turned into a set of active actions (Action enum)
#    see usage example below.
# - 'game_mode' is turned into GameMode enum.
# - 'designated' field is removed, as it always equals to 'active'
#    when a single player is controlled on the team.
# - 'left_team_roles'/'right_team_roles' are turned into PlayerRole enums.
# - Action enum is to be returned by the agent function.
@human_readable_agent
def agent(obs):
    # Make sure player is running.
    if Action.Sprint not in obs['sticky_actions']:
        return Action.Sprint
    # We always control left team (observations and actions
    # are mirrored appropriately by the environment).
    controlled_player_pos = obs['left_team'][obs['active']]
    # Does the player we control have the ball?
    if obs['ball_owned_player'] == obs['active'] and obs['ball_owned_team'] == 0:
        # Shot if we are 'close' to the goal (based on 'x' coordinate).
        if controlled_player_pos[0] > 0.7:
            return Action.Shot
        # Run towards the goal otherwise.
        return Action.Right
    else:
        # Run towards the ball.
        if obs['ball'][0] > controlled_player_pos[0] + 0.01:
            return Action.Right
        if obs['ball'][0] < controlled_player_pos[0] - 0.01:
            return Action.Left
        if obs['ball'][1] > controlled_player_pos[1] + 0.01:
            return Action.Bottom
        if obs['ball'][1] < controlled_player_pos[1] - 0.01:
            return Action.Top
        # Try to take over the ball if close to the ball.
        return Action.Slide",0,No Code Smell
9904,45436490,2,"# Set up the Environment.
from kaggle_environments import make
env = make(""football"", configuration={""save_video"": True, ""scenario_name"": ""11_vs_11_kaggle"", ""running_in_notebook"": True})
output = env.run([""/kaggle/working/submission.py"", ""do_nothing""])[-1]
print('Left player: reward = %s, status = %s, info = %s' % (output[0]['reward'], output[0]['status'], output[0]['info']))
print('Right player: reward = %s, status = %s, info = %s' % (output[1]['reward'], output[1]['status'], output[1]['info']))
env.render(mode=""human"", width=800, height=600)",0,No Code Smell
9905,44361370,0,"# Install:
# Kaggle environments.
!git clone https://github.com/Kaggle/kaggle-environments.git
!cd kaggle-environments && pip install .

# GFootball environment.
!apt-get update -y
!apt-get install -y libsdl2-gfx-dev libsdl2-ttf-dev

# Make sure that the Branch in git clone and in wget call matches !!
!git clone -b v2.6 https://github.com/google-research/football.git
!mkdir -p football/third_party/gfootball_engine/lib

!wget https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_v2.6.so -O football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so
!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install .",0,No Code Smell
9906,44361370,1,"%%writefile submission.py
from kaggle_environments.envs.football.helpers import *

# @human_readable_agent wrapper modifies raw observations 
# provided by the environment:
# https://github.com/google-research/football/blob/master/gfootball/doc/observation.md#raw-observations
# into a form easier to work with by humans.
# Following modifications are applied:
# - Action, PlayerRole and GameMode enums are introduced.
# - 'sticky_actions' are turned into a set of active actions (Action enum)
#    see usage example below.
# - 'game_mode' is turned into GameMode enum.
# - 'designated' field is removed, as it always equals to 'active'
#    when a single player is controlled on the team.
# - 'left_team_roles'/'right_team_roles' are turned into PlayerRole enums.
# - Action enum is to be returned by the agent function.
@human_readable_agent
def agent(obs):
    # Make sure player is running.
    if Action.Sprint not in obs['sticky_actions']:
        return Action.Sprint
    # We always control left team (observations and actions
    # are mirrored appropriately by the environment).
    controlled_player_pos = obs['left_team'][obs['active']]
    # Does the player we control have the ball?
    if obs['ball_owned_player'] == obs['active'] and obs['ball_owned_team'] == 0:
        # Shot if we are 'close' to the goal (based on 'x' coordinate).
        if controlled_player_pos[0] > 0.5:
            return Action.Shot
        # Run towards the goal otherwise.
        return Action.Right
    else:
        # Run towards the ball.
        if obs['ball'][0] > controlled_player_pos[0] + 0.05:
            return Action.Right
        if obs['ball'][0] < controlled_player_pos[0] - 0.05:
            return Action.Left
        if obs['ball'][1] > controlled_player_pos[1] + 0.05:
            return Action.Bottom
        if obs['ball'][1] < controlled_player_pos[1] - 0.05:
            return Action.Top
        # Try to take over the ball if close to the ball.
        return Action.Slide",0,No Code Smell
9907,44361370,2,"# Set up the Environment.
from kaggle_environments import make

env = make(""football"", configuration={""save_video"": True, ""scenario_name"": ""11_vs_11_kaggle"", ""running_in_notebook"": True})

output = env.run([""/kaggle/working/submission.py"", ""do_nothing""])[-1]

print('Left player: reward = %s, status = %s, info = %s' % (output[0]['reward'], output[0]['status'], output[0]['info']))
print('Right player: reward = %s, status = %s, info = %s' % (output[1]['reward'], output[1]['status'], output[1]['info']))

env.render(mode=""human"", width=800, height=600)",0,No Code Smell
9908,44361370,3,,0,No Code Smell
9909,45835722,0,"# Install:
# Kaggle environments.
!git clone https://github.com/Kaggle/kaggle-environments.git
!cd kaggle-environments && pip install .

# GFootball environment.
!apt-get update -y
!apt-get install -y libsdl2-gfx-dev libsdl2-ttf-dev

# Make sure that the Branch in git clone and in wget call matches !!
!git clone -b v2.8 https://github.com/google-research/football.git
!mkdir -p football/third_party/gfootball_engine/lib

!wget https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_v2.8.so -O football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so
!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install .",0,No Code Smell
9910,45835722,1,"%%writefile submission.py
from kaggle_environments.envs.football.helpers import *

# @human_readable_agent wrapper modifies raw observations 
# provided by the environment:
# https://github.com/google-research/football/blob/master/gfootball/doc/observation.md#raw-observations
# into a form easier to work with by humans.
# Following modifications are applied:
# - Action, PlayerRole and GameMode enums are introduced.
# - 'sticky_actions' are turned into a set of active actions (Action enum)
#    see usage example below.
# - 'game_mode' is turned into GameMode enum.
# - 'designated' field is removed, as it always equals to 'active'
#    when a single player is controlled on the team.
# - 'left_team_roles'/'right_team_roles' are turned into PlayerRole enums.
# - Action enum is to be returned by the agent function.
@human_readable_agent
def agent(obs):
    # Make sure player is running.
    if Action.Sprint not in obs['sticky_actions']:
        return Action.Sprint
    # We always control left team (observations and actions
    # are mirrored appropriately by the environment).
    controlled_player_pos = obs['left_team'][obs['active']]
    # Does the player we control have the ball?
    if obs['ball_owned_player'] == obs['active'] and obs['ball_owned_team'] == 0:
        # Shot if we are 'close' to the goal (based on 'x' coordinate).
        if controlled_player_pos[0] > 0.5:
            return Action.Shot
        # Run towards the goal otherwise.
        return Action.Right
    else:
        # Run towards the ball.
        if obs['ball'][0] > controlled_player_pos[0] + 0.05:
            return Action.Right
        if obs['ball'][0] < controlled_player_pos[0] - 0.05:
            return Action.Left
        if obs['ball'][1] > controlled_player_pos[1] + 0.05:
            return Action.Bottom
        if obs['ball'][1] < controlled_player_pos[1] - 0.05:
            return Action.Top
        # Try to take over the ball if close to the ball.
        return Action.Slide",0,No Code Smell
9911,45835722,2,"# Set up the Environment.
from kaggle_environments import make
env = make(""football"", configuration={""save_video"": True, ""scenario_name"": ""11_vs_11_kaggle"", ""running_in_notebook"": True})
output = env.run([""/kaggle/working/submission.py"", ""do_nothing""])[-1]
print('Left player: reward = %s, status = %s, info = %s' % (output[0]['reward'], output[0]['status'], output[0]['info']))
print('Right player: reward = %s, status = %s, info = %s' % (output[1]['reward'], output[1]['status'], output[1]['info']))
env.render(mode=""human"", width=800, height=600)",0,No Code Smell
9912,44888618,0,"#importing the tensorflow
!pip install tensorflow==1.15.3 --user",0,No Code Smell
9913,44888618,1,"#importing the numpy and tensorflow
import numpy as np
import tensorflow as tf",1,Code Smell
9914,44888618,2,"train_ds='../input/109-1-ntut-dl-app-hw1/emnist-byclass-train.npz'
test_ds='../input/109-1-ntut-dl-app-hw1/emnist-byclass-test.npz'
",0,No Code Smell
9915,44888618,3,"df=np.load(train_ds) #loading to the train set

ti=df['training_images'] #ti->train images, calling the label column",0,No Code Smell
9916,44888618,4,"df=np.load(train_ds) #loading to the train set
tl=df['training_labels'] #tl->train labels, calling the  label column
# detect and init the TPU
tpu = tf.distribute.cluster_resolver.TPUClusterResolver()
tf.config.experimental_connect_to_cluster(tpu)
tf.tpu.experimental.initialize_tpu_system(tpu)

# instantiate a distribution strategy
tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)

# instantiating the model in the strategy scope creates the model on the TPU
with tpu_strategy.scope():
    # define your model normally
    model= tf.keras.models.Sequential()
    model.add(tf.keras.layers.Conv2D(32, kernel_size=(5, 5),activation='relu'))
    model.add(tf.keras.layers.Dropout(0.1))
    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))

    model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu'))
    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))
    model.add(tf.keras.layers.Dropout(0.1))
    model.add(tf.keras.layers.Flatten())
    
    model.add(tf.keras.layers.Dense(128,activation=tf.nn.relu))
    model.add(tf.keras.layers.Dropout(0.15))
    model.add(tf.keras.layers.Dense(62, activation=tf.nn.softmax))
",0,No Code Smell
9917,44888618,5,model.summary(),0,No Code Smell
9918,44888618,6,"model.compile(optimizer='adam',loss='sparse_categorical_crossentropy', metrics=['accuracy'])
",0,No Code Smell
9919,44888618,7,"model.fit(ti, tl, epochs=9,batch_size=128)    
",0,No Code Smell
9920,44888618,8,ti = np.load(test_ds)['testing_images'],1,Code Smell
9921,44888618,9,r=model.predict_classes(ti),0,No Code Smell
9922,44888618,10,"        
# Print results in CSV format and upload to Kaggle
with open('Emnist_pred_results.csv', 'w') as f:
    f.write('Id,Category\n')
    for i in range(len(r)):
        f.write(str(i) + ',' + str(r[i]) + '\n')",0,No Code Smell
9923,44888618,11,"# Download your results!
from IPython.display import FileLink
FileLink('Emnist_pred_results.csv')",1,Code Smell
9924,44888618,12,,0,No Code Smell
9925,44439640,0,"#Importing the required libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.optimize as opt

plt.style.use('ggplot')
plt.rcParams['figure.figsize'] = (12,12)",0,No Code Smell
9926,44439640,1,"# data preprocessing
from sklearn.preprocessing import StandardScaler
# data splitting
from sklearn.model_selection import train_test_split
# data modeling
from sklearn.metrics import confusion_matrix,accuracy_score,roc_curve,classification_report
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from xgboost import XGBClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
#ensembling
from mlxtend.classifier import StackingCVClassifier",1,Code Smell
9927,44439640,2,"#loading the training and testing dataset
df = pd.read_csv('final.csv')
x_test = pd.read_csv('Test.csv')
test_ID = x_test['ID']
#This shows the first 5 rows of the dataset
df.head()",1,Code Smell
9928,44439640,3,"# Initializing the training data
y = df[""target""]
X = df.drop('target',axis=1)
X_train = X
y_train = y",0,No Code Smell
9929,44439640,4,"# Grouping the data in terms of the positive - 1 and negative cases - 0 with their mean values
df.groupby('target').mean()",0,No Code Smell
9930,44439640,5,"# Describing the dataset 
df.describe()",1,Code Smell
9931,44439640,6,"# Getting the required information about the dataset
df.info()",1,Code Smell
9932,44439640,7,"# Calculating the mean for each column
df.mean()",1,Code Smell
9933,44439640,8,"# Checking if there are any null values in the dataset
df.isnull().sum()",0,No Code Smell
9934,44439640,9,"# Using the groupby function to calculate the mean
df.groupby('target').mean()",0,No Code Smell
9935,44439640,10,"# Plotting a histogram representation of all the features
df.hist(figsize = (12, 8))
plt.show()",0,No Code Smell
9936,44439640,11,"
negative_cases = len(df[y == 0])
positive_cases = len(df[y == 1])
per_negative_cases = (negative_cases/len(y))*100 # percentage of negative cases
per_positive_cases = (positive_cases/len(y))*100 # percentage of positive cases

# We know that cardiovascular or Heart diseases affect more women than men and are responsible for more than 40% of all deaths in American women.
# Assuming 0 - Female and 1 - Male

female = len(df[df['sex']== 0])
male = len(df[df['sex'] == 1])
per_female = (female/len(y))*100
per_male = (male/len(y))*100

#This figure consists of two subplots to present the data graphical
fig, ax =plt.subplots(1,2)
sns.countplot(df['target'], ax=ax[0])# counts the number of positive and negative cases in the dataset
sns.countplot(df['sex'], ax=ax[1])# counts the number of male and female patients in the dataset
fig.show()
",0,No Code Smell
9937,44439640,12,"# Analysing the 'Chest Pain Type' feature
df[""cp""].unique()",0,No Code Smell
9938,44439640,13,"# Plotting a barplot for the 'Chest Pain' feature
sns.barplot(df[""cp""],y)",0,No Code Smell
9939,44439640,14,"# Analysing the FBS feature
df['fbs'].unique()",0,No Code Smell
9940,44439640,15,"# Plotting a barplot for FBS feature
sns.barplot(df[""fbs""],y)",0,No Code Smell
9941,44439640,16,"# Analysing the 'ca' feature
df[""ca""].unique()",0,No Code Smell
9942,44439640,17,"# Using a count plot to count
sns.countplot(df[""ca""])",0,No Code Smell
9943,44439640,18,"# Plotting a graph for Age Vs Maximum Heart rate achieved
plt.scatter(x=df.age[df['target']==1], y=df.thalach[(df['target']==1)], c=""red"")
plt.scatter(x=df.age[df['target']==0], y=df.thalach[(df['target']==0)], c = ""green"")
plt.legend([""+ve"", ""-ve""])
plt.xlabel(""Age"")
plt.ylabel(""maximum heart rate achieved"")
plt.show()",0,No Code Smell
9944,44439640,19,"# Finding the correlation between the features
sns.heatmap(df.corr(),annot=True,fmt='.2f')
plt.show()",0,No Code Smell
9945,44439640,20,"from xgboost import XGBClassifier


scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(x_test)

m4 = 'Extreme Gradient Boost'
xgb = XGBClassifier(learning_rate=0.01, n_estimators=25, max_depth=15,gamma=0.6, subsample=0.52,colsample_bytree=0.6,seed=27, 
                    reg_lambda=2, booster='dart', colsample_bylevel=0.6, colsample_bynode=0.5)
xgb.fit(X_train, y_train)
xgb_predicted = xgb.predict(X_test)
",0,No Code Smell
9946,44439640,21,"df_2 = pd.DataFrame({'ID':test_ID,'target':xgb_predicted})
df_2.to_csv('submission.csv',index=False)",0,No Code Smell
9947,44439640,22,,0,No Code Smell
9975,45365843,0,"import numpy as np
import pandas as pd 
import xgboost as xgb
from xgboost.sklearn import XGBClassifier
from sklearn.metrics import roc_auc_score, confusion_matrix
from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV
from xgboost import plot_importance
import seaborn as sns
import matplotlib.pyplot as plt
import operator as op",1,Code Smell
9976,45365843,1,"train = pd.read_csv(""../input/stat441datachallenge1/online_jewellery_shop_train.csv"")
test = pd.read_csv(""../input/stat441datachallenge1/online_jewellery_shop_test_final.csv"")",1,Code Smell
9977,45365843,2,train.describe(),0,No Code Smell
9978,45365843,3,"# seperate labels and explanatory variables
y_train = train[""Revenue""]
X_train = train.drop([""Revenue""], axis=1)
X_test = test
n = len(y_train)",0,No Code Smell
9979,45365843,4,X_train.columns,0,No Code Smell
9980,45365843,5,"# inspect variables by using a heat map (a correlation matrix using spearman as it a non-parametric test)
data = pd.concat([X_train,y_train], axis = 1)
corrmat = data.corr(method=""spearman"")
top_corr_features = corrmat.index
plt.figure(figsize=(25,25))
# plt heat map
g = sns.heatmap(data[top_corr_features].corr(), annot=True, cmap=""RdYlGn"")",0,No Code Smell
9981,45365843,6,"# modify string variables into numerical variables
def encode(X):
    # make VisitorType and Month into continuous variables
    visitor_type_dict = {""New_Visitor"": 1, ""Returning_Visitor"": 0, ""Other"": 2}
    X[""VisitorType""] = X[""VisitorType""].map(visitor_type_dict)
    month_dict = {""Jan"":1, ""Feb"":2, ""Mar"":3, ""Apr"":4, ""May"": 5, ""June"": 6, ""Jul"": 7, ""Aug"": 8, ""Sep"": 9, ""Oct"": 10, ""Nov"": 11, ""Dec"": 12}
    X[""Month""] = X[""Month""].map(month_dict)
    # setting up interaction variables
    interac_vars = [""Administrative"", ""Administrative_Duration"", ""Informational"", ""Informational_Duration"", ""ProductRelated"", ""ProductRelated_Duration""]
    for i in range(0,6):
        for j in range(1,6):
            var1 = interac_vars[i]
            var2 = interac_vars[j]
            if(var1 < var2):
                X[var1+""*""+var2] = X[var1]* X[var2]
    return X

X_train = encode(X_train)",0,No Code Smell
9982,45365843,7,"# inspect variables by using a heat map a correlation matrix using spearman as it a non-parametric test after adding in interation terms
data = pd.concat([X_train,y_train], axis = 1)
corrmat = data.corr(method=""spearman"")
top_corr_features = corrmat.index
plt.figure(figsize=(25,25))
# plt heat map
g = sns.heatmap(data[top_corr_features].corr(), annot=True, cmap=""RdYlGn"")",0,No Code Smell
9983,45365843,8,"# top 15 potential statistically insignificant features using the spearman test
(abs(corrmat.iloc[-1]).sort_values()).head(10)",0,No Code Smell
9984,45365843,9,print(X_train.isnull().sum().max()),0,No Code Smell
9985,45365843,10,"# check ratio of 0s and 1s
print(sum(y_train == 0)/n) ",0,No Code Smell
9986,45365843,11,"def grid_search(X, y, default_param, param_grid):
    grid_search = GridSearchCV (estimator=XGBClassifier(**default_param), param_grid=param_grid, scoring='roc_auc', n_jobs=-1, return_train_score=True, verbose=3, refit='roc_auc')
    grid_search.fit(X, y)
    print(grid_search.best_estimator_)
    print(grid_search.best_score_)",0,No Code Smell
9987,45365843,12,"# scale_pos_weight is set because this is a highly imbalanced dataset
# use gamma, alpha, lambda to perform regulization
# use min_child_weight, max_depth to make my model more conservative
params = {
    'objective': 'binary:logistic',
    'learning_rate': 0.1, 
    'eval_metric': 'auc',
    'scale_pos_weight': (sum(y_train == 1))/ (sum(y_train == 0)),
    'tree_method': 'exact',
    # grid1
    'min_child_weight': 1,
    # grid11
    'max_depth': 4,
    # grid2
    'gamma': 1,
    # grid3
    'subsample': 1,
    # grid4
    'colsample_bytree': 0.6,
    # grid5
    'max_delta_step': 2,
    # grid6
    'reg_alpha': 1e-5,
    'reg_lambda': 1,
}
param_grid1 = {
    'max_depth':range(0,10),
}
param_grid11 = {
    'min_child_weight':range(0,10)
}
param_grid2 = {
    'gamma':[i/2 for i in range(0,10)]
}
param_grid3 = {
    'subsample':[i/10 for i in range(6,11)],
}

param_grid4 = {
    'colsample_bytree':[i/10 for i in range(6,11)],
}
param_grid5 = {
    'max_delta_step':[0, 0.5, 1, 1.5, 2, 2.5, 3],
}
param_grid6 = {
    'reg_alpha':[1e-5, 1e-4, 1e-3, 1e-2, 0, 0.1, 1],
    'reg_lambda':[1e-5, 1e-4, 1e-3, 1e-2, 0, 0.1, 1]
}
#grid_search(X_train, y_train, params, param_grid6)",0,No Code Smell
9988,45365843,13,"model = XGBClassifier(**params)
model.fit(X_train, y_train)
sorted((model.get_booster().get_score(importance_type = 'gain')).items(), key = op.itemgetter(1), reverse=True)[::-1][0:20]",0,No Code Smell
9989,45365843,14,"def preprocess(X):
    X = encode(X)
    return X",0,No Code Smell
9990,45365843,15,"# # Prepare 20% of test data 
# sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)
# X_untouched_test = 0
# y_untouched_test = 0
# for train_index, test_index in sss.split(X_train, y_train):
#     X_train, X_untouched_test = X_train.iloc[train_index], X_train.iloc[test_index]
#     y_train ,y_untouched_test = y_train.iloc[train_index], y_train.iloc[test_index]",0,No Code Smell
9991,45365843,16,"# Cross Validation
s = StratifiedShuffleSplit(n_splits=4, test_size=0.2, random_state=0)
best_X_train = []
best_X_test = []
best_y_train = []
best_y_test = []
best_precision_score = 0
for train_index, test_index in s.split(X_train, y_train):
    print(""TRAIN:"", train_index, ""TEST:"", test_index)
    X_new_train, X_new_test = X_train.iloc[train_index], X_train.iloc[test_index]
    y_new_train, y_new_test = y_train.iloc[train_index], y_train.iloc[test_index]

    model.fit(X_new_train, y_new_train)
  
    y_new_test_pred = model.predict_proba(X_new_test)[:,1]
    
    auc = roc_auc_score(y_new_test, y_new_test_pred)
    if auc > best_precision_score:
      print(""New best with a AUC score of {0} "".format(auc))
      best_precision_score = auc
      best_X_train = X_new_train
      best_X_test = X_new_test
      best_y_train = y_new_train
      best_y_test = y_new_test
      
    else:
      print(""Current AUC score is {0}"".format(auc))",0,No Code Smell
9992,45365843,17,"X_train = best_X_train
y_train = best_y_train
X_test = best_X_test
y_test = best_y_test",0,No Code Smell
9993,45365843,18,"def print_report(true_y, pred_y_prob):
    cm = confusion_matrix(true_y, pred_y_prob>0.5)
    auc = roc_auc_score(true_y, pred_y_prob)
    print(""AUC Score: %f"" % roc_auc_score(true_y, pred_y_prob))
    tn, fp, fn, tp = cm.ravel()
    print(""True Negative: {0}   False Positive: {1}\nFalse Negative: {2}   True Positive: {3}"".format(tn, fp, fn, tp))",0,No Code Smell
9994,45365843,19,"# Model check on train data
print_report(y_train, model.predict_proba(X_train)[:,1])",0,No Code Smell
9995,45365843,20,"# Model check on test data
print_report(y_test, model.predict_proba(X_test)[:,1])",0,No Code Smell
9996,45365843,21,"# # Model auc check on untouched test data
# print_report(y_untouched_test, model.predict_proba(X_untouched_test)[:,1])",0,No Code Smell
9997,45365843,22,X_test = preprocess(test),0,No Code Smell
9998,45365843,23,print(X_test.isnull().sum().max()),0,No Code Smell
9999,45365843,24,X_test,0,No Code Smell
10000,45365843,25,"y_pred_test = model.predict_proba(X_test)[:,1]",0,No Code Smell
10001,45365843,26,"submission = pd.DataFrame({""ID"": test[""ID""], ""Revenue"": y_pred_test})",0,No Code Smell
10002,45365843,27,submission,0,No Code Smell
10003,45365843,28,"sum(submission[""Revenue""] <= 0.5) / 3700",0,No Code Smell
10004,45365843,29,"submission.to_csv(""submission.csv"", index=False)",0,No Code Smell
10005,46662684,0,########################### LOADING DATA ############################################,0,No Code Smell
10006,46662684,1,#!unzip tau-ethiopic-digit-recognition.zip,0,No Code Smell
10007,46662684,2,"# LOAD TRAINING DATA
import numpy as np
from PIL import Image
from sklearn.datasets import load_files
path=""train/train/""
data=load_files(path)
X=np.zeros((data.filenames.shape[0],28,28))
for i,file in enumerate(data.filenames):
    X[i,:]=np.array(Image.open(file))
y=data.target
# LOADING TEST DATA
import os
path=""test/test/""
X_test=np.zeros((len(os.listdir(path)),28,28))
ID=[]
for i,file in enumerate(os.listdir(path)):
    X_test[i,:]=np.array(Image.open(path+file))
    ID.append(int(file.split(""."")[0]))",0,No Code Smell
10008,46662684,3,###########################SPLITTING DATA ##################################,0,No Code Smell
10009,46662684,4,"from sklearn.model_selection import train_test_split
X_train,X_val,y_train,y_val=train_test_split(X, y,test_size=0.2)",0,No Code Smell
10010,46662684,5,########################### CNN MODEL ##################################,0,No Code Smell
10011,46662684,6,"from keras.models import Model, Sequential
from keras.constraints import maxnorm
from keras.layers import *

def make_model():
    model = Sequential()

    model.add(Conv2D(56, (3, 3),input_shape=(28, 28,1), activation='relu', padding='same'))
    model.add(GaussianNoise(0.2))
    model.add(BatchNormalization(axis=-1))
    model.add(Conv2D(56, (3, 3), activation='relu', padding='same'))
    model.add(BatchNormalization(axis=-1))
    model.add(MaxPooling2D(pool_size=(2, 2), strides=2))
    model.add(SpatialDropout2D(0.1))

    model.add(Conv2D(112, (3, 3), activation='relu', padding='same'))
    model.add(BatchNormalization(axis=-1))
    model.add(Conv2D(112, (3, 3), activation='relu', padding='same'))
    model.add(BatchNormalization(axis=-1))
    model.add(MaxPooling2D(pool_size=(2, 2), strides=2))
    model.add(SpatialDropout2D(0.1))


    model.add(Conv2D(224, (3, 3), activation='relu', padding='same'))
    model.add(BatchNormalization(axis=-1))
    model.add(Conv2D(224, (3, 3), activation='relu', padding='same'))
    model.add(BatchNormalization(axis=-1))
    model.add(MaxPooling2D(pool_size=(2, 2), strides=2))
    model.add(SpatialDropout2D(0.1))
    
    model.add(Flatten())
    model.add(Dense(784, activation='relu', kernel_constraint=maxnorm(3)))
    model.add(BatchNormalization())
    model.add(Dropout(0.2))
    model.add(Dense(98, activation='relu', kernel_constraint=maxnorm(3)))
    model.add(BatchNormalization())
    model.add(Dropout(0.2))
    model.add(Dense(10, activation='softmax'))
    return model",0,No Code Smell
10012,46662684,7,"# Creating the CNN model and printing summary
model=make_model()
model.summary()",0,No Code Smell
10013,46662684,8,"# Creating optimizer and compiling model
from keras.optimizers import Adam
epochs=100
lrate=0.001
decay=lrate/100
optimizer=Adam(lr=lrate,decay=decay, beta_1=0.9, beta_2=0.999, epsilon=1e-08)
model.compile(loss='sparse_categorical_crossentropy',optimizer=optimizer, metrics=['accuracy']) ",0,No Code Smell
10014,46662684,9,"# Creating an image augmentation object and fitting it
from tensorflow.keras.preprocessing.image import ImageDataGenerator
aug = ImageDataGenerator(
    #featurewise_center=True,
    #featurewise_std_normalization=True,
    rotation_range=45,
    width_shift_range=0.2,
    height_shift_range=0.2,
    horizontal_flip=True,
    zoom_range=0.3)
aug.fit(X.reshape(X.shape[0],28,28,1))",0,No Code Smell
10015,46662684,10,"import tensorflow as tf
checkpoint_filepath = 'checkpoint'
model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_filepath,
    save_weights_only=True,
    monitor='val_accuracy',
    mode='max',
    save_best_only=True)",0,No Code Smell
10016,46662684,11,"from keras.utils import np_utils
history=model.fit_generator(aug.flow(X_train.reshape(-1,28,28,1),
                                     y_train, batch_size=128), # np_utils.to_categorical(y_train)
                            steps_per_epoch = X_train.shape[0] / 128
                  ,epochs=epochs, validation_data=(X_val.reshape(-1,28,28,1),
                                                  y_val ), verbose=1, # np_utils.to_categorical(y_val)
                           callbacks=[model_checkpoint_callback]) ",0,No Code Smell
10017,46662684,12,"import time
model.load_weights(checkpoint_filepath)
model.save(""model-100-csc-{}"".format(time.time()))",0,No Code Smell
10018,46662684,13,"y_test_pred=model.predict(X_test.reshape(-1,28,28,1))
y_test_pred",0,No Code Smell
10019,46662684,14,"y=np.argmax(y_test_pred, axis=1)
y",0,No Code Smell
10020,46662684,15,ids=np.array(ID),0,No Code Smell
10021,46662684,16,"data.target_names, data.target",0,No Code Smell
10022,46662684,17,"target_dict={}
for i,t in enumerate(data.target_names):
    target_dict[i]=t",0,No Code Smell
10023,46662684,18,"y_real=[]
for Y in y:
    y_real.append(target_dict[Y])",0,No Code Smell
10024,46662684,19,y_real,0,No Code Smell
10025,46662684,20,"import pandas as pd
res=pd.DataFrame(data=ids, columns=[""Id""])
res[""Category""]=y_real
res",0,No Code Smell
10026,46662684,21,"res.to_csv(""submission.csv"", index=False)",0,No Code Smell
10027,46662684,22,"model.evaluate(X_val.reshape(-1,28,28,1),y_val)",0,No Code Smell
